0: gpu021.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-ebfd6e2d-0f81-ada3-ee79-f73a42db1693)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Sun Aug 14 03:34:03 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B4:00.0 Off |                    0 |
| N/A   36C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b757dee18e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m49.785s
user	0m3.502s
sys	0m2.465s
[03:35:54] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.2487],
        [-0.0734],
        [-0.1809],
        ...,
        [ 0.2602],
        [-1.0476],
        [ 1.0814]], device='cuda:0', requires_grad=True) 
node features sum: tensor(70.8263, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N3
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1338, -0.1378, -0.0554,  0.0261,  0.0065,  0.1405,  0.1363,  0.0029,
         -0.0461,  0.0833, -0.0265, -0.1515, -0.1289,  0.1346,  0.1432,  0.1489,
         -0.0253, -0.1060, -0.1431, -0.0096,  0.0582,  0.0098,  0.1323,  0.1383,
         -0.1246, -0.1309, -0.1266, -0.0403, -0.1371, -0.0015,  0.1173, -0.1046,
          0.0018,  0.0645,  0.0627,  0.1463, -0.1085, -0.1428, -0.0361, -0.0418,
          0.0589,  0.0915,  0.1108, -0.1513,  0.0563,  0.0697, -0.0271,  0.1440,
          0.0725, -0.1370, -0.0500,  0.1290, -0.0380,  0.0638, -0.0007, -0.1202,
         -0.0243, -0.1385, -0.1417,  0.0254, -0.1479,  0.0228,  0.0081, -0.1371,
         -0.0123, -0.1225, -0.0447, -0.1141,  0.1522,  0.1233, -0.0518, -0.0124,
          0.0665,  0.0085, -0.0023,  0.1236, -0.0022, -0.0043,  0.0054,  0.0111,
          0.1044,  0.0262,  0.1256, -0.0473,  0.1058, -0.0778, -0.0527,  0.0931,
          0.0990,  0.1105, -0.1211, -0.1358,  0.1117,  0.0660, -0.1053,  0.0360,
         -0.0997, -0.0304, -0.0935,  0.1447,  0.0023, -0.1507, -0.0017,  0.1458,
         -0.0625,  0.1432,  0.0987,  0.1390, -0.0194,  0.1384, -0.0778,  0.0316,
          0.0751,  0.0440, -0.1497, -0.0514, -0.1189, -0.0198,  0.0499,  0.0968,
         -0.0630, -0.1330,  0.0020, -0.0663,  0.0693, -0.0350, -0.0611,  0.1147,
          0.0465, -0.0349,  0.0124,  0.1041,  0.0464,  0.1234, -0.0220, -0.1315,
         -0.0643,  0.0848,  0.0660,  0.0340, -0.0949,  0.1018,  0.1384,  0.0558,
          0.0164,  0.0390, -0.0998, -0.0878, -0.1132, -0.0953,  0.0923, -0.0117,
          0.0620,  0.0788, -0.1111,  0.0084,  0.0804, -0.0561, -0.1073, -0.0062,
         -0.0322, -0.1004,  0.1208,  0.1349, -0.0141,  0.0922,  0.0675,  0.0647,
          0.0531,  0.0615, -0.0789,  0.0737,  0.0973,  0.1353,  0.0049,  0.0321,
          0.0309,  0.0299, -0.1106, -0.0280, -0.0728,  0.0815,  0.0273, -0.0246,
          0.0446,  0.0800, -0.0474, -0.0686,  0.1523,  0.1309,  0.0612, -0.1239,
          0.0524, -0.0985, -0.0682, -0.1212, -0.1021,  0.1329, -0.0623,  0.0358,
          0.0938, -0.0491,  0.1265,  0.0392, -0.0110,  0.0950, -0.0105,  0.1515,
          0.1039, -0.0088,  0.1070,  0.1120,  0.0653,  0.1521,  0.0879, -0.1074,
         -0.1408, -0.0234,  0.0283, -0.1475,  0.0905, -0.1428, -0.0395,  0.0341,
          0.0068, -0.0885, -0.1302, -0.0929,  0.1099, -0.1267,  0.0365, -0.0570,
         -0.1002, -0.0790, -0.0481, -0.1294, -0.0885,  0.0800,  0.0644, -0.0673,
          0.0135, -0.1189,  0.0274,  0.0139,  0.0783, -0.0276, -0.0103, -0.1508,
         -0.1246, -0.1118,  0.0951, -0.0667,  0.0004, -0.1491,  0.0888, -0.1152]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1338, -0.1378, -0.0554,  0.0261,  0.0065,  0.1405,  0.1363,  0.0029,
         -0.0461,  0.0833, -0.0265, -0.1515, -0.1289,  0.1346,  0.1432,  0.1489,
         -0.0253, -0.1060, -0.1431, -0.0096,  0.0582,  0.0098,  0.1323,  0.1383,
         -0.1246, -0.1309, -0.1266, -0.0403, -0.1371, -0.0015,  0.1173, -0.1046,
          0.0018,  0.0645,  0.0627,  0.1463, -0.1085, -0.1428, -0.0361, -0.0418,
          0.0589,  0.0915,  0.1108, -0.1513,  0.0563,  0.0697, -0.0271,  0.1440,
          0.0725, -0.1370, -0.0500,  0.1290, -0.0380,  0.0638, -0.0007, -0.1202,
         -0.0243, -0.1385, -0.1417,  0.0254, -0.1479,  0.0228,  0.0081, -0.1371,
         -0.0123, -0.1225, -0.0447, -0.1141,  0.1522,  0.1233, -0.0518, -0.0124,
          0.0665,  0.0085, -0.0023,  0.1236, -0.0022, -0.0043,  0.0054,  0.0111,
          0.1044,  0.0262,  0.1256, -0.0473,  0.1058, -0.0778, -0.0527,  0.0931,
          0.0990,  0.1105, -0.1211, -0.1358,  0.1117,  0.0660, -0.1053,  0.0360,
         -0.0997, -0.0304, -0.0935,  0.1447,  0.0023, -0.1507, -0.0017,  0.1458,
         -0.0625,  0.1432,  0.0987,  0.1390, -0.0194,  0.1384, -0.0778,  0.0316,
          0.0751,  0.0440, -0.1497, -0.0514, -0.1189, -0.0198,  0.0499,  0.0968,
         -0.0630, -0.1330,  0.0020, -0.0663,  0.0693, -0.0350, -0.0611,  0.1147,
          0.0465, -0.0349,  0.0124,  0.1041,  0.0464,  0.1234, -0.0220, -0.1315,
         -0.0643,  0.0848,  0.0660,  0.0340, -0.0949,  0.1018,  0.1384,  0.0558,
          0.0164,  0.0390, -0.0998, -0.0878, -0.1132, -0.0953,  0.0923, -0.0117,
          0.0620,  0.0788, -0.1111,  0.0084,  0.0804, -0.0561, -0.1073, -0.0062,
         -0.0322, -0.1004,  0.1208,  0.1349, -0.0141,  0.0922,  0.0675,  0.0647,
          0.0531,  0.0615, -0.0789,  0.0737,  0.0973,  0.1353,  0.0049,  0.0321,
          0.0309,  0.0299, -0.1106, -0.0280, -0.0728,  0.0815,  0.0273, -0.0246,
          0.0446,  0.0800, -0.0474, -0.0686,  0.1523,  0.1309,  0.0612, -0.1239,
          0.0524, -0.0985, -0.0682, -0.1212, -0.1021,  0.1329, -0.0623,  0.0358,
          0.0938, -0.0491,  0.1265,  0.0392, -0.0110,  0.0950, -0.0105,  0.1515,
          0.1039, -0.0088,  0.1070,  0.1120,  0.0653,  0.1521,  0.0879, -0.1074,
         -0.1408, -0.0234,  0.0283, -0.1475,  0.0905, -0.1428, -0.0395,  0.0341,
          0.0068, -0.0885, -0.1302, -0.0929,  0.1099, -0.1267,  0.0365, -0.0570,
         -0.1002, -0.0790, -0.0481, -0.1294, -0.0885,  0.0800,  0.0644, -0.0673,
          0.0135, -0.1189,  0.0274,  0.0139,  0.0783, -0.0276, -0.0103, -0.1508,
         -0.1246, -0.1118,  0.0951, -0.0667,  0.0004, -0.1491,  0.0888, -0.1152]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0939, -0.1185, -0.0650,  ..., -0.0108, -0.0333, -0.0971],
        [-0.0894,  0.0348, -0.1220,  ...,  0.0749, -0.0581,  0.0092],
        [ 0.0314, -0.0239, -0.0530,  ...,  0.0699, -0.0630, -0.0279],
        ...,
        [ 0.0139, -0.1004,  0.0894,  ..., -0.0978, -0.1127, -0.0149],
        [-0.0341,  0.0614, -0.0621,  ...,  0.0495,  0.0049, -0.0689],
        [ 0.0901,  0.1243, -0.0306,  ..., -0.0299,  0.0783, -0.0348]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0939, -0.1185, -0.0650,  ..., -0.0108, -0.0333, -0.0971],
        [-0.0894,  0.0348, -0.1220,  ...,  0.0749, -0.0581,  0.0092],
        [ 0.0314, -0.0239, -0.0530,  ...,  0.0699, -0.0630, -0.0279],
        ...,
        [ 0.0139, -0.1004,  0.0894,  ..., -0.0978, -0.1127, -0.0149],
        [-0.0341,  0.0614, -0.0621,  ...,  0.0495,  0.0049, -0.0689],
        [ 0.0901,  0.1243, -0.0306,  ..., -0.0299,  0.0783, -0.0348]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0159,  0.1449, -0.0412,  ..., -0.0443, -0.1086,  0.0702],
        [-0.0809, -0.0076,  0.1600,  ...,  0.0354, -0.1266,  0.1407],
        [ 0.0743,  0.1359,  0.0355,  ...,  0.1245,  0.1042,  0.0355],
        ...,
        [ 0.0577,  0.0756, -0.0191,  ..., -0.0852, -0.0019,  0.0447],
        [ 0.0174, -0.1395, -0.0287,  ..., -0.1728, -0.1464, -0.1574],
        [-0.1341, -0.1566,  0.1729,  ..., -0.1173, -0.1059,  0.1738]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0159,  0.1449, -0.0412,  ..., -0.0443, -0.1086,  0.0702],
        [-0.0809, -0.0076,  0.1600,  ...,  0.0354, -0.1266,  0.1407],
        [ 0.0743,  0.1359,  0.0355,  ...,  0.1245,  0.1042,  0.0355],
        ...,
        [ 0.0577,  0.0756, -0.0191,  ..., -0.0852, -0.0019,  0.0447],
        [ 0.0174, -0.1395, -0.0287,  ..., -0.1728, -0.1464, -0.1574],
        [-0.1341, -0.1566,  0.1729,  ..., -0.1173, -0.1059,  0.1738]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0844,  0.2480, -0.1912,  ..., -0.0388,  0.0398, -0.1683],
        [-0.0504, -0.2319, -0.0333,  ...,  0.1515,  0.1458, -0.2241],
        [ 0.1759, -0.1598,  0.0080,  ..., -0.1589, -0.1269, -0.0905],
        ...,
        [-0.2386, -0.2077, -0.1167,  ...,  0.2065,  0.0039,  0.0851],
        [-0.0558,  0.0798,  0.1982,  ..., -0.1099, -0.0753,  0.1905],
        [ 0.1530, -0.1046, -0.1885,  ...,  0.1641, -0.2341, -0.1943]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0844,  0.2480, -0.1912,  ..., -0.0388,  0.0398, -0.1683],
        [-0.0504, -0.2319, -0.0333,  ...,  0.1515,  0.1458, -0.2241],
        [ 0.1759, -0.1598,  0.0080,  ..., -0.1589, -0.1269, -0.0905],
        ...,
        [-0.2386, -0.2077, -0.1167,  ...,  0.2065,  0.0039,  0.0851],
        [-0.0558,  0.0798,  0.1982,  ..., -0.1099, -0.0753,  0.1905],
        [ 0.1530, -0.1046, -0.1885,  ...,  0.1641, -0.2341, -0.1943]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.0650],
        [ 0.2801],
        [ 0.4058],
        [ 0.1201],
        [ 0.0918],
        [ 0.2653],
        [-0.3794],
        [-0.1894],
        [-0.2806],
        [ 0.4175],
        [-0.1639],
        [-0.2495],
        [ 0.2405],
        [ 0.0759],
        [ 0.4012],
        [ 0.3689],
        [ 0.2602],
        [-0.2556],
        [ 0.0173],
        [ 0.3682],
        [ 0.1910],
        [ 0.1295],
        [ 0.1734],
        [-0.2151],
        [ 0.2410],
        [ 0.2813],
        [ 0.1869],
        [ 0.2827],
        [-0.3497],
        [ 0.3381],
        [-0.1907],
        [ 0.2531]], device='cuda:0') 
 Parameter containing:
tensor([[-0.0650],
        [ 0.2801],
        [ 0.4058],
        [ 0.1201],
        [ 0.0918],
        [ 0.2653],
        [-0.3794],
        [-0.1894],
        [-0.2806],
        [ 0.4175],
        [-0.1639],
        [-0.2495],
        [ 0.2405],
        [ 0.0759],
        [ 0.4012],
        [ 0.3689],
        [ 0.2602],
        [-0.2556],
        [ 0.0173],
        [ 0.3682],
        [ 0.1910],
        [ 0.1295],
        [ 0.1734],
        [-0.2151],
        [ 0.2410],
        [ 0.2813],
        [ 0.1869],
        [ 0.2827],
        [-0.3497],
        [ 0.3381],
        [-0.1907],
        [ 0.2531]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 5.2246e-02,  8.4451e-02, -9.6617e-02, -1.0648e-01, -7.3383e-02,
         -1.5063e-01,  2.4196e-02,  1.4109e-01, -9.2236e-02, -6.2390e-02,
         -1.0358e-01,  1.2186e-01,  6.4159e-02,  5.1945e-02,  2.3712e-03,
         -9.4380e-03, -2.2322e-02, -4.9125e-02,  9.6683e-02,  9.3816e-02,
          5.9305e-04, -1.3261e-01,  6.4481e-02,  3.5874e-02,  1.4821e-01,
         -1.1643e-02, -1.5978e-02, -1.2247e-01, -1.0491e-01,  8.7200e-02,
         -6.1108e-02, -1.7282e-02,  1.0667e-01, -1.3340e-01, -8.1445e-02,
         -3.2948e-02,  9.5382e-02,  3.1166e-02, -1.4651e-01,  9.4629e-02,
         -2.9836e-02,  1.1099e-01, -1.4557e-01,  4.2010e-03,  5.6514e-02,
         -5.6492e-02, -9.7819e-03,  4.5023e-02, -1.0917e-01,  8.0227e-02,
         -8.8456e-02,  7.8128e-02,  8.2059e-02,  1.4404e-01, -8.2033e-02,
          8.2030e-02, -5.6474e-02,  1.5163e-01, -5.6022e-02,  8.4568e-02,
          1.2459e-01, -1.1779e-01,  1.0886e-01, -1.5004e-01,  1.2287e-01,
         -1.4984e-01, -3.8366e-03,  1.5213e-01,  5.8838e-02,  5.3182e-02,
          8.8442e-02, -1.4054e-03, -6.4451e-02,  3.1259e-02, -2.0782e-02,
          1.2464e-02,  3.9515e-02,  8.2783e-03, -1.0487e-03,  5.7165e-02,
          8.0286e-02, -1.8024e-02,  6.6630e-03,  1.2735e-01,  1.1759e-01,
          6.5485e-02,  1.4325e-01, -1.0717e-01,  8.3465e-02, -1.0017e-01,
          2.3240e-02, -5.8092e-02,  1.1122e-01,  4.2787e-02,  4.1571e-02,
         -9.3215e-02, -5.7918e-02,  5.3506e-02, -4.4854e-02,  7.0095e-02,
         -3.0960e-02, -1.5149e-01,  1.3623e-01, -5.5319e-02, -7.8242e-02,
         -6.1227e-02,  9.1951e-02,  5.1343e-02, -9.5491e-02,  3.1461e-02,
         -4.4538e-02, -1.4530e-02,  6.2930e-02,  1.4256e-01, -4.8501e-02,
          1.1033e-02, -6.0240e-02,  1.0111e-01, -6.1412e-02,  9.0163e-02,
          6.2370e-03,  7.6064e-05,  4.3501e-02, -2.4211e-02,  1.1026e-01,
          8.0152e-02, -1.4572e-01, -8.3764e-02,  5.0801e-03,  2.6683e-02,
          1.2146e-01,  1.3092e-01,  1.2143e-01,  7.1013e-02, -1.0509e-01,
          1.1839e-01,  1.1919e-01,  4.1547e-02, -1.4350e-02, -1.5053e-01,
          6.7870e-03, -7.8293e-02, -1.3140e-01,  3.8388e-02, -1.5145e-01,
         -1.4720e-01,  7.5893e-02,  1.9763e-02,  5.3662e-02,  1.1753e-01,
         -1.2141e-01, -6.4251e-02, -1.2789e-01, -1.1754e-01, -9.9580e-02,
          1.3286e-01, -9.7419e-02, -1.1868e-01,  8.9753e-02, -1.4169e-01,
         -4.3512e-02,  1.2972e-01, -1.4798e-01, -5.2667e-02,  7.0572e-02,
          6.9381e-02,  5.2182e-02,  6.2540e-02, -1.0541e-01, -8.4921e-02,
         -1.0841e-01, -9.9287e-02, -6.5579e-02, -1.4949e-01, -5.5398e-02,
          2.2900e-02, -1.5695e-02, -3.5117e-02, -1.7315e-02,  3.3074e-02,
          3.3947e-02,  1.4686e-01, -6.5499e-02, -1.1965e-02,  9.9808e-02,
         -3.5258e-02,  6.3370e-02,  8.4045e-02,  7.8363e-02, -3.3103e-02,
          2.5267e-02,  1.0461e-01, -5.5126e-02, -7.8773e-02, -5.3784e-02,
         -6.5324e-03,  5.8890e-02, -2.8802e-02, -4.2706e-02,  3.9889e-02,
          7.0106e-02, -7.5114e-02, -1.0008e-01, -8.0037e-02, -7.6540e-02,
         -8.9435e-02, -1.0813e-02, -1.2835e-01, -1.2563e-01,  1.1257e-01,
          1.1539e-01,  5.4396e-02, -4.2991e-02, -1.4901e-01, -1.5213e-01,
         -6.4485e-03, -3.8597e-02, -4.6493e-02, -3.2666e-02, -1.0483e-01,
         -1.1635e-01, -7.7434e-02, -1.1601e-01,  4.2748e-02, -1.3660e-01,
         -9.8264e-02, -1.4414e-01,  5.8069e-02,  1.5279e-01, -2.3552e-03,
         -6.2857e-02,  1.2214e-01,  1.7113e-02,  1.9655e-02,  1.4438e-01,
          1.0560e-01, -1.0073e-01,  8.8446e-02, -7.2031e-02, -3.6690e-02,
         -8.9008e-02,  9.9145e-02,  8.0956e-02,  1.0545e-01, -5.2061e-02,
         -1.3178e-01,  7.9872e-02,  1.1021e-02,  1.2861e-02, -1.0371e-01,
         -7.9304e-02,  3.3047e-02, -1.1565e-01, -5.3686e-02,  1.2264e-01,
         -6.4371e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 5.2246e-02,  8.4451e-02, -9.6617e-02, -1.0648e-01, -7.3383e-02,
         -1.5063e-01,  2.4196e-02,  1.4109e-01, -9.2236e-02, -6.2390e-02,
         -1.0358e-01,  1.2186e-01,  6.4159e-02,  5.1945e-02,  2.3712e-03,
         -9.4380e-03, -2.2322e-02, -4.9125e-02,  9.6683e-02,  9.3816e-02,
          5.9305e-04, -1.3261e-01,  6.4481e-02,  3.5874e-02,  1.4821e-01,
         -1.1643e-02, -1.5978e-02, -1.2247e-01, -1.0491e-01,  8.7200e-02,
         -6.1108e-02, -1.7282e-02,  1.0667e-01, -1.3340e-01, -8.1445e-02,
         -3.2948e-02,  9.5382e-02,  3.1166e-02, -1.4651e-01,  9.4629e-02,
         -2.9836e-02,  1.1099e-01, -1.4557e-01,  4.2010e-03,  5.6514e-02,
         -5.6492e-02, -9.7819e-03,  4.5023e-02, -1.0917e-01,  8.0227e-02,
         -8.8456e-02,  7.8128e-02,  8.2059e-02,  1.4404e-01, -8.2033e-02,
          8.2030e-02, -5.6474e-02,  1.5163e-01, -5.6022e-02,  8.4568e-02,
          1.2459e-01, -1.1779e-01,  1.0886e-01, -1.5004e-01,  1.2287e-01,
         -1.4984e-01, -3.8366e-03,  1.5213e-01,  5.8838e-02,  5.3182e-02,
          8.8442e-02, -1.4054e-03, -6.4451e-02,  3.1259e-02, -2.0782e-02,
          1.2464e-02,  3.9515e-02,  8.2783e-03, -1.0487e-03,  5.7165e-02,
          8.0286e-02, -1.8024e-02,  6.6630e-03,  1.2735e-01,  1.1759e-01,
          6.5485e-02,  1.4325e-01, -1.0717e-01,  8.3465e-02, -1.0017e-01,
          2.3240e-02, -5.8092e-02,  1.1122e-01,  4.2787e-02,  4.1571e-02,
         -9.3215e-02, -5.7918e-02,  5.3506e-02, -4.4854e-02,  7.0095e-02,
         -3.0960e-02, -1.5149e-01,  1.3623e-01, -5.5319e-02, -7.8242e-02,
         -6.1227e-02,  9.1951e-02,  5.1343e-02, -9.5491e-02,  3.1461e-02,
         -4.4538e-02, -1.4530e-02,  6.2930e-02,  1.4256e-01, -4.8501e-02,
          1.1033e-02, -6.0240e-02,  1.0111e-01, -6.1412e-02,  9.0163e-02,
          6.2370e-03,  7.6064e-05,  4.3501e-02, -2.4211e-02,  1.1026e-01,
          8.0152e-02, -1.4572e-01, -8.3764e-02,  5.0801e-03,  2.6683e-02,
          1.2146e-01,  1.3092e-01,  1.2143e-01,  7.1013e-02, -1.0509e-01,
          1.1839e-01,  1.1919e-01,  4.1547e-02, -1.4350e-02, -1.5053e-01,
          6.7870e-03, -7.8293e-02, -1.3140e-01,  3.8388e-02, -1.5145e-01,
         -1.4720e-01,  7.5893e-02,  1.9763e-02,  5.3662e-02,  1.1753e-01,
         -1.2141e-01, -6.4251e-02, -1.2789e-01, -1.1754e-01, -9.9580e-02,
          1.3286e-01, -9.7419e-02, -1.1868e-01,  8.9753e-02, -1.4169e-01,
         -4.3512e-02,  1.2972e-01, -1.4798e-01, -5.2667e-02,  7.0572e-02,
          6.9381e-02,  5.2182e-02,  6.2540e-02, -1.0541e-01, -8.4921e-02,
         -1.0841e-01, -9.9287e-02, -6.5579e-02, -1.4949e-01, -5.5398e-02,
          2.2900e-02, -1.5695e-02, -3.5117e-02, -1.7315e-02,  3.3074e-02,
          3.3947e-02,  1.4686e-01, -6.5499e-02, -1.1965e-02,  9.9808e-02,
         -3.5258e-02,  6.3370e-02,  8.4045e-02,  7.8363e-02, -3.3103e-02,
          2.5267e-02,  1.0461e-01, -5.5126e-02, -7.8773e-02, -5.3784e-02,
         -6.5324e-03,  5.8890e-02, -2.8802e-02, -4.2706e-02,  3.9889e-02,
          7.0106e-02, -7.5114e-02, -1.0008e-01, -8.0037e-02, -7.6540e-02,
         -8.9435e-02, -1.0813e-02, -1.2835e-01, -1.2563e-01,  1.1257e-01,
          1.1539e-01,  5.4396e-02, -4.2991e-02, -1.4901e-01, -1.5213e-01,
         -6.4485e-03, -3.8597e-02, -4.6493e-02, -3.2666e-02, -1.0483e-01,
         -1.1635e-01, -7.7434e-02, -1.1601e-01,  4.2748e-02, -1.3660e-01,
         -9.8264e-02, -1.4414e-01,  5.8069e-02,  1.5279e-01, -2.3552e-03,
         -6.2857e-02,  1.2214e-01,  1.7113e-02,  1.9655e-02,  1.4438e-01,
          1.0560e-01, -1.0073e-01,  8.8446e-02, -7.2031e-02, -3.6690e-02,
         -8.9008e-02,  9.9145e-02,  8.0956e-02,  1.0545e-01, -5.2061e-02,
         -1.3178e-01,  7.9872e-02,  1.1021e-02,  1.2861e-02, -1.0371e-01,
         -7.9304e-02,  3.3047e-02, -1.1565e-01, -5.3686e-02,  1.2264e-01,
         -6.4371e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0684,  0.1167,  0.0261,  ..., -0.0599, -0.0289,  0.0519],
        [ 0.0082, -0.0599,  0.0006,  ...,  0.0357, -0.1096, -0.1093],
        [ 0.0914, -0.0553,  0.0971,  ...,  0.0836,  0.0388, -0.0452],
        ...,
        [-0.1067, -0.1070,  0.0217,  ..., -0.0478, -0.0831,  0.0950],
        [ 0.1024,  0.0592, -0.0875,  ..., -0.1046,  0.0583,  0.0080],
        [ 0.0957,  0.0264,  0.0476,  ..., -0.0213,  0.0468, -0.0734]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0684,  0.1167,  0.0261,  ..., -0.0599, -0.0289,  0.0519],
        [ 0.0082, -0.0599,  0.0006,  ...,  0.0357, -0.1096, -0.1093],
        [ 0.0914, -0.0553,  0.0971,  ...,  0.0836,  0.0388, -0.0452],
        ...,
        [-0.1067, -0.1070,  0.0217,  ..., -0.0478, -0.0831,  0.0950],
        [ 0.1024,  0.0592, -0.0875,  ..., -0.1046,  0.0583,  0.0080],
        [ 0.0957,  0.0264,  0.0476,  ..., -0.0213,  0.0468, -0.0734]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0699, -0.0680, -0.0958,  ...,  0.1649, -0.0477,  0.1540],
        [-0.1056, -0.0508, -0.0400,  ..., -0.1342,  0.0662,  0.1746],
        [ 0.0336,  0.1226,  0.0238,  ..., -0.1641,  0.0996, -0.0315],
        ...,
        [ 0.1442, -0.0259,  0.1366,  ...,  0.0336, -0.1677, -0.0057],
        [ 0.0087,  0.1603,  0.0445,  ...,  0.1045, -0.0527,  0.1264],
        [ 0.0606,  0.1147, -0.1381,  ..., -0.0482,  0.1762,  0.1243]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0699, -0.0680, -0.0958,  ...,  0.1649, -0.0477,  0.1540],
        [-0.1056, -0.0508, -0.0400,  ..., -0.1342,  0.0662,  0.1746],
        [ 0.0336,  0.1226,  0.0238,  ..., -0.1641,  0.0996, -0.0315],
        ...,
        [ 0.1442, -0.0259,  0.1366,  ...,  0.0336, -0.1677, -0.0057],
        [ 0.0087,  0.1603,  0.0445,  ...,  0.1045, -0.0527,  0.1264],
        [ 0.0606,  0.1147, -0.1381,  ..., -0.0482,  0.1762,  0.1243]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.2005,  0.1201,  0.1617,  ..., -0.0171, -0.1905,  0.1340],
        [ 0.1746,  0.2210, -0.0082,  ...,  0.1139, -0.2214,  0.1399],
        [ 0.1110,  0.1855,  0.1972,  ..., -0.1859,  0.1698,  0.0217],
        ...,
        [ 0.0045, -0.1896,  0.1893,  ..., -0.2220, -0.0654, -0.1352],
        [ 0.1302, -0.1128, -0.1758,  ..., -0.1005, -0.2327,  0.1701],
        [ 0.1505,  0.1504, -0.1869,  ...,  0.1662,  0.2016,  0.1388]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.2005,  0.1201,  0.1617,  ..., -0.0171, -0.1905,  0.1340],
        [ 0.1746,  0.2210, -0.0082,  ...,  0.1139, -0.2214,  0.1399],
        [ 0.1110,  0.1855,  0.1972,  ..., -0.1859,  0.1698,  0.0217],
        ...,
        [ 0.0045, -0.1896,  0.1893,  ..., -0.2220, -0.0654, -0.1352],
        [ 0.1302, -0.1128, -0.1758,  ..., -0.1005, -0.2327,  0.1701],
        [ 0.1505,  0.1504, -0.1869,  ...,  0.1662,  0.2016,  0.1388]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.1712],
        [-0.3665],
        [ 0.1907],
        [ 0.3955],
        [ 0.2157],
        [ 0.0503],
        [-0.3401],
        [-0.3944],
        [-0.1928],
        [-0.0670],
        [-0.0533],
        [ 0.1114],
        [-0.0664],
        [-0.1767],
        [-0.1297],
        [ 0.3683],
        [-0.2168],
        [ 0.1213],
        [-0.0735],
        [-0.2500],
        [ 0.4143],
        [ 0.1000],
        [ 0.1261],
        [ 0.4015],
        [ 0.0446],
        [-0.0135],
        [-0.1527],
        [-0.1217],
        [-0.2902],
        [ 0.0968],
        [ 0.4136],
        [ 0.2481]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.1712],
        [-0.3665],
        [ 0.1907],
        [ 0.3955],
        [ 0.2157],
        [ 0.0503],
        [-0.3401],
        [-0.3944],
        [-0.1928],
        [-0.0670],
        [-0.0533],
        [ 0.1114],
        [-0.0664],
        [-0.1767],
        [-0.1297],
        [ 0.3683],
        [-0.2168],
        [ 0.1213],
        [-0.0735],
        [-0.2500],
        [ 0.4143],
        [ 0.1000],
        [ 0.1261],
        [ 0.4015],
        [ 0.0446],
        [-0.0135],
        [-0.1527],
        [-0.1217],
        [-0.2902],
        [ 0.0968],
        [ 0.4136],
        [ 0.2481]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0039,  0.0033,  ...,  0.0022, -0.0002, -0.0053],
        [-0.0008, -0.0129,  0.0110,  ...,  0.0073, -0.0005, -0.0175],
        [-0.0005, -0.0086,  0.0073,  ...,  0.0048, -0.0003, -0.0116],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-138.8252, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6977, device='cuda:0')



h[100].sum tensor(17.9383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6365, device='cuda:0')



h[200].sum tensor(-6.5911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-6.8476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0251,  ..., 0.0167, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0221,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0304,  ..., 0.0202, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(17804.9980, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1391, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0320],
        [0.1364, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0314],
        [0.1371, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0316],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(93668.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2235.6851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.6335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(876.4329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(55.9152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2178.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(139.0155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1581],
        [-0.1522],
        [-0.1367],
        ...,
        [-0.0012],
        [-0.0012],
        [-0.0012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-8128.1851, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.1581],
        [-0.1522],
        [-0.1367],
        ...,
        [-0.0012],
        [-0.0012],
        [-0.0012]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101,  0.0157,  0.0126,  ...,  0.0087, -0.0131, -0.0078],
        [ 0.0224,  0.0349,  0.0280,  ...,  0.0193, -0.0291, -0.0172],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(318.9846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.5528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0976, device='cuda:0')



h[100].sum tensor(27.2359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8167, device='cuda:0')



h[200].sum tensor(22.5424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0895, 0.1396, 0.1120,  ..., 0.0770, 0.0000, 0.0000],
        [0.0451, 0.0703, 0.0564,  ..., 0.0388, 0.0000, 0.0000],
        [0.0506, 0.0790, 0.0633,  ..., 0.0436, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(31078.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0624, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(141074.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.7191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4950.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(341.0489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7147.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(492.3569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7355e+00],
        [-1.5010e+00],
        [-1.3095e+00],
        ...,
        [-2.1295e-05],
        [-3.5405e-05],
        [-5.0610e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-37420.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1581],
        [-0.1522],
        [-0.1367],
        ...,
        [-0.0012],
        [-0.0012],
        [-0.0012]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 3001 
BatchSize 30 
EpochNum 60 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0850,  0.0436,  0.0742, -0.0060,  0.0145,  0.0378, -0.0159,  0.0138,
         -0.1469, -0.0394,  0.0883,  0.1219, -0.1342, -0.1232, -0.0503,  0.1096,
         -0.0425,  0.0515, -0.0147, -0.0356,  0.0365,  0.0802, -0.1069,  0.0727,
          0.1491, -0.0305,  0.0526, -0.0877, -0.0309, -0.1387,  0.0709, -0.1384,
         -0.0209,  0.0936, -0.0012,  0.0628, -0.0957,  0.0926, -0.1499, -0.0597,
         -0.0044, -0.0118,  0.1230,  0.0067,  0.0035,  0.0821,  0.1448,  0.1140,
         -0.0351, -0.1156,  0.1324,  0.0114,  0.0829,  0.0695,  0.0112,  0.0805,
          0.0891, -0.0718, -0.1034,  0.1479,  0.0106, -0.1461,  0.0300,  0.1484,
          0.1402,  0.0623,  0.1045, -0.0483,  0.1413, -0.0328, -0.0371, -0.0221,
          0.0200,  0.1170,  0.1263, -0.0977, -0.1102,  0.1479, -0.1209,  0.0870,
          0.1223,  0.0319, -0.1503, -0.0416, -0.0083, -0.0074, -0.0617, -0.1173,
          0.0133,  0.0162, -0.0951, -0.0283,  0.0688, -0.0420,  0.0958,  0.1005,
          0.1261,  0.1488, -0.0153,  0.0316,  0.1079, -0.0276,  0.1228, -0.1215,
          0.0705, -0.0833, -0.1390,  0.0477, -0.0544, -0.1322,  0.1364, -0.0499,
          0.1251,  0.0187, -0.0543,  0.0914,  0.0161, -0.1335,  0.1032, -0.0413,
          0.0400, -0.1263, -0.1217,  0.1053, -0.1137, -0.0612,  0.0092,  0.0168,
          0.1368, -0.1178,  0.1517,  0.0252, -0.1521,  0.0716,  0.0847,  0.1208,
          0.0604,  0.1059,  0.0186, -0.1470, -0.0557, -0.1039, -0.1386, -0.1035,
         -0.1308,  0.0960,  0.1355, -0.0713,  0.0157, -0.0254, -0.0329, -0.0464,
          0.1006,  0.0249,  0.1164, -0.0585,  0.0207,  0.0354,  0.0206,  0.1428,
          0.0412,  0.0684,  0.0615,  0.0406, -0.0031,  0.0865, -0.0775, -0.0960,
          0.0959, -0.0170, -0.0945, -0.1038, -0.0634,  0.1362,  0.0905, -0.0963,
          0.0807, -0.1373, -0.0319, -0.0602,  0.0498,  0.0829, -0.1013,  0.0175,
          0.0251, -0.0973,  0.0875,  0.0212, -0.0595, -0.0389,  0.1334,  0.0409,
          0.0813,  0.0952,  0.0029, -0.1244, -0.0485,  0.0024, -0.1182,  0.0833,
         -0.1242,  0.0853, -0.0786, -0.1134, -0.1178,  0.1253,  0.0046,  0.0536,
         -0.0328, -0.1029,  0.1070,  0.0368,  0.0892, -0.1247, -0.0158,  0.1193,
          0.1378, -0.0926,  0.0180, -0.0096, -0.1000,  0.0403,  0.0987,  0.1176,
          0.1448, -0.1372,  0.1260,  0.0107, -0.0172,  0.1212,  0.1122,  0.0837,
          0.1457,  0.1376,  0.0219,  0.1113,  0.0978, -0.0493, -0.0366,  0.0175,
          0.0736,  0.1517, -0.0041,  0.0506,  0.0798,  0.1276,  0.0737, -0.0746,
         -0.0543,  0.0140,  0.1202,  0.0264, -0.0461, -0.0586,  0.1397,  0.1112]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0969,  0.0590, -0.0489,  ..., -0.0322,  0.0805,  0.0050],
        [-0.0846, -0.0971,  0.0029,  ..., -0.0967, -0.0412,  0.0147],
        [ 0.0701,  0.0633, -0.0323,  ...,  0.0732,  0.1080,  0.1112],
        ...,
        [ 0.0542,  0.0938, -0.0401,  ...,  0.0079, -0.0165, -0.0658],
        [ 0.1054,  0.0476, -0.0545,  ..., -0.0125,  0.0762, -0.0552],
        [-0.0827,  0.0005, -0.0892,  ...,  0.0422,  0.0282, -0.0698]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0190,  0.0170,  0.0569,  ..., -0.1571, -0.0586,  0.1220],
        [-0.0857,  0.1479, -0.1734,  ..., -0.0828,  0.1208, -0.0232],
        [-0.0611,  0.0241,  0.0671,  ..., -0.1395,  0.1031, -0.0817],
        ...,
        [-0.1627,  0.0823,  0.1476,  ..., -0.1396, -0.0863, -0.0111],
        [-0.0642, -0.1747,  0.0234,  ..., -0.1001,  0.0897, -0.0386],
        [-0.0471,  0.1481,  0.1289,  ..., -0.1651,  0.1303, -0.0966]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1819, -0.1942, -0.2265,  ..., -0.1419, -0.0441, -0.2313],
        [ 0.1952, -0.1553, -0.1217,  ...,  0.2181,  0.0177, -0.0958],
        [-0.1087,  0.0796,  0.1576,  ..., -0.0350,  0.2111,  0.0677],
        ...,
        [-0.2274,  0.2251, -0.0617,  ..., -0.0419,  0.1035,  0.1432],
        [ 0.1988,  0.1585,  0.1241,  ...,  0.0799, -0.2414, -0.0685],
        [ 0.2484,  0.1109, -0.1812,  ...,  0.2072,  0.0493, -0.1081]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.3897],
        [ 0.3161],
        [ 0.1676],
        [ 0.1243],
        [-0.1607],
        [-0.1095],
        [-0.0123],
        [-0.2028],
        [-0.0372],
        [-0.2106],
        [-0.1078],
        [-0.0812],
        [ 0.0354],
        [ 0.0067],
        [-0.2815],
        [ 0.3773],
        [-0.3450],
        [-0.2499],
        [-0.0216],
        [-0.1219],
        [-0.4178],
        [-0.1048],
        [-0.1352],
        [-0.2640],
        [-0.0349],
        [ 0.1325],
        [ 0.0156],
        [-0.3035],
        [-0.1867],
        [ 0.2771],
        [ 0.2344],
        [-0.3014]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0850,  0.0436,  0.0742, -0.0060,  0.0145,  0.0378, -0.0159,  0.0138,
         -0.1469, -0.0394,  0.0883,  0.1219, -0.1342, -0.1232, -0.0503,  0.1096,
         -0.0425,  0.0515, -0.0147, -0.0356,  0.0365,  0.0802, -0.1069,  0.0727,
          0.1491, -0.0305,  0.0526, -0.0877, -0.0309, -0.1387,  0.0709, -0.1384,
         -0.0209,  0.0936, -0.0012,  0.0628, -0.0957,  0.0926, -0.1499, -0.0597,
         -0.0044, -0.0118,  0.1230,  0.0067,  0.0035,  0.0821,  0.1448,  0.1140,
         -0.0351, -0.1156,  0.1324,  0.0114,  0.0829,  0.0695,  0.0112,  0.0805,
          0.0891, -0.0718, -0.1034,  0.1479,  0.0106, -0.1461,  0.0300,  0.1484,
          0.1402,  0.0623,  0.1045, -0.0483,  0.1413, -0.0328, -0.0371, -0.0221,
          0.0200,  0.1170,  0.1263, -0.0977, -0.1102,  0.1479, -0.1209,  0.0870,
          0.1223,  0.0319, -0.1503, -0.0416, -0.0083, -0.0074, -0.0617, -0.1173,
          0.0133,  0.0162, -0.0951, -0.0283,  0.0688, -0.0420,  0.0958,  0.1005,
          0.1261,  0.1488, -0.0153,  0.0316,  0.1079, -0.0276,  0.1228, -0.1215,
          0.0705, -0.0833, -0.1390,  0.0477, -0.0544, -0.1322,  0.1364, -0.0499,
          0.1251,  0.0187, -0.0543,  0.0914,  0.0161, -0.1335,  0.1032, -0.0413,
          0.0400, -0.1263, -0.1217,  0.1053, -0.1137, -0.0612,  0.0092,  0.0168,
          0.1368, -0.1178,  0.1517,  0.0252, -0.1521,  0.0716,  0.0847,  0.1208,
          0.0604,  0.1059,  0.0186, -0.1470, -0.0557, -0.1039, -0.1386, -0.1035,
         -0.1308,  0.0960,  0.1355, -0.0713,  0.0157, -0.0254, -0.0329, -0.0464,
          0.1006,  0.0249,  0.1164, -0.0585,  0.0207,  0.0354,  0.0206,  0.1428,
          0.0412,  0.0684,  0.0615,  0.0406, -0.0031,  0.0865, -0.0775, -0.0960,
          0.0959, -0.0170, -0.0945, -0.1038, -0.0634,  0.1362,  0.0905, -0.0963,
          0.0807, -0.1373, -0.0319, -0.0602,  0.0498,  0.0829, -0.1013,  0.0175,
          0.0251, -0.0973,  0.0875,  0.0212, -0.0595, -0.0389,  0.1334,  0.0409,
          0.0813,  0.0952,  0.0029, -0.1244, -0.0485,  0.0024, -0.1182,  0.0833,
         -0.1242,  0.0853, -0.0786, -0.1134, -0.1178,  0.1253,  0.0046,  0.0536,
         -0.0328, -0.1029,  0.1070,  0.0368,  0.0892, -0.1247, -0.0158,  0.1193,
          0.1378, -0.0926,  0.0180, -0.0096, -0.1000,  0.0403,  0.0987,  0.1176,
          0.1448, -0.1372,  0.1260,  0.0107, -0.0172,  0.1212,  0.1122,  0.0837,
          0.1457,  0.1376,  0.0219,  0.1113,  0.0978, -0.0493, -0.0366,  0.0175,
          0.0736,  0.1517, -0.0041,  0.0506,  0.0798,  0.1276,  0.0737, -0.0746,
         -0.0543,  0.0140,  0.1202,  0.0264, -0.0461, -0.0586,  0.1397,  0.1112]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0969,  0.0590, -0.0489,  ..., -0.0322,  0.0805,  0.0050],
        [-0.0846, -0.0971,  0.0029,  ..., -0.0967, -0.0412,  0.0147],
        [ 0.0701,  0.0633, -0.0323,  ...,  0.0732,  0.1080,  0.1112],
        ...,
        [ 0.0542,  0.0938, -0.0401,  ...,  0.0079, -0.0165, -0.0658],
        [ 0.1054,  0.0476, -0.0545,  ..., -0.0125,  0.0762, -0.0552],
        [-0.0827,  0.0005, -0.0892,  ...,  0.0422,  0.0282, -0.0698]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0190,  0.0170,  0.0569,  ..., -0.1571, -0.0586,  0.1220],
        [-0.0857,  0.1479, -0.1734,  ..., -0.0828,  0.1208, -0.0232],
        [-0.0611,  0.0241,  0.0671,  ..., -0.1395,  0.1031, -0.0817],
        ...,
        [-0.1627,  0.0823,  0.1476,  ..., -0.1396, -0.0863, -0.0111],
        [-0.0642, -0.1747,  0.0234,  ..., -0.1001,  0.0897, -0.0386],
        [-0.0471,  0.1481,  0.1289,  ..., -0.1651,  0.1303, -0.0966]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1819, -0.1942, -0.2265,  ..., -0.1419, -0.0441, -0.2313],
        [ 0.1952, -0.1553, -0.1217,  ...,  0.2181,  0.0177, -0.0958],
        [-0.1087,  0.0796,  0.1576,  ..., -0.0350,  0.2111,  0.0677],
        ...,
        [-0.2274,  0.2251, -0.0617,  ..., -0.0419,  0.1035,  0.1432],
        [ 0.1988,  0.1585,  0.1241,  ...,  0.0799, -0.2414, -0.0685],
        [ 0.2484,  0.1109, -0.1812,  ...,  0.2072,  0.0493, -0.1081]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.3897],
        [ 0.3161],
        [ 0.1676],
        [ 0.1243],
        [-0.1607],
        [-0.1095],
        [-0.0123],
        [-0.2028],
        [-0.0372],
        [-0.2106],
        [-0.1078],
        [-0.0812],
        [ 0.0354],
        [ 0.0067],
        [-0.2815],
        [ 0.3773],
        [-0.3450],
        [-0.2499],
        [-0.0216],
        [-0.1219],
        [-0.4178],
        [-0.1048],
        [-0.1352],
        [-0.2640],
        [-0.0349],
        [ 0.1325],
        [ 0.0156],
        [-0.3035],
        [-0.1867],
        [ 0.2771],
        [ 0.2344],
        [-0.3014]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9890.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(262.8761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(302.7996, device='cuda:0')



h[100].sum tensor(333.8391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(322.7449, device='cuda:0')



h[200].sum tensor(-384.2278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(267.1271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0076, 0.0130,  ..., 0.0000, 0.0244, 0.0194],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(381415.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1194, 0.1275,  ..., 0.0000, 0.0641, 0.0000],
        [0.0000, 0.0508, 0.0542,  ..., 0.0000, 0.0273, 0.0000],
        [0.0000, 0.0346, 0.0369,  ..., 0.0000, 0.0186, 0.0000],
        ...,
        [0.0000, 0.0281, 0.0300,  ..., 0.0000, 0.0151, 0.0000],
        [0.0000, 0.0474, 0.0506,  ..., 0.0000, 0.0255, 0.0000],
        [0.0000, 0.0698, 0.0745,  ..., 0.0000, 0.0374, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1793121., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3049.5432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-144.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1891.1138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0176],
        [-0.8933],
        [-0.8761],
        ...,
        [-0.3369],
        [-0.4372],
        [-0.5368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-760065.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(271.1972, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.8662e-03,  4.7212e-03,  7.7256e-03,  ..., -6.1783e-03,
          1.4644e-02,  1.1639e-02],
        [ 1.9827e-02,  1.0370e-02,  1.7292e-02,  ..., -1.3731e-02,
          3.2668e-02,  2.5990e-02],
        [-1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
         -1.0000e-04, -1.0000e-04],
        ...,
        [-1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
         -1.0000e-04, -1.0000e-04],
        [-1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
         -1.0000e-04, -1.0000e-04],
        [-1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
         -1.0000e-04, -1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9525.8887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(242.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(302.8776, device='cuda:0')



h[100].sum tensor(355.0500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(322.8280, device='cuda:0')



h[200].sum tensor(-384.5090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(267.1959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0792, 0.0414, 0.0691,  ..., 0.0000, 0.1306, 0.1039],
        [0.0398, 0.0211, 0.0347,  ..., 0.0000, 0.0656, 0.0522],
        [0.0447, 0.0236, 0.0390,  ..., 0.0000, 0.0737, 0.0587],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0053, 0.0032, 0.0046,  ..., 0.0000, 0.0088, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(393993., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4378, 0.4835,  ..., 0.0000, 0.2346, 0.0000],
        [0.0000, 0.3338, 0.3698,  ..., 0.0000, 0.1791, 0.0000],
        [0.0000, 0.3073, 0.3409,  ..., 0.0000, 0.1650, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0044,  ..., 0.0000, 0.0011, 0.0010],
        [0.0000, 0.0216, 0.0268,  ..., 0.0000, 0.0119, 0.0006],
        [0.0000, 0.0648, 0.0750,  ..., 0.0000, 0.0354, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1860262., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3183.9624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.3489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2009.1427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5125],
        [-2.1559],
        [-1.8637],
        ...,
        [-0.0538],
        [-0.1440],
        [-0.2817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-528315.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9272e-04,  1.9891e-04, -1.9654e-04,  ...,  0.0000e+00,
         -1.9996e-04, -3.5894e-05],
        [ 5.3952e-03,  3.0885e-03,  4.6798e-03,  ..., -3.8482e-03,
          8.9928e-03,  7.2928e-03],
        [-1.9272e-04,  1.9891e-04, -1.9654e-04,  ...,  0.0000e+00,
         -1.9996e-04, -3.5894e-05],
        ...,
        [-1.9272e-04,  1.9891e-04, -1.9654e-04,  ...,  0.0000e+00,
         -1.9996e-04, -3.5894e-05],
        [-1.9272e-04,  1.9891e-04, -1.9654e-04,  ...,  0.0000e+00,
         -1.9996e-04, -3.5894e-05],
        [-1.9272e-04,  1.9891e-04, -1.9654e-04,  ...,  0.0000e+00,
         -1.9996e-04, -3.5894e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10068.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(244.8652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(327.5671, device='cuda:0')



h[100].sum tensor(402.1794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(349.1439, device='cuda:0')



h[200].sum tensor(-415.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(288.9768, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.0154, 0.0239,  ..., 0.0000, 0.0458, 0.0370],
        [0.0084, 0.0052, 0.0073,  ..., 0.0000, 0.0139, 0.0112],
        [0.0054, 0.0037, 0.0047,  ..., 0.0000, 0.0090, 0.0073],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(433528.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.9453e-02, 1.2142e-01,  ..., 0.0000e+00, 5.5646e-02,
         0.0000e+00],
        [0.0000e+00, 7.8651e-02, 9.7413e-02,  ..., 0.0000e+00, 4.4233e-02,
         0.0000e+00],
        [0.0000e+00, 9.7812e-02, 1.1873e-01,  ..., 0.0000e+00, 5.4378e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.4499e-03,  ..., 0.0000e+00, 6.9657e-05,
         2.2768e-03],
        [0.0000e+00, 0.0000e+00, 4.4499e-03,  ..., 0.0000e+00, 6.9657e-05,
         2.2768e-03],
        [0.0000e+00, 0.0000e+00, 4.4499e-03,  ..., 0.0000e+00, 6.9657e-05,
         2.2768e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2073266.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3408.0098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(93.1858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2317.0862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2879],
        [-0.4229],
        [-0.6489],
        ...,
        [-0.0087],
        [-0.0086],
        [-0.0086]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-442529.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05],
        [-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05],
        [-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05],
        ...,
        [-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05],
        [-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05],
        [-2.8325e-04,  2.8970e-04, -2.8974e-04,  ...,  0.0000e+00,
         -2.9701e-04,  4.4765e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8962.1104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(203.1653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(300.5271, device='cuda:0')



h[100].sum tensor(388.6099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(320.3227, device='cuda:0')



h[200].sum tensor(-381.7563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(265.1223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(409765.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0254,  ..., 0.0000, 0.0091, 0.0013],
        [0.0000, 0.0010, 0.0094,  ..., 0.0000, 0.0016, 0.0023],
        [0.0000, 0.0022, 0.0100,  ..., 0.0000, 0.0019, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0001, 0.0036],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0001, 0.0036],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0001, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2004967., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3120.8870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(238.5771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2260.7480, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0865],
        [-0.0599],
        [-0.0500],
        ...,
        [-0.0168],
        [-0.0166],
        [-0.0165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-338311.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0004, -0.0004,  ...,  0.0000, -0.0004,  0.0001],
        [ 0.0051,  0.0032,  0.0044,  ..., -0.0038,  0.0086,  0.0073],
        [-0.0004,  0.0004, -0.0004,  ...,  0.0000, -0.0004,  0.0001],
        ...,
        [-0.0004,  0.0004, -0.0004,  ...,  0.0000, -0.0004,  0.0001],
        [-0.0004,  0.0004, -0.0004,  ...,  0.0000, -0.0004,  0.0001],
        [-0.0004,  0.0004, -0.0004,  ...,  0.0000, -0.0004,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8505.4238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(177.5154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(290.7571, device='cuda:0')



h[100].sum tensor(390.1292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(309.9091, device='cuda:0')



h[200].sum tensor(-368.6925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(256.5033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0119, 0.0159,  ..., 0.0000, 0.0313, 0.0267],
        [0.0041, 0.0038, 0.0035,  ..., 0.0000, 0.0070, 0.0064],
        [0.0051, 0.0043, 0.0044,  ..., 0.0000, 0.0086, 0.0077],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(404985.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0554, 0.0858,  ..., 0.0000, 0.0361, 0.0000],
        [0.0000, 0.0313, 0.0552,  ..., 0.0000, 0.0220, 0.0000],
        [0.0000, 0.0357, 0.0602,  ..., 0.0000, 0.0243, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0002, 0.0047],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0002, 0.0047],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0002, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2013459.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2965.9333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(349.4541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2266.7407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2150],
        [-0.2269],
        [-0.2511],
        ...,
        [-0.0260],
        [-0.0244],
        [-0.0236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-285421.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0209,  0.0116,  0.0182,  ..., -0.0147,  0.0347,  0.0284],
        [ 0.0217,  0.0120,  0.0188,  ..., -0.0152,  0.0359,  0.0293],
        [ 0.0208,  0.0115,  0.0181,  ..., -0.0146,  0.0345,  0.0282],
        ...,
        [ 0.0132,  0.0076,  0.0115,  ..., -0.0094,  0.0220,  0.0182],
        [ 0.0117,  0.0068,  0.0102,  ..., -0.0084,  0.0196,  0.0163],
        [ 0.0116,  0.0068,  0.0101,  ..., -0.0083,  0.0194,  0.0162]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7654.9873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(141.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(266.3177, device='cuda:0')



h[100].sum tensor(373.6268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(283.8599, device='cuda:0')



h[200].sum tensor(-337.1840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(234.9430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0897, 0.0496, 0.0781,  ..., 0.0000, 0.1488, 0.1215],
        [0.0756, 0.0422, 0.0657,  ..., 0.0000, 0.1255, 0.1028],
        [0.0775, 0.0432, 0.0674,  ..., 0.0000, 0.1286, 0.1054],
        ...,
        [0.0398, 0.0235, 0.0346,  ..., 0.0000, 0.0666, 0.0557],
        [0.0436, 0.0255, 0.0379,  ..., 0.0000, 0.0729, 0.0607],
        [0.0417, 0.0245, 0.0362,  ..., 0.0000, 0.0698, 0.0582]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376627.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4742, 0.5923,  ..., 0.0000, 0.2602, 0.0000],
        [0.0000, 0.4486, 0.5620,  ..., 0.0000, 0.2468, 0.0000],
        [0.0000, 0.4194, 0.5274,  ..., 0.0000, 0.2315, 0.0000],
        ...,
        [0.0000, 0.1724, 0.2320,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.1991, 0.2641,  ..., 0.0000, 0.1147, 0.0000],
        [0.0000, 0.2057, 0.2724,  ..., 0.0000, 0.1184, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1894583., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2630.7234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(458.1870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2146.7864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5593],
        [-0.5482],
        [-0.4815],
        ...,
        [-0.1532],
        [-0.1832],
        [-0.2018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-229765.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0064,  0.0092,  ..., -0.0076,  0.0178,  0.0150],
        [ 0.0039,  0.0028,  0.0034,  ..., -0.0030,  0.0068,  0.0062],
        [ 0.0062,  0.0040,  0.0054,  ..., -0.0046,  0.0105,  0.0092],
        ...,
        [ 0.0064,  0.0041,  0.0055,  ..., -0.0047,  0.0108,  0.0094],
        [ 0.0060,  0.0039,  0.0052,  ..., -0.0044,  0.0101,  0.0089],
        [ 0.0124,  0.0073,  0.0107,  ..., -0.0088,  0.0206,  0.0173]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8532.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(150.8558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(290.9082, device='cuda:0')



h[100].sum tensor(411.2360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(310.0703, device='cuda:0')



h[200].sum tensor(-369.0549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(256.6366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0117, 0.0147,  ..., 0.0000, 0.0287, 0.0255],
        [0.0402, 0.0242, 0.0349,  ..., 0.0000, 0.0673, 0.0569],
        [0.0174, 0.0117, 0.0151,  ..., 0.0000, 0.0293, 0.0255],
        ...,
        [0.0365, 0.0223, 0.0317,  ..., 0.0000, 0.0614, 0.0521],
        [0.0285, 0.0181, 0.0247,  ..., 0.0000, 0.0481, 0.0415],
        [0.0253, 0.0164, 0.0219,  ..., 0.0000, 0.0428, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(416779.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1076, 0.1627,  ..., 0.0000, 0.0689, 0.0000],
        [0.0000, 0.1469, 0.2111,  ..., 0.0000, 0.0902, 0.0000],
        [0.0000, 0.0975, 0.1486,  ..., 0.0000, 0.0623, 0.0000],
        ...,
        [0.0000, 0.1879, 0.2617,  ..., 0.0000, 0.1124, 0.0000],
        [0.0000, 0.1819, 0.2548,  ..., 0.0000, 0.1095, 0.0000],
        [0.0000, 0.1775, 0.2497,  ..., 0.0000, 0.1073, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2113345.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2864.5149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(528.5466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2383.9800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1357],
        [-0.1148],
        [-0.0906],
        ...,
        [-0.1962],
        [-0.1893],
        [-0.1807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-222052.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0126,  0.0075,  0.0110,  ..., -0.0090,  0.0211,  0.0178],
        [ 0.0149,  0.0087,  0.0130,  ..., -0.0106,  0.0249,  0.0208],
        [ 0.0063,  0.0042,  0.0055,  ..., -0.0047,  0.0107,  0.0095],
        ...,
        [-0.0005,  0.0006, -0.0005,  ...,  0.0000, -0.0006,  0.0004],
        [ 0.0103,  0.0063,  0.0090,  ..., -0.0074,  0.0173,  0.0147],
        [ 0.0043,  0.0031,  0.0037,  ..., -0.0033,  0.0073,  0.0067]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9816.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(171.7200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(327.5502, device='cuda:0')



h[100].sum tensor(460.4458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(349.1258, device='cuda:0')



h[200].sum tensor(-414.8030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(288.9618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0475, 0.0284, 0.0414,  ..., 0.0000, 0.0796, 0.0673],
        [0.0417, 0.0254, 0.0363,  ..., 0.0000, 0.0699, 0.0595],
        [0.0442, 0.0267, 0.0385,  ..., 0.0000, 0.0741, 0.0629],
        ...,
        [0.0175, 0.0121, 0.0152,  ..., 0.0000, 0.0294, 0.0261],
        [0.0196, 0.0135, 0.0171,  ..., 0.0000, 0.0333, 0.0297],
        [0.0322, 0.0204, 0.0281,  ..., 0.0000, 0.0544, 0.0471]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(468560.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2291, 0.3200,  ..., 0.0000, 0.1364, 0.0000],
        [0.0000, 0.1916, 0.2736,  ..., 0.0000, 0.1164, 0.0000],
        [0.0000, 0.1750, 0.2536,  ..., 0.0000, 0.1079, 0.0000],
        ...,
        [0.0000, 0.0961, 0.1525,  ..., 0.0000, 0.0634, 0.0000],
        [0.0000, 0.1191, 0.1833,  ..., 0.0000, 0.0772, 0.0000],
        [0.0000, 0.1261, 0.1920,  ..., 0.0000, 0.0811, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2383374.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3197.8276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(565.9841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2691.9424, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1427],
        [-0.1103],
        [-0.0828],
        ...,
        [-0.0531],
        [-0.0615],
        [-0.0540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-196989.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0006, -0.0005,  ...,  0.0000, -0.0006,  0.0005],
        [ 0.0040,  0.0031,  0.0036,  ..., -0.0032,  0.0070,  0.0066],
        [ 0.0040,  0.0031,  0.0036,  ..., -0.0032,  0.0070,  0.0066],
        ...,
        [-0.0006,  0.0006, -0.0005,  ...,  0.0000, -0.0006,  0.0005],
        [-0.0006,  0.0006, -0.0005,  ...,  0.0000, -0.0006,  0.0005],
        [-0.0006,  0.0006, -0.0005,  ...,  0.0000, -0.0006,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8414.6611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(124.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(283.7646, device='cuda:0')



h[100].sum tensor(419.7455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(302.4561, device='cuda:0')



h[200].sum tensor(-358.2003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(250.3346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0126, 0.0146,  ..., 0.0000, 0.0288, 0.0269],
        [0.0072, 0.0070, 0.0064,  ..., 0.0000, 0.0126, 0.0129],
        [0.0072, 0.0070, 0.0064,  ..., 0.0000, 0.0126, 0.0129],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(416583.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0661, 0.1259,  ..., 0.0000, 0.0535, 0.0000],
        [0.0000, 0.0510, 0.1046,  ..., 0.0000, 0.0437, 0.0000],
        [0.0000, 0.0650, 0.1218,  ..., 0.0000, 0.0509, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0122,  ..., 0.0000, 0.0011, 0.0062],
        [0.0000, 0.0000, 0.0122,  ..., 0.0000, 0.0011, 0.0062],
        [0.0000, 0.0000, 0.0122,  ..., 0.0000, 0.0011, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2148061.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2717.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(631.7478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2480.6055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0024],
        [ 0.0010],
        [-0.0168],
        ...,
        [-0.0349],
        [-0.0347],
        [-0.0347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-145373.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0007, -0.0005,  ...,  0.0000, -0.0007,  0.0005],
        [ 0.0037,  0.0030,  0.0033,  ..., -0.0030,  0.0065,  0.0063],
        [ 0.0141,  0.0085,  0.0124,  ..., -0.0101,  0.0237,  0.0201],
        ...,
        [-0.0006,  0.0007, -0.0005,  ...,  0.0000, -0.0007,  0.0005],
        [-0.0006,  0.0007, -0.0005,  ...,  0.0000, -0.0007,  0.0005],
        [-0.0006,  0.0007, -0.0005,  ...,  0.0000, -0.0007,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7730.6729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.7685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(260.4180, device='cuda:0')



h[100].sum tensor(402.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(277.5716, device='cuda:0')



h[200].sum tensor(-328.4097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(229.7384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0070, 0.0060,  ..., 0.0000, 0.0117, 0.0125],
        [0.0243, 0.0166, 0.0215,  ..., 0.0000, 0.0412, 0.0367],
        [0.0371, 0.0237, 0.0328,  ..., 0.0000, 0.0627, 0.0546],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(399436.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0588, 0.1152,  ..., 0.0000, 0.0475, 0.0000],
        [0.0000, 0.1311, 0.2113,  ..., 0.0000, 0.0891, 0.0000],
        [0.0000, 0.1877, 0.2857,  ..., 0.0000, 0.1214, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0128,  ..., 0.0000, 0.0016, 0.0062],
        [0.0000, 0.0000, 0.0150,  ..., 0.0000, 0.0026, 0.0050],
        [0.0000, 0.0110, 0.0388,  ..., 0.0000, 0.0131, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2109648.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2531.9573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(679.2856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2459.2383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0251],
        [ 0.0369],
        [ 0.0359],
        ...,
        [-0.0305],
        [-0.0217],
        [-0.0064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-107464.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0037,  0.0045,  ..., -0.0039,  0.0086,  0.0080],
        [ 0.0231,  0.0133,  0.0204,  ..., -0.0163,  0.0386,  0.0321],
        [ 0.0089,  0.0058,  0.0079,  ..., -0.0065,  0.0151,  0.0132],
        ...,
        [-0.0007,  0.0007, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [-0.0007,  0.0007, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [-0.0007,  0.0007, -0.0004,  ...,  0.0000, -0.0007,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7998.3960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.4167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(264.7474, device='cuda:0')



h[100].sum tensor(414.7419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(282.1862, device='cuda:0')



h[200].sum tensor(-334.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(233.5578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0786, 0.0459, 0.0692,  ..., 0.0000, 0.1312, 0.1100],
        [0.0527, 0.0323, 0.0466,  ..., 0.0000, 0.0886, 0.0757],
        [0.0626, 0.0375, 0.0552,  ..., 0.0000, 0.1048, 0.0888],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(410015.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3475, 0.4948,  ..., 0.0000, 0.2077, 0.0000],
        [0.0000, 0.3687, 0.5237,  ..., 0.0000, 0.2204, 0.0000],
        [0.0000, 0.3961, 0.5596,  ..., 0.0000, 0.2356, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0021, 0.0062],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0021, 0.0062],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0021, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2172410.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2546.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(695.4751, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2618.8157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1113],
        [-0.1528],
        [-0.1759],
        ...,
        [-0.0313],
        [-0.0311],
        [-0.0311]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-61944.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 300 loss: tensor(2073.3735, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [ 0.0111,  0.0071,  0.0099,  ..., -0.0081,  0.0188,  0.0163],
        ...,
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0006],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7944.4717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.2514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(260.0302, device='cuda:0')



h[100].sum tensor(414.9682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(277.1582, device='cuda:0')



h[200].sum tensor(-328.4611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(229.3963, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0150, 0.0118, 0.0135,  ..., 0.0000, 0.0256, 0.0242],
        [0.0132, 0.0112, 0.0120,  ..., 0.0000, 0.0230, 0.0227],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(414848.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0115, 0.0469,  ..., 0.0000, 0.0178, 0.0018],
        [0.0000, 0.0487, 0.1114,  ..., 0.0000, 0.0464, 0.0000],
        [0.0000, 0.0662, 0.1376,  ..., 0.0000, 0.0587, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0026, 0.0062],
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0026, 0.0062],
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0026, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2232553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2516.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(727.9929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2730.4829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0283],
        [ 0.0636],
        [ 0.0964],
        ...,
        [-0.0318],
        [-0.0316],
        [-0.0316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-30666.9883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007],
        ...,
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0004,  ...,  0.0000, -0.0007,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10213.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(136.4292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(326.2295, device='cuda:0')



h[100].sum tensor(488.6168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(347.7181, device='cuda:0')



h[200].sum tensor(-410.9512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(287.7967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(493095.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0024, 0.0319,  ..., 0.0000, 0.0111, 0.0030],
        [0.0000, 0.0000, 0.0183,  ..., 0.0000, 0.0050, 0.0044],
        [0.0000, 0.0196, 0.0559,  ..., 0.0000, 0.0210, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0138,  ..., 0.0000, 0.0029, 0.0064],
        [0.0000, 0.0000, 0.0138,  ..., 0.0000, 0.0029, 0.0064],
        [0.0000, 0.0000, 0.0138,  ..., 0.0000, 0.0029, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2594185., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3037.2524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(745.0222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3218.1611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0402],
        [ 0.0287],
        [ 0.0441],
        ...,
        [-0.0374],
        [-0.0372],
        [-0.0371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-9296.2305, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0008, -0.0005,  ...,  0.0000, -0.0007,  0.0007],
        [ 0.0052,  0.0040,  0.0047,  ..., -0.0040,  0.0090,  0.0086],
        [-0.0007,  0.0008, -0.0005,  ...,  0.0000, -0.0007,  0.0007],
        ...,
        [-0.0007,  0.0008, -0.0005,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0005,  ...,  0.0000, -0.0007,  0.0007],
        [-0.0007,  0.0008, -0.0005,  ...,  0.0000, -0.0007,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7353.9834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(59.0172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(243.0226, device='cuda:0')



h[100].sum tensor(390.4854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(259.0304, device='cuda:0')



h[200].sum tensor(-306.0072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(214.3923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0229, 0.0171, 0.0208,  ..., 0.0000, 0.0397, 0.0373],
        [0.0041, 0.0059, 0.0038,  ..., 0.0000, 0.0072, 0.0093],
        [0.0127, 0.0113, 0.0117,  ..., 0.0000, 0.0223, 0.0227],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(397788., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0657, 0.1411,  ..., 0.0000, 0.0606, 0.0000],
        [0.0000, 0.0460, 0.1130,  ..., 0.0000, 0.0482, 0.0000],
        [0.0000, 0.0635, 0.1387,  ..., 0.0000, 0.0598, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0030, 0.0069],
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0030, 0.0069],
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0030, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2176295., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2174.8538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(797.9578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2722.3794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0950],
        [ 0.0999],
        [ 0.1015],
        ...,
        [-0.0498],
        [-0.0496],
        [-0.0495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-92458.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        [ 0.0085,  0.0058,  0.0076,  ..., -0.0063,  0.0145,  0.0131],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        ...,
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8436.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.6178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(275.7846, device='cuda:0')



h[100].sum tensor(420.7600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(293.9505, device='cuda:0')



h[200].sum tensor(-346.5903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(243.2947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0136, 0.0115, 0.0123,  ..., 0.0000, 0.0235, 0.0233],
        [0.0068, 0.0075, 0.0061,  ..., 0.0000, 0.0117, 0.0132],
        [0.0305, 0.0213, 0.0274,  ..., 0.0000, 0.0522, 0.0478],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(435100.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0419, 0.1063,  ..., 0.0000, 0.0448, 0.0000],
        [0.0000, 0.0324, 0.0921,  ..., 0.0000, 0.0383, 0.0000],
        [0.0000, 0.0669, 0.1407,  ..., 0.0000, 0.0594, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0136,  ..., 0.0000, 0.0030, 0.0075],
        [0.0000, 0.0000, 0.0136,  ..., 0.0000, 0.0030, 0.0075],
        [0.0000, 0.0000, 0.0136,  ..., 0.0000, 0.0030, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2343289., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2352.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(811.9724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2942.9360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0313],
        [ 0.0309],
        [ 0.0339],
        ...,
        [-0.0610],
        [-0.0607],
        [-0.0606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-134146.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096,  0.0064,  0.0085,  ..., -0.0070,  0.0163,  0.0147],
        [ 0.0194,  0.0117,  0.0172,  ..., -0.0137,  0.0326,  0.0278],
        [ 0.0262,  0.0153,  0.0231,  ..., -0.0184,  0.0438,  0.0369],
        ...,
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8069.4277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(68.2906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(265.1480, device='cuda:0')



h[100].sum tensor(403.5914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(282.6132, device='cuda:0')



h[200].sum tensor(-333.6647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(233.9112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0734, 0.0443, 0.0649,  ..., 0.0000, 0.1232, 0.1055],
        [0.0699, 0.0424, 0.0618,  ..., 0.0000, 0.1174, 0.1008],
        [0.0817, 0.0487, 0.0721,  ..., 0.0000, 0.1368, 0.1165],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(426126.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3683, 0.5413,  ..., 0.0000, 0.2251, 0.0000],
        [0.0000, 0.3643, 0.5364,  ..., 0.0000, 0.2233, 0.0000],
        [0.0000, 0.3572, 0.5271,  ..., 0.0000, 0.2194, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0031, 0.0081],
        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0031, 0.0081],
        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0031, 0.0081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2314564.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2188.3252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(836.8564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2881.0117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1437],
        [ 0.1448],
        [ 0.1406],
        ...,
        [-0.0716],
        [-0.0712],
        [-0.0711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-190327.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0130,  0.0082,  0.0115,  ..., -0.0093,  0.0219,  0.0193],
        [ 0.0230,  0.0136,  0.0202,  ..., -0.0161,  0.0384,  0.0326],
        [ 0.0252,  0.0148,  0.0222,  ..., -0.0176,  0.0421,  0.0356],
        ...,
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0008,  0.0009, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [ 0.0097,  0.0065,  0.0086,  ..., -0.0071,  0.0165,  0.0149]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8142.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(66.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(267.5506, device='cuda:0')



h[100].sum tensor(401.2447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(285.1740, device='cuda:0')



h[200].sum tensor(-336.5497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(236.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0523, 0.0332, 0.0464,  ..., 0.0000, 0.0885, 0.0778],
        [0.0730, 0.0442, 0.0644,  ..., 0.0000, 0.1225, 0.1053],
        [0.0837, 0.0499, 0.0738,  ..., 0.0000, 0.1403, 0.1197],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        [0.0179, 0.0140, 0.0160,  ..., 0.0000, 0.0307, 0.0297],
        [0.0313, 0.0216, 0.0278,  ..., 0.0000, 0.0532, 0.0486]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(442245.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2287, 0.3560,  ..., 0.0000, 0.1491, 0.0000],
        [0.0000, 0.3029, 0.4530,  ..., 0.0000, 0.1884, 0.0000],
        [0.0000, 0.3396, 0.5015,  ..., 0.0000, 0.2083, 0.0000],
        ...,
        [0.0000, 0.0385, 0.0859,  ..., 0.0000, 0.0344, 0.0019],
        [0.0000, 0.1155, 0.2017,  ..., 0.0000, 0.0838, 0.0000],
        [0.0000, 0.1802, 0.2900,  ..., 0.0000, 0.1212, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2451700.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2211.7153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(848.4504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2975.4092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0584],
        [0.0665],
        [0.0637],
        ...,
        [0.0028],
        [0.0378],
        [0.0587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-227370.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0009],
        [ 0.0108,  0.0071,  0.0096,  ..., -0.0079,  0.0184,  0.0165],
        ...,
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7263.1543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(39.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(242.0413, device='cuda:0')



h[100].sum tensor(369.9402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(257.9845, device='cuda:0')



h[200].sum tensor(-304.0974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(213.5267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0145, 0.0123, 0.0130,  ..., 0.0000, 0.0251, 0.0254],
        [0.0162, 0.0137, 0.0147,  ..., 0.0000, 0.0285, 0.0288],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0074, 0.0081, 0.0066,  ..., 0.0000, 0.0128, 0.0148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(408309.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0142, 0.0505,  ..., 0.0000, 0.0200, 0.0038],
        [0.0000, 0.0562, 0.1242,  ..., 0.0000, 0.0524, 0.0000],
        [0.0000, 0.0892, 0.1720,  ..., 0.0000, 0.0737, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0046, 0.0074],
        [0.0000, 0.0063, 0.0359,  ..., 0.0000, 0.0131, 0.0046],
        [0.0000, 0.0444, 0.0998,  ..., 0.0000, 0.0405, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2269548.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1867.2056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(865.4565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2780.3108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0355],
        [-0.0033],
        [ 0.0287],
        ...,
        [-0.0695],
        [-0.0479],
        [-0.0181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-263667.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [ 0.0097,  0.0066,  0.0086,  ..., -0.0071,  0.0166,  0.0151],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        ...,
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0008,  0.0009, -0.0006,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8795.8418, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.8228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(285.5722, device='cuda:0')



h[100].sum tensor(418.4074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(304.3827, device='cuda:0')



h[200].sum tensor(-357.9262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(251.9292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0094, 0.0086,  ..., 0.0000, 0.0166, 0.0180],
        [0.0078, 0.0084, 0.0069,  ..., 0.0000, 0.0134, 0.0154],
        [0.0349, 0.0242, 0.0312,  ..., 0.0000, 0.0599, 0.0551],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(454415.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0260, 0.0822,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0325, 0.0907,  ..., 0.0000, 0.0375, 0.0000],
        [0.0000, 0.0678, 0.1395,  ..., 0.0000, 0.0582, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0032, 0.0093],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0032, 0.0093],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0032, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2469550.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2127.8289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(845.5771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3064.5718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.0401],
        [-0.0460],
        ...,
        [-0.0843],
        [-0.0839],
        [-0.0837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-250815.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0048,  0.0057,  ..., -0.0048,  0.0109,  0.0105],
        [-0.0009,  0.0010, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        ...,
        [-0.0009,  0.0010, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0006,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0006,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8487.1357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(59.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(275.4135, device='cuda:0')



h[100].sum tensor(409.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(293.5548, device='cuda:0')



h[200].sum tensor(-344.1225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(242.9673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0070, 0.0045,  ..., 0.0000, 0.0088, 0.0117],
        [0.0062, 0.0077, 0.0057,  ..., 0.0000, 0.0109, 0.0135],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(448058.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0116, 0.0619,  ..., 0.0000, 0.0255, 0.0000],
        [0.0000, 0.0092, 0.0529,  ..., 0.0000, 0.0215, 0.0006],
        [0.0000, 0.0000, 0.0288,  ..., 0.0000, 0.0110, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0034, 0.0094],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0034, 0.0094],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0034, 0.0094]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2461329., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1995.3135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(825.2117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3057.9824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0296],
        [-0.0288],
        [-0.0251],
        ...,
        [-0.0821],
        [-0.0816],
        [-0.0815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-235628.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0129,  0.0083,  0.0115,  ..., -0.0093,  0.0219,  0.0194],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [ 0.0100,  0.0068,  0.0090,  ..., -0.0073,  0.0172,  0.0156],
        ...,
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7904.9717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(39.3750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(256.3596, device='cuda:0')



h[100].sum tensor(393.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(273.2459, device='cuda:0')



h[200].sum tensor(-319.9272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(226.1582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0100, 0.0093,  ..., 0.0000, 0.0177, 0.0190],
        [0.0308, 0.0219, 0.0276,  ..., 0.0000, 0.0528, 0.0487],
        [0.0302, 0.0215, 0.0270,  ..., 0.0000, 0.0518, 0.0479],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(431072.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1091, 0.1962,  ..., 0.0000, 0.0809, 0.0000],
        [0.0000, 0.1807, 0.2930,  ..., 0.0000, 0.1207, 0.0000],
        [0.0000, 0.2239, 0.3513,  ..., 0.0000, 0.1446, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0129,  ..., 0.0000, 0.0036, 0.0093],
        [0.0000, 0.0000, 0.0129,  ..., 0.0000, 0.0036, 0.0093],
        [0.0000, 0.0000, 0.0129,  ..., 0.0000, 0.0036, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2381993., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1812.6350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(797.3306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2944.2117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0707],
        [ 0.0732],
        [ 0.0762],
        ...,
        [-0.0762],
        [-0.0758],
        [-0.0757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-218923.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0305,  0.0178,  0.0269,  ..., -0.0212,  0.0510,  0.0430],
        [ 0.0337,  0.0195,  0.0297,  ..., -0.0233,  0.0563,  0.0473],
        [ 0.0371,  0.0213,  0.0327,  ..., -0.0256,  0.0620,  0.0519],
        ...,
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7967.1758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.9067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(257.4844, device='cuda:0')



h[100].sum tensor(395.6117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(274.4448, device='cuda:0')



h[200].sum tensor(-322.1583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(227.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0857, 0.0517, 0.0758,  ..., 0.0000, 0.1442, 0.1234],
        [0.1192, 0.0696, 0.1050,  ..., 0.0000, 0.1995, 0.1682],
        [0.1045, 0.0617, 0.0922,  ..., 0.0000, 0.1752, 0.1485],
        ...,
        [0.0151, 0.0130, 0.0136,  ..., 0.0000, 0.0262, 0.0265],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(422785.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4655, 0.6661,  ..., 0.0000, 0.2698, 0.0000],
        [0.0000, 0.5184, 0.7334,  ..., 0.0000, 0.2959, 0.0000],
        [0.0000, 0.4764, 0.6776,  ..., 0.0000, 0.2733, 0.0000],
        ...,
        [0.0000, 0.0915, 0.1727,  ..., 0.0000, 0.0714, 0.0000],
        [0.0000, 0.0307, 0.0780,  ..., 0.0000, 0.0315, 0.0021],
        [0.0000, 0.0018, 0.0290,  ..., 0.0000, 0.0107, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2315367., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1728.3857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(801.5085, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2938.4761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0706],
        [ 0.0694],
        [ 0.0684],
        ...,
        [ 0.0207],
        [-0.0054],
        [-0.0334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-207834.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0187,  0.0115,  0.0166,  ..., -0.0132,  0.0316,  0.0272],
        [ 0.0092,  0.0065,  0.0083,  ..., -0.0068,  0.0160,  0.0146],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        ...,
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0010, -0.0005,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8555.5449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(49.4938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(271.6081, device='cuda:0')



h[100].sum tensor(418.0890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(289.4988, device='cuda:0')



h[200].sum tensor(-339.4933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(239.6102, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0802, 0.0489, 0.0711,  ..., 0.0000, 0.1353, 0.1161],
        [0.0333, 0.0229, 0.0296,  ..., 0.0000, 0.0564, 0.0509],
        [0.0296, 0.0214, 0.0267,  ..., 0.0000, 0.0511, 0.0472],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(444029.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3593, 0.5279,  ..., 0.0000, 0.2127, 0.0000],
        [0.0000, 0.2347, 0.3636,  ..., 0.0000, 0.1474, 0.0000],
        [0.0000, 0.1930, 0.3107,  ..., 0.0000, 0.1271, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0131,  ..., 0.0000, 0.0039, 0.0091],
        [0.0000, 0.0000, 0.0131,  ..., 0.0000, 0.0039, 0.0091],
        [0.0000, 0.0000, 0.0131,  ..., 0.0000, 0.0039, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2407171.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1836.2524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(758.7478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3066.3835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0812],
        [ 0.0832],
        [ 0.0850],
        ...,
        [-0.0664],
        [-0.0610],
        [-0.0488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-163715.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        ...,
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8439.6367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(41.0043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(265.9947, device='cuda:0')



h[100].sum tensor(418.1642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(283.5157, device='cuda:0')



h[200].sum tensor(-331.4905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(234.6582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(441102.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0042, 0.0090],
        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0042, 0.0090],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0043, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0042, 0.0089],
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0042, 0.0089],
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0042, 0.0089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2402287., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1748.2887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(726.2292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3095.6074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0047],
        [-0.0105],
        [-0.0164],
        ...,
        [-0.0591],
        [-0.0588],
        [-0.0587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-106509.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        ...,
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009],
        [-0.0009,  0.0011, -0.0005,  ...,  0.0000, -0.0008,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8111.1992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(254.0466, device='cuda:0')



h[100].sum tensor(410.9135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(270.7806, device='cuda:0')



h[200].sum tensor(-316.1807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(224.1177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(427559.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0135,  ..., 0.0000, 0.0046, 0.0089],
        [0.0000, 0.0047, 0.0307,  ..., 0.0000, 0.0116, 0.0051],
        [0.0000, 0.0152, 0.0545,  ..., 0.0000, 0.0213, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0046, 0.0088],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0046, 0.0088],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0046, 0.0088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2347511.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1597.0514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(699.9756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3055.0862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0666],
        [-0.0491],
        [-0.0275],
        ...,
        [-0.0512],
        [-0.0509],
        [-0.0508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-65570.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009],
        [-0.0009,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009],
        [-0.0009,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009],
        ...,
        [ 0.0160,  0.0102,  0.0144,  ..., -0.0114,  0.0273,  0.0237],
        [ 0.0050,  0.0043,  0.0047,  ..., -0.0040,  0.0090,  0.0089],
        [ 0.0047,  0.0041,  0.0045,  ..., -0.0038,  0.0086,  0.0085]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8162.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(252.5288, device='cuda:0')



h[100].sum tensor(414.0313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(269.1627, device='cuda:0')



h[200].sum tensor(-315.1785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(222.7786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0255, 0.0202, 0.0239,  ..., 0.0000, 0.0454, 0.0431],
        [0.0372, 0.0264, 0.0341,  ..., 0.0000, 0.0648, 0.0588],
        [0.0134, 0.0127, 0.0125,  ..., 0.0000, 0.0239, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(421372.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0069, 0.0073],
        [0.0000, 0.0000, 0.0209,  ..., 0.0000, 0.0086, 0.0052],
        [0.0000, 0.0011, 0.0360,  ..., 0.0000, 0.0150, 0.0032],
        ...,
        [0.0000, 0.1687, 0.2827,  ..., 0.0000, 0.1154, 0.0000],
        [0.0000, 0.1401, 0.2429,  ..., 0.0000, 0.0991, 0.0000],
        [0.0000, 0.0618, 0.1356,  ..., 0.0000, 0.0558, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2277279.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1513.7124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(686.0886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3031.6152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0002],
        [ 0.0139],
        [ 0.0316],
        ...,
        [ 0.0589],
        [ 0.0522],
        [ 0.0348]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-47938.8555, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0183,  0.0114,  0.0164,  ..., -0.0129,  0.0310,  0.0267],
        [ 0.0211,  0.0130,  0.0189,  ..., -0.0148,  0.0358,  0.0306],
        [ 0.0094,  0.0067,  0.0086,  ..., -0.0069,  0.0164,  0.0149],
        ...,
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9853.5039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(61.4396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(299.2209, device='cuda:0')



h[100].sum tensor(467.2408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(318.9304, device='cuda:0')



h[200].sum tensor(-372.4590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(263.9700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0922, 0.0560, 0.0822,  ..., 0.0000, 0.1558, 0.1326],
        [0.0819, 0.0505, 0.0733,  ..., 0.0000, 0.1389, 0.1188],
        [0.0811, 0.0501, 0.0725,  ..., 0.0000, 0.1375, 0.1177],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(495885.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4053, 0.5886,  ..., 0.0000, 0.2325, 0.0000],
        [0.0000, 0.4157, 0.6037,  ..., 0.0000, 0.2388, 0.0000],
        [0.0000, 0.4118, 0.5998,  ..., 0.0000, 0.2377, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0051, 0.0095],
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0051, 0.0095],
        [0.0000, 0.0000, 0.0132,  ..., 0.0000, 0.0051, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2696212.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2060.6604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(669.2004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3433.0576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0902],
        [ 0.0902],
        [ 0.0908],
        ...,
        [-0.0447],
        [-0.0444],
        [-0.0443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-18382.8555, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0039,  0.0041,  ..., -0.0034,  0.0078,  0.0079],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0011, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8187.0127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(250.7633, device='cuda:0')



h[100].sum tensor(412.5142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(267.2810, device='cuda:0')



h[200].sum tensor(-312.5436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(221.2212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0146, 0.0150,  ..., 0.0000, 0.0286, 0.0288],
        [0.0074, 0.0096, 0.0074,  ..., 0.0000, 0.0140, 0.0164],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(426357.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0755, 0.1567,  ..., 0.0000, 0.0660, 0.0000],
        [0.0000, 0.0292, 0.0888,  ..., 0.0000, 0.0380, 0.0000],
        [0.0000, 0.0035, 0.0384,  ..., 0.0000, 0.0164, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0052, 0.0103],
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0052, 0.0103],
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0052, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2329173.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1487.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(691.7917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3058.0815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0791],
        [ 0.0601],
        [ 0.0343],
        ...,
        [-0.0466],
        [-0.0461],
        [-0.0459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-40745.8398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [ 0.0055,  0.0047,  0.0053,  ..., -0.0044,  0.0101,  0.0097],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10463.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(69.6666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(314.2621, device='cuda:0')



h[100].sum tensor(480.3002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(334.9624, device='cuda:0')



h[200].sum tensor(-391.1381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(277.2392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0239, 0.0196, 0.0227,  ..., 0.0000, 0.0431, 0.0413],
        [0.0116, 0.0119, 0.0111,  ..., 0.0000, 0.0210, 0.0222],
        [0.0087, 0.0103, 0.0085,  ..., 0.0000, 0.0162, 0.0182],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(510590.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0656, 0.1431,  ..., 0.0000, 0.0610, 0.0000],
        [0.0000, 0.0483, 0.1183,  ..., 0.0000, 0.0505, 0.0000],
        [0.0000, 0.0305, 0.0922,  ..., 0.0000, 0.0398, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0052, 0.0112],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0052, 0.0112],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0052, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2746437.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2111.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(692.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3496.7935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0730],
        [ 0.0649],
        [ 0.0538],
        ...,
        [-0.0531],
        [-0.0528],
        [-0.0527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-54469.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [ 0.0047,  0.0042,  0.0045,  ..., -0.0038,  0.0086,  0.0086],
        ...,
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8423.6348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(257.0681, device='cuda:0')



h[100].sum tensor(411.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(274.0011, device='cuda:0')



h[200].sum tensor(-319.4567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(226.7832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        [0.0047, 0.0077, 0.0045,  ..., 0.0000, 0.0086, 0.0116],
        [0.0254, 0.0193, 0.0231,  ..., 0.0000, 0.0438, 0.0407],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(440176.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0268, 0.0862,  ..., 0.0000, 0.0371, 0.0000],
        [0.0000, 0.0594, 0.1300,  ..., 0.0000, 0.0546, 0.0000],
        [0.0000, 0.1289, 0.2229,  ..., 0.0000, 0.0915, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0052, 0.0123],
        [0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0052, 0.0123],
        [0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0052, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2414002.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1555.5529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(721.7493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3085.1001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0684],
        [ 0.0679],
        [ 0.0671],
        ...,
        [-0.0613],
        [-0.0610],
        [-0.0609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-102665.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8738.9395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(265.2126, device='cuda:0')



h[100].sum tensor(416.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(282.6821, device='cuda:0')



h[200].sum tensor(-329.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(233.9682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(449423.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0271,  ..., 0.0000, 0.0119, 0.0066],
        [0.0000, 0.0000, 0.0168,  ..., 0.0000, 0.0074, 0.0110],
        [0.0000, 0.0000, 0.0246,  ..., 0.0000, 0.0110, 0.0075],
        ...,
        [0.0000, 0.0325, 0.0780,  ..., 0.0000, 0.0325, 0.0041],
        [0.0000, 0.0020, 0.0274,  ..., 0.0000, 0.0118, 0.0078],
        [0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0053, 0.0133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2464547.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1615.0215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(737.8923, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3102.8804, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0180],
        [ 0.0081],
        [ 0.0030],
        ...,
        [ 0.0008],
        [-0.0226],
        [-0.0459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-128790.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 900 loss: tensor(3127.1086, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [ 0.0096,  0.0068,  0.0088,  ..., -0.0070,  0.0167,  0.0152],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8495.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(258.2296, device='cuda:0')



h[100].sum tensor(404.5306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(275.2390, device='cuda:0')



h[200].sum tensor(-320.1792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(227.8078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        ...,
        [0.0191, 0.0166, 0.0182,  ..., 0.0000, 0.0344, 0.0339],
        [0.0167, 0.0147, 0.0156,  ..., 0.0000, 0.0295, 0.0293],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(438865.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0306,  ..., 0.0000, 0.0137, 0.0069],
        [0.0000, 0.0000, 0.0156,  ..., 0.0000, 0.0072, 0.0126],
        [0.0000, 0.0000, 0.0122,  ..., 0.0000, 0.0056, 0.0143],
        ...,
        [0.0000, 0.1531, 0.2536,  ..., 0.0000, 0.1046, 0.0000],
        [0.0000, 0.1254, 0.2157,  ..., 0.0000, 0.0889, 0.0000],
        [0.0000, 0.0722, 0.1431,  ..., 0.0000, 0.0592, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2421372.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1511.0496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(756.2222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3045.0361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0086],
        [-0.0222],
        [-0.0275],
        ...,
        [ 0.0159],
        [ 0.0167],
        [ 0.0182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-147336.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [ 0.0123,  0.0083,  0.0112,  ..., -0.0088,  0.0212,  0.0189],
        [ 0.0056,  0.0047,  0.0054,  ..., -0.0044,  0.0103,  0.0100],
        [ 0.0066,  0.0053,  0.0063,  ..., -0.0050,  0.0118,  0.0113]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8618.4199, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.5939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(260.4473, device='cuda:0')



h[100].sum tensor(404.1515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(277.6028, device='cuda:0')



h[200].sum tensor(-322.9413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(229.7642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        ...,
        [0.0365, 0.0265, 0.0339,  ..., 0.0000, 0.0641, 0.0586],
        [0.0347, 0.0255, 0.0323,  ..., 0.0000, 0.0611, 0.0562],
        [0.0150, 0.0144, 0.0146,  ..., 0.0000, 0.0277, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(441744., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0176,  ..., 0.0000, 0.0083, 0.0127],
        [0.0000, 0.0000, 0.0128,  ..., 0.0000, 0.0059, 0.0149],
        [0.0000, 0.0000, 0.0117,  ..., 0.0000, 0.0054, 0.0155],
        ...,
        [0.0000, 0.1942, 0.3120,  ..., 0.0000, 0.1291, 0.0000],
        [0.0000, 0.1825, 0.2963,  ..., 0.0000, 0.1228, 0.0000],
        [0.0000, 0.1469, 0.2486,  ..., 0.0000, 0.1037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2441961.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1520.2052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(765.4197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3050.9944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0345],
        [-0.0503],
        [-0.0651],
        ...,
        [ 0.0279],
        [ 0.0283],
        [ 0.0286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-161361.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032,  0.0034,  0.0033,  ..., -0.0028,  0.0062,  0.0067],
        [ 0.0143,  0.0094,  0.0130,  ..., -0.0102,  0.0246,  0.0217],
        [ 0.0051,  0.0045,  0.0050,  ..., -0.0041,  0.0094,  0.0093],
        ...,
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0004,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2494.0586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-130.9865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.2231, device='cuda:0')



h[100].sum tensor(211.3891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.1001, device='cuda:0')



h[200].sum tensor(-111.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(78.7118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0526, 0.0352, 0.0481,  ..., 0.0000, 0.0908, 0.0803],
        [0.0231, 0.0193, 0.0223,  ..., 0.0000, 0.0421, 0.0407],
        [0.0247, 0.0197, 0.0232,  ..., 0.0000, 0.0437, 0.0415],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(246911.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1679, 0.2753,  ..., 0.0000, 0.1135, 0.0000],
        [0.0000, 0.1388, 0.2388,  ..., 0.0000, 0.1001, 0.0000],
        [0.0000, 0.1299, 0.2262,  ..., 0.0000, 0.0947, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0053, 0.0162],
        [0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0053, 0.0162],
        [0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0053, 0.0162]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1552033.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.5619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(769.7402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1813.7148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0605],
        [ 0.0617],
        [ 0.0601],
        ...,
        [-0.0830],
        [-0.0826],
        [-0.0825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-280619.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-737.4717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-209.2835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(108.8802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(144534.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0054, 0.0171],
        [0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0054, 0.0171],
        [0.0000, 0.0000, 0.0116,  ..., 0.0000, 0.0055, 0.0171],
        ...,
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0054, 0.0170],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0054, 0.0170],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0054, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1095336.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(636.2950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(775.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1169.6985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1385],
        [-0.1399],
        [-0.1424],
        ...,
        [-0.0858],
        [-0.0854],
        [-0.0852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-347143.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-718.9214, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-210.3984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(106.8739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(145068.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0055, 0.0178],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0055, 0.0178],
        [0.0000, 0.0000, 0.0115,  ..., 0.0000, 0.0055, 0.0178],
        ...,
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0054, 0.0177],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0054, 0.0177],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0054, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1103483.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(639.3871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(780.4840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1167.6514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1423],
        [-0.1438],
        [-0.1463],
        ...,
        [-0.0882],
        [-0.0878],
        [-0.0877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-354953.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-702.0250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-211.4138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(105.0467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(145554.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0055, 0.0184],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0055, 0.0184],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0056, 0.0185],
        ...,
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0183],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0183],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0183]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1110913.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(642.2032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(785.0516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1165.7874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1451],
        [-0.1466],
        [-0.1491],
        ...,
        [-0.0900],
        [-0.0895],
        [-0.0894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-360680.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-686.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-212.3385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(103.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(146021.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0190],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0190],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0056, 0.0191],
        ...,
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0189],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0189],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0189]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1117554.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(645.2344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(789.0853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1166.0676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1471],
        [-0.1486],
        [-0.1512],
        ...,
        [-0.0908],
        [-0.0903],
        [-0.0902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-365941.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-672.6307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-213.1805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(101.8676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(146467.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0196],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0196],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0056, 0.0196],
        ...,
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0195],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0195],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0195]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1123508.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(648.3923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(792.6500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1168.0065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1483],
        [-0.1498],
        [-0.1524],
        ...,
        [-0.0916],
        [-0.0911],
        [-0.0910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-370710.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0010,  0.0012, -0.0003,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-659.8776, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-213.9471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(100.4881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(146872.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0201],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0201],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0200],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0200],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0054, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1128943.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(651.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(795.8958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1169.7717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1493],
        [-0.1509],
        [-0.1535],
        ...,
        [-0.0923],
        [-0.0918],
        [-0.0917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-374498.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-648.2648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-214.6449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(99.2325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(147242.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0205],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0205],
        [0.0000, 0.0000, 0.0113,  ..., 0.0000, 0.0055, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0204],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0204],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1133902., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(653.8842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(798.8502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1171.3787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1503],
        [-0.1519],
        [-0.1545],
        ...,
        [-0.0929],
        [-0.0925],
        [-0.0923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-376904.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 1200 loss: tensor(401.2539, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-637.6961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-215.2800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(98.0896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(147578.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0209],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0209],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0210],
        ...,
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0208],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0208],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1138486.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(656.2665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(801.5392, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1172.8409, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1512],
        [-0.1528],
        [-0.1554],
        ...,
        [-0.0936],
        [-0.0931],
        [-0.0930]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-378863.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-628.0787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-215.8580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(97.0495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(147884.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0213],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0213],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0213],
        ...,
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0212],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0212],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1142779.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(658.4345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(803.9869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1174.1722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1520],
        [-0.1536],
        [-0.1563],
        ...,
        [-0.0941],
        [-0.0937],
        [-0.0935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-380622.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-619.3308, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-216.3840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(96.1031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(148162.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0055, 0.0216],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0055, 0.0216],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0217],
        ...,
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0215],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0215],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1146692.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(660.4066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(806.2136, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1175.3831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1528],
        [-0.1544],
        [-0.1570],
        ...,
        [-0.0946],
        [-0.0942],
        [-0.0940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-382224.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-611.3663, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-216.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(95.2421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(148415.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0219],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0219],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0055, 0.0220],
        ...,
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0218],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0218],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0218]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1150259.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(662.2010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(808.2390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1176.4850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1534],
        [-0.1550],
        [-0.1577],
        ...,
        [-0.0951],
        [-0.0946],
        [-0.0945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-383685.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-604.1248, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-217.2977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(94.4589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(148652.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0222],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0222],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0055, 0.0223],
        ...,
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0221],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0221],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1153515.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(664.1058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(809.5643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1177.3579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1540],
        [-0.1556],
        [-0.1583],
        ...,
        [-0.0955],
        [-0.0950],
        [-0.0948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-384924.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-597.5364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-217.6935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(93.7465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(148876.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0225],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0054, 0.0225],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0055, 0.0225],
        ...,
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0224],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0224],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1156504., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(665.7021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(811.2126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.1234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1545],
        [-0.1561],
        [-0.1588],
        ...,
        [-0.0958],
        [-0.0953],
        [-0.0952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-385946.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-591.5511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-218.0535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(93.0988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149079.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0227],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0227],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0055, 0.0228],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0226],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0226],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0226]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1159237.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(667.1541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(812.7120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.8196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1549],
        [-0.1566],
        [-0.1592],
        ...,
        [-0.0961],
        [-0.0956],
        [-0.0955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-386873.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-586.1065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-218.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(92.5098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149264.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0229],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0229],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0230],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0228],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0228],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1161736.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(668.4741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(814.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1179.4524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1554],
        [-0.1570],
        [-0.1597],
        ...,
        [-0.0964],
        [-0.0959],
        [-0.0958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-387684.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-581.1523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-218.6784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(91.9743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149433.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0231],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0231],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0232],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0230],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0230],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1164014.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(669.6737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(815.3137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1180.0281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1557],
        [-0.1573],
        [-0.1600],
        ...,
        [-0.0966],
        [-0.0962],
        [-0.0960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-388371.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-576.6532, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-218.9489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(91.4876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149590.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0233],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0233],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0234],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0232],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0232],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1166039.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.8227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(816.2593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1180.8921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1560],
        [-0.1576],
        [-0.1604],
        ...,
        [-0.0969],
        [-0.0964],
        [-0.0962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-388935.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-572.5614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-219.1947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(91.0453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149737.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0235],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0054, 0.0235],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0235],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0234],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0234],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1167834.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(671.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(816.9514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1181.9934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1563],
        [-0.1579],
        [-0.1606],
        ...,
        [-0.0971],
        [-0.0966],
        [-0.0964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-389397.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0012, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-568.8447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-219.4180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(90.6433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149870.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0236],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0236],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0237],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0235],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0235],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1169466.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(672.9186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(817.5803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1182.9944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1565],
        [-0.1581],
        [-0.1609],
        ...,
        [-0.0972],
        [-0.0968],
        [-0.0966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-389811.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-565.4711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-219.6210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(90.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(149992.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0238],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0238],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0238],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0236],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0236],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1170949.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(673.8252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(818.1517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1183.9038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1567],
        [-0.1584],
        [-0.1611],
        ...,
        [-0.0974],
        [-0.0969],
        [-0.0968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-390172.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-562.4012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-219.8054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(89.9463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150102.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0239],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0239],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0239],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0237],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0237],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1172297.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(674.6491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(818.6708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1184.7297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1569],
        [-0.1586],
        [-0.1613],
        ...,
        [-0.0975],
        [-0.0970],
        [-0.0969]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-390485.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-559.6141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-219.9728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(89.6450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150202.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0240],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0239],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0239],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1173521.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(675.3970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(819.1420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1185.4802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1571],
        [-0.1587],
        [-0.1615],
        ...,
        [-0.0977],
        [-0.0972],
        [-0.0970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-390768.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-557.0850, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.1250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(89.3712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150293.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0241],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0241],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0241],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1174633., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(676.0760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(819.5698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1186.1614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1573],
        [-0.1589],
        [-0.1616],
        ...,
        [-0.0978],
        [-0.0973],
        [-0.0971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391025.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-554.7882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(89.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150378.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0242],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1175632.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(676.7094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(820.1603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1186.8782, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1574],
        [-0.1590],
        [-0.1618],
        ...,
        [-0.0979],
        [-0.0974],
        [-0.0972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391232.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-554.7882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(89.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150378.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0242],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1175632.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(676.7094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(820.1603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1186.8782, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1574],
        [-0.1590],
        [-0.1618],
        ...,
        [-0.0979],
        [-0.0974],
        [-0.0972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391232.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-552.6989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.3885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.8970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150455.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0243],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0241],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0241],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0241]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1176539.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(677.2845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(820.7054, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1187.5331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1575],
        [-0.1592],
        [-0.1619],
        ...,
        [-0.0980],
        [-0.0975],
        [-0.0973]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391421.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0002,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-550.8040, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.5024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.6921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150526.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0243],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0243],
        [0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0055, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0242],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1177363.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(677.8073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(821.2000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1188.1272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1576],
        [-0.1593],
        [-0.1620],
        ...,
        [-0.0981],
        [-0.0976],
        [-0.0974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391591.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-549.0858, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.5061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150590.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0244],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0244],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0055, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0243],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0243],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0054, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1178110.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(678.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(821.6483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1188.6667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1577],
        [-0.1594],
        [-0.1621],
        ...,
        [-0.0981],
        [-0.0977],
        [-0.0975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391745.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-547.5221, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.6996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.3373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150648.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0244],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0244],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0055, 0.0245],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0243],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0243],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1178789.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(678.7118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.0556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1189.1567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1578],
        [-0.1595],
        [-0.1622],
        ...,
        [-0.0982],
        [-0.0977],
        [-0.0976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-391884.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-546.1060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.7847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150700.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0245],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0245],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0055, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1179406., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(679.1030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.4255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1189.6007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1579],
        [-0.1595],
        [-0.1623],
        ...,
        [-0.0983],
        [-0.0978],
        [-0.0976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-392010.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-544.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(88.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150748.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0245],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0245],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0055, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1179965.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(679.4575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.7609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1190.0038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1580],
        [-0.1596],
        [-0.1624],
        ...,
        [-0.0983],
        [-0.0978],
        [-0.0977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-392123.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        ...,
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010],
        [-0.0011,  0.0013, -0.0001,  ...,  0.0000, -0.0007,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-543.6533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-220.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(87.9190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(150791.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0246],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0246],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0054, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0245],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0245],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0053, 0.0245]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1180472.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(679.7792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(823.0648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1190.3696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1581],
        [-0.1597],
        [-0.1624],
        ...,
        [-0.0984],
        [-0.0979],
        [-0.0977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-392225.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N3/./Training.py", line 78, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[203880, 1]' is invalid for input of size 135920

real	2m10.459s
user	0m14.154s
sys	0m9.066s
