0: gpu021.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-ebfd6e2d-0f81-ada3-ee79-f73a42db1693)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Sun Aug 14 03:53:58 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B4:00.0 Off |                    0 |
| N/A   38C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b436b5128e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.647s
user	0m2.707s
sys	0m0.792s
[03:54:05] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.7087],
        [-0.8325],
        [ 0.7470],
        ...,
        [ 0.0346],
        [ 1.1870],
        [-0.2783]], device='cuda:0', requires_grad=True) 
node features sum: tensor(31.0820, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N3
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1078, -0.0274,  0.1344,  0.0582,  0.1391,  0.0302,  0.1366,  0.0676,
         -0.0584, -0.0353,  0.1354, -0.0157, -0.1345, -0.1012,  0.0782, -0.1141,
          0.0235,  0.0996, -0.1235, -0.0122,  0.1300,  0.0861,  0.1185,  0.0499,
          0.0444, -0.0655,  0.1499,  0.1232,  0.0242,  0.0461, -0.0412,  0.1331,
         -0.0326, -0.0038,  0.1222,  0.1195, -0.1061, -0.0188, -0.0675,  0.1513,
          0.0206, -0.0717, -0.0533,  0.1065,  0.1377, -0.1267, -0.1500, -0.1040,
          0.0006, -0.0903, -0.0546,  0.0426, -0.0935,  0.1338, -0.0595,  0.1254,
         -0.1048, -0.0040,  0.0459,  0.0507,  0.1512,  0.0480,  0.0428,  0.0778,
         -0.0533,  0.0941,  0.1257, -0.0119,  0.0290, -0.1323, -0.0798,  0.1426,
          0.0258,  0.1290, -0.0484, -0.0348,  0.0945,  0.0656, -0.1259, -0.0438,
          0.0728, -0.1233,  0.0146, -0.1378,  0.0834, -0.1275, -0.1437,  0.0174,
          0.1162,  0.1106,  0.0591, -0.1232, -0.0756,  0.1203, -0.0135,  0.0395,
          0.0050, -0.1386,  0.0523, -0.0442, -0.1084, -0.1275, -0.0361,  0.0765,
          0.0694, -0.0634, -0.0403, -0.1031,  0.1363,  0.1263,  0.1173, -0.1234,
          0.1096, -0.0153,  0.0564, -0.0786, -0.1431,  0.1465, -0.0308,  0.1358,
          0.0248,  0.0122, -0.0886, -0.0337,  0.0023,  0.0070,  0.1333, -0.1204,
          0.0855,  0.0610, -0.0144,  0.1522,  0.0777, -0.1080,  0.0914,  0.0123,
         -0.0009, -0.0736, -0.0435, -0.0032, -0.1388,  0.0013, -0.1511, -0.0681,
          0.0210, -0.0053, -0.1102, -0.0106, -0.0192, -0.0814,  0.0523,  0.0421,
          0.0142, -0.0250, -0.0277,  0.0096,  0.0979,  0.0803,  0.0831,  0.0306,
         -0.1160,  0.0385,  0.1451,  0.0430, -0.0763, -0.0605, -0.0775, -0.0039,
          0.1375,  0.0379, -0.0956,  0.0186,  0.0612,  0.0326, -0.1297, -0.1397,
         -0.0773,  0.1192,  0.0398, -0.0510, -0.0624, -0.0836,  0.1370, -0.0985,
         -0.0554,  0.0971,  0.0455,  0.0217, -0.0800,  0.0647,  0.0143,  0.0265,
          0.1386, -0.1204,  0.0271,  0.1095,  0.1134,  0.0287,  0.0210,  0.0522,
          0.0328,  0.0637, -0.1195,  0.0158,  0.0528, -0.0377,  0.0679,  0.1376,
          0.0780,  0.0415, -0.0126,  0.1288, -0.0309, -0.0926, -0.0307, -0.0416,
          0.0763,  0.0013,  0.0582,  0.0433,  0.1064, -0.1133,  0.0290, -0.0475,
         -0.1361,  0.0521,  0.1127,  0.0075,  0.0525, -0.1482,  0.0003,  0.1351,
         -0.0284,  0.0806, -0.0871,  0.0552, -0.1500,  0.1467,  0.0998,  0.1395,
          0.1301,  0.1023, -0.0199, -0.0347,  0.0897, -0.1329, -0.1413, -0.0378,
          0.1237,  0.0084,  0.0111, -0.0018,  0.0263,  0.0225, -0.1121, -0.1142]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1078, -0.0274,  0.1344,  0.0582,  0.1391,  0.0302,  0.1366,  0.0676,
         -0.0584, -0.0353,  0.1354, -0.0157, -0.1345, -0.1012,  0.0782, -0.1141,
          0.0235,  0.0996, -0.1235, -0.0122,  0.1300,  0.0861,  0.1185,  0.0499,
          0.0444, -0.0655,  0.1499,  0.1232,  0.0242,  0.0461, -0.0412,  0.1331,
         -0.0326, -0.0038,  0.1222,  0.1195, -0.1061, -0.0188, -0.0675,  0.1513,
          0.0206, -0.0717, -0.0533,  0.1065,  0.1377, -0.1267, -0.1500, -0.1040,
          0.0006, -0.0903, -0.0546,  0.0426, -0.0935,  0.1338, -0.0595,  0.1254,
         -0.1048, -0.0040,  0.0459,  0.0507,  0.1512,  0.0480,  0.0428,  0.0778,
         -0.0533,  0.0941,  0.1257, -0.0119,  0.0290, -0.1323, -0.0798,  0.1426,
          0.0258,  0.1290, -0.0484, -0.0348,  0.0945,  0.0656, -0.1259, -0.0438,
          0.0728, -0.1233,  0.0146, -0.1378,  0.0834, -0.1275, -0.1437,  0.0174,
          0.1162,  0.1106,  0.0591, -0.1232, -0.0756,  0.1203, -0.0135,  0.0395,
          0.0050, -0.1386,  0.0523, -0.0442, -0.1084, -0.1275, -0.0361,  0.0765,
          0.0694, -0.0634, -0.0403, -0.1031,  0.1363,  0.1263,  0.1173, -0.1234,
          0.1096, -0.0153,  0.0564, -0.0786, -0.1431,  0.1465, -0.0308,  0.1358,
          0.0248,  0.0122, -0.0886, -0.0337,  0.0023,  0.0070,  0.1333, -0.1204,
          0.0855,  0.0610, -0.0144,  0.1522,  0.0777, -0.1080,  0.0914,  0.0123,
         -0.0009, -0.0736, -0.0435, -0.0032, -0.1388,  0.0013, -0.1511, -0.0681,
          0.0210, -0.0053, -0.1102, -0.0106, -0.0192, -0.0814,  0.0523,  0.0421,
          0.0142, -0.0250, -0.0277,  0.0096,  0.0979,  0.0803,  0.0831,  0.0306,
         -0.1160,  0.0385,  0.1451,  0.0430, -0.0763, -0.0605, -0.0775, -0.0039,
          0.1375,  0.0379, -0.0956,  0.0186,  0.0612,  0.0326, -0.1297, -0.1397,
         -0.0773,  0.1192,  0.0398, -0.0510, -0.0624, -0.0836,  0.1370, -0.0985,
         -0.0554,  0.0971,  0.0455,  0.0217, -0.0800,  0.0647,  0.0143,  0.0265,
          0.1386, -0.1204,  0.0271,  0.1095,  0.1134,  0.0287,  0.0210,  0.0522,
          0.0328,  0.0637, -0.1195,  0.0158,  0.0528, -0.0377,  0.0679,  0.1376,
          0.0780,  0.0415, -0.0126,  0.1288, -0.0309, -0.0926, -0.0307, -0.0416,
          0.0763,  0.0013,  0.0582,  0.0433,  0.1064, -0.1133,  0.0290, -0.0475,
         -0.1361,  0.0521,  0.1127,  0.0075,  0.0525, -0.1482,  0.0003,  0.1351,
         -0.0284,  0.0806, -0.0871,  0.0552, -0.1500,  0.1467,  0.0998,  0.1395,
          0.1301,  0.1023, -0.0199, -0.0347,  0.0897, -0.1329, -0.1413, -0.0378,
          0.1237,  0.0084,  0.0111, -0.0018,  0.0263,  0.0225, -0.1121, -0.1142]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0793,  0.0886,  0.0306,  ...,  0.1244, -0.0858,  0.0184],
        [-0.0699,  0.0657, -0.0393,  ..., -0.0644, -0.1082,  0.0197],
        [ 0.0775,  0.0345,  0.0316,  ..., -0.1138,  0.0688, -0.0416],
        ...,
        [-0.0251,  0.0383, -0.0582,  ..., -0.0347,  0.1197,  0.1094],
        [ 0.1032, -0.0689, -0.0674,  ...,  0.0576, -0.0737, -0.0709],
        [-0.0587,  0.0076, -0.0880,  ...,  0.0952, -0.0320, -0.1026]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0793,  0.0886,  0.0306,  ...,  0.1244, -0.0858,  0.0184],
        [-0.0699,  0.0657, -0.0393,  ..., -0.0644, -0.1082,  0.0197],
        [ 0.0775,  0.0345,  0.0316,  ..., -0.1138,  0.0688, -0.0416],
        ...,
        [-0.0251,  0.0383, -0.0582,  ..., -0.0347,  0.1197,  0.1094],
        [ 0.1032, -0.0689, -0.0674,  ...,  0.0576, -0.0737, -0.0709],
        [-0.0587,  0.0076, -0.0880,  ...,  0.0952, -0.0320, -0.1026]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0504, -0.0460, -0.0715,  ...,  0.1105,  0.0013, -0.1131],
        [ 0.1516, -0.0504, -0.0281,  ..., -0.1207,  0.1237, -0.1024],
        [ 0.1069,  0.1525,  0.0443,  ...,  0.1591,  0.1144,  0.0582],
        ...,
        [-0.0063, -0.0533, -0.1744,  ...,  0.0139, -0.0300, -0.0025],
        [-0.1254, -0.0476,  0.0921,  ...,  0.1533, -0.1707, -0.0623],
        [ 0.1214, -0.1130, -0.0467,  ...,  0.0196,  0.1766,  0.0162]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0504, -0.0460, -0.0715,  ...,  0.1105,  0.0013, -0.1131],
        [ 0.1516, -0.0504, -0.0281,  ..., -0.1207,  0.1237, -0.1024],
        [ 0.1069,  0.1525,  0.0443,  ...,  0.1591,  0.1144,  0.0582],
        ...,
        [-0.0063, -0.0533, -0.1744,  ...,  0.0139, -0.0300, -0.0025],
        [-0.1254, -0.0476,  0.0921,  ...,  0.1533, -0.1707, -0.0623],
        [ 0.1214, -0.1130, -0.0467,  ...,  0.0196,  0.1766,  0.0162]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.2117, -0.0600,  0.0492,  ..., -0.2351,  0.2118,  0.1968],
        [ 0.1690, -0.1160,  0.1201,  ..., -0.2421,  0.0546,  0.0613],
        [ 0.0435, -0.1904,  0.0527,  ..., -0.1492,  0.0991, -0.0156],
        ...,
        [ 0.0304, -0.2196,  0.1913,  ..., -0.1802, -0.1328, -0.1278],
        [ 0.1489, -0.0449, -0.0631,  ..., -0.0423, -0.1242,  0.2178],
        [-0.2057, -0.0929,  0.1036,  ..., -0.0666,  0.0901, -0.0227]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.2117, -0.0600,  0.0492,  ..., -0.2351,  0.2118,  0.1968],
        [ 0.1690, -0.1160,  0.1201,  ..., -0.2421,  0.0546,  0.0613],
        [ 0.0435, -0.1904,  0.0527,  ..., -0.1492,  0.0991, -0.0156],
        ...,
        [ 0.0304, -0.2196,  0.1913,  ..., -0.1802, -0.1328, -0.1278],
        [ 0.1489, -0.0449, -0.0631,  ..., -0.0423, -0.1242,  0.2178],
        [-0.2057, -0.0929,  0.1036,  ..., -0.0666,  0.0901, -0.0227]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3932],
        [ 0.3440],
        [ 0.0753],
        [-0.4153],
        [-0.1483],
        [ 0.3140],
        [-0.3762],
        [-0.4188],
        [-0.4195],
        [-0.3107],
        [-0.1440],
        [-0.0190],
        [ 0.4173],
        [-0.1095],
        [-0.0397],
        [-0.3807],
        [-0.3000],
        [ 0.2092],
        [ 0.1744],
        [ 0.4119],
        [-0.2251],
        [-0.2633],
        [ 0.1667],
        [-0.1947],
        [ 0.2729],
        [ 0.1474],
        [ 0.1088],
        [ 0.2047],
        [ 0.2936],
        [-0.0404],
        [ 0.2677],
        [ 0.4234]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3932],
        [ 0.3440],
        [ 0.0753],
        [-0.4153],
        [-0.1483],
        [ 0.3140],
        [-0.3762],
        [-0.4188],
        [-0.4195],
        [-0.3107],
        [-0.1440],
        [-0.0190],
        [ 0.4173],
        [-0.1095],
        [-0.0397],
        [-0.3807],
        [-0.3000],
        [ 0.2092],
        [ 0.1744],
        [ 0.4119],
        [-0.2251],
        [-0.2633],
        [ 0.1667],
        [-0.1947],
        [ 0.2729],
        [ 0.1474],
        [ 0.1088],
        [ 0.2047],
        [ 0.2936],
        [-0.0404],
        [ 0.2677],
        [ 0.4234]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0598,  0.1209,  0.0698,  0.0065,  0.0246,  0.0904,  0.0684, -0.0671,
         -0.1163,  0.0600, -0.0262, -0.1259, -0.0535, -0.0599, -0.0602,  0.0779,
          0.0708,  0.0487, -0.0568, -0.0982, -0.1098,  0.1040,  0.0349,  0.0249,
         -0.0936, -0.1458, -0.0321,  0.1092, -0.1102, -0.0321, -0.0269, -0.0297,
          0.1485,  0.0525,  0.1328, -0.0901, -0.1351,  0.0771, -0.1107, -0.1236,
         -0.0613, -0.0013, -0.0909,  0.0343, -0.1328, -0.1122,  0.1447, -0.1126,
         -0.1149, -0.0539, -0.0256,  0.1483,  0.1220, -0.0816, -0.0404,  0.0633,
          0.0271, -0.0575,  0.1207, -0.0979,  0.0086,  0.1181,  0.1343, -0.0499,
         -0.0714,  0.0424,  0.1374, -0.1045, -0.0061, -0.0157,  0.1073, -0.0955,
          0.1508,  0.0782, -0.1130,  0.0465, -0.0116,  0.0258, -0.0973, -0.0231,
          0.0585, -0.0409,  0.0326,  0.0922,  0.0394,  0.1403,  0.0261, -0.1012,
         -0.0981,  0.0073, -0.0307,  0.1309, -0.1330, -0.0188,  0.0420, -0.0644,
          0.0420, -0.0828, -0.0820, -0.1130, -0.0339,  0.1147,  0.1397,  0.0699,
          0.1318,  0.1319, -0.0261, -0.0084,  0.1228, -0.0377, -0.0629, -0.0612,
          0.0063, -0.0777,  0.0242, -0.0711, -0.0276, -0.0776,  0.0116, -0.0250,
         -0.0134, -0.0075, -0.0599,  0.0238,  0.0741, -0.1030,  0.1171, -0.0926,
          0.0013, -0.0720, -0.1243,  0.1199, -0.1443,  0.0424,  0.0104,  0.0548,
          0.1053, -0.0631,  0.0912,  0.0972,  0.1407, -0.1398,  0.0432, -0.0273,
          0.0759, -0.0431,  0.1134, -0.1250, -0.1453, -0.0855, -0.0636,  0.1518,
         -0.0147, -0.0479, -0.0271, -0.0642,  0.1094,  0.0560, -0.1413, -0.0274,
         -0.0878, -0.0867,  0.1303, -0.0674,  0.0216,  0.1174,  0.0471, -0.0452,
          0.0962, -0.0531,  0.0671, -0.0204,  0.0409,  0.1495,  0.0165, -0.1057,
         -0.0531,  0.1230,  0.0186, -0.0343,  0.0254, -0.0121, -0.0590, -0.0132,
          0.1193, -0.1098,  0.0730, -0.1339, -0.0642, -0.1502,  0.0095, -0.1009,
          0.1200,  0.1468, -0.0337,  0.0942, -0.0704,  0.1343,  0.1182,  0.0373,
          0.0862, -0.0920,  0.0713, -0.1474,  0.0546, -0.0080,  0.0231, -0.1377,
         -0.0829,  0.1123, -0.0446, -0.1404, -0.1460, -0.0549,  0.1038,  0.0011,
          0.0201, -0.0400, -0.0650, -0.1036, -0.0757, -0.0185, -0.0483, -0.0399,
         -0.0913,  0.0392,  0.1359, -0.0923, -0.1186,  0.0661,  0.0122,  0.1194,
         -0.0023, -0.0044, -0.1379,  0.0224,  0.1358,  0.0208,  0.1424, -0.1140,
          0.0757, -0.0644, -0.0218,  0.0342, -0.0519,  0.0824, -0.1371, -0.0186,
          0.0245, -0.0967,  0.1179,  0.1089,  0.0745,  0.0685, -0.0641, -0.1245]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0598,  0.1209,  0.0698,  0.0065,  0.0246,  0.0904,  0.0684, -0.0671,
         -0.1163,  0.0600, -0.0262, -0.1259, -0.0535, -0.0599, -0.0602,  0.0779,
          0.0708,  0.0487, -0.0568, -0.0982, -0.1098,  0.1040,  0.0349,  0.0249,
         -0.0936, -0.1458, -0.0321,  0.1092, -0.1102, -0.0321, -0.0269, -0.0297,
          0.1485,  0.0525,  0.1328, -0.0901, -0.1351,  0.0771, -0.1107, -0.1236,
         -0.0613, -0.0013, -0.0909,  0.0343, -0.1328, -0.1122,  0.1447, -0.1126,
         -0.1149, -0.0539, -0.0256,  0.1483,  0.1220, -0.0816, -0.0404,  0.0633,
          0.0271, -0.0575,  0.1207, -0.0979,  0.0086,  0.1181,  0.1343, -0.0499,
         -0.0714,  0.0424,  0.1374, -0.1045, -0.0061, -0.0157,  0.1073, -0.0955,
          0.1508,  0.0782, -0.1130,  0.0465, -0.0116,  0.0258, -0.0973, -0.0231,
          0.0585, -0.0409,  0.0326,  0.0922,  0.0394,  0.1403,  0.0261, -0.1012,
         -0.0981,  0.0073, -0.0307,  0.1309, -0.1330, -0.0188,  0.0420, -0.0644,
          0.0420, -0.0828, -0.0820, -0.1130, -0.0339,  0.1147,  0.1397,  0.0699,
          0.1318,  0.1319, -0.0261, -0.0084,  0.1228, -0.0377, -0.0629, -0.0612,
          0.0063, -0.0777,  0.0242, -0.0711, -0.0276, -0.0776,  0.0116, -0.0250,
         -0.0134, -0.0075, -0.0599,  0.0238,  0.0741, -0.1030,  0.1171, -0.0926,
          0.0013, -0.0720, -0.1243,  0.1199, -0.1443,  0.0424,  0.0104,  0.0548,
          0.1053, -0.0631,  0.0912,  0.0972,  0.1407, -0.1398,  0.0432, -0.0273,
          0.0759, -0.0431,  0.1134, -0.1250, -0.1453, -0.0855, -0.0636,  0.1518,
         -0.0147, -0.0479, -0.0271, -0.0642,  0.1094,  0.0560, -0.1413, -0.0274,
         -0.0878, -0.0867,  0.1303, -0.0674,  0.0216,  0.1174,  0.0471, -0.0452,
          0.0962, -0.0531,  0.0671, -0.0204,  0.0409,  0.1495,  0.0165, -0.1057,
         -0.0531,  0.1230,  0.0186, -0.0343,  0.0254, -0.0121, -0.0590, -0.0132,
          0.1193, -0.1098,  0.0730, -0.1339, -0.0642, -0.1502,  0.0095, -0.1009,
          0.1200,  0.1468, -0.0337,  0.0942, -0.0704,  0.1343,  0.1182,  0.0373,
          0.0862, -0.0920,  0.0713, -0.1474,  0.0546, -0.0080,  0.0231, -0.1377,
         -0.0829,  0.1123, -0.0446, -0.1404, -0.1460, -0.0549,  0.1038,  0.0011,
          0.0201, -0.0400, -0.0650, -0.1036, -0.0757, -0.0185, -0.0483, -0.0399,
         -0.0913,  0.0392,  0.1359, -0.0923, -0.1186,  0.0661,  0.0122,  0.1194,
         -0.0023, -0.0044, -0.1379,  0.0224,  0.1358,  0.0208,  0.1424, -0.1140,
          0.0757, -0.0644, -0.0218,  0.0342, -0.0519,  0.0824, -0.1371, -0.0186,
          0.0245, -0.0967,  0.1179,  0.1089,  0.0745,  0.0685, -0.0641, -0.1245]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0582, -0.0428,  0.0431,  ...,  0.0808, -0.1052,  0.0670],
        [-0.0591, -0.1104,  0.0518,  ..., -0.0288,  0.0046, -0.0543],
        [-0.0073,  0.0137,  0.0866,  ..., -0.0187,  0.0556, -0.0035],
        ...,
        [-0.0671,  0.0045,  0.0071,  ...,  0.1185, -0.1026,  0.0029],
        [ 0.0412, -0.0203,  0.0779,  ..., -0.0576, -0.0833, -0.1124],
        [ 0.0786, -0.1141,  0.1124,  ...,  0.1142, -0.0249,  0.1058]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0582, -0.0428,  0.0431,  ...,  0.0808, -0.1052,  0.0670],
        [-0.0591, -0.1104,  0.0518,  ..., -0.0288,  0.0046, -0.0543],
        [-0.0073,  0.0137,  0.0866,  ..., -0.0187,  0.0556, -0.0035],
        ...,
        [-0.0671,  0.0045,  0.0071,  ...,  0.1185, -0.1026,  0.0029],
        [ 0.0412, -0.0203,  0.0779,  ..., -0.0576, -0.0833, -0.1124],
        [ 0.0786, -0.1141,  0.1124,  ...,  0.1142, -0.0249,  0.1058]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0894, -0.0247,  0.1609,  ...,  0.0255, -0.1719, -0.1455],
        [-0.1307,  0.1361,  0.0044,  ...,  0.1030,  0.1036,  0.1465],
        [-0.1741, -0.0040, -0.0980,  ...,  0.0147, -0.0996, -0.1216],
        ...,
        [ 0.1615,  0.1040, -0.1588,  ..., -0.1757,  0.1435, -0.1335],
        [-0.1315,  0.0481, -0.0776,  ..., -0.1147,  0.1118,  0.1447],
        [-0.1166,  0.0499, -0.1490,  ..., -0.1643, -0.1116, -0.0739]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0894, -0.0247,  0.1609,  ...,  0.0255, -0.1719, -0.1455],
        [-0.1307,  0.1361,  0.0044,  ...,  0.1030,  0.1036,  0.1465],
        [-0.1741, -0.0040, -0.0980,  ...,  0.0147, -0.0996, -0.1216],
        ...,
        [ 0.1615,  0.1040, -0.1588,  ..., -0.1757,  0.1435, -0.1335],
        [-0.1315,  0.0481, -0.0776,  ..., -0.1147,  0.1118,  0.1447],
        [-0.1166,  0.0499, -0.1490,  ..., -0.1643, -0.1116, -0.0739]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0592,  0.1811,  0.0846,  ...,  0.0027,  0.0268, -0.0885],
        [ 0.2463, -0.1296, -0.0014,  ..., -0.1326,  0.0556,  0.1744],
        [-0.1606, -0.0985,  0.1113,  ...,  0.2007, -0.1067, -0.2292],
        ...,
        [-0.2019, -0.1907, -0.1712,  ..., -0.1122, -0.0940,  0.0517],
        [ 0.2019, -0.2187, -0.1967,  ..., -0.2133, -0.1819, -0.0689],
        [-0.2205, -0.1895,  0.0868,  ...,  0.1258, -0.1839, -0.0302]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0592,  0.1811,  0.0846,  ...,  0.0027,  0.0268, -0.0885],
        [ 0.2463, -0.1296, -0.0014,  ..., -0.1326,  0.0556,  0.1744],
        [-0.1606, -0.0985,  0.1113,  ...,  0.2007, -0.1067, -0.2292],
        ...,
        [-0.2019, -0.1907, -0.1712,  ..., -0.1122, -0.0940,  0.0517],
        [ 0.2019, -0.2187, -0.1967,  ..., -0.2133, -0.1819, -0.0689],
        [-0.2205, -0.1895,  0.0868,  ...,  0.1258, -0.1839, -0.0302]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2157],
        [ 0.2367],
        [ 0.2337],
        [-0.0505],
        [-0.2107],
        [-0.1759],
        [ 0.2806],
        [ 0.3791],
        [-0.3109],
        [ 0.2225],
        [ 0.1909],
        [-0.2134],
        [-0.1208],
        [-0.1546],
        [ 0.3856],
        [ 0.0149],
        [ 0.3653],
        [ 0.3166],
        [-0.2991],
        [-0.4119],
        [-0.1718],
        [ 0.4233],
        [-0.1429],
        [-0.2925],
        [-0.1782],
        [-0.0083],
        [ 0.0312],
        [-0.0743],
        [ 0.0609],
        [ 0.2669],
        [-0.2598],
        [-0.3282]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2157],
        [ 0.2367],
        [ 0.2337],
        [-0.0505],
        [-0.2107],
        [-0.1759],
        [ 0.2806],
        [ 0.3791],
        [-0.3109],
        [ 0.2225],
        [ 0.1909],
        [-0.2134],
        [-0.1208],
        [-0.1546],
        [ 0.3856],
        [ 0.0149],
        [ 0.3653],
        [ 0.3166],
        [-0.2991],
        [-0.4119],
        [-0.1718],
        [ 0.4233],
        [-0.1429],
        [-0.2925],
        [-0.1782],
        [-0.0083],
        [ 0.0312],
        [-0.0743],
        [ 0.0609],
        [ 0.2669],
        [-0.2598],
        [-0.3282]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086,  0.0023, -0.0082,  ...,  0.0037, -0.0045,  0.0090],
        [ 0.0284,  0.0075, -0.0272,  ...,  0.0122, -0.0148,  0.0299],
        [ 0.0189,  0.0050, -0.0180,  ...,  0.0081, -0.0098,  0.0199],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-5.9567, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.8332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7608, device='cuda:0')



h[100].sum tensor(-8.5591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.8923, device='cuda:0')



h[200].sum tensor(24.1331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0724, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0653, 0.0173, 0.0000,  ..., 0.0279, 0.0000, 0.0687],
        [0.0575, 0.0152, 0.0000,  ..., 0.0246, 0.0000, 0.0605],
        [0.0790, 0.0209, 0.0000,  ..., 0.0338, 0.0000, 0.0831],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(20324.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0506,  ..., 0.0000, 0.0000, 0.0519],
        [0.0000, 0.0155, 0.0496,  ..., 0.0000, 0.0000, 0.0509],
        [0.0000, 0.0156, 0.0499,  ..., 0.0000, 0.0000, 0.0511],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(123015.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.4280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-66.0164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-24.3812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.7214],
        [0.6944],
        [0.6236],
        ...,
        [0.0054],
        [0.0055],
        [0.0056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(37092.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.7214],
        [0.6944],
        [0.6236],
        ...,
        [0.0054],
        [0.0055],
        [0.0056]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0158, -0.0027, -0.0020,  ...,  0.0011,  0.0054,  0.0119],
        [ 0.0351, -0.0059, -0.0045,  ...,  0.0023,  0.0120,  0.0264],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-237.9478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.0549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.9090, device='cuda:0')



h[100].sum tensor(13.9445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2418, device='cuda:0')



h[200].sum tensor(18.7990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.1999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1403, 0.0000, 0.0000,  ..., 0.0094, 0.0480, 0.1056],
        [0.0706, 0.0000, 0.0000,  ..., 0.0047, 0.0242, 0.0532],
        [0.0793, 0.0000, 0.0000,  ..., 0.0053, 0.0271, 0.0597],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(28624.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1011, 0.1240, 0.5090,  ..., 0.0000, 0.2003, 0.0574],
        [0.0772, 0.0947, 0.3887,  ..., 0.0000, 0.1530, 0.0438],
        [0.0711, 0.0872, 0.3580,  ..., 0.0000, 0.1409, 0.0404],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(160513.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1149.9250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(79.2168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.2533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.1527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[9.5970e-01],
        [8.3006e-01],
        [7.2416e-01],
        ...,
        [1.1776e-05],
        [1.9578e-05],
        [2.7986e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(20693.2461, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.7214],
        [0.6944],
        [0.6236],
        ...,
        [0.0054],
        [0.0055],
        [0.0056]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 3001 
BatchSize 30 
EpochNum 60 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0294, -0.1348, -0.0940,  0.0458,  0.0600,  0.0226, -0.0121, -0.1509,
          0.0633,  0.1516, -0.0135, -0.0740,  0.0448,  0.0606, -0.0260,  0.0834,
          0.1391,  0.0593, -0.0224,  0.0598, -0.0502, -0.0465,  0.1013, -0.0178,
         -0.0388,  0.1280,  0.0257,  0.0674,  0.0345, -0.0300,  0.1073, -0.0227,
          0.1387,  0.0758, -0.0693, -0.0690, -0.0691,  0.0759,  0.0789,  0.0411,
         -0.1136,  0.1263, -0.0711,  0.0311, -0.0825,  0.0157, -0.1141, -0.0594,
          0.0367,  0.0134,  0.0235, -0.0894, -0.0456, -0.1024,  0.0238, -0.1333,
         -0.0304, -0.1376, -0.1446, -0.0723,  0.0469, -0.0569, -0.0086,  0.0799,
         -0.0736, -0.1202,  0.0641, -0.0951, -0.1222, -0.0878, -0.0342,  0.0059,
         -0.0990,  0.0753, -0.0820, -0.0807,  0.0828,  0.0859,  0.0387, -0.0509,
          0.1052,  0.0336, -0.0602, -0.1171,  0.0857, -0.1459, -0.1045,  0.1123,
         -0.1089, -0.0673,  0.1388, -0.0846, -0.1296, -0.1263, -0.1293,  0.1266,
          0.0751, -0.0855, -0.0421, -0.0698,  0.1379,  0.1182,  0.1200,  0.0322,
         -0.0490,  0.0273, -0.0219,  0.0210,  0.0602,  0.1007,  0.0197,  0.0544,
          0.0872,  0.0482, -0.0493, -0.0076, -0.1060,  0.0490,  0.0534,  0.0305,
         -0.0123,  0.0687,  0.0595,  0.0150, -0.1432,  0.0369, -0.0212,  0.0590,
          0.1255, -0.0081,  0.1341, -0.1414, -0.1432, -0.0929, -0.0830, -0.0917,
         -0.0832,  0.0015, -0.0759,  0.0570,  0.0393,  0.0266,  0.0254,  0.0442,
          0.0390, -0.0293,  0.1003,  0.0159, -0.0062, -0.1411,  0.0535,  0.0557,
         -0.0190,  0.0921,  0.0627,  0.0486,  0.0164, -0.1218,  0.0863,  0.1180,
          0.0093,  0.0076, -0.0687,  0.1241, -0.1176, -0.0423,  0.0497,  0.0921,
          0.0392,  0.0948, -0.1098,  0.0077,  0.0813,  0.0004,  0.1407, -0.0084,
          0.0380,  0.0724, -0.0595, -0.0756,  0.0729,  0.0475, -0.0626, -0.0691,
          0.0939,  0.0185,  0.0894, -0.0172,  0.0872,  0.1203, -0.0034,  0.0614,
         -0.1220, -0.0929, -0.1227,  0.0242,  0.0384, -0.0307, -0.0926, -0.0134,
         -0.0388,  0.0211,  0.0063,  0.0436,  0.0230, -0.0818, -0.0899, -0.0846,
         -0.0820,  0.0060, -0.0785,  0.1291,  0.0903,  0.0041,  0.1000, -0.0376,
          0.0349,  0.1156,  0.0547,  0.0147,  0.1278,  0.1197, -0.1128,  0.0353,
         -0.0057, -0.1264,  0.0695,  0.0084, -0.1345, -0.0896, -0.1010,  0.0040,
         -0.0319,  0.0773, -0.0381, -0.0982, -0.1344, -0.1219, -0.1023,  0.0462,
          0.1314, -0.1067, -0.0209,  0.0012,  0.0220,  0.1120,  0.1049,  0.1169,
         -0.0631,  0.0442, -0.0923,  0.0968,  0.1124,  0.0761,  0.0702, -0.1095]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0168, -0.0008, -0.1007,  ...,  0.1121, -0.0744,  0.0067],
        [-0.0886, -0.0154,  0.0947,  ...,  0.0698,  0.0959, -0.0722],
        [ 0.1090,  0.1118, -0.0703,  ..., -0.1010, -0.0282, -0.0550],
        ...,
        [ 0.1026, -0.1037,  0.0396,  ..., -0.0120, -0.0292, -0.0076],
        [ 0.0466,  0.0796, -0.0559,  ..., -0.0051,  0.0247, -0.1123],
        [ 0.0081,  0.0609, -0.0832,  ..., -0.0417, -0.0737,  0.1247]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1621,  0.1665, -0.1420,  ...,  0.0471, -0.0843, -0.1259],
        [-0.0626, -0.0241,  0.0156,  ...,  0.1743,  0.0182, -0.0726],
        [ 0.0138, -0.0662,  0.0808,  ..., -0.0261, -0.0604,  0.0966],
        ...,
        [ 0.1720, -0.1156, -0.0394,  ..., -0.0248,  0.1580,  0.1239],
        [ 0.1136, -0.1542, -0.0886,  ...,  0.0056,  0.0077,  0.0446],
        [ 0.0520, -0.1734, -0.1584,  ..., -0.1021,  0.1573, -0.0382]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1899,  0.1540,  0.0339,  ..., -0.2129,  0.2014,  0.0514],
        [-0.1397,  0.2320,  0.0962,  ..., -0.0572,  0.0751,  0.0725],
        [ 0.2341,  0.1993, -0.1769,  ...,  0.0671, -0.0397, -0.1791],
        ...,
        [-0.0511,  0.2248,  0.0609,  ...,  0.2408, -0.0008, -0.0203],
        [ 0.1522, -0.0465, -0.1612,  ...,  0.1257, -0.0455, -0.0582],
        [-0.1881,  0.0329,  0.1752,  ...,  0.0415, -0.1651, -0.2442]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1025],
        [ 0.1411],
        [ 0.2394],
        [-0.2803],
        [-0.2656],
        [ 0.0800],
        [-0.1947],
        [-0.0218],
        [-0.3134],
        [-0.0183],
        [-0.0918],
        [-0.0190],
        [-0.1617],
        [ 0.2559],
        [ 0.3194],
        [ 0.2249],
        [-0.2123],
        [-0.1960],
        [ 0.1606],
        [-0.2407],
        [-0.3544],
        [ 0.0957],
        [ 0.4006],
        [ 0.0724],
        [-0.0188],
        [-0.1865],
        [ 0.0649],
        [ 0.4165],
        [ 0.0204],
        [-0.3405],
        [-0.3604],
        [-0.3256]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0294, -0.1348, -0.0940,  0.0458,  0.0600,  0.0226, -0.0121, -0.1509,
          0.0633,  0.1516, -0.0135, -0.0740,  0.0448,  0.0606, -0.0260,  0.0834,
          0.1391,  0.0593, -0.0224,  0.0598, -0.0502, -0.0465,  0.1013, -0.0178,
         -0.0388,  0.1280,  0.0257,  0.0674,  0.0345, -0.0300,  0.1073, -0.0227,
          0.1387,  0.0758, -0.0693, -0.0690, -0.0691,  0.0759,  0.0789,  0.0411,
         -0.1136,  0.1263, -0.0711,  0.0311, -0.0825,  0.0157, -0.1141, -0.0594,
          0.0367,  0.0134,  0.0235, -0.0894, -0.0456, -0.1024,  0.0238, -0.1333,
         -0.0304, -0.1376, -0.1446, -0.0723,  0.0469, -0.0569, -0.0086,  0.0799,
         -0.0736, -0.1202,  0.0641, -0.0951, -0.1222, -0.0878, -0.0342,  0.0059,
         -0.0990,  0.0753, -0.0820, -0.0807,  0.0828,  0.0859,  0.0387, -0.0509,
          0.1052,  0.0336, -0.0602, -0.1171,  0.0857, -0.1459, -0.1045,  0.1123,
         -0.1089, -0.0673,  0.1388, -0.0846, -0.1296, -0.1263, -0.1293,  0.1266,
          0.0751, -0.0855, -0.0421, -0.0698,  0.1379,  0.1182,  0.1200,  0.0322,
         -0.0490,  0.0273, -0.0219,  0.0210,  0.0602,  0.1007,  0.0197,  0.0544,
          0.0872,  0.0482, -0.0493, -0.0076, -0.1060,  0.0490,  0.0534,  0.0305,
         -0.0123,  0.0687,  0.0595,  0.0150, -0.1432,  0.0369, -0.0212,  0.0590,
          0.1255, -0.0081,  0.1341, -0.1414, -0.1432, -0.0929, -0.0830, -0.0917,
         -0.0832,  0.0015, -0.0759,  0.0570,  0.0393,  0.0266,  0.0254,  0.0442,
          0.0390, -0.0293,  0.1003,  0.0159, -0.0062, -0.1411,  0.0535,  0.0557,
         -0.0190,  0.0921,  0.0627,  0.0486,  0.0164, -0.1218,  0.0863,  0.1180,
          0.0093,  0.0076, -0.0687,  0.1241, -0.1176, -0.0423,  0.0497,  0.0921,
          0.0392,  0.0948, -0.1098,  0.0077,  0.0813,  0.0004,  0.1407, -0.0084,
          0.0380,  0.0724, -0.0595, -0.0756,  0.0729,  0.0475, -0.0626, -0.0691,
          0.0939,  0.0185,  0.0894, -0.0172,  0.0872,  0.1203, -0.0034,  0.0614,
         -0.1220, -0.0929, -0.1227,  0.0242,  0.0384, -0.0307, -0.0926, -0.0134,
         -0.0388,  0.0211,  0.0063,  0.0436,  0.0230, -0.0818, -0.0899, -0.0846,
         -0.0820,  0.0060, -0.0785,  0.1291,  0.0903,  0.0041,  0.1000, -0.0376,
          0.0349,  0.1156,  0.0547,  0.0147,  0.1278,  0.1197, -0.1128,  0.0353,
         -0.0057, -0.1264,  0.0695,  0.0084, -0.1345, -0.0896, -0.1010,  0.0040,
         -0.0319,  0.0773, -0.0381, -0.0982, -0.1344, -0.1219, -0.1023,  0.0462,
          0.1314, -0.1067, -0.0209,  0.0012,  0.0220,  0.1120,  0.1049,  0.1169,
         -0.0631,  0.0442, -0.0923,  0.0968,  0.1124,  0.0761,  0.0702, -0.1095]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0168, -0.0008, -0.1007,  ...,  0.1121, -0.0744,  0.0067],
        [-0.0886, -0.0154,  0.0947,  ...,  0.0698,  0.0959, -0.0722],
        [ 0.1090,  0.1118, -0.0703,  ..., -0.1010, -0.0282, -0.0550],
        ...,
        [ 0.1026, -0.1037,  0.0396,  ..., -0.0120, -0.0292, -0.0076],
        [ 0.0466,  0.0796, -0.0559,  ..., -0.0051,  0.0247, -0.1123],
        [ 0.0081,  0.0609, -0.0832,  ..., -0.0417, -0.0737,  0.1247]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1621,  0.1665, -0.1420,  ...,  0.0471, -0.0843, -0.1259],
        [-0.0626, -0.0241,  0.0156,  ...,  0.1743,  0.0182, -0.0726],
        [ 0.0138, -0.0662,  0.0808,  ..., -0.0261, -0.0604,  0.0966],
        ...,
        [ 0.1720, -0.1156, -0.0394,  ..., -0.0248,  0.1580,  0.1239],
        [ 0.1136, -0.1542, -0.0886,  ...,  0.0056,  0.0077,  0.0446],
        [ 0.0520, -0.1734, -0.1584,  ..., -0.1021,  0.1573, -0.0382]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1899,  0.1540,  0.0339,  ..., -0.2129,  0.2014,  0.0514],
        [-0.1397,  0.2320,  0.0962,  ..., -0.0572,  0.0751,  0.0725],
        [ 0.2341,  0.1993, -0.1769,  ...,  0.0671, -0.0397, -0.1791],
        ...,
        [-0.0511,  0.2248,  0.0609,  ...,  0.2408, -0.0008, -0.0203],
        [ 0.1522, -0.0465, -0.1612,  ...,  0.1257, -0.0455, -0.0582],
        [-0.1881,  0.0329,  0.1752,  ...,  0.0415, -0.1651, -0.2442]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1025],
        [ 0.1411],
        [ 0.2394],
        [-0.2803],
        [-0.2656],
        [ 0.0800],
        [-0.1947],
        [-0.0218],
        [-0.3134],
        [-0.0183],
        [-0.0918],
        [-0.0190],
        [-0.1617],
        [ 0.2559],
        [ 0.3194],
        [ 0.2249],
        [-0.2123],
        [-0.1960],
        [ 0.1606],
        [-0.2407],
        [-0.3544],
        [ 0.0957],
        [ 0.4006],
        [ 0.0724],
        [-0.0188],
        [-0.1865],
        [ 0.0649],
        [ 0.4165],
        [ 0.0204],
        [-0.3405],
        [-0.3604],
        [-0.3256]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-73.4054, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-90.9464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(474.6494, device='cuda:0')



h[100].sum tensor(426.6810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.2418, device='cuda:0')



h[200].sum tensor(-120.1310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(222.7683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0123, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(296721.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0000, 0.0109,  ..., 0.0763, 0.0000, 0.0000],
        [0.0063, 0.0000, 0.0046,  ..., 0.0324, 0.0000, 0.0000],
        [0.0043, 0.0000, 0.0032,  ..., 0.0221, 0.0000, 0.0000],
        ...,
        [0.0035, 0.0000, 0.0026,  ..., 0.0179, 0.0000, 0.0000],
        [0.0059, 0.0000, 0.0043,  ..., 0.0303, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0064,  ..., 0.0446, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1512304.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7372.8374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1247.3105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3985.4878, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3576.1550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1510.8716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9546],
        [-0.8381],
        [-0.8220],
        ...,
        [-0.3161],
        [-0.4102],
        [-0.5037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-713068.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(263.5764, device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0952e-03, -1.4236e-02, -9.9155e-03,  ...,  8.1523e-03,
          7.3094e-03, -1.1556e-02],
        [-6.8788e-03, -3.1638e-02, -2.2036e-02,  ...,  1.7996e-02,
          1.6367e-02, -2.5682e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-575.2971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-90.7784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(474.7716, device='cuda:0')



h[100].sum tensor(406.6389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.2844, device='cuda:0')



h[200].sum tensor(-120.0073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(222.8257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0719, 0.0654, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0364, 0.0328, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0369, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(306925.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0488, 0.0000, 0.0354,  ..., 0.2880, 0.0000, 0.0000],
        [0.0368, 0.0000, 0.0266,  ..., 0.2201, 0.0000, 0.0000],
        [0.0337, 0.0000, 0.0244,  ..., 0.2028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0010,  ..., 0.0157, 0.0000, 0.0000],
        [0.0060, 0.0000, 0.0041,  ..., 0.0443, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1570168.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4879.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1392.9976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8531.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3357.6526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1562.1501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4034],
        [-2.0640],
        [-1.7859],
        ...,
        [-0.0592],
        [-0.1467],
        [-0.2773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-555070.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.8748e-04,
         -8.3189e-05,  0.0000e+00],
        [-1.9246e-03, -8.8755e-03, -6.1800e-03,  ...,  5.2172e-03,
          4.5409e-03, -7.2033e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.8748e-04,
         -8.3189e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.8748e-04,
         -8.3189e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.8748e-04,
         -8.3189e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.8748e-04,
         -8.3189e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-935.3910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-97.8700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(513.4734, device='cuda:0')



h[100].sum tensor(434.2441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(178.7578, device='cuda:0')



h[200].sum tensor(-129.4889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(240.9897, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0262, 0.0231, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0045, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337371.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0057,  ..., 0.0704, 0.0000, 0.0000],
        [0.0058, 0.0000, 0.0039,  ..., 0.0563, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0054,  ..., 0.0691, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1740312.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4060.7817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1482.0332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13747.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3441.2173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1721.4298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3331],
        [-0.4650],
        [-0.6825],
        ...,
        [-0.0075],
        [-0.0074],
        [-0.0074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-497979.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.8024e-04,
         -3.1452e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1253.4185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-89.6618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(471.0872, device='cuda:0')



h[100].sum tensor(397.4604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(164.0017, device='cuda:0')



h[200].sum tensor(-118.7272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(221.0965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318116.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0003, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1670287.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3023.4514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1362.8303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18999.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2976.1831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1639.7510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0975],
        [-0.0624],
        [-0.0497],
        ...,
        [-0.0120],
        [-0.0119],
        [-0.0118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-367640.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.7544e-04,
          3.4185e-05,  0.0000e+00],
        [-1.8797e-03, -8.7146e-03, -6.0640e-03,  ...,  5.3333e-03,
          4.5893e-03, -7.0703e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.7544e-04,
          3.4185e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.7544e-04,
          3.4185e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.7544e-04,
          3.4185e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.7544e-04,
          3.4185e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1559.5125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-86.3677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(455.7723, device='cuda:0')



h[100].sum tensor(387.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(158.6701, device='cuda:0')



h[200].sum tensor(-114.4606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(213.9087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0167, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0039, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(313803.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9361e-03, 0.0000e+00, 6.1423e-04,  ..., 4.7720e-02, 0.0000e+00,
         0.0000e+00],
        [4.0858e-04, 0.0000e+00, 6.9026e-05,  ..., 2.9562e-02, 0.0000e+00,
         0.0000e+00],
        [4.5204e-04, 0.0000e+00, 2.7969e-05,  ..., 3.2602e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.2677e-03, 0.0000e+00,  ..., 1.3674e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2677e-03, 0.0000e+00,  ..., 1.3674e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2677e-03, 0.0000e+00,  ..., 1.3674e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1666217., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2549.6577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1301.7437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24430.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2751.9419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1639.5759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2524],
        [-0.2715],
        [-0.3076],
        ...,
        [-0.0159],
        [-0.0144],
        [-0.0136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-273731.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073, -0.0339, -0.0236,  ...,  0.0198,  0.0179, -0.0275],
        [-0.0076, -0.0351, -0.0244,  ...,  0.0205,  0.0185, -0.0285],
        [-0.0073, -0.0337, -0.0235,  ...,  0.0197,  0.0178, -0.0273],
        ...,
        [-0.0047, -0.0217, -0.0151,  ...,  0.0128,  0.0115, -0.0176],
        [-0.0042, -0.0194, -0.0135,  ...,  0.0115,  0.0103, -0.0157],
        [-0.0041, -0.0192, -0.0134,  ...,  0.0114,  0.0102, -0.0156]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1848.0297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-78.7799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(417.4626, device='cuda:0')



h[100].sum tensor(359.4641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(145.3331, device='cuda:0')



h[200].sum tensor(-104.4922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(195.9287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0848, 0.0766, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0719, 0.0648, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0664, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0396, 0.0351, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0430, 0.0382, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0366, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(291761.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0468, 0.0000, 0.0289,  ..., 0.3527, 0.0000, 0.0000],
        [0.0440, 0.0000, 0.0271,  ..., 0.3344, 0.0000, 0.0000],
        [0.0409, 0.0000, 0.0251,  ..., 0.3136, 0.0000, 0.0000],
        ...,
        [0.0146, 0.0000, 0.0077,  ..., 0.1356, 0.0000, 0.0000],
        [0.0174, 0.0000, 0.0097,  ..., 0.1549, 0.0000, 0.0000],
        [0.0181, 0.0000, 0.0102,  ..., 0.1599, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1564194.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2025.7899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1171.1030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29154.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2389.5962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1557.6819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7174],
        [-0.7055],
        [-0.6278],
        ...,
        [-0.2020],
        [-0.2459],
        [-0.2719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-172672.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038, -0.0177, -0.0123,  ...,  0.0106,  0.0095, -0.0144],
        [-0.0015, -0.0070, -0.0049,  ...,  0.0046,  0.0039, -0.0057],
        [-0.0023, -0.0107, -0.0074,  ...,  0.0066,  0.0058, -0.0086],
        ...,
        [-0.0023, -0.0109, -0.0076,  ...,  0.0068,  0.0059, -0.0089],
        [-0.0022, -0.0103, -0.0072,  ...,  0.0064,  0.0056, -0.0083],
        [-0.0044, -0.0204, -0.0142,  ...,  0.0122,  0.0109, -0.0166]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2102.3677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-85.9996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(456.0092, device='cuda:0')



h[100].sum tensor(402.8809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(158.7526, device='cuda:0')



h[200].sum tensor(-114.1644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(214.0199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0161, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0404, 0.0359, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0161, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0371, 0.0329, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0262, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0269, 0.0235, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(322857.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0022,  ..., 0.0940, 0.0000, 0.0000],
        [0.0128, 0.0000, 0.0043,  ..., 0.1234, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0021,  ..., 0.0854, 0.0000, 0.0000],
        ...,
        [0.0172, 0.0000, 0.0070,  ..., 0.1542, 0.0000, 0.0000],
        [0.0165, 0.0000, 0.0067,  ..., 0.1500, 0.0000, 0.0000],
        [0.0161, 0.0000, 0.0064,  ..., 0.1469, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1741601.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2530.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1305.7957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35490.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2675.6536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1742.4844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1377],
        [-0.1091],
        [-0.0762],
        ...,
        [-0.2666],
        [-0.2556],
        [-0.2425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-143587.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0209, -0.0145,  ...,  0.0126,  0.0113, -0.0169],
        [-0.0053, -0.0246, -0.0171,  ...,  0.0147,  0.0132, -0.0199],
        [-0.0023, -0.0109, -0.0076,  ...,  0.0069,  0.0060, -0.0089],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0006,  0.0003,  0.0000],
        [-0.0037, -0.0172, -0.0120,  ...,  0.0105,  0.0093, -0.0140],
        [-0.0016, -0.0076, -0.0053,  ...,  0.0050,  0.0043, -0.0062]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2332.5398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-96.4051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(513.4468, device='cuda:0')



h[100].sum tensor(461.6051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(178.7486, device='cuda:0')



h[200].sum tensor(-128.0862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(240.9772, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0426, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0423, 0.0377, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.0398, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0166, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0189, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0337, 0.0299, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(363037.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0236, 0.0000, 0.0069,  ..., 0.1903, 0.0000, 0.0000],
        [0.0194, 0.0000, 0.0047,  ..., 0.1620, 0.0000, 0.0000],
        [0.0175, 0.0000, 0.0039,  ..., 0.1497, 0.0000, 0.0000],
        ...,
        [0.0086, 0.0000, 0.0009,  ..., 0.0879, 0.0000, 0.0000],
        [0.0112, 0.0000, 0.0010,  ..., 0.1069, 0.0000, 0.0000],
        [0.0119, 0.0000, 0.0011,  ..., 0.1122, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1965111.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3562.0530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1486.5493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41618.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3078.9138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1971.2740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1733],
        [-0.1240],
        [-0.0854],
        ...,
        [-0.0576],
        [-0.0660],
        [-0.0580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-128271.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0003,  0.0000],
        [-0.0016, -0.0074, -0.0051,  ...,  0.0049,  0.0042, -0.0060],
        [-0.0016, -0.0074, -0.0051,  ...,  0.0049,  0.0042, -0.0060],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0003,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0003,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2593.1138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-83.0295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(444.8114, device='cuda:0')



h[100].sum tensor(405.8568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(154.8542, device='cuda:0')



h[200].sum tensor(-110.4090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(208.7644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0173, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0084, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323534.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1786111.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3293.4292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1327.9464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44848.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2551.4060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1812.5731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0337],
        [ 0.0214],
        [-0.0171],
        ...,
        [-0.0381],
        [-0.0379],
        [-0.0379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-89364.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000],
        [-0.0015, -0.0069, -0.0048,  ...,  0.0047,  0.0041, -0.0056],
        [-0.0050, -0.0235, -0.0163,  ...,  0.0142,  0.0128, -0.0190],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2829.4165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-75.9219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(408.2147, device='cuda:0')



h[100].sum tensor(375.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.1136, device='cuda:0')



h[200].sum tensor(-101.0440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(191.5884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0083, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0268, 0.0236, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0391, 0.0349, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(310873.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0000],
        [0.0164, 0.0000, 0.0002,  ..., 0.1248, 0.0000, 0.0000],
        [0.0235, 0.0000, 0.0005,  ..., 0.1713, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0012, 0.0030, 0.0000,  ..., 0.0172, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1770190., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3627.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1291.3196, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48796.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2338.9609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1794.2659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0258],
        [ 0.0198],
        [ 0.0113],
        ...,
        [-0.0425],
        [-0.0318],
        [-0.0185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-85829.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019, -0.0090, -0.0062,  ...,  0.0059,  0.0052, -0.0073],
        [-0.0081, -0.0378, -0.0263,  ...,  0.0225,  0.0204, -0.0307],
        [-0.0032, -0.0152, -0.0106,  ...,  0.0095,  0.0084, -0.0123],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0007,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2826.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-77.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(415.0013, device='cuda:0')



h[100].sum tensor(382.5173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(144.4763, device='cuda:0')



h[200].sum tensor(-102.8825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(194.7735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0771, 0.0698, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0535, 0.0481, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0625, 0.0564, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314303., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0441, 0.0000, 0.0063,  ..., 0.3002, 0.0000, 0.0000],
        [0.0468, 0.0000, 0.0074,  ..., 0.3183, 0.0000, 0.0000],
        [0.0503, 0.0000, 0.0086,  ..., 0.3405, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1782739.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3659.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1315.8046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49231.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2374.8394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1807.9976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4318],
        [-0.5369],
        [-0.5979],
        ...,
        [-0.0503],
        [-0.0500],
        [-0.0500]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-75337.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 300 loss: tensor(1968.0680, device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0008,  0.0005,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0008,  0.0005,  0.0000],
        [-0.0040, -0.0188, -0.0131,  ...,  0.0116,  0.0104, -0.0152],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0008,  0.0005,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0008,  0.0005,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0008,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3048.0898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-75.7926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(407.6068, device='cuda:0')



h[100].sum tensor(376.9132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(141.9020, device='cuda:0')



h[200].sum tensor(-100.9588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(191.3031, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0157, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0148, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318273.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0020, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0622, 0.0000, 0.0000],
        [0.0101, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1850146.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4131.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1376.8350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53940.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2349.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1876.3508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0239],
        [ 0.0541],
        [ 0.0768],
        ...,
        [-0.0633],
        [-0.0629],
        [-0.0628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-84985.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3199.0266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-94.5735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(511.3766, device='cuda:0')



h[100].sum tensor(467.9502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(178.0278, device='cuda:0')



h[200].sum tensor(-126.0846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(240.0056, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(378759.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0036, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0030, 0.0034, 0.0000,  ..., 0.0266, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2160935.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5390.9497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1712.9409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60606.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2960.2866, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2224.5081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0198],
        [ 0.0138],
        [ 0.0201],
        ...,
        [-0.0779],
        [-0.0775],
        [-0.0774]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-98219.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [-0.0020, -0.0094, -0.0065,  ...,  0.0063,  0.0056, -0.0076],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3484.4980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.2333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(380.9468, device='cuda:0')



h[100].sum tensor(345.8271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.6207, device='cuda:0')



h[200].sum tensor(-93.7158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(178.7907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.0241, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0149, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(307112.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0000, 0.0000,  ..., 0.0800, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0000],
        [0.0099, 0.0000, 0.0000,  ..., 0.0784, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1866649.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3192.8367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1416.2949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59706.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2020.6901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1927.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0925],
        [ 0.0985],
        [ 0.1013],
        ...,
        [-0.0920],
        [-0.0915],
        [-0.0914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-193890.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [-0.0031, -0.0147, -0.0102,  ...,  0.0094,  0.0084, -0.0119],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3622.2131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-79.3334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(432.3025, device='cuda:0')



h[100].sum tensor(389.0678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(150.4994, device='cuda:0')



h[200].sum tensor(-105.9510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(202.8936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0153, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0089, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0343, 0.0308, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336751.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0482, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0791, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2026453., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3154.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1600.6671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63500.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2268.4727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2090.5100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0323],
        [ 0.0306],
        [ 0.0329],
        ...,
        [-0.1027],
        [-0.1021],
        [-0.1020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-231636.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035, -0.0164, -0.0114,  ...,  0.0104,  0.0094, -0.0133],
        [-0.0067, -0.0321, -0.0223,  ...,  0.0195,  0.0177, -0.0260],
        [-0.0090, -0.0429, -0.0298,  ...,  0.0258,  0.0235, -0.0348],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3762.0596, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-76.1683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(415.6291, device='cuda:0')



h[100].sum tensor(374.1705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(144.6948, device='cuda:0')



h[200].sum tensor(-101.8132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(195.0682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0672, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0708, 0.0643, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0817, 0.0742, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(331546.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0000, 0.0000,  ..., 0.3349, 0.0000, 0.0000],
        [0.0486, 0.0000, 0.0000,  ..., 0.3318, 0.0000, 0.0000],
        [0.0477, 0.0000, 0.0000,  ..., 0.3257, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2033258.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2758.2673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1574.2960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64202.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2141.8914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2067.5547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0227],
        [ 0.0225],
        [ 0.0254],
        ...,
        [-0.1102],
        [-0.1096],
        [-0.1095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-273591.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0046, -0.0219, -0.0152,  ...,  0.0136,  0.0123, -0.0177],
        [-0.0079, -0.0377, -0.0262,  ...,  0.0228,  0.0208, -0.0306],
        [-0.0086, -0.0413, -0.0286,  ...,  0.0249,  0.0227, -0.0334],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [-0.0035, -0.0166, -0.0115,  ...,  0.0106,  0.0096, -0.0135]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3850.0803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-76.6185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(419.3953, device='cuda:0')



h[100].sum tensor(378.8385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(146.0060, device='cuda:0')



h[200].sum tensor(-102.5053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(196.8358, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0548, 0.0497, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0738, 0.0671, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0837, 0.0762, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.0193, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0313, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(344789.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0296, 0.0000, 0.0000,  ..., 0.2177, 0.0000, 0.0000],
        [0.0394, 0.0000, 0.0000,  ..., 0.2799, 0.0000, 0.0000],
        [0.0443, 0.0000, 0.0000,  ..., 0.3110, 0.0000, 0.0000],
        ...,
        [0.0042, 0.0031, 0.0000,  ..., 0.0450, 0.0000, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.0000],
        [0.0229, 0.0000, 0.0000,  ..., 0.1754, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2165216.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2826.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1644.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66592.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2229.6870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2105.2927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0202],
        [ 0.0067],
        [-0.0060],
        ...,
        [-0.0216],
        [ 0.0038],
        [ 0.0174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-280281.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [-0.0039, -0.0185, -0.0128,  ...,  0.0117,  0.0106, -0.0150],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3941.2383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-69.0421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(379.4086, device='cuda:0')



h[100].sum tensor(345.7908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.0852, device='cuda:0')



h[200].sum tensor(-92.4508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(178.0687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0167, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0188, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320685.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0057, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.1003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0112, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0002, 0.0066, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0048, 0.0010, 0.0000,  ..., 0.0540, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2046417.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2153.4780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1488.4280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65069.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1914.5664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1952.4585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0176],
        [ 0.0181],
        [ 0.0502],
        ...,
        [-0.0889],
        [-0.0603],
        [-0.0248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-287232.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [-0.0035, -0.0167, -0.0116,  ...,  0.0106,  0.0097, -0.0135],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3904.7073, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-81.0415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(447.6448, device='cuda:0')



h[100].sum tensor(413.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(155.8406, device='cuda:0')



h[200].sum tensor(-108.6152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(210.0942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0121, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0104, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0390, 0.0355, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0032, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355976., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0433, 0.0000, 0.0000],
        [0.0022, 0.0000, 0.0000,  ..., 0.0489, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0807, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2219437.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2508.1899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1640.4751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68136.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2287.2898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2061.5420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.2346e-02],
        [ 1.0049e-02],
        [ 2.0196e-05],
        ...,
        [-1.1335e-01],
        [-1.1279e-01],
        [-1.1261e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-223606.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023, -0.0113, -0.0078,  ...,  0.0075,  0.0069, -0.0091],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3931.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-77.7027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(431.7206, device='cuda:0')



h[100].sum tensor(404.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(150.2968, device='cuda:0')



h[200].sum tensor(-104.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(202.6205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0082, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0093, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0033, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(352214.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0000],
        [0.0009, 0.0016, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2235257.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2372.9331, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1578.3645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68203.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2212.8101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1987.6836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0276],
        [ 0.0294],
        [ 0.0330],
        ...,
        [-0.1098],
        [-0.1094],
        [-0.1093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-196457.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0218, -0.0151,  ...,  0.0137,  0.0125, -0.0176],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000],
        [-0.0036, -0.0172, -0.0119,  ...,  0.0110,  0.0101, -0.0140],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3961.1355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-72.0410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(401.8530, device='cuda:0')



h[100].sum tensor(382.5745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(139.8989, device='cuda:0')



h[200].sum tensor(-96.7253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(188.6026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0130, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0318, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0341, 0.0312, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0034, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341254.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000],
        [0.0216, 0.0000, 0.0000,  ..., 0.1825, 0.0000, 0.0000],
        [0.0274, 0.0000, 0.0000,  ..., 0.2207, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2205270.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2407.3159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1438.3353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65665.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2082.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1890.7236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0932],
        [ 0.0977],
        [ 0.1024],
        ...,
        [-0.1071],
        [-0.1065],
        [-0.1064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-215365.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 600 loss: tensor(1586.7480, device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0102, -0.0497, -0.0344,  ...,  0.0300,  0.0274, -0.0402],
        [-0.0113, -0.0547, -0.0379,  ...,  0.0329,  0.0302, -0.0443],
        [-0.0124, -0.0602, -0.0417,  ...,  0.0361,  0.0331, -0.0487],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0009,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3946.3389, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-72.2846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(403.6161, device='cuda:0')



h[100].sum tensor(389.7989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.5127, device='cuda:0')



h[200].sum tensor(-97.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.4301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0864, 0.0791, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1174, 0.1075, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0950, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0178, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(335712.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0000, 0.0000,  ..., 0.4276, 0.0000, 0.0000],
        [0.0648, 0.0000, 0.0000,  ..., 0.4717, 0.0000, 0.0000],
        [0.0592, 0.0000, 0.0000,  ..., 0.4352, 0.0000, 0.0000],
        ...,
        [0.0099, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.0000],
        [0.0026, 0.0040, 0.0000,  ..., 0.0420, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2170318., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1719.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1403.4282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66302.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1965.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1798.4346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0689],
        [ 0.0626],
        [ 0.0600],
        ...,
        [ 0.0214],
        [-0.0042],
        [-0.0344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-166595.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064, -0.0310, -0.0215,  ...,  0.0191,  0.0175, -0.0251],
        [-0.0033, -0.0161, -0.0111,  ...,  0.0104,  0.0095, -0.0130],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3906.0181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-75.9636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(425.7555, device='cuda:0')



h[100].sum tensor(411.8022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.2202, device='cuda:0')



h[200].sum tensor(-102.1764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(199.8209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0745, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0364, 0.0333, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.0309, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(352681.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0426, 0.0000, 0.0000,  ..., 0.3359, 0.0000, 0.0000],
        [0.0267, 0.0000, 0.0000,  ..., 0.2282, 0.0000, 0.0000],
        [0.0218, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0163, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0163, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0163, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2269513., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1913.2831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1429.3663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66057.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2128.0645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1839.4878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0580],
        [ 0.0631],
        [ 0.0656],
        ...,
        [-0.0999],
        [-0.0901],
        [-0.0710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-180397.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3899.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-73.9674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(416.9564, device='cuda:0')



h[100].sum tensor(404.2327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(145.1569, device='cuda:0')



h[200].sum tensor(-99.5816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(195.6912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350284.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2285048., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1553.8098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1394.0205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66050.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2052.1636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1783.0197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0357],
        [-0.0457],
        [-0.0541],
        ...,
        [-0.1033],
        [-0.1028],
        [-0.1027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-170921.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3907.2429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.3550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(398.2274, device='cuda:0')



h[100].sum tensor(387.6269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(138.6367, device='cuda:0')



h[200].sum tensor(-94.8049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(186.9010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(340231.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2260500.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1339.4304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1311.5028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65301.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1896.1790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1699.3934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1368],
        [-0.1129],
        [-0.0851],
        ...,
        [-0.1042],
        [-0.1037],
        [-0.1035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-176611.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        ...,
        [-0.0055, -0.0268, -0.0185,  ...,  0.0167,  0.0153, -0.0216],
        [-0.0019, -0.0093, -0.0065,  ...,  0.0065,  0.0059, -0.0075],
        [-0.0018, -0.0089, -0.0062,  ...,  0.0062,  0.0057, -0.0072]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3887.9170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-69.9365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(395.8480, device='cuda:0')



h[100].sum tensor(386.5541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(137.8084, device='cuda:0')



h[200].sum tensor(-94.3273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(185.7843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0036, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0312, 0.0285, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0384, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(335978.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0164, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0140, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0000, 0.0000,  ..., 0.1753, 0.0000, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.1493, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0789, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2219723.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1091.4193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1253.8118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64294.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1813.7444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1650.4753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0387],
        [-0.0191],
        [ 0.0042],
        ...,
        [ 0.0242],
        [ 0.0097],
        [-0.0098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-183962.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062, -0.0304, -0.0210,  ...,  0.0189,  0.0172, -0.0245],
        [-0.0071, -0.0349, -0.0241,  ...,  0.0215,  0.0197, -0.0282],
        [-0.0033, -0.0164, -0.0113,  ...,  0.0106,  0.0097, -0.0132],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3755.5029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-82.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(469.0396, device='cuda:0')



h[100].sum tensor(451.1611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.2889, device='cuda:0')



h[200].sum tensor(-111.2609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(220.1355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0932, 0.0851, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0837, 0.0764, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0829, 0.0757, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(392774.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0445, 0.0000, 0.0000,  ..., 0.3793, 0.0000, 0.0000],
        [0.0457, 0.0000, 0.0000,  ..., 0.3893, 0.0000, 0.0000],
        [0.0453, 0.0000, 0.0000,  ..., 0.3867, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0189, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0189, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0189, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2560073., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2017.2678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1488.5283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67372.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2456.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1898.2206, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0656],
        [ 0.0656],
        [ 0.0637],
        ...,
        [-0.1108],
        [-0.1103],
        [-0.1101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-154068.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017, -0.0081, -0.0056,  ...,  0.0058,  0.0053, -0.0066],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3856.6079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-68.9640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(393.0807, device='cuda:0')



h[100].sum tensor(380.7397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(136.8449, device='cuda:0')



h[200].sum tensor(-93.1872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(184.4855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0195, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0117, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(340328.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.0000],
        [0.0019, 0.0016, 0.0000,  ..., 0.0488, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0163, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0194, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2275039., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1038.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1221.8497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64409.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1812.6729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1626.0454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0450],
        [ 0.0192],
        [-0.0141],
        ...,
        [-0.1130],
        [-0.1120],
        [-0.1116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-156876.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [-0.0021, -0.0103, -0.0071,  ...,  0.0071,  0.0065, -0.0083],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3665.6067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-86.0636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(492.6173, device='cuda:0')



h[100].sum tensor(466.7053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(171.4971, device='cuda:0')



h[200].sum tensor(-116.4007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(231.2013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0274, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0153, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0128, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(404729.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0694, 0.0000, 0.0000],
        [0.0016, 0.0007, 0.0000,  ..., 0.0521, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0200, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0200, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0200, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2611858.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2056.1870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1508., device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67524.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2534.5842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1921.8453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0849],
        [ 0.0745],
        [ 0.0596],
        ...,
        [-0.1181],
        [-0.1176],
        [-0.1174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-137821.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [-0.0018, -0.0089, -0.0061,  ...,  0.0063,  0.0057, -0.0072],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3800.6777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.0931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(402.9636, device='cuda:0')



h[100].sum tensor(378.6867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.2855, device='cuda:0')



h[200].sum tensor(-94.8889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.1239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0086, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0295, 0.0270, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(352231., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5999e-05, 0.0000e+00, 0.0000e+00,  ..., 4.8749e-02, 0.0000e+00,
         0.0000e+00],
        [3.9466e-03, 0.0000e+00, 0.0000e+00,  ..., 7.8304e-02, 0.0000e+00,
         0.0000e+00],
        [1.0864e-02, 0.0000e+00, 0.0000e+00,  ..., 1.4106e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.0910e-02, 0.0000e+00,  ..., 2.2927e-04, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0910e-02, 0.0000e+00,  ..., 2.2927e-04, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0910e-02, 0.0000e+00,  ..., 2.2927e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2357559., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1298.9506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1239.8762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64078.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1881.7178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1668.3281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0722],
        [ 0.0725],
        [ 0.0726],
        ...,
        [-0.1246],
        [-0.1241],
        [-0.1239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-188588.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3750.9644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-72.1295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(415.7305, device='cuda:0')



h[100].sum tensor(384.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(144.7301, device='cuda:0')



h[200].sum tensor(-97.7370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(195.1158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(359612.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0189, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        ...,
        [0.0013, 0.0078, 0.0000,  ..., 0.0443, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0216, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2402439.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1284.5219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1264.4535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63862.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1937.7603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1700.4893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0074],
        [-0.0222],
        [-0.0188],
        ...,
        [-0.0162],
        [-0.0568],
        [-0.0916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-211484.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 900 loss: tensor(1621.8871, device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        ...,
        [-0.0033, -0.0166, -0.0115,  ...,  0.0108,  0.0099, -0.0134],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3743.8164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-69.8545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(404.7842, device='cuda:0')



h[100].sum tensor(368.2227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.9194, device='cuda:0')



h[200].sum tensor(-94.7431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.9783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0247, 0.0226, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0197, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351568.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0123, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        [0.0000, 0.0202, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0222, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0130, 0.0000, 0.0000,  ..., 0.1632, 0.0000, 0.0000],
        [0.0095, 0.0000, 0.0000,  ..., 0.1378, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2370329.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(980.2595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1232.1587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63576.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1805.8999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1652.5658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0202],
        [-0.0432],
        [-0.0556],
        ...,
        [ 0.0687],
        [ 0.0635],
        [ 0.0556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-223932.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        ...,
        [-0.0042, -0.0209, -0.0144,  ...,  0.0134,  0.0122, -0.0169],
        [-0.0021, -0.0105, -0.0072,  ...,  0.0072,  0.0066, -0.0084],
        [-0.0024, -0.0120, -0.0083,  ...,  0.0081,  0.0074, -0.0097]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3702.7471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.2568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(408.2606, device='cuda:0')



h[100].sum tensor(366.9727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.1296, device='cuda:0')



h[200].sum tensor(-95.3785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(191.6099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0039, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0039, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0039, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0418, 0.0383, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0401, 0.0368, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0192, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353982.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9319e-02, 0.0000e+00,  ..., 3.8873e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2099e-02, 0.0000e+00,  ..., 8.9085e-04, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2830e-02, 0.0000e+00,  ..., 1.3099e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7450e-02, 0.0000e+00, 0.0000e+00,  ..., 2.0299e-01, 0.0000e+00,
         0.0000e+00],
        [1.6184e-02, 0.0000e+00, 0.0000e+00,  ..., 1.9216e-01, 0.0000e+00,
         0.0000e+00],
        [1.2135e-02, 0.0000e+00, 0.0000e+00,  ..., 1.5966e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2389971., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(961.6122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1239.3560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63265.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1816.3235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1658.0548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0526],
        [-0.0793],
        [-0.1037],
        ...,
        [ 0.0732],
        [ 0.0701],
        [ 0.0665]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-238303.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0066, -0.0046,  ...,  0.0049,  0.0046, -0.0053],
        [-0.0048, -0.0241, -0.0166,  ...,  0.0153,  0.0140, -0.0195],
        [-0.0019, -0.0097, -0.0067,  ...,  0.0067,  0.0062, -0.0078],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0010,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4226.8848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(139.8604, device='cuda:0')



h[100].sum tensor(121.5043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.6902, device='cuda:0')



h[200].sum tensor(-32.8261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(65.6410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0568, 0.0521, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0271, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0276, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0040, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0040, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(211173.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3584e-02, 0.0000e+00, 0.0000e+00,  ..., 1.7731e-01, 0.0000e+00,
         0.0000e+00],
        [1.0700e-02, 0.0000e+00, 0.0000e+00,  ..., 1.5206e-01, 0.0000e+00,
         0.0000e+00],
        [9.4043e-03, 0.0000e+00, 0.0000e+00,  ..., 1.4381e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.2985e-02, 0.0000e+00,  ..., 1.1027e-04, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2985e-02, 0.0000e+00,  ..., 1.1027e-04, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2985e-02, 0.0000e+00,  ..., 1.1027e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1712134.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(326.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-386.4572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50138.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(229.1855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1028.7402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0626],
        [ 0.0668],
        [ 0.0650],
        ...,
        [-0.1508],
        [-0.1501],
        [-0.1499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-456225.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4515.3213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-9.0670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0041, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(135992.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0234, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0234, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0234, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0232, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0000, 0.0232, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0000, 0.0232, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1363121.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.8324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43538.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-616.4651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-690.1980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2412],
        [-0.2436],
        [-0.2475],
        ...,
        [-0.1550],
        [-0.1543],
        [-0.1540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-570221.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4508.6499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-11.9736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(136585.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0236, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0235, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0235, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0235, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1370126., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.2510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43345.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-619.3289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.4249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2467],
        [-0.2492],
        [-0.2532],
        ...,
        [-0.1588],
        [-0.1580],
        [-0.1578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-580708.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4502.5723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-14.6208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(137125.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0238, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0238, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0237, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0237, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0237, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1376537.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.7223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43168.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-621.9370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-679.1675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2517],
        [-0.2541],
        [-0.2583],
        ...,
        [-0.1622],
        [-0.1615],
        [-0.1613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-590366.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4497.0342, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-17.0315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(137618., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0240, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0240, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0241, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1382414.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.2405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43007.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.3115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.3794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2561],
        [-0.2586],
        [-0.2628],
        ...,
        [-0.1654],
        [-0.1646],
        [-0.1644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-599153.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4497.0342, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-17.0315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(137618., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0240, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0240, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0241, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1382414.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.2405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43007.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.3115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.3794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2561],
        [-0.2586],
        [-0.2628],
        ...,
        [-0.1654],
        [-0.1646],
        [-0.1644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-599153.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4491.9937, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-19.2266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(138082.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0241, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0241, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0241, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1387819.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.9734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42871.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-625.4169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.2357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2601],
        [-0.2627],
        [-0.2670],
        ...,
        [-0.1682],
        [-0.1675],
        [-0.1672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-606990.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4487.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-21.2250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(138516.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0243, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0243, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0244, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0242, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1392786., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.6479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42754.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-625.6931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-661.4098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2638],
        [-0.2663],
        [-0.2706],
        ...,
        [-0.1708],
        [-0.1701],
        [-0.1698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-613557.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 1200 loss: tensor(396.2379, device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4483.2285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-23.0442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(138911.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0244, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0244, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0244, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1397340.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.6201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42648.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-625.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-655.1963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2670],
        [-0.2696],
        [-0.2740],
        ...,
        [-0.1732],
        [-0.1724],
        [-0.1722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-619104.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4479.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-24.7000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(139271.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0246, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0246, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0245, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1401502.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(42.8650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42551.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-626.1729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.5407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2700],
        [-0.2727],
        [-0.2770],
        ...,
        [-0.1754],
        [-0.1746],
        [-0.1743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-623832.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4475.9673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-26.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(139604.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0247, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0246, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0246, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0246, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1405257., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(39.9977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42472., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-626.4617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.8127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2727],
        [-0.2753],
        [-0.2798],
        ...,
        [-0.1773],
        [-0.1765],
        [-0.1763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-627986.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4472.8208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-27.5779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(139912.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0247, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1408640.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.1172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42406.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-626.7852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-640.8262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2751],
        [-0.2778],
        [-0.2822],
        ...,
        [-0.1791],
        [-0.1782],
        [-0.1780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-631692.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4469.9565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-28.8253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(140192.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0248, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1411723.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(34.4982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42346.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-627.0791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-637.1989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2772],
        [-0.2799],
        [-0.2844],
        ...,
        [-0.1806],
        [-0.1798],
        [-0.1796]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-635072.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4467.3506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-29.9600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(140447.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1414534.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42291.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-627.3466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-633.8995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2791],
        [-0.2818],
        [-0.2863],
        ...,
        [-0.1821],
        [-0.1812],
        [-0.1810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-638141.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4467.3506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-29.9600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(140447.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0249, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1414534.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42291.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-627.3466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-633.8995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2791],
        [-0.2818],
        [-0.2863],
        ...,
        [-0.1821],
        [-0.1812],
        [-0.1810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-638141.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4464.9829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-30.9920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(140678.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1417094.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(29.9470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42242.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-627.5900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.8984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2807],
        [-0.2834],
        [-0.2879],
        ...,
        [-0.1834],
        [-0.1825],
        [-0.1823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-640933.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4462.8271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-31.9304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(140889.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1419428.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(27.9753, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42197.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-627.8114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-628.1700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2821],
        [-0.2848],
        [-0.2894],
        ...,
        [-0.1845],
        [-0.1837],
        [-0.1835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-643468.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4460.8677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-32.7837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141081.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1421553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.1833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42156.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.0123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.6882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2834],
        [-0.2861],
        [-0.2907],
        ...,
        [-0.1856],
        [-0.1848],
        [-0.1845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-645771.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4459.0854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-33.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141255.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1423487.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.5535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42118.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-623.4330, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2845],
        [-0.2873],
        [-0.2918],
        ...,
        [-0.1865],
        [-0.1857],
        [-0.1854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-647865.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4459.0854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-33.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141255.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1423487.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.5535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42118.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-623.4330, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2845],
        [-0.2873],
        [-0.2918],
        ...,
        [-0.1865],
        [-0.1857],
        [-0.1854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-647865.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4457.4668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-34.2646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141413.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0252, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0252, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1425246.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(23.0728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42084.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.3613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-621.3828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2856],
        [-0.2883],
        [-0.2928],
        ...,
        [-0.1873],
        [-0.1865],
        [-0.1862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-649770., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4455.9971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-34.9054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141557.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1426847., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.7267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42053.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.5124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.5192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2865],
        [-0.2892],
        [-0.2938],
        ...,
        [-0.1881],
        [-0.1872],
        [-0.1870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-651503., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4454.6587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-35.4878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141688.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1428302., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.5037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42025.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.6504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.8254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2873],
        [-0.2900],
        [-0.2946],
        ...,
        [-0.1888],
        [-0.1879],
        [-0.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-653080., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4453.4434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-36.0169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141806.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1429626.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.3928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42000.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.7748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.2869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2880],
        [-0.2908],
        [-0.2954],
        ...,
        [-0.1894],
        [-0.1886],
        [-0.1883]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-654512.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4452.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-36.4976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(141914.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1430833.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.3826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41976.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.8881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.8893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2887],
        [-0.2914],
        [-0.2960],
        ...,
        [-0.1899],
        [-0.1891],
        [-0.1889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-655813.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4451.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-36.9342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142012.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1431932., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.4653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41955.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-628.9915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-613.6198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2893],
        [-0.2920],
        [-0.2966],
        ...,
        [-0.1904],
        [-0.1896],
        [-0.1893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-656995.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4450.4287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-37.3307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142101.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1432930.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.6331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41936.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.0846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.4668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2898],
        [-0.2925],
        [-0.2971],
        ...,
        [-0.1909],
        [-0.1901],
        [-0.1898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-658069.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4449.6025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-37.6908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142182.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1433839.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.8770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41919.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.1692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-611.4197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2902],
        [-0.2930],
        [-0.2976],
        ...,
        [-0.1913],
        [-0.1905],
        [-0.1902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-659044.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4448.8530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-38.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142256.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1434665., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41903.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.2466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.4686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2907],
        [-0.2934],
        [-0.2980],
        ...,
        [-0.1917],
        [-0.1908],
        [-0.1906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-659929.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4448.8530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-38.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142256.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1434665., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41903.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.2466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.4686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2907],
        [-0.2934],
        [-0.2980],
        ...,
        [-0.1917],
        [-0.1908],
        [-0.1906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-659929.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4448.1675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-38.3146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142322.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1435416.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.5658, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41888.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.3167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-609.6060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2911],
        [-0.2938],
        [-0.2984],
        ...,
        [-0.1920],
        [-0.1912],
        [-0.1909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-660732.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4447.5508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-38.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0255, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1436100.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14., device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41875.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.3809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.8224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2914],
        [-0.2942],
        [-0.2988],
        ...,
        [-0.1923],
        [-0.1915],
        [-0.1912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-661459.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-4446.9883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(-38.8287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142438.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1436721.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(13.4863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41863.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.4380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.1111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2917],
        [-0.2945],
        [-0.2991],
        ...,
        [-0.1926],
        [-0.1918],
        [-0.1915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-662118.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
30 [   0   30   60   90  120  150  180  210  240  270  300  330  360  390
  420  450  480  510  540  570  600  630  660  690  720  750  780  810
  840  870  900  930  960  990 1020 1050 1080 1110 1140 1170 1200 1230
 1260 1290 1320 1350 1380 1410 1440 1470 1500 1530 1560 1590 1620 1650
 1680 1710 1740 1770 1800 1830 1860 1890 1920 1950 1980 2010 2040 2070
 2100 2130 2160 2190 2220 2250 2280 2310 2340 2370 2400 2430 2460 2490
 2520 2550 2580 2610 2640 2670 2700 2730 2760 2790 2820 2850 2880 2910
 2940 2970 3000] tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4993, 0.3171,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2974,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N3/./Training.py", line 79, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[203880, 1]' is invalid for input of size 135920

real	0m21.134s
user	0m13.261s
sys	0m5.071s
