0: gpu034.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-07f659b6-fc78-8e02-1667-20934fa516c9)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Sun Aug 14 01:47:45 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |
| N/A   32C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b40000308e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m3.671s
user	0m1.853s
sys	0m0.586s
[01:47:50] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.3197],
        [ 0.8944],
        [ 1.4141],
        ...,
        [ 1.2298],
        [-1.4124],
        [ 1.3723]], device='cuda:0', requires_grad=True) 
node features sum: tensor(130.3466, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N3
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0360,  0.0565, -0.1029,  0.1195, -0.0326,  0.1002,  0.0981, -0.1476,
         -0.1299, -0.1101, -0.1365, -0.0509,  0.0662, -0.0548,  0.0986,  0.0730,
          0.0614,  0.1090,  0.1329,  0.0395, -0.1473, -0.0056,  0.0763, -0.1086,
         -0.1362, -0.1253, -0.1502,  0.1138, -0.0640,  0.1380, -0.0865, -0.0851,
         -0.0311, -0.0949, -0.0581,  0.0651,  0.0486,  0.0064, -0.0392,  0.1507,
          0.0887, -0.1351,  0.0731, -0.0055, -0.1017, -0.1408,  0.0949,  0.0028,
         -0.1267,  0.0460, -0.1395,  0.0143,  0.1360, -0.0389, -0.0891,  0.0196,
          0.1041,  0.0682, -0.0221,  0.0109,  0.0882,  0.0767,  0.1291,  0.0808,
         -0.0714, -0.0874, -0.0028,  0.1461,  0.0467, -0.0165,  0.1242, -0.1382,
          0.1433, -0.0325, -0.0136,  0.0642, -0.1354,  0.1227, -0.1473, -0.0894,
          0.0026,  0.0826, -0.0342, -0.0068,  0.1258,  0.1418,  0.0762,  0.1400,
         -0.0559, -0.0024, -0.0536,  0.1412, -0.0654,  0.1227, -0.0030, -0.0754,
         -0.1088, -0.0299, -0.0402, -0.0798, -0.1278, -0.0351,  0.1144,  0.0618,
         -0.0715, -0.0291,  0.1060, -0.0297,  0.0651,  0.0442,  0.1438, -0.0779,
         -0.0358, -0.1310, -0.1105,  0.0510,  0.0599, -0.0428,  0.0331,  0.0683,
         -0.0614, -0.1346, -0.0321,  0.1321, -0.0909,  0.0781, -0.1057,  0.0348,
         -0.0185,  0.0982,  0.0200, -0.1066,  0.0872, -0.0191, -0.1463, -0.1444,
          0.1231, -0.1462,  0.1152, -0.1165,  0.0505, -0.1072, -0.0039, -0.0367,
          0.0642,  0.0079, -0.0277,  0.0212,  0.0221, -0.0487,  0.0891, -0.1459,
         -0.1155,  0.0934, -0.0961, -0.0961,  0.0721,  0.1186,  0.1421,  0.1491,
          0.0850, -0.0506, -0.0633, -0.1432, -0.0444, -0.0486,  0.1006, -0.0953,
         -0.0577,  0.1527,  0.0490,  0.1249, -0.0783,  0.1206,  0.0106,  0.1266,
         -0.0666, -0.0694, -0.1231, -0.1106, -0.1105, -0.1527, -0.1306,  0.0535,
         -0.0613,  0.1186, -0.1403, -0.1475, -0.0885,  0.0978,  0.0462, -0.0353,
         -0.0406,  0.0378, -0.0893,  0.0213,  0.0485, -0.1036,  0.0020,  0.0477,
         -0.0416, -0.0227,  0.0449, -0.0528, -0.0498, -0.1475,  0.1383,  0.0319,
         -0.0527, -0.0494, -0.1464,  0.0886, -0.0145,  0.0716, -0.1089, -0.0159,
          0.0301, -0.1352, -0.0556, -0.0684,  0.1430, -0.0271,  0.0788, -0.1006,
         -0.0383, -0.0574,  0.0759, -0.0723, -0.0500, -0.0096, -0.0840, -0.1313,
          0.1078,  0.0868, -0.0559, -0.0519, -0.0741, -0.1120,  0.0969,  0.0014,
         -0.1299, -0.1144, -0.1401, -0.1439, -0.0844, -0.0068,  0.0113, -0.0797,
          0.0741,  0.0711,  0.1469,  0.0923,  0.0009, -0.0785, -0.0913,  0.1160]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0360,  0.0565, -0.1029,  0.1195, -0.0326,  0.1002,  0.0981, -0.1476,
         -0.1299, -0.1101, -0.1365, -0.0509,  0.0662, -0.0548,  0.0986,  0.0730,
          0.0614,  0.1090,  0.1329,  0.0395, -0.1473, -0.0056,  0.0763, -0.1086,
         -0.1362, -0.1253, -0.1502,  0.1138, -0.0640,  0.1380, -0.0865, -0.0851,
         -0.0311, -0.0949, -0.0581,  0.0651,  0.0486,  0.0064, -0.0392,  0.1507,
          0.0887, -0.1351,  0.0731, -0.0055, -0.1017, -0.1408,  0.0949,  0.0028,
         -0.1267,  0.0460, -0.1395,  0.0143,  0.1360, -0.0389, -0.0891,  0.0196,
          0.1041,  0.0682, -0.0221,  0.0109,  0.0882,  0.0767,  0.1291,  0.0808,
         -0.0714, -0.0874, -0.0028,  0.1461,  0.0467, -0.0165,  0.1242, -0.1382,
          0.1433, -0.0325, -0.0136,  0.0642, -0.1354,  0.1227, -0.1473, -0.0894,
          0.0026,  0.0826, -0.0342, -0.0068,  0.1258,  0.1418,  0.0762,  0.1400,
         -0.0559, -0.0024, -0.0536,  0.1412, -0.0654,  0.1227, -0.0030, -0.0754,
         -0.1088, -0.0299, -0.0402, -0.0798, -0.1278, -0.0351,  0.1144,  0.0618,
         -0.0715, -0.0291,  0.1060, -0.0297,  0.0651,  0.0442,  0.1438, -0.0779,
         -0.0358, -0.1310, -0.1105,  0.0510,  0.0599, -0.0428,  0.0331,  0.0683,
         -0.0614, -0.1346, -0.0321,  0.1321, -0.0909,  0.0781, -0.1057,  0.0348,
         -0.0185,  0.0982,  0.0200, -0.1066,  0.0872, -0.0191, -0.1463, -0.1444,
          0.1231, -0.1462,  0.1152, -0.1165,  0.0505, -0.1072, -0.0039, -0.0367,
          0.0642,  0.0079, -0.0277,  0.0212,  0.0221, -0.0487,  0.0891, -0.1459,
         -0.1155,  0.0934, -0.0961, -0.0961,  0.0721,  0.1186,  0.1421,  0.1491,
          0.0850, -0.0506, -0.0633, -0.1432, -0.0444, -0.0486,  0.1006, -0.0953,
         -0.0577,  0.1527,  0.0490,  0.1249, -0.0783,  0.1206,  0.0106,  0.1266,
         -0.0666, -0.0694, -0.1231, -0.1106, -0.1105, -0.1527, -0.1306,  0.0535,
         -0.0613,  0.1186, -0.1403, -0.1475, -0.0885,  0.0978,  0.0462, -0.0353,
         -0.0406,  0.0378, -0.0893,  0.0213,  0.0485, -0.1036,  0.0020,  0.0477,
         -0.0416, -0.0227,  0.0449, -0.0528, -0.0498, -0.1475,  0.1383,  0.0319,
         -0.0527, -0.0494, -0.1464,  0.0886, -0.0145,  0.0716, -0.1089, -0.0159,
          0.0301, -0.1352, -0.0556, -0.0684,  0.1430, -0.0271,  0.0788, -0.1006,
         -0.0383, -0.0574,  0.0759, -0.0723, -0.0500, -0.0096, -0.0840, -0.1313,
          0.1078,  0.0868, -0.0559, -0.0519, -0.0741, -0.1120,  0.0969,  0.0014,
         -0.1299, -0.1144, -0.1401, -0.1439, -0.0844, -0.0068,  0.0113, -0.0797,
          0.0741,  0.0711,  0.1469,  0.0923,  0.0009, -0.0785, -0.0913,  0.1160]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0903, -0.0891,  0.0550,  ..., -0.1073,  0.1030, -0.1169],
        [-0.0772,  0.0140,  0.0355,  ..., -0.1222, -0.0176, -0.0156],
        [-0.0906, -0.0819, -0.1033,  ..., -0.0128,  0.0559,  0.0095],
        ...,
        [-0.1231, -0.0072,  0.0717,  ..., -0.0891, -0.0044, -0.0084],
        [-0.1198,  0.0622,  0.0699,  ...,  0.0041,  0.0729, -0.0843],
        [-0.0213, -0.1108, -0.1247,  ...,  0.1056,  0.0500, -0.0526]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0903, -0.0891,  0.0550,  ..., -0.1073,  0.1030, -0.1169],
        [-0.0772,  0.0140,  0.0355,  ..., -0.1222, -0.0176, -0.0156],
        [-0.0906, -0.0819, -0.1033,  ..., -0.0128,  0.0559,  0.0095],
        ...,
        [-0.1231, -0.0072,  0.0717,  ..., -0.0891, -0.0044, -0.0084],
        [-0.1198,  0.0622,  0.0699,  ...,  0.0041,  0.0729, -0.0843],
        [-0.0213, -0.1108, -0.1247,  ...,  0.1056,  0.0500, -0.0526]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0752,  0.0150, -0.1012,  ..., -0.1012, -0.0802, -0.0344],
        [ 0.1753,  0.0288,  0.0770,  ...,  0.1511,  0.1024,  0.1007],
        [-0.0435, -0.1263, -0.0531,  ..., -0.0568,  0.1120,  0.1209],
        ...,
        [ 0.0064,  0.1467,  0.0446,  ..., -0.0614,  0.0912, -0.0577],
        [-0.0977,  0.0371,  0.1475,  ...,  0.0750, -0.1058,  0.1276],
        [-0.1711,  0.0043,  0.0500,  ..., -0.0963, -0.0011,  0.1513]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0752,  0.0150, -0.1012,  ..., -0.1012, -0.0802, -0.0344],
        [ 0.1753,  0.0288,  0.0770,  ...,  0.1511,  0.1024,  0.1007],
        [-0.0435, -0.1263, -0.0531,  ..., -0.0568,  0.1120,  0.1209],
        ...,
        [ 0.0064,  0.1467,  0.0446,  ..., -0.0614,  0.0912, -0.0577],
        [-0.0977,  0.0371,  0.1475,  ...,  0.0750, -0.1058,  0.1276],
        [-0.1711,  0.0043,  0.0500,  ..., -0.0963, -0.0011,  0.1513]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0631, -0.1205,  0.0571,  ...,  0.1639, -0.1793,  0.1735],
        [ 0.1733,  0.1648,  0.0153,  ..., -0.0332,  0.1968,  0.1182],
        [ 0.0632,  0.0634, -0.1016,  ...,  0.1675,  0.0909, -0.0515],
        ...,
        [-0.1710,  0.0803, -0.0908,  ..., -0.0647, -0.0157, -0.0597],
        [ 0.0098, -0.0333, -0.0968,  ..., -0.2401,  0.1190, -0.0981],
        [-0.1019, -0.0067, -0.0998,  ..., -0.0463,  0.0871,  0.1337]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0631, -0.1205,  0.0571,  ...,  0.1639, -0.1793,  0.1735],
        [ 0.1733,  0.1648,  0.0153,  ..., -0.0332,  0.1968,  0.1182],
        [ 0.0632,  0.0634, -0.1016,  ...,  0.1675,  0.0909, -0.0515],
        ...,
        [-0.1710,  0.0803, -0.0908,  ..., -0.0647, -0.0157, -0.0597],
        [ 0.0098, -0.0333, -0.0968,  ..., -0.2401,  0.1190, -0.0981],
        [-0.1019, -0.0067, -0.0998,  ..., -0.0463,  0.0871,  0.1337]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2703],
        [-0.0275],
        [-0.1751],
        [ 0.0056],
        [-0.2337],
        [-0.1886],
        [-0.1500],
        [-0.0912],
        [ 0.3884],
        [ 0.0412],
        [ 0.3714],
        [-0.3970],
        [ 0.2507],
        [-0.3098],
        [-0.0832],
        [ 0.2932],
        [ 0.3692],
        [-0.2918],
        [-0.2121],
        [ 0.2923],
        [-0.1370],
        [-0.2031],
        [-0.1482],
        [ 0.3549],
        [ 0.0684],
        [-0.3501],
        [ 0.2695],
        [ 0.1297],
        [-0.2155],
        [ 0.0663],
        [-0.0206],
        [ 0.2661]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2703],
        [-0.0275],
        [-0.1751],
        [ 0.0056],
        [-0.2337],
        [-0.1886],
        [-0.1500],
        [-0.0912],
        [ 0.3884],
        [ 0.0412],
        [ 0.3714],
        [-0.3970],
        [ 0.2507],
        [-0.3098],
        [-0.0832],
        [ 0.2932],
        [ 0.3692],
        [-0.2918],
        [-0.2121],
        [ 0.2923],
        [-0.1370],
        [-0.2031],
        [-0.1482],
        [ 0.3549],
        [ 0.0684],
        [-0.3501],
        [ 0.2695],
        [ 0.1297],
        [-0.2155],
        [ 0.0663],
        [-0.0206],
        [ 0.2661]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 4.0908e-03, -1.1195e-01,  1.3852e-01,  7.5852e-02,  1.3857e-01,
         -6.1181e-03, -8.9298e-02,  1.0969e-01,  3.9874e-02, -1.0814e-01,
          8.1799e-02,  1.1304e-01, -7.7789e-02, -3.3773e-02, -1.2844e-01,
          7.8414e-02, -8.4615e-02,  2.0739e-03, -1.4969e-01, -1.0684e-01,
          6.1663e-02, -8.9949e-02, -4.3516e-02,  1.3960e-01, -5.6935e-02,
          1.4452e-01,  1.3590e-01,  1.0366e-01, -1.4122e-01, -6.3831e-02,
         -6.7388e-03, -5.6122e-02,  4.1456e-02, -1.0649e-01,  6.0231e-02,
          9.9094e-03,  1.2936e-02, -6.5357e-02, -6.5905e-02, -4.6710e-02,
         -1.0589e-02, -3.4372e-02, -5.8870e-02, -9.0677e-02, -8.2321e-02,
          3.3103e-02,  4.4406e-02, -1.1398e-01, -6.7285e-02, -9.8844e-02,
          5.9568e-02, -9.8744e-02, -9.6287e-02, -5.8738e-02, -9.8561e-02,
         -7.3002e-03,  1.0204e-01, -1.2173e-01, -6.1020e-02, -1.2054e-01,
          1.0302e-01, -1.3113e-01,  1.1682e-01, -8.1360e-02,  7.1713e-02,
         -4.1289e-02,  7.6996e-02,  1.2062e-01, -3.8984e-02,  3.6616e-02,
          4.2197e-02,  5.0359e-03, -1.4241e-01,  9.6298e-02,  5.2419e-02,
          1.3234e-01,  1.0959e-01,  8.9012e-02, -1.0733e-02,  3.1109e-02,
          4.7948e-03, -1.4519e-01,  1.0342e-01, -9.7160e-02,  1.4323e-01,
          2.3324e-02, -2.0519e-03, -1.0477e-01,  8.5184e-02, -5.0913e-02,
          1.7755e-02, -1.6124e-02, -2.6379e-02,  1.3758e-01,  4.6041e-02,
         -2.5962e-02, -1.0622e-01,  1.3780e-01,  3.3627e-02,  2.2805e-02,
         -5.8576e-02,  5.3639e-02,  1.4381e-01, -1.4623e-01, -4.6866e-02,
          4.6667e-02, -2.1588e-03, -4.1349e-02, -1.3614e-02, -1.1827e-01,
         -1.1336e-01,  9.4935e-02, -1.3949e-01, -2.9670e-02,  3.2995e-02,
         -8.3086e-02,  6.4564e-03, -6.9109e-02, -8.2710e-02,  3.9270e-02,
          2.3840e-02,  6.2925e-02,  1.2666e-01, -1.3136e-01,  8.9022e-02,
          2.2870e-02,  5.3583e-03, -2.2795e-02,  6.0801e-03, -7.8519e-02,
         -2.0167e-02,  4.1988e-03, -1.2258e-01,  1.2253e-01, -1.1354e-01,
          1.2495e-01, -4.7085e-02, -1.4207e-01, -7.9401e-02, -1.2557e-02,
         -7.1306e-02, -1.1550e-01, -1.2223e-01,  3.9514e-02,  3.4328e-02,
          9.2340e-02,  1.2966e-01,  1.2572e-04, -1.2985e-01, -1.0595e-01,
          8.9280e-02,  9.2858e-02,  7.6507e-02,  6.0134e-02, -1.2013e-01,
         -6.1193e-02, -1.2867e-01,  3.7968e-02,  1.3754e-01,  4.6080e-02,
          3.7539e-02,  3.4649e-02, -1.1350e-01,  5.1929e-02,  2.1878e-02,
         -1.2314e-01, -7.1779e-02,  8.4982e-02,  6.6501e-02, -6.9519e-02,
         -1.2434e-01, -9.0746e-02,  9.9772e-02,  1.2220e-01,  1.4957e-01,
          2.9910e-02, -1.9887e-02, -3.9716e-02,  5.6150e-02, -5.8590e-02,
         -1.9119e-02,  9.5859e-02,  1.4003e-01, -1.1267e-01,  8.4550e-02,
         -1.2393e-02,  5.0780e-03, -8.8716e-02, -8.6402e-02, -3.5291e-02,
         -3.9168e-02, -1.0974e-01, -4.8969e-02, -5.5312e-02,  1.0084e-02,
          4.0648e-02,  5.4943e-02, -8.2439e-02,  1.6919e-02,  7.6223e-02,
         -8.5354e-02,  3.8564e-02, -1.2275e-01,  4.1139e-02,  1.0046e-01,
          1.3043e-01, -6.2444e-02, -5.9590e-03,  2.7398e-03,  1.0243e-03,
         -9.1982e-02,  1.3155e-01,  2.6226e-02, -1.2673e-01, -2.0822e-02,
          8.9679e-02, -1.4274e-01,  7.0304e-02,  1.5264e-01, -5.2366e-02,
          9.4223e-03,  2.8134e-02,  1.4258e-04, -1.7411e-02,  4.8360e-02,
          4.4923e-02,  7.6239e-02,  5.9349e-02, -1.2129e-01, -1.0128e-01,
         -1.2785e-01,  6.4417e-02,  7.6484e-02, -6.5714e-03,  3.6598e-02,
          5.2744e-02,  5.7818e-02,  1.4198e-02,  2.0547e-02, -1.4161e-01,
         -1.3472e-01,  1.0596e-01, -8.5149e-02,  1.1114e-01,  1.4462e-01,
          3.2271e-02,  1.1738e-01,  1.4350e-01,  4.2992e-02, -9.0720e-02,
         -4.0539e-02, -7.3380e-02,  9.9009e-02, -3.8870e-02,  1.2238e-02,
         -4.9035e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 4.0908e-03, -1.1195e-01,  1.3852e-01,  7.5852e-02,  1.3857e-01,
         -6.1181e-03, -8.9298e-02,  1.0969e-01,  3.9874e-02, -1.0814e-01,
          8.1799e-02,  1.1304e-01, -7.7789e-02, -3.3773e-02, -1.2844e-01,
          7.8414e-02, -8.4615e-02,  2.0739e-03, -1.4969e-01, -1.0684e-01,
          6.1663e-02, -8.9949e-02, -4.3516e-02,  1.3960e-01, -5.6935e-02,
          1.4452e-01,  1.3590e-01,  1.0366e-01, -1.4122e-01, -6.3831e-02,
         -6.7388e-03, -5.6122e-02,  4.1456e-02, -1.0649e-01,  6.0231e-02,
          9.9094e-03,  1.2936e-02, -6.5357e-02, -6.5905e-02, -4.6710e-02,
         -1.0589e-02, -3.4372e-02, -5.8870e-02, -9.0677e-02, -8.2321e-02,
          3.3103e-02,  4.4406e-02, -1.1398e-01, -6.7285e-02, -9.8844e-02,
          5.9568e-02, -9.8744e-02, -9.6287e-02, -5.8738e-02, -9.8561e-02,
         -7.3002e-03,  1.0204e-01, -1.2173e-01, -6.1020e-02, -1.2054e-01,
          1.0302e-01, -1.3113e-01,  1.1682e-01, -8.1360e-02,  7.1713e-02,
         -4.1289e-02,  7.6996e-02,  1.2062e-01, -3.8984e-02,  3.6616e-02,
          4.2197e-02,  5.0359e-03, -1.4241e-01,  9.6298e-02,  5.2419e-02,
          1.3234e-01,  1.0959e-01,  8.9012e-02, -1.0733e-02,  3.1109e-02,
          4.7948e-03, -1.4519e-01,  1.0342e-01, -9.7160e-02,  1.4323e-01,
          2.3324e-02, -2.0519e-03, -1.0477e-01,  8.5184e-02, -5.0913e-02,
          1.7755e-02, -1.6124e-02, -2.6379e-02,  1.3758e-01,  4.6041e-02,
         -2.5962e-02, -1.0622e-01,  1.3780e-01,  3.3627e-02,  2.2805e-02,
         -5.8576e-02,  5.3639e-02,  1.4381e-01, -1.4623e-01, -4.6866e-02,
          4.6667e-02, -2.1588e-03, -4.1349e-02, -1.3614e-02, -1.1827e-01,
         -1.1336e-01,  9.4935e-02, -1.3949e-01, -2.9670e-02,  3.2995e-02,
         -8.3086e-02,  6.4564e-03, -6.9109e-02, -8.2710e-02,  3.9270e-02,
          2.3840e-02,  6.2925e-02,  1.2666e-01, -1.3136e-01,  8.9022e-02,
          2.2870e-02,  5.3583e-03, -2.2795e-02,  6.0801e-03, -7.8519e-02,
         -2.0167e-02,  4.1988e-03, -1.2258e-01,  1.2253e-01, -1.1354e-01,
          1.2495e-01, -4.7085e-02, -1.4207e-01, -7.9401e-02, -1.2557e-02,
         -7.1306e-02, -1.1550e-01, -1.2223e-01,  3.9514e-02,  3.4328e-02,
          9.2340e-02,  1.2966e-01,  1.2572e-04, -1.2985e-01, -1.0595e-01,
          8.9280e-02,  9.2858e-02,  7.6507e-02,  6.0134e-02, -1.2013e-01,
         -6.1193e-02, -1.2867e-01,  3.7968e-02,  1.3754e-01,  4.6080e-02,
          3.7539e-02,  3.4649e-02, -1.1350e-01,  5.1929e-02,  2.1878e-02,
         -1.2314e-01, -7.1779e-02,  8.4982e-02,  6.6501e-02, -6.9519e-02,
         -1.2434e-01, -9.0746e-02,  9.9772e-02,  1.2220e-01,  1.4957e-01,
          2.9910e-02, -1.9887e-02, -3.9716e-02,  5.6150e-02, -5.8590e-02,
         -1.9119e-02,  9.5859e-02,  1.4003e-01, -1.1267e-01,  8.4550e-02,
         -1.2393e-02,  5.0780e-03, -8.8716e-02, -8.6402e-02, -3.5291e-02,
         -3.9168e-02, -1.0974e-01, -4.8969e-02, -5.5312e-02,  1.0084e-02,
          4.0648e-02,  5.4943e-02, -8.2439e-02,  1.6919e-02,  7.6223e-02,
         -8.5354e-02,  3.8564e-02, -1.2275e-01,  4.1139e-02,  1.0046e-01,
          1.3043e-01, -6.2444e-02, -5.9590e-03,  2.7398e-03,  1.0243e-03,
         -9.1982e-02,  1.3155e-01,  2.6226e-02, -1.2673e-01, -2.0822e-02,
          8.9679e-02, -1.4274e-01,  7.0304e-02,  1.5264e-01, -5.2366e-02,
          9.4223e-03,  2.8134e-02,  1.4258e-04, -1.7411e-02,  4.8360e-02,
          4.4923e-02,  7.6239e-02,  5.9349e-02, -1.2129e-01, -1.0128e-01,
         -1.2785e-01,  6.4417e-02,  7.6484e-02, -6.5714e-03,  3.6598e-02,
          5.2744e-02,  5.7818e-02,  1.4198e-02,  2.0547e-02, -1.4161e-01,
         -1.3472e-01,  1.0596e-01, -8.5149e-02,  1.1114e-01,  1.4462e-01,
          3.2271e-02,  1.1738e-01,  1.4350e-01,  4.2992e-02, -9.0720e-02,
         -4.0539e-02, -7.3380e-02,  9.9009e-02, -3.8870e-02,  1.2238e-02,
         -4.9035e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0060,  0.0648,  0.0814,  ..., -0.0272, -0.0577,  0.0471],
        [-0.0012, -0.0151, -0.0401,  ...,  0.0668, -0.0248,  0.0248],
        [-0.0416,  0.0476,  0.0739,  ...,  0.1235, -0.0846, -0.0148],
        ...,
        [ 0.1011, -0.0922, -0.0748,  ...,  0.0844,  0.0264, -0.0370],
        [-0.1092,  0.0903,  0.0777,  ...,  0.0698,  0.0948,  0.0452],
        [ 0.0613,  0.0555, -0.0955,  ..., -0.0329, -0.0388, -0.0128]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0060,  0.0648,  0.0814,  ..., -0.0272, -0.0577,  0.0471],
        [-0.0012, -0.0151, -0.0401,  ...,  0.0668, -0.0248,  0.0248],
        [-0.0416,  0.0476,  0.0739,  ...,  0.1235, -0.0846, -0.0148],
        ...,
        [ 0.1011, -0.0922, -0.0748,  ...,  0.0844,  0.0264, -0.0370],
        [-0.1092,  0.0903,  0.0777,  ...,  0.0698,  0.0948,  0.0452],
        [ 0.0613,  0.0555, -0.0955,  ..., -0.0329, -0.0388, -0.0128]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0866,  0.0549,  0.0255,  ...,  0.0132,  0.1508,  0.0733],
        [ 0.0287, -0.0833, -0.0903,  ..., -0.0717, -0.0441, -0.0921],
        [ 0.0544,  0.0928,  0.0332,  ...,  0.1075, -0.1694, -0.0522],
        ...,
        [-0.1693,  0.0061,  0.0408,  ..., -0.0503, -0.1442, -0.0740],
        [-0.0159, -0.0991, -0.1076,  ...,  0.1167, -0.0795,  0.1686],
        [-0.1752, -0.0731,  0.1251,  ...,  0.1434, -0.1398, -0.0123]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0866,  0.0549,  0.0255,  ...,  0.0132,  0.1508,  0.0733],
        [ 0.0287, -0.0833, -0.0903,  ..., -0.0717, -0.0441, -0.0921],
        [ 0.0544,  0.0928,  0.0332,  ...,  0.1075, -0.1694, -0.0522],
        ...,
        [-0.1693,  0.0061,  0.0408,  ..., -0.0503, -0.1442, -0.0740],
        [-0.0159, -0.0991, -0.1076,  ...,  0.1167, -0.0795,  0.1686],
        [-0.1752, -0.0731,  0.1251,  ...,  0.1434, -0.1398, -0.0123]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1547, -0.0570, -0.0225,  ...,  0.0443, -0.1943,  0.1342],
        [ 0.0009, -0.1371,  0.2151,  ..., -0.0789, -0.0219, -0.1364],
        [-0.1472,  0.2081, -0.0177,  ...,  0.0987,  0.0498,  0.1468],
        ...,
        [-0.1145,  0.1540, -0.1813,  ...,  0.0109,  0.1969, -0.0596],
        [-0.0310, -0.1305, -0.0152,  ...,  0.1649, -0.0292,  0.1352],
        [ 0.1064, -0.1968, -0.0426,  ..., -0.0906,  0.0508, -0.0039]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1547, -0.0570, -0.0225,  ...,  0.0443, -0.1943,  0.1342],
        [ 0.0009, -0.1371,  0.2151,  ..., -0.0789, -0.0219, -0.1364],
        [-0.1472,  0.2081, -0.0177,  ...,  0.0987,  0.0498,  0.1468],
        ...,
        [-0.1145,  0.1540, -0.1813,  ...,  0.0109,  0.1969, -0.0596],
        [-0.0310, -0.1305, -0.0152,  ...,  0.1649, -0.0292,  0.1352],
        [ 0.1064, -0.1968, -0.0426,  ..., -0.0906,  0.0508, -0.0039]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3483],
        [ 0.2417],
        [-0.2390],
        [ 0.3932],
        [-0.2739],
        [ 0.2868],
        [-0.2693],
        [-0.3737],
        [-0.0597],
        [ 0.0550],
        [ 0.2770],
        [ 0.1165],
        [ 0.1430],
        [ 0.1621],
        [-0.0778],
        [-0.0700],
        [ 0.2141],
        [ 0.3683],
        [ 0.3231],
        [-0.3778],
        [ 0.2506],
        [ 0.2284],
        [-0.2830],
        [-0.2444],
        [ 0.0523],
        [-0.0759],
        [-0.3897],
        [ 0.1041],
        [ 0.2330],
        [ 0.0646],
        [ 0.0027],
        [-0.1762]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3483],
        [ 0.2417],
        [-0.2390],
        [ 0.3932],
        [-0.2739],
        [ 0.2868],
        [-0.2693],
        [-0.3737],
        [-0.0597],
        [ 0.0550],
        [ 0.2770],
        [ 0.1165],
        [ 0.1430],
        [ 0.1621],
        [-0.0778],
        [-0.0700],
        [ 0.2141],
        [ 0.3683],
        [ 0.3231],
        [-0.3778],
        [ 0.2506],
        [ 0.2284],
        [-0.2830],
        [-0.2444],
        [ 0.0523],
        [-0.0759],
        [-0.3897],
        [ 0.1041],
        [ 0.2330],
        [ 0.0646],
        [ 0.0027],
        [-0.1762]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0052,  0.0027,  ...,  0.0015,  0.0032, -0.0089],
        [ 0.0172,  0.0171,  0.0090,  ...,  0.0050,  0.0105, -0.0296],
        [ 0.0114,  0.0113,  0.0060,  ...,  0.0033,  0.0069, -0.0197],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(80.4933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.4491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0115, device='cuda:0')



h[100].sum tensor(13.1953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7089, device='cuda:0')



h[200].sum tensor(5.9857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0396, 0.0392, 0.0207,  ..., 0.0115, 0.0240, 0.0000],
        [0.0348, 0.0345, 0.0183,  ..., 0.0101, 0.0211, 0.0000],
        [0.0479, 0.0474, 0.0251,  ..., 0.0139, 0.0290, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(19726.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0000, 0.0951,  ..., 0.0000, 0.0625, 0.0000],
        [0.0686, 0.0000, 0.0933,  ..., 0.0000, 0.0613, 0.0000],
        [0.0689, 0.0000, 0.0937,  ..., 0.0000, 0.0616, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(100543.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1124.4736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(71.7398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-193.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-77.3489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2046],
        [-0.1970],
        [-0.1769],
        ...,
        [-0.0015],
        [-0.0016],
        [-0.0016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-10520.1738, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.2046],
        [-0.1970],
        [-0.1769],
        ...,
        [-0.0015],
        [-0.0016],
        [-0.0016]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.5499, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0022,  0.0014,  ..., -0.0039,  0.0138, -0.0093],
        [ 0.0033,  0.0049,  0.0031,  ..., -0.0088,  0.0308, -0.0208],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-18.7018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.7355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(3.8151, device='cuda:0')



h[100].sum tensor(-14.1546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4564, device='cuda:0')



h[200].sum tensor(-3.8773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-3.9600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0194, 0.0124,  ..., 0.0000, 0.1229, 0.0000],
        [0.0066, 0.0098, 0.0063,  ..., 0.0000, 0.0619, 0.0000],
        [0.0074, 0.0110, 0.0070,  ..., 0.0000, 0.0695, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(28616.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2717, 0.0621, 0.0356,  ..., 0.1664, 0.0678, 0.0000],
        [0.2075, 0.0474, 0.0272,  ..., 0.1271, 0.0518, 0.0000],
        [0.1911, 0.0437, 0.0250,  ..., 0.1171, 0.0477, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(187028.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3090.5222, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.9020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2820.9292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.3302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5596.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(385.5054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7875e+00],
        [-2.4109e+00],
        [-2.1034e+00],
        ...,
        [-3.4203e-05],
        [-5.6867e-05],
        [-8.1288e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-60104.4766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2046],
        [-0.1970],
        [-0.1769],
        ...,
        [-0.0015],
        [-0.0016],
        [-0.0016]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 3001 
BatchSize 30 
EpochNum 60 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-4.6367e-02,  1.4229e-01,  1.2363e-01, -9.1626e-02, -1.4148e-01,
          4.4501e-02,  1.2770e-01, -1.2739e-01, -1.1096e-01,  1.1290e-01,
          1.1593e-01,  8.8623e-02, -7.3719e-02, -1.1763e-01, -1.0430e-01,
         -1.1870e-02, -2.6326e-02,  2.8448e-02, -1.3875e-01,  1.1674e-01,
          6.4767e-02,  1.4463e-02, -4.5336e-02,  1.3265e-01,  1.2215e-01,
          1.1340e-02, -4.9207e-02, -1.3903e-01,  1.1123e-01,  1.4337e-02,
         -6.5248e-02, -9.0598e-02, -7.6750e-02,  3.5556e-02,  1.5050e-01,
         -1.4095e-01, -1.3358e-01,  6.4994e-02,  1.3213e-01,  1.2206e-01,
          9.0832e-02, -8.7264e-02,  1.3882e-01,  5.6564e-04,  1.4850e-01,
          8.9446e-02,  4.3244e-02, -1.2893e-01, -5.6817e-02, -1.0381e-01,
          4.5397e-02,  2.9997e-02, -1.3311e-01, -9.4878e-02, -7.8794e-02,
          2.7501e-03, -7.8510e-02, -1.0877e-01,  1.5221e-01, -4.8288e-03,
         -1.3307e-01,  1.9359e-02, -2.8366e-02, -6.0451e-02, -1.1052e-02,
         -1.1122e-01, -6.3996e-02, -3.0496e-02,  1.1715e-01, -7.6642e-02,
         -1.3885e-01,  6.8332e-02,  1.0547e-01, -5.3907e-02,  1.3935e-01,
          7.3526e-02, -1.0147e-01,  1.3515e-01, -9.3346e-02, -3.4694e-02,
         -1.1021e-01, -5.1373e-02, -5.1798e-02,  2.8872e-02,  1.5050e-01,
          4.1383e-02, -1.4267e-01, -1.0295e-01,  1.0809e-02, -7.6580e-03,
          2.5903e-02, -6.0426e-02,  8.5037e-02,  7.0593e-02, -1.3831e-01,
          2.4440e-02, -5.2163e-02, -1.7038e-02, -2.3910e-02,  1.2530e-01,
         -3.1350e-02,  1.8266e-02,  1.2236e-03,  1.4651e-01,  8.0387e-02,
          9.4622e-02, -1.5131e-01, -8.6646e-02,  1.0075e-01,  2.2392e-02,
         -8.8551e-02,  1.3374e-01,  9.9849e-02,  1.0835e-01,  2.4100e-02,
         -2.9322e-02,  1.1662e-01,  1.3719e-01,  9.8675e-02, -1.1091e-01,
          2.1119e-02, -1.8869e-02,  3.5585e-03, -5.9845e-02, -1.3484e-01,
          2.1712e-02, -8.5226e-02, -9.3481e-02,  2.8479e-02, -3.8150e-02,
         -1.4280e-01,  4.5473e-02, -5.8490e-02,  3.4969e-02,  8.1740e-02,
          1.3258e-01, -1.2678e-01, -4.2167e-02,  5.7910e-02,  6.5723e-02,
          8.4743e-02, -2.0557e-02, -5.1190e-02,  1.2299e-01,  8.2613e-02,
         -3.9671e-02,  6.3273e-02,  6.4228e-02, -9.7080e-03, -1.4641e-01,
         -8.8077e-03,  1.1179e-01, -5.7796e-02,  1.3553e-01, -7.5765e-03,
          7.7101e-02,  6.7193e-02, -1.3591e-01, -8.8465e-02,  1.2905e-01,
         -1.0244e-02,  5.3459e-02, -3.4149e-02,  9.0980e-02, -1.5475e-04,
          1.0485e-01,  8.4611e-02, -1.1150e-01, -1.3674e-01,  1.5165e-01,
          7.0304e-02, -3.9442e-02, -7.2445e-02,  5.8870e-02, -1.0186e-01,
          1.3042e-01,  1.4426e-01,  6.1231e-02, -4.8288e-02, -1.9257e-02,
          1.4098e-01, -7.6407e-03,  4.2615e-03,  3.5373e-02, -8.4036e-02,
         -1.1426e-01,  6.3628e-02,  9.2173e-02, -7.0253e-02,  1.1483e-01,
         -1.0115e-01,  1.4150e-01,  4.1490e-03,  5.1570e-02,  1.2801e-01,
          2.9427e-02, -2.5754e-02, -1.9278e-02, -6.0949e-02, -1.0313e-01,
          6.6176e-02,  1.3289e-01, -1.1627e-01, -3.8945e-02, -5.8968e-02,
          1.2997e-01, -1.2272e-01,  1.5112e-01,  1.4399e-01,  9.2071e-02,
          1.4641e-01, -7.8369e-02, -6.4530e-02,  1.1183e-01,  4.4172e-02,
         -4.8206e-02, -8.9611e-02,  3.1789e-02, -1.4304e-01, -1.5134e-01,
         -4.7244e-02, -1.4711e-01, -8.7563e-02, -7.0891e-02, -2.6771e-02,
         -8.1973e-02,  4.5180e-02, -1.0147e-01,  1.4728e-02,  9.1716e-02,
         -2.4038e-02,  8.2731e-03,  5.8695e-03,  2.9971e-02,  1.0904e-01,
         -5.3751e-02,  4.8410e-02, -7.0762e-02, -7.9417e-02, -8.0928e-02,
          1.0707e-01,  1.0669e-01,  1.4407e-01, -1.1650e-02,  8.7744e-02,
          8.3991e-02, -1.3229e-01, -1.2116e-01, -1.8727e-02,  5.2844e-03,
         -7.3168e-05, -1.3732e-01,  9.6372e-02,  5.1407e-02, -1.1857e-01,
          9.6450e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0849,  0.0379, -0.1106,  ...,  0.1203,  0.0907,  0.0595],
        [ 0.0451, -0.0131,  0.1158,  ..., -0.0157, -0.0968,  0.0813],
        [ 0.0924, -0.0085, -0.0949,  ...,  0.0079,  0.0658,  0.0926],
        ...,
        [-0.0423, -0.0552,  0.0072,  ..., -0.0937, -0.0100,  0.1017],
        [ 0.0718, -0.0612,  0.0808,  ...,  0.0735, -0.0131,  0.0535],
        [ 0.1003,  0.0261,  0.1226,  ..., -0.0659, -0.0192,  0.0732]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1613,  0.1636, -0.1087,  ...,  0.1245, -0.0233,  0.1621],
        [ 0.1561, -0.1251, -0.0067,  ..., -0.0050, -0.1411, -0.1699],
        [ 0.1546, -0.0773,  0.1581,  ...,  0.0508,  0.1006, -0.1705],
        ...,
        [-0.0955, -0.1453, -0.1069,  ..., -0.1164,  0.0317,  0.1112],
        [-0.1753,  0.1597,  0.1023,  ..., -0.0904,  0.1696, -0.0422],
        [-0.0973,  0.1445,  0.0932,  ...,  0.0648, -0.0195, -0.1522]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1403,  0.0463,  0.2289,  ...,  0.1435,  0.0763,  0.1087],
        [ 0.0655, -0.0568,  0.0776,  ..., -0.1053, -0.1649,  0.2327],
        [ 0.2373, -0.2329,  0.0399,  ...,  0.0525, -0.0187,  0.0636],
        ...,
        [-0.0934, -0.2154,  0.1585,  ..., -0.0954, -0.0949, -0.0180],
        [-0.2154, -0.1958, -0.2218,  ...,  0.1103, -0.0864, -0.2171],
        [ 0.2473, -0.2264, -0.0545,  ..., -0.1710, -0.0189,  0.0285]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1488],
        [-0.3201],
        [ 0.1221],
        [-0.2111],
        [-0.1966],
        [ 0.1578],
        [-0.1389],
        [ 0.4107],
        [-0.3782],
        [-0.0612],
        [ 0.0183],
        [ 0.4258],
        [ 0.1046],
        [ 0.1731],
        [ 0.0295],
        [ 0.3713],
        [-0.0986],
        [ 0.1945],
        [-0.1980],
        [ 0.3582],
        [-0.1751],
        [-0.1184],
        [ 0.3554],
        [ 0.0310],
        [ 0.3979],
        [-0.1509],
        [-0.0028],
        [-0.3187],
        [ 0.0024],
        [ 0.0038],
        [ 0.0917],
        [-0.3373]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-4.6367e-02,  1.4229e-01,  1.2363e-01, -9.1626e-02, -1.4148e-01,
          4.4501e-02,  1.2770e-01, -1.2739e-01, -1.1096e-01,  1.1290e-01,
          1.1593e-01,  8.8623e-02, -7.3719e-02, -1.1763e-01, -1.0430e-01,
         -1.1870e-02, -2.6326e-02,  2.8448e-02, -1.3875e-01,  1.1674e-01,
          6.4767e-02,  1.4463e-02, -4.5336e-02,  1.3265e-01,  1.2215e-01,
          1.1340e-02, -4.9207e-02, -1.3903e-01,  1.1123e-01,  1.4337e-02,
         -6.5248e-02, -9.0598e-02, -7.6750e-02,  3.5556e-02,  1.5050e-01,
         -1.4095e-01, -1.3358e-01,  6.4994e-02,  1.3213e-01,  1.2206e-01,
          9.0832e-02, -8.7264e-02,  1.3882e-01,  5.6564e-04,  1.4850e-01,
          8.9446e-02,  4.3244e-02, -1.2893e-01, -5.6817e-02, -1.0381e-01,
          4.5397e-02,  2.9997e-02, -1.3311e-01, -9.4878e-02, -7.8794e-02,
          2.7501e-03, -7.8510e-02, -1.0877e-01,  1.5221e-01, -4.8288e-03,
         -1.3307e-01,  1.9359e-02, -2.8366e-02, -6.0451e-02, -1.1052e-02,
         -1.1122e-01, -6.3996e-02, -3.0496e-02,  1.1715e-01, -7.6642e-02,
         -1.3885e-01,  6.8332e-02,  1.0547e-01, -5.3907e-02,  1.3935e-01,
          7.3526e-02, -1.0147e-01,  1.3515e-01, -9.3346e-02, -3.4694e-02,
         -1.1021e-01, -5.1373e-02, -5.1798e-02,  2.8872e-02,  1.5050e-01,
          4.1383e-02, -1.4267e-01, -1.0295e-01,  1.0809e-02, -7.6580e-03,
          2.5903e-02, -6.0426e-02,  8.5037e-02,  7.0593e-02, -1.3831e-01,
          2.4440e-02, -5.2163e-02, -1.7038e-02, -2.3910e-02,  1.2530e-01,
         -3.1350e-02,  1.8266e-02,  1.2236e-03,  1.4651e-01,  8.0387e-02,
          9.4622e-02, -1.5131e-01, -8.6646e-02,  1.0075e-01,  2.2392e-02,
         -8.8551e-02,  1.3374e-01,  9.9849e-02,  1.0835e-01,  2.4100e-02,
         -2.9322e-02,  1.1662e-01,  1.3719e-01,  9.8675e-02, -1.1091e-01,
          2.1119e-02, -1.8869e-02,  3.5585e-03, -5.9845e-02, -1.3484e-01,
          2.1712e-02, -8.5226e-02, -9.3481e-02,  2.8479e-02, -3.8150e-02,
         -1.4280e-01,  4.5473e-02, -5.8490e-02,  3.4969e-02,  8.1740e-02,
          1.3258e-01, -1.2678e-01, -4.2167e-02,  5.7910e-02,  6.5723e-02,
          8.4743e-02, -2.0557e-02, -5.1190e-02,  1.2299e-01,  8.2613e-02,
         -3.9671e-02,  6.3273e-02,  6.4228e-02, -9.7080e-03, -1.4641e-01,
         -8.8077e-03,  1.1179e-01, -5.7796e-02,  1.3553e-01, -7.5765e-03,
          7.7101e-02,  6.7193e-02, -1.3591e-01, -8.8465e-02,  1.2905e-01,
         -1.0244e-02,  5.3459e-02, -3.4149e-02,  9.0980e-02, -1.5475e-04,
          1.0485e-01,  8.4611e-02, -1.1150e-01, -1.3674e-01,  1.5165e-01,
          7.0304e-02, -3.9442e-02, -7.2445e-02,  5.8870e-02, -1.0186e-01,
          1.3042e-01,  1.4426e-01,  6.1231e-02, -4.8288e-02, -1.9257e-02,
          1.4098e-01, -7.6407e-03,  4.2615e-03,  3.5373e-02, -8.4036e-02,
         -1.1426e-01,  6.3628e-02,  9.2173e-02, -7.0253e-02,  1.1483e-01,
         -1.0115e-01,  1.4150e-01,  4.1490e-03,  5.1570e-02,  1.2801e-01,
          2.9427e-02, -2.5754e-02, -1.9278e-02, -6.0949e-02, -1.0313e-01,
          6.6176e-02,  1.3289e-01, -1.1627e-01, -3.8945e-02, -5.8968e-02,
          1.2997e-01, -1.2272e-01,  1.5112e-01,  1.4399e-01,  9.2071e-02,
          1.4641e-01, -7.8369e-02, -6.4530e-02,  1.1183e-01,  4.4172e-02,
         -4.8206e-02, -8.9611e-02,  3.1789e-02, -1.4304e-01, -1.5134e-01,
         -4.7244e-02, -1.4711e-01, -8.7563e-02, -7.0891e-02, -2.6771e-02,
         -8.1973e-02,  4.5180e-02, -1.0147e-01,  1.4728e-02,  9.1716e-02,
         -2.4038e-02,  8.2731e-03,  5.8695e-03,  2.9971e-02,  1.0904e-01,
         -5.3751e-02,  4.8410e-02, -7.0762e-02, -7.9417e-02, -8.0928e-02,
          1.0707e-01,  1.0669e-01,  1.4407e-01, -1.1650e-02,  8.7744e-02,
          8.3991e-02, -1.3229e-01, -1.2116e-01, -1.8727e-02,  5.2844e-03,
         -7.3168e-05, -1.3732e-01,  9.6372e-02,  5.1407e-02, -1.1857e-01,
          9.6450e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0849,  0.0379, -0.1106,  ...,  0.1203,  0.0907,  0.0595],
        [ 0.0451, -0.0131,  0.1158,  ..., -0.0157, -0.0968,  0.0813],
        [ 0.0924, -0.0085, -0.0949,  ...,  0.0079,  0.0658,  0.0926],
        ...,
        [-0.0423, -0.0552,  0.0072,  ..., -0.0937, -0.0100,  0.1017],
        [ 0.0718, -0.0612,  0.0808,  ...,  0.0735, -0.0131,  0.0535],
        [ 0.1003,  0.0261,  0.1226,  ..., -0.0659, -0.0192,  0.0732]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1613,  0.1636, -0.1087,  ...,  0.1245, -0.0233,  0.1621],
        [ 0.1561, -0.1251, -0.0067,  ..., -0.0050, -0.1411, -0.1699],
        [ 0.1546, -0.0773,  0.1581,  ...,  0.0508,  0.1006, -0.1705],
        ...,
        [-0.0955, -0.1453, -0.1069,  ..., -0.1164,  0.0317,  0.1112],
        [-0.1753,  0.1597,  0.1023,  ..., -0.0904,  0.1696, -0.0422],
        [-0.0973,  0.1445,  0.0932,  ...,  0.0648, -0.0195, -0.1522]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1403,  0.0463,  0.2289,  ...,  0.1435,  0.0763,  0.1087],
        [ 0.0655, -0.0568,  0.0776,  ..., -0.1053, -0.1649,  0.2327],
        [ 0.2373, -0.2329,  0.0399,  ...,  0.0525, -0.0187,  0.0636],
        ...,
        [-0.0934, -0.2154,  0.1585,  ..., -0.0954, -0.0949, -0.0180],
        [-0.2154, -0.1958, -0.2218,  ...,  0.1103, -0.0864, -0.2171],
        [ 0.2473, -0.2264, -0.0545,  ..., -0.1710, -0.0189,  0.0285]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1488],
        [-0.3201],
        [ 0.1221],
        [-0.2111],
        [-0.1966],
        [ 0.1578],
        [-0.1389],
        [ 0.4107],
        [-0.3782],
        [-0.0612],
        [ 0.0183],
        [ 0.4258],
        [ 0.1046],
        [ 0.1731],
        [ 0.0295],
        [ 0.3713],
        [-0.0986],
        [ 0.1945],
        [-0.1980],
        [ 0.3582],
        [-0.1751],
        [-0.1184],
        [ 0.3554],
        [ 0.0310],
        [ 0.3979],
        [-0.1509],
        [-0.0028],
        [-0.3187],
        [ 0.0024],
        [ 0.0038],
        [ 0.0917],
        [-0.3373]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3173.8818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2294.4282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.4521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.2652, device='cuda:0')



h[100].sum tensor(-96.9928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-167.7319, device='cuda:0')



h[200].sum tensor(204.7411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.9460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0248, 0.0216,  ..., 0.0090, 0.0000, 0.0168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(347935.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0218, 0.0722, 0.0718,  ..., 0.0190, 0.0000, 0.0000],
        [0.0093, 0.0307, 0.0305,  ..., 0.0081, 0.0000, 0.0000],
        [0.0063, 0.0209, 0.0208,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0170, 0.0169,  ..., 0.0045, 0.0000, 0.0000],
        [0.0086, 0.0287, 0.0285,  ..., 0.0075, 0.0000, 0.0000],
        [0.0127, 0.0422, 0.0419,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1772726.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10781.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-563.9974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78227.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4010.0813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33553.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5550.1592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3215],
        [-0.2823],
        [-0.2769],
        ...,
        [-0.1065],
        [-0.1382],
        [-0.1696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-240176.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(189.7278, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3174.6992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8881e-03,  1.5143e-02,  1.3172e-02,  ...,  5.3206e-03,
         -1.2516e-02,  1.0301e-02],
        [-1.0864e-02,  3.3532e-02,  2.9152e-02,  ...,  1.1947e-02,
         -2.7817e-02,  2.2770e-02],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        ...,
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2109.0371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.3641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.2766, device='cuda:0')



h[100].sum tensor(-96.8339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-167.7751, device='cuda:0')



h[200].sum tensor(184.3579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.9578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1340, 0.1165,  ..., 0.0477, 0.0000, 0.0910],
        [0.0000, 0.0677, 0.0588,  ..., 0.0239, 0.0000, 0.0460],
        [0.0000, 0.0759, 0.0660,  ..., 0.0269, 0.0000, 0.0516],
        ...,
        [0.0000, 0.0004, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0004, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0095, 0.0083,  ..., 0.0032, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(360185.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0769, 0.2785, 0.2745,  ..., 0.0608, 0.0000, 0.0000],
        [0.0587, 0.2136, 0.2103,  ..., 0.0455, 0.0000, 0.0000],
        [0.0540, 0.1970, 0.1939,  ..., 0.0417, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0043, 0.0041,  ..., 0.0000, 0.0000, 0.0000],
        [0.0038, 0.0172, 0.0166,  ..., 0.0017, 0.0000, 0.0000],
        [0.0114, 0.0449, 0.0438,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1844159.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10409.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-808.6802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82630.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4105.4604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32681.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5758.0142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5153e-01],
        [-5.5191e-01],
        [-4.6978e-01],
        ...,
        [ 1.0236e-04],
        [-2.0448e-02],
        [-5.4684e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-63129.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.4902, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [-3.0498e-03,  9.4858e-03,  8.2559e-03,  ...,  3.2820e-03,
         -7.8093e-03,  6.4644e-03],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        ...,
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [ 0.0000e+00,  1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2299.8711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-155.0894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.8859, device='cuda:0')



h[100].sum tensor(-104.7537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-181.4515, device='cuda:0')



h[200].sum tensor(201.1034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.7041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0480, 0.0417,  ..., 0.0167, 0.0000, 0.0327],
        [0.0000, 0.0148, 0.0129,  ..., 0.0051, 0.0000, 0.0102],
        [0.0000, 0.0098, 0.0086,  ..., 0.0033, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0004, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0004, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0004, 0.0004,  ..., 0.0000, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390111., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0698, 0.0679,  ..., 0.0116, 0.0000, 0.0000],
        [0.0145, 0.0561, 0.0546,  ..., 0.0087, 0.0000, 0.0000],
        [0.0179, 0.0681, 0.0666,  ..., 0.0116, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0029,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2000182., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11314.7979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-857.7124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89578.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4450.2949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35593.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6235.3535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0707],
        [-0.1148],
        [-0.1872],
        ...,
        [ 0.0082],
        [ 0.0081],
        [ 0.0081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-79960.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3150.0620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002],
        ...,
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0002,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1628.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.2603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.9330, device='cuda:0')



h[100].sum tensor(-95.9895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-166.4731, device='cuda:0')



h[200].sum tensor(162.5352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.6012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0006, 0.0007,  ..., 0.0000, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(365849.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5099e-03, 1.6183e-02, 1.5763e-02,  ..., 0.0000e+00, 2.4428e-05,
         0.0000e+00],
        [1.7242e-04, 6.9146e-03, 6.8103e-03,  ..., 0.0000e+00, 5.5792e-05,
         0.0000e+00],
        [3.9824e-04, 7.2011e-03, 7.1329e-03,  ..., 0.0000e+00, 8.7597e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.9642e-03, 4.9510e-03,  ..., 0.0000e+00, 1.1070e-04,
         0.0000e+00],
        [0.0000e+00, 4.9642e-03, 4.9510e-03,  ..., 0.0000e+00, 1.1070e-04,
         0.0000e+00],
        [0.0000e+00, 4.9642e-03, 4.9510e-03,  ..., 0.0000e+00, 1.1070e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1908505.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10302.4639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-983.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(85662.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4334.6108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33079.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5797.4492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0216],
        [0.0217],
        [0.0204],
        ...,
        [0.0027],
        [0.0026],
        [0.0025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(15787.9258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3047.6548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0002,  ..., -0.0003,  0.0000,  0.0003],
        [-0.0030,  0.0094,  0.0082,  ...,  0.0030, -0.0077,  0.0066],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0003,  0.0000,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000,  0.0002,  0.0002,  ..., -0.0003,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1075.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-137.2064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.5047, device='cuda:0')



h[100].sum tensor(-92.4836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-161.0611, device='cuda:0')



h[200].sum tensor(135.9337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.1187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0343, 0.0300,  ..., 0.0108, 0.0000, 0.0240],
        [0.0000, 0.0083, 0.0074,  ..., 0.0024, 0.0000, 0.0063],
        [0.0000, 0.0099, 0.0089,  ..., 0.0030, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0007, 0.0009,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0007, 0.0009,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0007, 0.0009,  ..., 0.0000, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(357567.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0527, 0.0520,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0345, 0.0346,  ..., 0.0000, 0.0000, 0.0000],
        [0.0079, 0.0373, 0.0375,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0076,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0063, 0.0076,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0063, 0.0076,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1885772.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10625.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1153.3279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84409.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4408.9775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30766.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5589.0356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0721],
        [ 0.0704],
        [ 0.0696],
        ...,
        [-0.0016],
        [-0.0034],
        [-0.0039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(74277.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(2791.4858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0116,  0.0362,  0.0315,  ...,  0.0126, -0.0299,  0.0248],
        [-0.0120,  0.0374,  0.0326,  ...,  0.0130, -0.0309,  0.0257],
        [-0.0115,  0.0359,  0.0313,  ...,  0.0125, -0.0297,  0.0247],
        ...,
        [-0.0074,  0.0231,  0.0202,  ...,  0.0079, -0.0191,  0.0160],
        [-0.0066,  0.0207,  0.0181,  ...,  0.0070, -0.0170,  0.0143],
        [-0.0066,  0.0205,  0.0179,  ...,  0.0070, -0.0169,  0.0142]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(379.5529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-125.3107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.9320, device='cuda:0')



h[100].sum tensor(-84.3774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-147.5232, device='cuda:0')



h[200].sum tensor(105.9535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.4103, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1550, 0.1349,  ..., 0.0539, 0.0000, 0.1063],
        [0.0000, 0.1311, 0.1141,  ..., 0.0453, 0.0000, 0.0901],
        [0.0000, 0.1343, 0.1170,  ..., 0.0465, 0.0000, 0.0923],
        ...,
        [0.0000, 0.0707, 0.0619,  ..., 0.0238, 0.0000, 0.0491],
        [0.0000, 0.0772, 0.0675,  ..., 0.0261, 0.0000, 0.0535],
        [0.0000, 0.0740, 0.0647,  ..., 0.0249, 0.0000, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(326577.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0968, 0.3467, 0.3431,  ..., 0.0419, 0.0000, 0.0000],
        [0.0917, 0.3292, 0.3258,  ..., 0.0387, 0.0000, 0.0000],
        [0.0858, 0.3092, 0.3060,  ..., 0.0352, 0.0000, 0.0000],
        ...,
        [0.0361, 0.1379, 0.1367,  ..., 0.0066, 0.0000, 0.0000],
        [0.0415, 0.1566, 0.1551,  ..., 0.0095, 0.0000, 0.0000],
        [0.0429, 0.1615, 0.1599,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1728690.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8821.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1249.6981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76874.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4178.2583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25367.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5003.7280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0609],
        [0.0590],
        [0.0571],
        ...,
        [0.0420],
        [0.0460],
        [0.0489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(57865.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3049.2393, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0189,  0.0166,  ...,  0.0064, -0.0156,  0.0132],
        [-0.0024,  0.0076,  0.0068,  ...,  0.0024, -0.0062,  0.0055],
        [-0.0036,  0.0114,  0.0101,  ...,  0.0037, -0.0094,  0.0081],
        ...,
        [-0.0037,  0.0117,  0.0104,  ...,  0.0039, -0.0096,  0.0083],
        [-0.0035,  0.0110,  0.0098,  ...,  0.0036, -0.0091,  0.0078],
        [-0.0070,  0.0218,  0.0191,  ...,  0.0075, -0.0180,  0.0152]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(292.4552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-136.9687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.5268, device='cuda:0')



h[100].sum tensor(-92.1310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-161.1448, device='cuda:0')



h[200].sum tensor(121.2111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.1416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0314, 0.0283,  ..., 0.0102, 0.0000, 0.0228],
        [0.0000, 0.0715, 0.0630,  ..., 0.0242, 0.0000, 0.0500],
        [0.0000, 0.0314, 0.0283,  ..., 0.0105, 0.0000, 0.0228],
        ...,
        [0.0000, 0.0654, 0.0577,  ..., 0.0220, 0.0000, 0.0458],
        [0.0000, 0.0518, 0.0459,  ..., 0.0172, 0.0000, 0.0366],
        [0.0000, 0.0464, 0.0413,  ..., 0.0153, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358626.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2089e-02, 9.8020e-02, 9.7306e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.0162e-02, 1.2630e-01, 1.2508e-01,  ..., 1.1643e-04, 0.0000e+00,
         0.0000e+00],
        [1.9757e-02, 8.9171e-02, 8.9077e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.8632e-02, 1.5600e-01, 1.5418e-01,  ..., 2.6235e-03, 0.0000e+00,
         0.0000e+00],
        [3.7495e-02, 1.5212e-01, 1.5024e-01,  ..., 1.7609e-03, 0.0000e+00,
         0.0000e+00],
        [3.6648e-02, 1.4920e-01, 1.4728e-01,  ..., 1.1159e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1904477.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8135.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1419.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(85407.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4601.8452, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25447.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5418.2979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0509],
        [0.0436],
        [0.0315],
        ...,
        [0.0429],
        [0.0424],
        [0.0409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-5225.2041, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(3433.3123, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1291e-03,  2.2228e-02,  1.9641e-02,  ...,  7.6732e-03,
         -1.8375e-02,  1.5530e-02],
        [-8.3778e-03,  2.6122e-02,  2.3012e-02,  ...,  9.0653e-03,
         -2.1594e-02,  1.8175e-02],
        [-3.7294e-03,  1.1627e-02,  1.0461e-02,  ...,  3.8831e-03,
         -9.6124e-03,  8.3285e-03],
        ...,
        [ 0.0000e+00, -1.4525e-06,  3.9125e-04,  ..., -2.7455e-04,
          0.0000e+00,  4.2881e-04],
        [-5.8732e-03,  1.8312e-02,  1.6250e-02,  ...,  6.2731e-03,
         -1.5138e-02,  1.2870e-02],
        [-2.6048e-03,  8.1208e-03,  7.4246e-03,  ...,  2.6294e-03,
         -6.7139e-03,  5.9464e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(374.0374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-153.7376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.8834, device='cuda:0')



h[100].sum tensor(-103.3018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-181.4421, device='cuda:0')



h[200].sum tensor(145.4169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.7016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0840, 0.0743,  ..., 0.0289, 0.0000, 0.0588],
        [0.0000, 0.0742, 0.0658,  ..., 0.0254, 0.0000, 0.0521],
        [0.0000, 0.0784, 0.0695,  ..., 0.0269, 0.0000, 0.0550],
        ...,
        [0.0000, 0.0314, 0.0288,  ..., 0.0107, 0.0000, 0.0231],
        [0.0000, 0.0360, 0.0327,  ..., 0.0120, 0.0000, 0.0262],
        [0.0000, 0.0582, 0.0520,  ..., 0.0197, 0.0000, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401860.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.1894, 0.1877,  ..., 0.0036, 0.0000, 0.0000],
        [0.0379, 0.1625, 0.1611,  ..., 0.0012, 0.0000, 0.0000],
        [0.0346, 0.1511, 0.1497,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0185, 0.0911, 0.0912,  ..., 0.0000, 0.0000, 0.0000],
        [0.0233, 0.1098, 0.1090,  ..., 0.0000, 0.0000, 0.0000],
        [0.0248, 0.1149, 0.1141,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2137808.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8052.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1586.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(96407.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5148.1558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26721.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6024.1992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0026],
        [-0.0013],
        [-0.0053],
        ...,
        [-0.0056],
        [-0.0012],
        [-0.0042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-67885.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2974.3613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.9743e-05,  4.2869e-04,  ..., -2.6671e-04,
          0.0000e+00,  4.4129e-04],
        [-2.5058e-03,  7.7825e-03,  7.2088e-03,  ...,  2.5320e-03,
         -6.4674e-03,  5.7635e-03],
        [-2.5058e-03,  7.7825e-03,  7.2088e-03,  ...,  2.5320e-03,
         -6.4674e-03,  5.7635e-03],
        ...,
        [ 0.0000e+00, -4.9743e-05,  4.2869e-04,  ..., -2.6671e-04,
          0.0000e+00,  4.4129e-04],
        [ 0.0000e+00, -4.9743e-05,  4.2869e-04,  ..., -2.6671e-04,
          0.0000e+00,  4.4129e-04],
        [ 0.0000e+00, -4.9743e-05,  4.2869e-04,  ..., -2.6671e-04,
          0.0000e+00,  4.4129e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-191.6847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-132.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.4825, device='cuda:0')



h[100].sum tensor(-88.9896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-157.1877, device='cuda:0')



h[200].sum tensor(111.6815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.0577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0319, 0.0295,  ..., 0.0104, 0.0000, 0.0236],
        [0.0000, 0.0141, 0.0140,  ..., 0.0046, 0.0000, 0.0114],
        [0.0000, 0.0141, 0.0140,  ..., 0.0046, 0.0000, 0.0114],
        ...,
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350245.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0780, 0.0769,  ..., 0.0000, 0.0000, 0.0000],
        [0.0092, 0.0646, 0.0644,  ..., 0.0000, 0.0000, 0.0000],
        [0.0120, 0.0745, 0.0743,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0102,  ..., 0.0000, 0.0080, 0.0000],
        [0.0000, 0.0067, 0.0102,  ..., 0.0000, 0.0080, 0.0000],
        [0.0000, 0.0067, 0.0102,  ..., 0.0000, 0.0080, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1885951.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5764.5107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1559.6149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84149.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4612.9644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20520.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5123.9297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0250],
        [-0.0240],
        [-0.0204],
        ...,
        [-0.0413],
        [-0.0411],
        [-0.0411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-103538.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.6470, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -8.3076e-05,  4.5612e-04,  ..., -2.7399e-04,
          0.0000e+00,  4.4147e-04],
        [-2.3482e-03,  7.2744e-03,  6.8227e-03,  ...,  2.3531e-03,
         -6.0686e-03,  5.4420e-03],
        [-7.9860e-03,  2.4939e-02,  2.2108e-02,  ...,  8.6605e-03,
         -2.0639e-02,  1.7448e-02],
        ...,
        [ 0.0000e+00, -8.3076e-05,  4.5612e-04,  ..., -2.7399e-04,
          0.0000e+00,  4.4147e-04],
        [ 0.0000e+00, -8.3076e-05,  4.5612e-04,  ..., -2.7399e-04,
          0.0000e+00,  4.4147e-04],
        [ 0.0000e+00, -8.3076e-05,  4.5612e-04,  ..., -2.7399e-04,
          0.0000e+00,  4.4147e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-555.1523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-121.3850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.0696, device='cuda:0')



h[100].sum tensor(-81.3904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-144.2551, device='cuda:0')



h[200].sum tensor(89.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.5151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0134,  ..., 0.0042, 0.0000, 0.0108],
        [0.0000, 0.0441, 0.0402,  ..., 0.0150, 0.0000, 0.0319],
        [0.0000, 0.0668, 0.0599,  ..., 0.0229, 0.0000, 0.0474],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(330833.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0704, 0.0708,  ..., 0.0000, 0.0018, 0.0000],
        [0.0258, 0.1279, 0.1267,  ..., 0.0000, 0.0000, 0.0000],
        [0.0378, 0.1724, 0.1699,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0108,  ..., 0.0000, 0.0094, 0.0000],
        [0.0000, 0.0084, 0.0121,  ..., 0.0000, 0.0091, 0.0000],
        [0.0013, 0.0229, 0.0259,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1829140.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4864.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1579.6185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81112.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4441.2026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18395.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4762.1729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0235],
        [-0.0122],
        [-0.0033],
        ...,
        [-0.0471],
        [-0.0431],
        [-0.0359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-116393.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2775.0273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0095,  0.0088,  ...,  0.0031, -0.0079,  0.0069],
        [-0.0128,  0.0402,  0.0354,  ...,  0.0141, -0.0332,  0.0279],
        [-0.0052,  0.0161,  0.0145,  ...,  0.0055, -0.0133,  0.0114],
        ...,
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-649.2626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-123.3239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7025, device='cuda:0')



h[100].sum tensor(-82.6025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-146.6534, device='cuda:0')



h[200].sum tensor(84.9197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.1721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1373, 0.1210,  ..., 0.0480, 0.0000, 0.0953],
        [0.0000, 0.0935, 0.0831,  ..., 0.0323, 0.0000, 0.0655],
        [0.0000, 0.1102, 0.0976,  ..., 0.0383, 0.0000, 0.0769],
        ...,
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336988.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0717, 0.2953, 0.2919,  ..., 0.0059, 0.0000, 0.0000],
        [0.0763, 0.3131, 0.3088,  ..., 0.0062, 0.0000, 0.0000],
        [0.0821, 0.3343, 0.3296,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0112,  ..., 0.0000, 0.0106, 0.0000],
        [0.0000, 0.0075, 0.0112,  ..., 0.0000, 0.0106, 0.0000],
        [0.0000, 0.0075, 0.0112,  ..., 0.0000, 0.0106, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1874566.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4514.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1654.3618, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83087.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4565.0215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18283.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4830.8291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1247],
        [ 0.1502],
        [ 0.1660],
        ...,
        [-0.0547],
        [-0.0545],
        [-0.0544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-115442.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2725.5818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [-0.0064,  0.0200,  0.0178,  ...,  0.0069, -0.0165,  0.0141],
        ...,
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-686.3707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-121.3355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.0129, device='cuda:0')



h[100].sum tensor(-81.2706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-144.0403, device='cuda:0')



h[200].sum tensor(82.0510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.4563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0276, 0.0260,  ..., 0.0093, 0.0000, 0.0206],
        [0.0000, 0.0255, 0.0243,  ..., 0.0084, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336697.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6433e-03, 2.8859e-02, 3.0997e-02,  ..., 0.0000e+00, 5.6757e-03,
         0.0000e+00],
        [9.1759e-03, 6.9023e-02, 6.8933e-02,  ..., 0.0000e+00, 1.6719e-03,
         0.0000e+00],
        [1.3023e-02, 8.6016e-02, 8.4534e-02,  ..., 0.0000e+00, 8.0385e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.5071e-03, 1.1249e-02,  ..., 0.0000e+00, 1.0612e-02,
         0.0000e+00],
        [0.0000e+00, 7.5071e-03, 1.1249e-02,  ..., 0.0000e+00, 1.0612e-02,
         0.0000e+00],
        [0.0000e+00, 7.5071e-03, 1.1249e-02,  ..., 0.0000e+00, 1.0612e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1891727., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4519.6597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1661.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84051.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4563.1353, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18462.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4825.0508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0372],
        [-0.0197],
        [-0.0056],
        ...,
        [-0.0547],
        [-0.0545],
        [-0.0544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-113202.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(3419.4692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        ...,
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0001,  0.0005,  ..., -0.0003,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-208.4025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-151.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.6903, device='cuda:0')



h[100].sum tensor(-101.4324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-180.7106, device='cuda:0')



h[200].sum tensor(117.7007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-49.5012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(406500.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0197, 0.0222,  ..., 0.0000, 0.0092, 0.0000],
        [0.0000, 0.0111, 0.0142,  ..., 0.0000, 0.0113, 0.0000],
        [0.0031, 0.0340, 0.0363,  ..., 0.0000, 0.0074, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0115,  ..., 0.0000, 0.0118, 0.0000],
        [0.0000, 0.0081, 0.0115,  ..., 0.0000, 0.0118, 0.0000],
        [0.0000, 0.0081, 0.0115,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2235310.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5763.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1854.5858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(100417.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5418.2188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23521.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5934.5273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0267],
        [-0.0324],
        [-0.0257],
        ...,
        [-0.0604],
        [-0.0601],
        [-0.0601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-95080.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2547.3120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [-0.0032,  0.0099,  0.0092,  ...,  0.0033, -0.0083,  0.0072],
        [ 0.0000, -0.0002,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        ...,
        [ 0.0000, -0.0002,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0002,  0.0005,  ..., -0.0003,  0.0000,  0.0004],
        [ 0.0000, -0.0002,  0.0005,  ..., -0.0003,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1005.2983, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-112.7295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.5266, device='cuda:0')



h[100].sum tensor(-75.3445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-134.6192, device='cuda:0')



h[200].sum tensor(56.2636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8756, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0433, 0.0402,  ..., 0.0145, 0.0000, 0.0313],
        [0.0000, 0.0080, 0.0093,  ..., 0.0026, 0.0000, 0.0070],
        [0.0000, 0.0248, 0.0241,  ..., 0.0081, 0.0000, 0.0187],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317425., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0910, 0.0867,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0733, 0.0702,  ..., 0.0000, 0.0010, 0.0000],
        [0.0111, 0.0899, 0.0854,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0114,  ..., 0.0000, 0.0131, 0.0000],
        [0.0000, 0.0085, 0.0114,  ..., 0.0000, 0.0131, 0.0000],
        [0.0000, 0.0085, 0.0114,  ..., 0.0000, 0.0131, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1828943.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3133.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1693.1210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80504.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4412.4990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16604.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4555.7373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0149],
        [ 0.0175],
        [ 0.0185],
        ...,
        [-0.0665],
        [-0.0663],
        [-0.0662]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-132022.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2890.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [-0.0050,  0.0155,  0.0141,  ...,  0.0053, -0.0129,  0.0110],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-753.6918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-127.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.3160, device='cuda:0')



h[100].sum tensor(-85.1267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-152.7673, device='cuda:0')



h[200].sum tensor(71.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.8468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0254, 0.0246,  ..., 0.0086, 0.0000, 0.0188],
        [0.0000, 0.0126, 0.0134,  ..., 0.0043, 0.0000, 0.0100],
        [0.0000, 0.0562, 0.0516,  ..., 0.0191, 0.0000, 0.0400],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350868.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.0881e-03, 6.9843e-02, 6.6001e-02,  ..., 0.0000e+00, 3.1125e-03,
         0.0000e+00],
        [2.9827e-03, 6.0293e-02, 5.7568e-02,  ..., 0.0000e+00, 4.0015e-03,
         0.0000e+00],
        [1.0372e-02, 9.0934e-02, 8.6004e-02,  ..., 0.0000e+00, 2.2644e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.9965e-03, 1.1259e-02,  ..., 0.0000e+00, 1.4266e-02,
         0.0000e+00],
        [0.0000e+00, 8.9965e-03, 1.1259e-02,  ..., 0.0000e+00, 1.4266e-02,
         0.0000e+00],
        [0.0000e+00, 8.9965e-03, 1.1259e-02,  ..., 0.0000e+00, 1.4266e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1990899.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3310.6414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1793.0808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88219.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4826.9087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18916.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5134.0376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0046],
        [ 0.0042],
        [ 0.0056],
        ...,
        [-0.0720],
        [-0.0717],
        [-0.0716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-130861.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.2256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0174,  0.0158,  ...,  0.0060, -0.0145,  0.0122],
        [-0.0109,  0.0342,  0.0304,  ...,  0.0120, -0.0282,  0.0236],
        [-0.0145,  0.0457,  0.0404,  ...,  0.0161, -0.0377,  0.0315],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-870.1041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-122.5780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7610, device='cuda:0')



h[100].sum tensor(-81.7497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-146.8752, device='cuda:0')



h[200].sum tensor(56.2202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.2328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1293, 0.1150,  ..., 0.0452, 0.0000, 0.0895],
        [0.0000, 0.1233, 0.1099,  ..., 0.0431, 0.0000, 0.0855],
        [0.0000, 0.1433, 0.1272,  ..., 0.0502, 0.0000, 0.0991],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341615.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.1233e-02, 3.3396e-01, 3.1922e-01,  ..., 1.1406e-04, 0.0000e+00,
         0.0000e+00],
        [7.0455e-02, 3.3130e-01, 3.1643e-01,  ..., 1.9319e-04, 0.0000e+00,
         0.0000e+00],
        [6.8944e-02, 3.2570e-01, 3.1105e-01,  ..., 1.1406e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 9.6041e-03, 1.1136e-02,  ..., 0.0000e+00, 1.5408e-02,
         0.0000e+00],
        [0.0000e+00, 9.6041e-03, 1.1136e-02,  ..., 0.0000e+00, 1.5408e-02,
         0.0000e+00],
        [0.0000e+00, 9.6041e-03, 1.1136e-02,  ..., 0.0000e+00, 1.5408e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1966454.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2985.2629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1772.5236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86343.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4734.9204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18713.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5039.6255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0241],
        [ 0.0245],
        [ 0.0245],
        ...,
        [-0.0769],
        [-0.0766],
        [-0.0764]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-135197.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2804.4092, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0074,  0.0232,  0.0209,  ...,  0.0080, -0.0192,  0.0162],
        [-0.0127,  0.0402,  0.0356,  ...,  0.0141, -0.0332,  0.0277],
        [-0.0139,  0.0440,  0.0389,  ...,  0.0155, -0.0363,  0.0303],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [-0.0056,  0.0176,  0.0160,  ...,  0.0060, -0.0146,  0.0123]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-860.2175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-123.4660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1123, device='cuda:0')



h[100].sum tensor(-82.2523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-148.2061, device='cuda:0')



h[200].sum tensor(49.5664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.5974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0936, 0.0843,  ..., 0.0325, 0.0000, 0.0651],
        [0.0000, 0.1286, 0.1146,  ..., 0.0450, 0.0000, 0.0889],
        [0.0000, 0.1469, 0.1305,  ..., 0.0515, 0.0000, 0.1014],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0328, 0.0312,  ..., 0.0112, 0.0000, 0.0235],
        [0.0000, 0.0566, 0.0520,  ..., 0.0195, 0.0000, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(356095.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.2263, 0.2120,  ..., 0.0000, 0.0000, 0.0000],
        [0.0561, 0.2840, 0.2684,  ..., 0.0000, 0.0000, 0.0000],
        [0.0635, 0.3131, 0.2967,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0568, 0.0538,  ..., 0.0000, 0.0080, 0.0000],
        [0.0186, 0.1296, 0.1215,  ..., 0.0000, 0.0026, 0.0000],
        [0.0314, 0.1850, 0.1732,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2105276., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3140.4426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1821.9941, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92832.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4906.0801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20975.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5327.3096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0337],
        [ 0.0268],
        [ 0.0197],
        ...,
        [-0.0159],
        [ 0.0021],
        [ 0.0108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-119510.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2537.0264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [-0.0062,  0.0196,  0.0178,  ...,  0.0067, -0.0162,  0.0137],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1104.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-111.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.3832, device='cuda:0')



h[100].sum tensor(-74.1363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-134.0756, device='cuda:0')



h[200].sum tensor(24.3479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.7267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0271, 0.0263,  ..., 0.0092, 0.0000, 0.0195],
        [0.0000, 0.0312, 0.0300,  ..., 0.0104, 0.0000, 0.0224],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0138, 0.0146,  ..., 0.0047, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(324310.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0367, 0.0332,  ..., 0.0000, 0.0137, 0.0000],
        [0.0071, 0.0854, 0.0766,  ..., 0.0000, 0.0069, 0.0000],
        [0.0122, 0.1174, 0.1048,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0132, 0.0127,  ..., 0.0000, 0.0173, 0.0000],
        [0.0000, 0.0261, 0.0244,  ..., 0.0000, 0.0150, 0.0000],
        [0.0058, 0.0670, 0.0619,  ..., 0.0000, 0.0087, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1940161.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2225.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1744.6990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84288.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4528.8442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18353.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4867.8052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0105],
        [ 0.0387],
        [ 0.0615],
        ...,
        [-0.0647],
        [-0.0416],
        [-0.0131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-123605.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2993.3081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [-0.0056,  0.0177,  0.0161,  ...,  0.0061, -0.0147,  0.0124],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0003,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-696.7275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-131.2320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.7468, device='cuda:0')



h[100].sum tensor(-87.3303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-158.1890, device='cuda:0')



h[200].sum tensor(53.3076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.3319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0177, 0.0179,  ..., 0.0061, 0.0000, 0.0129],
        [0.0000, 0.0144, 0.0151,  ..., 0.0049, 0.0000, 0.0107],
        [0.0000, 0.0642, 0.0588,  ..., 0.0219, 0.0000, 0.0450],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(365509.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0581, 0.0520,  ..., 0.0000, 0.0102, 0.0000],
        [0.0020, 0.0632, 0.0570,  ..., 0.0000, 0.0090, 0.0000],
        [0.0076, 0.0942, 0.0855,  ..., 0.0000, 0.0032, 0.0000],
        ...,
        [0.0000, 0.0111, 0.0108,  ..., 0.0000, 0.0177, 0.0000],
        [0.0000, 0.0111, 0.0108,  ..., 0.0000, 0.0177, 0.0000],
        [0.0000, 0.0111, 0.0108,  ..., 0.0000, 0.0177, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2123292.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2536.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1856.8341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93719.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5008.0786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21231.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5527.9922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0250],
        [ 0.0160],
        [ 0.0087],
        ...,
        [-0.0852],
        [-0.0848],
        [-0.0847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-98465.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2886.8262, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038,  0.0119,  0.0111,  ...,  0.0040, -0.0099,  0.0084],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-791.3304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-125.9948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.2617, device='cuda:0')



h[100].sum tensor(-83.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-152.5617, device='cuda:0')



h[200].sum tensor(39.9042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.7905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0096, 0.0109,  ..., 0.0032, 0.0000, 0.0073],
        [0.0000, 0.0119, 0.0128,  ..., 0.0040, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(359183.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0456, 0.0398,  ..., 0.0000, 0.0142, 0.0000],
        [0.0004, 0.0397, 0.0345,  ..., 0.0000, 0.0151, 0.0000],
        [0.0000, 0.0238, 0.0204,  ..., 0.0000, 0.0178, 0.0000],
        ...,
        [0.0000, 0.0117, 0.0107,  ..., 0.0000, 0.0189, 0.0000],
        [0.0000, 0.0117, 0.0107,  ..., 0.0000, 0.0189, 0.0000],
        [0.0000, 0.0117, 0.0107,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2130635.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2351.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1849.9691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93848.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4926.2612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21026.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5459.5576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0211],
        [ 0.0233],
        [ 0.0288],
        ...,
        [-0.0894],
        [-0.0890],
        [-0.0889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-96751.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2687.1079, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073,  0.0231,  0.0209,  ...,  0.0080, -0.0191,  0.0160],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [-0.0058,  0.0183,  0.0166,  ...,  0.0063, -0.0152,  0.0127],
        ...,
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001],
        [ 0.0000, -0.0002,  0.0006,  ..., -0.0004,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-958.1771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-116.9723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4763, device='cuda:0')



h[100].sum tensor(-77.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-142.0071, device='cuda:0')



h[200].sum tensor(23.0568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.8993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0189, 0.0189,  ..., 0.0065, 0.0000, 0.0135],
        [0.0000, 0.0562, 0.0516,  ..., 0.0193, 0.0000, 0.0392],
        [0.0000, 0.0551, 0.0507,  ..., 0.0189, 0.0000, 0.0385],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(342484.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0144, 0.1308, 0.1179,  ..., 0.0000, 0.0014, 0.0000],
        [0.0285, 0.1917, 0.1745,  ..., 0.0000, 0.0000, 0.0000],
        [0.0369, 0.2281, 0.2086,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0121, 0.0106,  ..., 0.0000, 0.0201, 0.0000],
        [0.0000, 0.0121, 0.0106,  ..., 0.0000, 0.0201, 0.0000],
        [0.0000, 0.0121, 0.0106,  ..., 0.0000, 0.0201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2074431., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2569.4451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1776.4323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89971.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4715.0986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19937.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5198.5361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0573],
        [ 0.0608],
        [ 0.0653],
        ...,
        [-0.0947],
        [-0.0943],
        [-0.0942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-142881.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 600 loss: tensor(2794.8816, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2698.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6664e-02,  5.3098e-02,  4.6858e-02,  ...,  1.8742e-02,
         -4.3652e-02,  3.6382e-02],
        [-1.8371e-02,  5.8562e-02,  5.1599e-02,  ...,  2.0698e-02,
         -4.8123e-02,  4.0098e-02],
        [-2.0192e-02,  6.4394e-02,  5.6659e-02,  ...,  2.2785e-02,
         -5.2894e-02,  4.4064e-02],
        ...,
        [ 0.0000e+00, -2.5423e-04,  5.6675e-04,  ..., -3.5386e-04,
          0.0000e+00,  9.7219e-05],
        [ 0.0000e+00, -2.5423e-04,  5.6675e-04,  ..., -3.5386e-04,
          0.0000e+00,  9.7219e-05],
        [ 0.0000e+00, -2.5423e-04,  5.6675e-04,  ..., -3.5386e-04,
          0.0000e+00,  9.7219e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-915.2115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-117.5270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.6407, device='cuda:0')



h[100].sum tensor(-77.9518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-142.6301, device='cuda:0')



h[200].sum tensor(21.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.0700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1508, 0.1340,  ..., 0.0529, 0.0000, 0.1036],
        [0.0000, 0.2077, 0.1834,  ..., 0.0733, 0.0000, 0.1423],
        [0.0000, 0.1827, 0.1617,  ..., 0.0644, 0.0000, 0.1254],
        ...,
        [0.0000, 0.0281, 0.0271,  ..., 0.0095, 0.0000, 0.0198],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336353.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0816, 0.4186, 0.3923,  ..., 0.0018, 0.0000, 0.0000],
        [0.0914, 0.4580, 0.4318,  ..., 0.0042, 0.0000, 0.0000],
        [0.0833, 0.4236, 0.3993,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0117, 0.1168, 0.1040,  ..., 0.0000, 0.0066, 0.0000],
        [0.0022, 0.0553, 0.0487,  ..., 0.0000, 0.0149, 0.0000],
        [0.0000, 0.0234, 0.0200,  ..., 0.0000, 0.0198, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2026330.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1520.7009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1796.9827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88590.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4629.4824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18343.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5114.7383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0306],
        [ 0.0276],
        [ 0.0294],
        ...,
        [ 0.0130],
        [-0.0093],
        [-0.0407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-132960.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2846.9390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0394e-02,  3.3081e-02,  2.9503e-02,  ...,  1.1590e-02,
         -2.7266e-02,  2.2762e-02],
        [-5.3780e-03,  1.6987e-02,  1.5534e-02,  ...,  5.8274e-03,
         -1.4107e-02,  1.1815e-02],
        [ 0.0000e+00, -2.6821e-04,  5.5837e-04,  ..., -3.5113e-04,
          0.0000e+00,  7.8469e-05],
        ...,
        [ 0.0000e+00, -2.6821e-04,  5.5837e-04,  ..., -3.5113e-04,
          0.0000e+00,  7.8469e-05],
        [ 0.0000e+00, -2.6821e-04,  5.5837e-04,  ..., -3.5113e-04,
          0.0000e+00,  7.8469e-05],
        [ 0.0000e+00, -2.6821e-04,  5.5837e-04,  ..., -3.5113e-04,
          0.0000e+00,  7.8469e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-741.0929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-123.6770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.7054, device='cuda:0')



h[100].sum tensor(-81.9398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-150.4537, device='cuda:0')



h[200].sum tensor(28.6494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.2131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1415, 0.1260,  ..., 0.0496, 0.0000, 0.0973],
        [0.0000, 0.0592, 0.0540,  ..., 0.0207, 0.0000, 0.0409],
        [0.0000, 0.0542, 0.0500,  ..., 0.0187, 0.0000, 0.0377],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355059.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.0170e-02, 3.3359e-01, 3.1060e-01,  ..., 1.7379e-04, 0.0000e+00,
         0.0000e+00],
        [3.6690e-02, 2.3396e-01, 2.1493e-01,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8916e-02, 2.0341e-01, 1.8412e-01,  ..., 0.0000e+00, 6.6165e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.2482e-02, 1.0478e-02,  ..., 0.0000e+00, 2.2394e-02,
         0.0000e+00],
        [0.0000e+00, 1.2482e-02, 1.0478e-02,  ..., 0.0000e+00, 2.2394e-02,
         0.0000e+00],
        [0.0000e+00, 1.2482e-02, 1.0478e-02,  ..., 0.0000e+00, 2.2394e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2131145.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2096.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1816.7295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93422.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4822.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19491.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5408.2256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0055],
        [ 0.0250],
        [ 0.0413],
        ...,
        [-0.1054],
        [-0.0977],
        [-0.0809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-165040.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2788.1011, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05],
        [ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05],
        [ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05],
        ...,
        [ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05],
        [ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05],
        [ 0.0000e+00, -2.7663e-04,  5.4787e-04,  ..., -3.5138e-04,
          0.0000e+00,  6.0036e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-758.3318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-120.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.8848, device='cuda:0')



h[100].sum tensor(-79.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-147.3443, device='cuda:0')



h[200].sum tensor(22.1992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.3613, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351882.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0106,  ..., 0.0000, 0.0236, 0.0000],
        [0.0000, 0.0128, 0.0106,  ..., 0.0000, 0.0236, 0.0000],
        [0.0000, 0.0129, 0.0106,  ..., 0.0000, 0.0237, 0.0000],
        ...,
        [0.0000, 0.0127, 0.0105,  ..., 0.0000, 0.0235, 0.0000],
        [0.0000, 0.0127, 0.0105,  ..., 0.0000, 0.0235, 0.0000],
        [0.0000, 0.0127, 0.0105,  ..., 0.0000, 0.0235, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2140572., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1696.5988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1816.6091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94238.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4763.3076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19043.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5357.3916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0362],
        [-0.0456],
        [-0.0556],
        ...,
        [-0.1129],
        [-0.1124],
        [-0.1123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-169320.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2662.8638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05],
        [ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05],
        [ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05],
        ...,
        [ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05],
        [ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05],
        [ 0.0000e+00, -2.8040e-04,  5.3631e-04,  ..., -3.5448e-04,
          0.0000e+00,  4.0717e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-828.2156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.8597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1382, device='cuda:0')



h[100].sum tensor(-75.9281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-140.7258, device='cuda:0')



h[200].sum tensor(11.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.5483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338885.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0106,  ..., 0.0000, 0.0247, 0.0000],
        [0.0000, 0.0241, 0.0206,  ..., 0.0000, 0.0229, 0.0000],
        [0.0000, 0.0392, 0.0344,  ..., 0.0000, 0.0205, 0.0000],
        ...,
        [0.0000, 0.0130, 0.0105,  ..., 0.0000, 0.0245, 0.0000],
        [0.0000, 0.0130, 0.0105,  ..., 0.0000, 0.0245, 0.0000],
        [0.0000, 0.0130, 0.0105,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2103804.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1455.6018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1787.9926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92126.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4592.0200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17979.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5150.2720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1617],
        [-0.1369],
        [-0.1060],
        ...,
        [-0.1181],
        [-0.1176],
        [-0.1174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-187582.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2646.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8096e-04,  5.2731e-04,  ..., -3.5820e-04,
          0.0000e+00,  2.1889e-05],
        [ 0.0000e+00, -2.8096e-04,  5.2731e-04,  ..., -3.5820e-04,
          0.0000e+00,  2.1889e-05],
        [ 0.0000e+00, -2.8096e-04,  5.2731e-04,  ..., -3.5820e-04,
          0.0000e+00,  2.1889e-05],
        ...,
        [-8.9278e-03,  2.8552e-02,  2.5561e-02,  ...,  9.9722e-03,
         -2.3515e-02,  1.9636e-02],
        [-3.1134e-03,  9.7741e-03,  9.2573e-03,  ...,  3.2443e-03,
         -8.2006e-03,  6.8622e-03],
        [-2.9718e-03,  9.3166e-03,  8.8601e-03,  ...,  3.0804e-03,
         -7.8275e-03,  6.5509e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-774.3263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.3337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.9163, device='cuda:0')



h[100].sum tensor(-75.4954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-139.8850, device='cuda:0')



h[200].sum tensor(7.9097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.3180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.1092e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.7558e-05],
        [0.0000e+00, 0.0000e+00, 2.1092e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.7558e-05],
        [0.0000e+00, 0.0000e+00, 2.1092e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.7558e-05],
        ...,
        [0.0000e+00, 4.8760e-02, 4.5420e-02,  ..., 1.6440e-02, 0.0000e+00,
         3.4023e-02],
        [0.0000e+00, 6.8685e-02, 6.2719e-02,  ..., 2.3579e-02, 0.0000e+00,
         4.7577e-02],
        [0.0000e+00, 2.5539e-02, 2.4771e-02,  ..., 8.6353e-03, 0.0000e+00,
         1.7844e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(332892.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0171, 0.0130,  ..., 0.0000, 0.0257, 0.0000],
        [0.0000, 0.0196, 0.0152,  ..., 0.0000, 0.0253, 0.0000],
        [0.0000, 0.0300, 0.0240,  ..., 0.0000, 0.0241, 0.0000],
        ...,
        [0.0228, 0.1888, 0.1672,  ..., 0.0000, 0.0016, 0.0000],
        [0.0176, 0.1628, 0.1440,  ..., 0.0000, 0.0048, 0.0000],
        [0.0060, 0.0944, 0.0817,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2050710.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1204.9047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1771.3539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89259.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4500.4390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16771.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5054.2305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.9848e-02],
        [-4.8125e-02],
        [-2.2519e-02],
        ...,
        [ 3.3845e-05],
        [-2.9445e-03],
        [-1.7893e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-204848.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3136.3706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0113e-02,  3.2457e-02,  2.8942e-02,  ...,  1.1362e-02,
         -2.6673e-02,  2.2268e-02],
        [-1.1622e-02,  3.7343e-02,  3.3184e-02,  ...,  1.3113e-02,
         -3.0654e-02,  2.5591e-02],
        [-5.4594e-03,  1.7396e-02,  1.5865e-02,  ...,  5.9656e-03,
         -1.4400e-02,  1.2022e-02],
        ...,
        [ 0.0000e+00, -2.7553e-04,  5.2207e-04,  ..., -3.6580e-04,
          0.0000e+00,  6.2906e-07],
        [ 0.0000e+00, -2.7553e-04,  5.2207e-04,  ..., -3.6580e-04,
          0.0000e+00,  6.2906e-07],
        [ 0.0000e+00, -2.7553e-04,  5.2207e-04,  ..., -3.6580e-04,
          0.0000e+00,  6.2906e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-222.5075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-134.9212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.7420, device='cuda:0')



h[100].sum tensor(-88.9891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-165.7495, device='cuda:0')



h[200].sum tensor(35.1305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.4030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.6238e-01, 1.4403e-01,  ..., 5.7111e-02, 0.0000e+00,
         1.1121e-01],
        [0.0000e+00, 1.4496e-01, 1.2890e-01,  ..., 5.0868e-02, 0.0000e+00,
         9.9362e-02],
        [0.0000e+00, 1.4354e-01, 1.2767e-01,  ..., 5.0360e-02, 0.0000e+00,
         9.8397e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 2.0883e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5162e-06],
        [0.0000e+00, 0.0000e+00, 2.0883e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5162e-06],
        [0.0000e+00, 0.0000e+00, 2.0883e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5162e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401479.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0643, 0.3723, 0.3466,  ..., 0.0000, 0.0000, 0.0000],
        [0.0662, 0.3826, 0.3554,  ..., 0.0000, 0.0000, 0.0000],
        [0.0655, 0.3811, 0.3530,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0137, 0.0108,  ..., 0.0000, 0.0263, 0.0000],
        [0.0000, 0.0137, 0.0108,  ..., 0.0000, 0.0263, 0.0000],
        [0.0000, 0.0137, 0.0108,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2464836., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2447.8394, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1911.5439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109305.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5275.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23480.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6156.2163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0081],
        [-0.0110],
        [-0.0080],
        ...,
        [-0.1277],
        [-0.1272],
        [-0.1270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-184601.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2628.4487, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.7050e-03,  8.5100e-03,  8.1329e-03,  ...,  2.7676e-03,
         -7.1444e-03,  5.9493e-03],
        [ 0.0000e+00, -2.6583e-04,  5.1449e-04,  ..., -3.7618e-04,
          0.0000e+00, -2.0067e-05],
        [ 0.0000e+00, -2.6583e-04,  5.1449e-04,  ..., -3.7618e-04,
          0.0000e+00, -2.0067e-05],
        ...,
        [ 0.0000e+00, -2.6583e-04,  5.1449e-04,  ..., -3.7618e-04,
          0.0000e+00, -2.0067e-05],
        [ 0.0000e+00, -2.6583e-04,  5.1449e-04,  ..., -3.7618e-04,
          0.0000e+00, -2.0067e-05],
        [ 0.0000e+00, -2.6583e-04,  5.1449e-04,  ..., -3.7618e-04,
          0.0000e+00, -2.0067e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-650.0247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-113.0564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.6582, device='cuda:0')



h[100].sum tensor(-74.4834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-138.9071, device='cuda:0')



h[200].sum tensor(-1.0981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.0501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0309, 0.0296,  ..., 0.0102, 0.0000, 0.0215],
        [0.0000, 0.0154, 0.0159,  ..., 0.0050, 0.0000, 0.0108],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337852.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.1138, 0.0956,  ..., 0.0000, 0.0133, 0.0000],
        [0.0007, 0.0687, 0.0558,  ..., 0.0000, 0.0206, 0.0000],
        [0.0000, 0.0329, 0.0261,  ..., 0.0000, 0.0253, 0.0000],
        ...,
        [0.0000, 0.0142, 0.0110,  ..., 0.0000, 0.0271, 0.0000],
        [0.0000, 0.0142, 0.0110,  ..., 0.0000, 0.0271, 0.0000],
        [0.0000, 0.0142, 0.0110,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2123396.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1068.8988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1790.2845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92240., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4532.1094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17676.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5148.0098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0371],
        [ 0.0092],
        [-0.0277],
        ...,
        [-0.1315],
        [-0.1308],
        [-0.1305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-193296.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(3294.0298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5374e-04,  5.1092e-04,  ..., -3.8657e-04,
          0.0000e+00, -3.8511e-05],
        [-3.4180e-03,  1.0861e-02,  1.0158e-02,  ...,  3.5944e-03,
         -9.0402e-03,  7.5213e-03],
        [ 0.0000e+00, -2.5374e-04,  5.1092e-04,  ..., -3.8657e-04,
          0.0000e+00, -3.8511e-05],
        ...,
        [ 0.0000e+00, -2.5374e-04,  5.1092e-04,  ..., -3.8657e-04,
          0.0000e+00, -3.8511e-05],
        [ 0.0000e+00, -2.5374e-04,  5.1092e-04,  ..., -3.8657e-04,
          0.0000e+00, -3.8511e-05],
        [ 0.0000e+00, -2.5374e-04,  5.1092e-04,  ..., -3.8657e-04,
          0.0000e+00, -3.8511e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(109.6281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.9409, device='cuda:0')



h[100].sum tensor(-92.9752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-174.0814, device='cuda:0')



h[200].sum tensor(37.5711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.6853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0464, 0.0432,  ..., 0.0154, 0.0000, 0.0321],
        [0.0000, 0.0227, 0.0222,  ..., 0.0075, 0.0000, 0.0157],
        [0.0000, 0.0177, 0.0178,  ..., 0.0057, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(415777.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.1071, 0.0885,  ..., 0.0000, 0.0150, 0.0000],
        [0.0012, 0.0899, 0.0739,  ..., 0.0000, 0.0176, 0.0000],
        [0.0006, 0.0727, 0.0586,  ..., 0.0000, 0.0206, 0.0000],
        ...,
        [0.0000, 0.0146, 0.0112,  ..., 0.0000, 0.0277, 0.0000],
        [0.0000, 0.0146, 0.0112,  ..., 0.0000, 0.0277, 0.0000],
        [0.0000, 0.0146, 0.0112,  ..., 0.0000, 0.0277, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2532945.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2435.1555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1950.4338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111571.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5422.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24517.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6407.6562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0602],
        [ 0.0506],
        [ 0.0357],
        ...,
        [-0.1355],
        [-0.1349],
        [-0.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-173542.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2694.5339, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4246e-04,  5.0213e-04,  ..., -3.9411e-04,
          0.0000e+00, -5.0520e-05],
        [ 0.0000e+00, -2.4246e-04,  5.0213e-04,  ..., -3.9411e-04,
          0.0000e+00, -5.0520e-05],
        [-2.9533e-03,  9.3842e-03,  8.8547e-03,  ...,  3.0523e-03,
         -7.8220e-03,  6.4962e-03],
        ...,
        [ 0.0000e+00, -2.4246e-04,  5.0213e-04,  ..., -3.9411e-04,
          0.0000e+00, -5.0520e-05],
        [ 0.0000e+00, -2.4246e-04,  5.0213e-04,  ..., -3.9411e-04,
          0.0000e+00, -5.0520e-05],
        [ 0.0000e+00, -2.4246e-04,  5.0213e-04,  ..., -3.9411e-04,
          0.0000e+00, -5.0520e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-429.7025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.2282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5799, device='cuda:0')



h[100].sum tensor(-75.7415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-142.3995, device='cuda:0')



h[200].sum tensor(-3.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.0068, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0104,  ..., 0.0031, 0.0000, 0.0065],
        [0.0000, 0.0462, 0.0425,  ..., 0.0159, 0.0000, 0.0316],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351285.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0683, 0.0558,  ..., 0.0000, 0.0209, 0.0000],
        [0.0043, 0.0966, 0.0818,  ..., 0.0000, 0.0152, 0.0000],
        [0.0136, 0.1558, 0.1367,  ..., 0.0000, 0.0073, 0.0000],
        ...,
        [0.0000, 0.0149, 0.0115,  ..., 0.0000, 0.0284, 0.0000],
        [0.0000, 0.0149, 0.0115,  ..., 0.0000, 0.0284, 0.0000],
        [0.0000, 0.0149, 0.0115,  ..., 0.0000, 0.0284, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2222749.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1511.1884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1815.1525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95501., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4671.6157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19229.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5368.3403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0790],
        [ 0.0825],
        [ 0.0852],
        ...,
        [-0.1397],
        [-0.1391],
        [-0.1389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-199126.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2779.9036, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05],
        [ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05],
        [ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05],
        ...,
        [ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05],
        [ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05],
        [ 0.0000e+00, -2.3177e-04,  5.0444e-04,  ..., -3.9784e-04,
          0.0000e+00, -5.8997e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-253.0204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-118.7422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7705, device='cuda:0')



h[100].sum tensor(-77.9621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-146.9111, device='cuda:0')



h[200].sum tensor(0.6358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.2426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(360167.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0262, 0.0209,  ..., 0.0000, 0.0275, 0.0000],
        [0.0000, 0.0188, 0.0146,  ..., 0.0000, 0.0286, 0.0000],
        [0.0000, 0.0250, 0.0195,  ..., 0.0000, 0.0280, 0.0000],
        ...,
        [0.0010, 0.0599, 0.0512,  ..., 0.0000, 0.0206, 0.0000],
        [0.0000, 0.0261, 0.0210,  ..., 0.0000, 0.0272, 0.0000],
        [0.0000, 0.0151, 0.0117,  ..., 0.0000, 0.0289, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2276664.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1520.8489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1833.0750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97746.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4764.0063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20046.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5501.2583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0158],
        [-0.0035],
        [-0.0078],
        ...,
        [-0.0104],
        [-0.0576],
        [-0.1004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-200421.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 900 loss: tensor(1628.1456, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2706.7080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.2654e-04,  5.1662e-04,  ..., -3.9332e-04,
          0.0000e+00, -6.1161e-05],
        [ 0.0000e+00, -2.2654e-04,  5.1662e-04,  ..., -3.9332e-04,
          0.0000e+00, -6.1161e-05],
        [ 0.0000e+00, -2.2654e-04,  5.1662e-04,  ..., -3.9332e-04,
          0.0000e+00, -6.1161e-05],
        ...,
        [-5.5004e-03,  1.7787e-02,  1.6138e-02,  ...,  6.0529e-03,
         -1.4609e-02,  1.2188e-02],
        [ 0.0000e+00, -2.2654e-04,  5.1662e-04,  ..., -3.9332e-04,
          0.0000e+00, -6.1161e-05],
        [ 0.0000e+00, -2.2654e-04,  5.1662e-04,  ..., -3.9332e-04,
          0.0000e+00, -6.1161e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-246.9494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.1588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.7497, device='cuda:0')



h[100].sum tensor(-75.5226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-143.0429, device='cuda:0')



h[200].sum tensor(-3.3991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1830, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0371, 0.0348,  ..., 0.0123, 0.0000, 0.0255],
        [0.0000, 0.0315, 0.0298,  ..., 0.0106, 0.0000, 0.0216],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350812.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0296, 0.0235,  ..., 0.0000, 0.0277, 0.0000],
        [0.0000, 0.0187, 0.0144,  ..., 0.0000, 0.0294, 0.0000],
        [0.0000, 0.0157, 0.0122,  ..., 0.0000, 0.0297, 0.0000],
        ...,
        [0.0166, 0.1769, 0.1570,  ..., 0.0000, 0.0032, 0.0000],
        [0.0125, 0.1512, 0.1342,  ..., 0.0000, 0.0049, 0.0000],
        [0.0049, 0.1030, 0.0907,  ..., 0.0000, 0.0117, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2237673., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1133.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1828.1532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95984.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4636.2324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18828.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5343.3701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0187],
        [-0.0417],
        [-0.0515],
        ...,
        [ 0.0472],
        [ 0.0446],
        [ 0.0409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-212041.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(2729.9536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.2097e-04,  5.3542e-04,  ..., -3.8761e-04,
          0.0000e+00, -6.0922e-05],
        [ 0.0000e+00, -2.2097e-04,  5.3542e-04,  ..., -3.8761e-04,
          0.0000e+00, -6.0922e-05],
        [ 0.0000e+00, -2.2097e-04,  5.3542e-04,  ..., -3.8761e-04,
          0.0000e+00, -6.0922e-05],
        ...,
        [-6.8942e-03,  2.2409e-02,  2.0157e-02,  ...,  7.7103e-03,
         -1.8336e-02,  1.5328e-02],
        [-3.4537e-03,  1.1116e-02,  1.0365e-02,  ...,  3.6691e-03,
         -9.1858e-03,  7.6483e-03],
        [-3.9502e-03,  1.2745e-02,  1.1778e-02,  ...,  4.2522e-03,
         -1.0506e-02,  8.7564e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-124.2774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.9857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.0739, device='cuda:0')



h[100].sum tensor(-75.9772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-144.2713, device='cuda:0')



h[200].sum tensor(-0.2188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.5196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0683, 0.0621,  ..., 0.0232, 0.0000, 0.0468],
        [0.0000, 0.0651, 0.0594,  ..., 0.0221, 0.0000, 0.0447],
        [0.0000, 0.0301, 0.0289,  ..., 0.0099, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353631.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0204, 0.0158,  ..., 0.0000, 0.0297, 0.0000],
        [0.0000, 0.0162, 0.0127,  ..., 0.0000, 0.0301, 0.0000],
        [0.0000, 0.0153, 0.0120,  ..., 0.0000, 0.0302, 0.0000],
        ...,
        [0.0230, 0.2170, 0.1928,  ..., 0.0000, 0.0000, 0.0000],
        [0.0211, 0.2069, 0.1834,  ..., 0.0000, 0.0000, 0.0000],
        [0.0152, 0.1761, 0.1548,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2261786., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1182.7312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1839.4559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97006.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4647.6611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18767.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5377.0889, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0621],
        [-0.0907],
        [-0.1192],
        ...,
        [ 0.0561],
        [ 0.0551],
        [ 0.0532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-234388.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1821e-03,  6.9654e-03,  6.7854e-03,  ...,  2.1862e-03,
         -5.8118e-03,  4.8215e-03],
        [-7.9509e-03,  2.5945e-02,  2.3240e-02,  ...,  8.9777e-03,
         -2.1176e-02,  1.7728e-02],
        [-3.1890e-03,  1.0278e-02,  9.6575e-03,  ...,  3.3716e-03,
         -8.4936e-03,  7.0743e-03],
        ...,
        [ 0.0000e+00, -2.1373e-04,  5.6117e-04,  ..., -3.8270e-04,
          0.0000e+00, -6.0707e-05],
        [ 0.0000e+00, -2.1373e-04,  5.6117e-04,  ..., -3.8270e-04,
          0.0000e+00, -6.0707e-05],
        [ 0.0000e+00, -2.1373e-04,  5.6117e-04,  ..., -3.8270e-04,
          0.0000e+00, -6.0707e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1952.5447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-39.9373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0432, device='cuda:0')



h[100].sum tensor(-26.1309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-49.4239, device='cuda:0')



h[200].sum tensor(-113.1801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0958, 0.0860,  ..., 0.0331, 0.0000, 0.0655],
        [0.0000, 0.0455, 0.0424,  ..., 0.0151, 0.0000, 0.0313],
        [0.0000, 0.0467, 0.0433,  ..., 0.0158, 0.0000, 0.0320],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(172550.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0181, 0.1917, 0.1713,  ..., 0.0000, 0.0000, 0.0000],
        [0.0131, 0.1709, 0.1499,  ..., 0.0000, 0.0012, 0.0000],
        [0.0117, 0.1618, 0.1421,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0304, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0304, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0304, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1440153.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(442.9401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1371.4851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52876.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2502.0571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6399.1802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2405.9971, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0473],
        [ 0.0495],
        [ 0.0475],
        ...,
        [-0.1570],
        [-0.1563],
        [-0.1561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-439140.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        ...,
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2901.3428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-172.4480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(77515.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0152, 0.0119,  ..., 0.0000, 0.0309, 0.0000],
        [0.0000, 0.0152, 0.0119,  ..., 0.0000, 0.0309, 0.0000],
        [0.0000, 0.0153, 0.0119,  ..., 0.0000, 0.0310, 0.0000],
        ...,
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1019157.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1135.6270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30353.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1364.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(852.2416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2431],
        [-0.2454],
        [-0.2492],
        ...,
        [-0.1607],
        [-0.1600],
        [-0.1598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-542791.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        ...,
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05],
        [ 0.0000e+00, -2.0555e-04,  5.8720e-04,  ..., -3.7942e-04,
          0.0000e+00, -6.1137e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2901.3428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-172.4480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(77515.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0152, 0.0119,  ..., 0.0000, 0.0309, 0.0000],
        [0.0000, 0.0152, 0.0119,  ..., 0.0000, 0.0309, 0.0000],
        [0.0000, 0.0153, 0.0119,  ..., 0.0000, 0.0310, 0.0000],
        ...,
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0151, 0.0118,  ..., 0.0000, 0.0308, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1019157.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1135.6270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30353.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1364.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(852.2416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2431],
        [-0.2454],
        [-0.2492],
        ...,
        [-0.1607],
        [-0.1600],
        [-0.1598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-542791.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05],
        [ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05],
        [ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05],
        ...,
        [ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05],
        [ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05],
        [ 0.0000e+00, -1.9809e-04,  6.1090e-04,  ..., -3.7644e-04,
          0.0000e+00, -6.1529e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2837.9985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-171.5500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(78145.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0153, 0.0119,  ..., 0.0000, 0.0313, 0.0000],
        [0.0000, 0.0153, 0.0119,  ..., 0.0000, 0.0313, 0.0000],
        [0.0000, 0.0153, 0.0119,  ..., 0.0000, 0.0314, 0.0000],
        ...,
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0311, 0.0000],
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0311, 0.0000],
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0311, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1030786.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1141.3320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30862.1602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1350.8025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(862.2156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2479],
        [-0.2502],
        [-0.2541],
        ...,
        [-0.1639],
        [-0.1632],
        [-0.1630]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-552332.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05],
        [ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05],
        [ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05],
        ...,
        [ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05],
        [ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05],
        [ 0.0000e+00, -1.9129e-04,  6.3250e-04,  ..., -3.7372e-04,
          0.0000e+00, -6.1886e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2780.2925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-170.7321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(78719.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0316, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0316, 0.0000],
        [0.0000, 0.0154, 0.0119,  ..., 0.0000, 0.0317, 0.0000],
        ...,
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0315, 0.0000],
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0315, 0.0000],
        [0.0000, 0.0152, 0.0118,  ..., 0.0000, 0.0315, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1041394.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1146.5286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31325.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1338.6383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(871.3014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2522],
        [-0.2545],
        [-0.2585],
        ...,
        [-0.1668],
        [-0.1661],
        [-0.1659]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-561022.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05],
        [ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05],
        [ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05],
        ...,
        [ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05],
        [ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05],
        [ 0.0000e+00, -1.8510e-04,  6.5217e-04,  ..., -3.7124e-04,
          0.0000e+00, -6.2211e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2727.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-169.9870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(79242.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0319, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0319, 0.0000],
        [0.0000, 0.0155, 0.0119,  ..., 0.0000, 0.0320, 0.0000],
        ...,
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0318, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0318, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0318, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1051148.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1151.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31746.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1327.5583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(879.5774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2561],
        [-0.2584],
        [-0.2625],
        ...,
        [-0.1694],
        [-0.1687],
        [-0.1684]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-568910.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        ...,
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2679.8606, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-169.3085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(79738.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0322, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0322, 0.0000],
        [0.0000, 0.0155, 0.0119,  ..., 0.0000, 0.0323, 0.0000],
        ...,
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1060026.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1154.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32148.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1319.7251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(888.1849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2594],
        [-0.2618],
        [-0.2659],
        ...,
        [-0.1717],
        [-0.1709],
        [-0.1707]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-575860.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        ...,
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05],
        [ 0.0000e+00, -1.7946e-04,  6.7008e-04,  ..., -3.6899e-04,
          0.0000e+00, -6.2508e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2679.8606, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-169.3085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(79738.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0322, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0322, 0.0000],
        [0.0000, 0.0155, 0.0119,  ..., 0.0000, 0.0323, 0.0000],
        ...,
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0153, 0.0117,  ..., 0.0000, 0.0321, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1060026.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1154.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32148.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1319.7251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(888.1849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2594],
        [-0.2618],
        [-0.2659],
        ...,
        [-0.1717],
        [-0.1709],
        [-0.1707]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-575860.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 1200 loss: tensor(396.2950, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05],
        [ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05],
        [ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05],
        ...,
        [ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05],
        [ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05],
        [ 0.0000e+00, -1.7433e-04,  6.8640e-04,  ..., -3.6693e-04,
          0.0000e+00, -6.2777e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2636.2693, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-168.6905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(80201.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0325, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0325, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0326, 0.0000],
        ...,
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0323, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0323, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0323, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1068250.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1157.8147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32523.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1313.7280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(896.5620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2624],
        [-0.2648],
        [-0.2689],
        ...,
        [-0.1737],
        [-0.1730],
        [-0.1727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-581943.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        ...,
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2596.5747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-168.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(80622.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1075826.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1160.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32865.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1308.2677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(904.1895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2651],
        [-0.2675],
        [-0.2716],
        ...,
        [-0.1756],
        [-0.1748],
        [-0.1746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-587395.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        ...,
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05],
        [ 0.0000e+00, -1.6966e-04,  7.0125e-04,  ..., -3.6506e-04,
          0.0000e+00, -6.3023e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2596.5747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-168.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(80622.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000],
        [0.0000, 0.0154, 0.0118,  ..., 0.0000, 0.0326, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1075826.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1160.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32865.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1308.2677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(904.1895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2651],
        [-0.2675],
        [-0.2716],
        ...,
        [-0.1756],
        [-0.1748],
        [-0.1746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-587395.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05],
        [ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05],
        [ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05],
        ...,
        [ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05],
        [ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05],
        [ 0.0000e+00, -1.6540e-04,  7.1478e-04,  ..., -3.6336e-04,
          0.0000e+00, -6.3247e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2560.4360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-167.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(81006.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0331, 0.0000],
        ...,
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0328, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1082807.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1162.9272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33176.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1303.2969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(911.1341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2674],
        [-0.2699],
        [-0.2741],
        ...,
        [-0.1773],
        [-0.1765],
        [-0.1763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-592284.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05],
        [ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05],
        [ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05],
        ...,
        [ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05],
        [ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05],
        [ 0.0000e+00, -1.6153e-04,  7.2709e-04,  ..., -3.6181e-04,
          0.0000e+00, -6.3450e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2527.5415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-167.1493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(81355.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0156, 0.0119,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0333, 0.0000],
        ...,
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0330, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1089204.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.1450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33459.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1298.7719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(917.4552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2696],
        [-0.2721],
        [-0.2762],
        ...,
        [-0.1788],
        [-0.1780],
        [-0.1778]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-596690.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05],
        [ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05],
        [ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05],
        ...,
        [ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05],
        [ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05],
        [ 0.0000e+00, -1.5800e-04,  7.3830e-04,  ..., -3.6040e-04,
          0.0000e+00, -6.3636e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2497.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-166.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(81691.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0334, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0334, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0335, 0.0000],
        ...,
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0155, 0.0118,  ..., 0.0000, 0.0332, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1095095., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.0066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33700.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1292.7220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(922.1415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2714],
        [-0.2739],
        [-0.2781],
        ...,
        [-0.1801],
        [-0.1793],
        [-0.1791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-600584.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05],
        [ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05],
        [ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05],
        ...,
        [ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05],
        [ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05],
        [ 0.0000e+00, -1.5479e-04,  7.4849e-04,  ..., -3.5912e-04,
          0.0000e+00, -6.3804e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2470.3486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-166.3386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(82013.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0335, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0335, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0336, 0.0000],
        ...,
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0334, 0.0000],
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0334, 0.0000],
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0334, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1100511.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.7861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33905.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1285.5394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(925.4789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2731],
        [-0.2756],
        [-0.2798],
        ...,
        [-0.1812],
        [-0.1804],
        [-0.1802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-604050.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05],
        [ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05],
        [ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05],
        ...,
        [ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05],
        [ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05],
        [ 0.0000e+00, -1.5187e-04,  7.5777e-04,  ..., -3.5795e-04,
          0.0000e+00, -6.3958e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2445.5542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-165.9872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(82308.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0338, 0.0000],
        ...,
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0335, 0.0000],
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0335, 0.0000],
        [0.0000, 0.0156, 0.0118,  ..., 0.0000, 0.0335, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1105423.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.7928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34092.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1278.9963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(928.7728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2745],
        [-0.2770],
        [-0.2812],
        ...,
        [-0.1822],
        [-0.1814],
        [-0.1812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-607201.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05],
        [ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05],
        [ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05],
        ...,
        [ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05],
        [ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05],
        [ 0.0000e+00, -1.4922e-04,  7.6621e-04,  ..., -3.5689e-04,
          0.0000e+00, -6.4097e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2422.9961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-165.6674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(82583.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0338, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0338, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0339, 0.0000],
        ...,
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0337, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1109842., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34263.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1273.0256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(932.3813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2758],
        [-0.2783],
        [-0.2825],
        ...,
        [-0.1831],
        [-0.1823],
        [-0.1821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-610053.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05],
        [ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05],
        [ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05],
        ...,
        [ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05],
        [ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05],
        [ 0.0000e+00, -1.4680e-04,  7.7389e-04,  ..., -3.5592e-04,
          0.0000e+00, -6.4224e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2402.4795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-165.3766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(82832.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0338, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0338, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0338, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1113875.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.7502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34420.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1267.5944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(935.6644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2769],
        [-0.2794],
        [-0.2837],
        ...,
        [-0.1840],
        [-0.1832],
        [-0.1829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-612612.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05],
        [ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05],
        [ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05],
        ...,
        [ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05],
        [ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05],
        [ 0.0000e+00, -1.4460e-04,  7.8088e-04,  ..., -3.5504e-04,
          0.0000e+00, -6.4340e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2383.8174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-165.1121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83059.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0342, 0.0000],
        ...,
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0339, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0339, 0.0000],
        [0.0000, 0.0157, 0.0119,  ..., 0.0000, 0.0339, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1117579.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1167.1643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34562., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1262.6543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(938.6500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2779],
        [-0.2804],
        [-0.2847],
        ...,
        [-0.1847],
        [-0.1839],
        [-0.1837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-614851.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05],
        [ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05],
        [ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05],
        ...,
        [ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05],
        [ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05],
        [ 0.0000e+00, -1.4260e-04,  7.8723e-04,  ..., -3.5424e-04,
          0.0000e+00, -6.4445e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2366.8472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-164.8715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83265.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0342, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0342, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0343, 0.0000],
        ...,
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0340, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1120950.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1167.5413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34691.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1258.1621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(941.3653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2788],
        [-0.2813],
        [-0.2856],
        ...,
        [-0.1854],
        [-0.1846],
        [-0.1843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-616630.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05],
        [ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05],
        [ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05],
        ...,
        [ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05],
        [ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05],
        [ 0.0000e+00, -1.4079e-04,  7.9300e-04,  ..., -3.5351e-04,
          0.0000e+00, -6.4540e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2351.4155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-164.6528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83453.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0344, 0.0000],
        ...,
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0341, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1124027.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1167.8840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34808.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1254.0776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(943.8340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2796],
        [-0.2821],
        [-0.2864],
        ...,
        [-0.1860],
        [-0.1852],
        [-0.1849]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-618115.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05],
        [ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05],
        [ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05],
        ...,
        [ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05],
        [ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05],
        [ 0.0000e+00, -1.3914e-04,  7.9825e-04,  ..., -3.5285e-04,
          0.0000e+00, -6.4627e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2337.3887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-164.4539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83623.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0344, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0344, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0345, 0.0000],
        ...,
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0342, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0342, 0.0000],
        [0.0000, 0.0158, 0.0119,  ..., 0.0000, 0.0342, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1126837.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1168.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34915.1602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1250.3645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(946.0786, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2803],
        [-0.2829],
        [-0.2872],
        ...,
        [-0.1865],
        [-0.1857],
        [-0.1855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-619484.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05],
        [ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05],
        [ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05],
        ...,
        [ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05],
        [ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05],
        [ 0.0000e+00, -1.3763e-04,  8.0302e-04,  ..., -3.5225e-04,
          0.0000e+00, -6.4706e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2324.6382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-164.2732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83781.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        ...,
        [0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0158, 0.0120,  ..., 0.0000, 0.0343, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1129448.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1168.1782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35010.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1247.1792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(948.1746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2810],
        [-0.2835],
        [-0.2878],
        ...,
        [-0.1870],
        [-0.1862],
        [-0.1860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-620741.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05],
        [ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05],
        [ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05],
        ...,
        [ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05],
        [ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05],
        [ 0.0000e+00, -1.3627e-04,  8.0736e-04,  ..., -3.5171e-04,
          0.0000e+00, -6.4778e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2313.0493, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-164.1089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(83930.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        ...,
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0344, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0344, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0344, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1131909.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1167.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35095.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1244.6051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(950.1748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2816],
        [-0.2841],
        [-0.2884],
        ...,
        [-0.1875],
        [-0.1867],
        [-0.1864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-621883., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05],
        [ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05],
        [ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05],
        ...,
        [ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05],
        [ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05],
        [ 0.0000e+00, -1.3503e-04,  8.1130e-04,  ..., -3.5121e-04,
          0.0000e+00, -6.4843e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2302.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.9596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84064.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0347, 0.0000],
        ...,
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1134151., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1167.1831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35173.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.2664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(951.9920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2821],
        [-0.2847],
        [-0.2890],
        ...,
        [-0.1879],
        [-0.1870],
        [-0.1868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-622917.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05],
        [ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05],
        [ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05],
        ...,
        [ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05],
        [ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05],
        [ 0.0000e+00, -1.3390e-04,  8.1488e-04,  ..., -3.5076e-04,
          0.0000e+00, -6.4902e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2292.9509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.8240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84187.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        ...,
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0345, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1136193., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.7523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35243.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1240.1416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(953.6429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2826],
        [-0.2851],
        [-0.2895],
        ...,
        [-0.1882],
        [-0.1874],
        [-0.1871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-623854.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05],
        [ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05],
        [ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05],
        ...,
        [ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05],
        [ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05],
        [ 0.0000e+00, -1.3288e-04,  8.1814e-04,  ..., -3.5035e-04,
          0.0000e+00, -6.4956e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2284.2593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.7008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84298.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        ...,
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0159, 0.0120,  ..., 0.0000, 0.0346, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1138049.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.3611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35307.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1238.2107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(955.1431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2830],
        [-0.2856],
        [-0.2899],
        ...,
        [-0.1885],
        [-0.1877],
        [-0.1875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-624701.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05],
        [ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05],
        [ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05],
        ...,
        [ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05],
        [ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05],
        [ 0.0000e+00, -1.3195e-04,  8.2109e-04,  ..., -3.4998e-04,
          0.0000e+00, -6.5005e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2276.3638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.5889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84400., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0349, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0346, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0346, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1139735.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1166.0054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35365.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1236.4570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(956.5055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2834],
        [-0.2860],
        [-0.2903],
        ...,
        [-0.1888],
        [-0.1880],
        [-0.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-625468.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05],
        [ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05],
        [ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05],
        ...,
        [ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05],
        [ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05],
        [ 0.0000e+00, -1.3110e-04,  8.2377e-04,  ..., -3.4964e-04,
          0.0000e+00, -6.5049e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2269.1924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84491.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0349, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0349, 0.0000],
        [0.0000, 0.0162, 0.0122,  ..., 0.0000, 0.0350, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1141266.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35417.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1234.8643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(957.7433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2837],
        [-0.2863],
        [-0.2907],
        ...,
        [-0.1891],
        [-0.1883],
        [-0.1880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-626165.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05],
        [ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05],
        [ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05],
        ...,
        [ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05],
        [ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05],
        [ 0.0000e+00, -1.3034e-04,  8.2621e-04,  ..., -3.4933e-04,
          0.0000e+00, -6.5089e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2262.6790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.3949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84575.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0349, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0349, 0.0000],
        [0.0000, 0.0162, 0.0122,  ..., 0.0000, 0.0350, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0160, 0.0120,  ..., 0.0000, 0.0347, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1142655., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.3896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35465.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1233.4175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(958.8667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2841],
        [-0.2866],
        [-0.2910],
        ...,
        [-0.1893],
        [-0.1885],
        [-0.1882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-626796.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05],
        [ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05],
        [ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05],
        ...,
        [ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05],
        [ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05],
        [ 0.0000e+00, -1.2964e-04,  8.2842e-04,  ..., -3.4905e-04,
          0.0000e+00, -6.5126e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2256.7686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.3111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84650.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0162, 0.0122,  ..., 0.0000, 0.0350, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1143914.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1165.1235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35508.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1232.1049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.8874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2843],
        [-0.2869],
        [-0.2913],
        ...,
        [-0.1895],
        [-0.1887],
        [-0.1884]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-627369.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05],
        [ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05],
        [ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05],
        ...,
        [ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05],
        [ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05],
        [ 0.0000e+00, -1.2901e-04,  8.3043e-04,  ..., -3.4880e-04,
          0.0000e+00, -6.5159e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2251.4014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84719.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0161, 0.0121,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0162, 0.0122,  ..., 0.0000, 0.0351, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1145057.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1164.8816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35548.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1230.9124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(960.8132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2846],
        [-0.2872],
        [-0.2915],
        ...,
        [-0.1897],
        [-0.1889],
        [-0.1886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-627891.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05],
        [ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05],
        [ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05],
        ...,
        [ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05],
        [ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05],
        [ 0.0000e+00, -1.2844e-04,  8.3226e-04,  ..., -3.4857e-04,
          0.0000e+00, -6.5189e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2246.5278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(-163.1660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(84782.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0161, 0.0122,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0161, 0.0122,  ..., 0.0000, 0.0350, 0.0000],
        [0.0000, 0.0162, 0.0122,  ..., 0.0000, 0.0351, 0.0000],
        ...,
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0160, 0.0121,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1146095.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1164.6621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35584.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1229.8303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(961.6542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2848],
        [-0.2874],
        [-0.2918],
        ...,
        [-0.1899],
        [-0.1891],
        [-0.1888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-628363.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N3/./Training.py", line 78, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[203880, 1]' is invalid for input of size 135920

real	0m17.559s
user	0m10.758s
sys	0m4.792s
