0: gpu018.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-7449098f-fc64-77b7-cfa8-7be53c656a67)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Tue Aug  9 18:03:36 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1F:00.0 Off |                    0 |
| N/A   46C    P0    46W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2abf25e54880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.357s
user	0m2.648s
sys	0m0.966s
[18:03:44] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.0450],
        [ 0.7129],
        [-0.8240],
        ...,
        [-0.1265],
        [-0.3960],
        [-0.3114]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-16.7403, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1314,  0.0722, -0.0775, -0.1484, -0.0888, -0.0445,  0.0313, -0.1242,
          0.0260, -0.0803, -0.1441,  0.0700,  0.0823, -0.0748,  0.1471,  0.0724,
         -0.0900, -0.0258, -0.0828,  0.0784,  0.1526, -0.0372,  0.1058, -0.1027,
         -0.0615,  0.1323,  0.1299,  0.1286, -0.1199, -0.1180, -0.0334,  0.1072,
         -0.1437,  0.0442,  0.1006,  0.1144, -0.0896,  0.0010, -0.0599, -0.0511,
          0.1247, -0.1316, -0.1177, -0.1406, -0.0925, -0.0307, -0.0898,  0.0347,
          0.0257, -0.0978, -0.1115,  0.0204, -0.0144,  0.1011,  0.1499,  0.1501,
          0.0461, -0.0543,  0.0312,  0.0652, -0.0552,  0.0070, -0.0335,  0.0580,
          0.0652, -0.0616, -0.0643,  0.0272,  0.1072, -0.0305, -0.0560, -0.0562,
         -0.1449,  0.0236,  0.0724,  0.0144, -0.1262, -0.0287,  0.1492, -0.1510,
         -0.0417,  0.0467, -0.1518, -0.1225, -0.1489, -0.0119,  0.1255,  0.0329,
         -0.1001, -0.1376,  0.1369, -0.1302,  0.0178,  0.0921, -0.0107, -0.0251,
          0.0871,  0.0338, -0.0667, -0.1272,  0.1485, -0.0966,  0.1121,  0.0772,
          0.0981,  0.0731,  0.0673, -0.1327,  0.0220,  0.0144, -0.0232,  0.0572,
          0.0831,  0.0287,  0.0591, -0.0901,  0.0041,  0.1070, -0.1191,  0.1383,
         -0.0133, -0.0408,  0.0069, -0.0327,  0.0766, -0.1420,  0.1395,  0.1495,
          0.0700,  0.1093,  0.0240, -0.0472,  0.1239,  0.1120, -0.0874, -0.0669,
         -0.0873, -0.0484,  0.1233,  0.1022,  0.1509, -0.0862, -0.0405, -0.1410,
         -0.0906, -0.1103,  0.0807,  0.1102,  0.1520,  0.1324,  0.0324,  0.0078,
         -0.0747,  0.0037,  0.1450,  0.1030, -0.1486,  0.0820, -0.0868,  0.1318,
         -0.1385, -0.0282, -0.0568,  0.0010, -0.1061, -0.1014,  0.1328, -0.1148,
         -0.1399,  0.0896, -0.1195,  0.0726, -0.0058, -0.0208, -0.1470, -0.0505,
          0.1011, -0.0756,  0.0697,  0.0064,  0.1340, -0.0490, -0.0621, -0.1000,
         -0.0885,  0.0952, -0.1245, -0.1052,  0.1333,  0.0041, -0.0507,  0.0495,
          0.0177,  0.0247, -0.0388, -0.1267, -0.0297, -0.0347, -0.0887,  0.0804,
         -0.0058,  0.0878,  0.1135,  0.0757, -0.1019, -0.1460,  0.0965, -0.0419,
         -0.0869,  0.0811, -0.0381, -0.0563, -0.1285, -0.0856,  0.0950, -0.0578,
         -0.1189,  0.0072,  0.1336, -0.0734,  0.1055, -0.1262, -0.0041,  0.0855,
          0.1326, -0.0361, -0.0080,  0.0758, -0.0136,  0.0370,  0.1263, -0.1235,
         -0.1467, -0.1436, -0.1218,  0.1450,  0.0959,  0.0314,  0.1309,  0.1374,
          0.0488,  0.1483, -0.0203, -0.0221,  0.0241,  0.1395,  0.0259, -0.0332,
          0.0372,  0.1278, -0.1323, -0.0477,  0.1174, -0.0499,  0.0760, -0.1387]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1314,  0.0722, -0.0775, -0.1484, -0.0888, -0.0445,  0.0313, -0.1242,
          0.0260, -0.0803, -0.1441,  0.0700,  0.0823, -0.0748,  0.1471,  0.0724,
         -0.0900, -0.0258, -0.0828,  0.0784,  0.1526, -0.0372,  0.1058, -0.1027,
         -0.0615,  0.1323,  0.1299,  0.1286, -0.1199, -0.1180, -0.0334,  0.1072,
         -0.1437,  0.0442,  0.1006,  0.1144, -0.0896,  0.0010, -0.0599, -0.0511,
          0.1247, -0.1316, -0.1177, -0.1406, -0.0925, -0.0307, -0.0898,  0.0347,
          0.0257, -0.0978, -0.1115,  0.0204, -0.0144,  0.1011,  0.1499,  0.1501,
          0.0461, -0.0543,  0.0312,  0.0652, -0.0552,  0.0070, -0.0335,  0.0580,
          0.0652, -0.0616, -0.0643,  0.0272,  0.1072, -0.0305, -0.0560, -0.0562,
         -0.1449,  0.0236,  0.0724,  0.0144, -0.1262, -0.0287,  0.1492, -0.1510,
         -0.0417,  0.0467, -0.1518, -0.1225, -0.1489, -0.0119,  0.1255,  0.0329,
         -0.1001, -0.1376,  0.1369, -0.1302,  0.0178,  0.0921, -0.0107, -0.0251,
          0.0871,  0.0338, -0.0667, -0.1272,  0.1485, -0.0966,  0.1121,  0.0772,
          0.0981,  0.0731,  0.0673, -0.1327,  0.0220,  0.0144, -0.0232,  0.0572,
          0.0831,  0.0287,  0.0591, -0.0901,  0.0041,  0.1070, -0.1191,  0.1383,
         -0.0133, -0.0408,  0.0069, -0.0327,  0.0766, -0.1420,  0.1395,  0.1495,
          0.0700,  0.1093,  0.0240, -0.0472,  0.1239,  0.1120, -0.0874, -0.0669,
         -0.0873, -0.0484,  0.1233,  0.1022,  0.1509, -0.0862, -0.0405, -0.1410,
         -0.0906, -0.1103,  0.0807,  0.1102,  0.1520,  0.1324,  0.0324,  0.0078,
         -0.0747,  0.0037,  0.1450,  0.1030, -0.1486,  0.0820, -0.0868,  0.1318,
         -0.1385, -0.0282, -0.0568,  0.0010, -0.1061, -0.1014,  0.1328, -0.1148,
         -0.1399,  0.0896, -0.1195,  0.0726, -0.0058, -0.0208, -0.1470, -0.0505,
          0.1011, -0.0756,  0.0697,  0.0064,  0.1340, -0.0490, -0.0621, -0.1000,
         -0.0885,  0.0952, -0.1245, -0.1052,  0.1333,  0.0041, -0.0507,  0.0495,
          0.0177,  0.0247, -0.0388, -0.1267, -0.0297, -0.0347, -0.0887,  0.0804,
         -0.0058,  0.0878,  0.1135,  0.0757, -0.1019, -0.1460,  0.0965, -0.0419,
         -0.0869,  0.0811, -0.0381, -0.0563, -0.1285, -0.0856,  0.0950, -0.0578,
         -0.1189,  0.0072,  0.1336, -0.0734,  0.1055, -0.1262, -0.0041,  0.0855,
          0.1326, -0.0361, -0.0080,  0.0758, -0.0136,  0.0370,  0.1263, -0.1235,
         -0.1467, -0.1436, -0.1218,  0.1450,  0.0959,  0.0314,  0.1309,  0.1374,
          0.0488,  0.1483, -0.0203, -0.0221,  0.0241,  0.1395,  0.0259, -0.0332,
          0.0372,  0.1278, -0.1323, -0.0477,  0.1174, -0.0499,  0.0760, -0.1387]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0545, -0.0636,  0.1127,  ...,  0.0211,  0.0063,  0.1072],
        [ 0.0213,  0.0866,  0.0928,  ...,  0.0518, -0.1006,  0.0326],
        [ 0.0164,  0.0787,  0.0560,  ...,  0.1197,  0.0593,  0.1111],
        ...,
        [-0.0954, -0.0548, -0.0441,  ...,  0.0084,  0.1212, -0.1032],
        [-0.0550, -0.0456, -0.0170,  ...,  0.0987,  0.0783,  0.0888],
        [ 0.0382,  0.1136,  0.0585,  ...,  0.0142, -0.0888, -0.0021]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0545, -0.0636,  0.1127,  ...,  0.0211,  0.0063,  0.1072],
        [ 0.0213,  0.0866,  0.0928,  ...,  0.0518, -0.1006,  0.0326],
        [ 0.0164,  0.0787,  0.0560,  ...,  0.1197,  0.0593,  0.1111],
        ...,
        [-0.0954, -0.0548, -0.0441,  ...,  0.0084,  0.1212, -0.1032],
        [-0.0550, -0.0456, -0.0170,  ...,  0.0987,  0.0783,  0.0888],
        [ 0.0382,  0.1136,  0.0585,  ...,  0.0142, -0.0888, -0.0021]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0814,  0.0318, -0.0057,  ...,  0.1267,  0.0770,  0.0118],
        [ 0.0172, -0.1460, -0.0209,  ..., -0.0961,  0.0192,  0.1242],
        [ 0.1557, -0.1653, -0.0059,  ..., -0.1642,  0.0738, -0.0634],
        ...,
        [ 0.1558, -0.1348, -0.0353,  ..., -0.1400, -0.0281, -0.1031],
        [ 0.1677,  0.1496, -0.1244,  ...,  0.0130, -0.0709, -0.1643],
        [-0.1735,  0.0247, -0.1282,  ...,  0.1745,  0.1399,  0.0996]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0814,  0.0318, -0.0057,  ...,  0.1267,  0.0770,  0.0118],
        [ 0.0172, -0.1460, -0.0209,  ..., -0.0961,  0.0192,  0.1242],
        [ 0.1557, -0.1653, -0.0059,  ..., -0.1642,  0.0738, -0.0634],
        ...,
        [ 0.1558, -0.1348, -0.0353,  ..., -0.1400, -0.0281, -0.1031],
        [ 0.1677,  0.1496, -0.1244,  ...,  0.0130, -0.0709, -0.1643],
        [-0.1735,  0.0247, -0.1282,  ...,  0.1745,  0.1399,  0.0996]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0195, -0.1747,  0.2352,  ...,  0.1182,  0.0673, -0.0156],
        [-0.1323, -0.1529, -0.2140,  ..., -0.0188,  0.2051, -0.1408],
        [-0.2193, -0.2264, -0.0589,  ..., -0.1834,  0.2255,  0.2496],
        ...,
        [-0.0099,  0.1700,  0.0695,  ..., -0.2201,  0.0581, -0.1896],
        [ 0.0213,  0.1784, -0.1634,  ..., -0.1698,  0.0592,  0.0943],
        [ 0.1320,  0.2317, -0.1012,  ..., -0.1048,  0.1596,  0.0432]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0195, -0.1747,  0.2352,  ...,  0.1182,  0.0673, -0.0156],
        [-0.1323, -0.1529, -0.2140,  ..., -0.0188,  0.2051, -0.1408],
        [-0.2193, -0.2264, -0.0589,  ..., -0.1834,  0.2255,  0.2496],
        ...,
        [-0.0099,  0.1700,  0.0695,  ..., -0.2201,  0.0581, -0.1896],
        [ 0.0213,  0.1784, -0.1634,  ..., -0.1698,  0.0592,  0.0943],
        [ 0.1320,  0.2317, -0.1012,  ..., -0.1048,  0.1596,  0.0432]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.1975],
        [ 0.2462],
        [ 0.0112],
        [ 0.1249],
        [ 0.3909],
        [ 0.3503],
        [-0.3969],
        [-0.0706],
        [ 0.4034],
        [-0.2531],
        [ 0.0183],
        [ 0.1979],
        [-0.2018],
        [ 0.2574],
        [-0.2986],
        [ 0.0489],
        [ 0.2124],
        [-0.0811],
        [-0.1630],
        [-0.2605],
        [ 0.3946],
        [ 0.1869],
        [ 0.3629],
        [-0.1967],
        [ 0.2253],
        [ 0.0570],
        [-0.2866],
        [ 0.2895],
        [-0.3655],
        [-0.0796],
        [ 0.0036],
        [-0.0924]], device='cuda:0') 
 Parameter containing:
tensor([[-0.1975],
        [ 0.2462],
        [ 0.0112],
        [ 0.1249],
        [ 0.3909],
        [ 0.3503],
        [-0.3969],
        [-0.0706],
        [ 0.4034],
        [-0.2531],
        [ 0.0183],
        [ 0.1979],
        [-0.2018],
        [ 0.2574],
        [-0.2986],
        [ 0.0489],
        [ 0.2124],
        [-0.0811],
        [-0.1630],
        [-0.2605],
        [ 0.3946],
        [ 0.1869],
        [ 0.3629],
        [-0.1967],
        [ 0.2253],
        [ 0.0570],
        [-0.2866],
        [ 0.2895],
        [-0.3655],
        [-0.0796],
        [ 0.0036],
        [-0.0924]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 9.7975e-02,  1.5186e-01, -6.9683e-02, -7.0263e-02, -3.3951e-03,
          5.8807e-02, -5.8377e-03, -9.2254e-02,  1.0636e-01, -8.7780e-02,
          7.6782e-02,  1.2898e-01, -1.8496e-02, -1.3985e-02, -8.5440e-02,
          1.8720e-02, -1.0502e-01, -1.5013e-01, -9.7237e-02,  9.1903e-02,
          8.5478e-02,  4.1265e-02,  9.9890e-02,  1.4194e-01,  1.1612e-01,
          3.4068e-02,  1.2282e-01,  1.4844e-02,  6.7838e-04,  3.8498e-02,
          4.5502e-02, -2.0974e-02,  1.4957e-01,  1.0653e-02, -1.4054e-01,
         -1.4075e-01, -5.3080e-02,  9.1202e-03,  1.6584e-02,  4.7898e-02,
          2.8325e-02, -8.8846e-02, -5.5261e-03,  8.4056e-02, -4.7899e-02,
          1.3419e-01, -4.4352e-02,  1.2789e-01,  1.2275e-01,  3.9822e-02,
          7.2935e-02, -1.2515e-01,  1.1153e-01,  1.1858e-01,  1.8433e-02,
          1.2603e-01, -1.3813e-01, -1.1088e-01, -3.6191e-02,  1.4593e-01,
          2.0913e-02,  1.0341e-01, -4.4974e-02,  1.3786e-01,  1.1297e-01,
         -4.2222e-02,  1.1519e-01,  9.2603e-03, -7.8168e-03,  1.4051e-01,
          1.1658e-02,  1.1757e-01, -4.9730e-02, -6.9390e-02, -1.3526e-01,
         -1.1951e-04, -1.3543e-01,  3.4742e-02, -2.4268e-02,  7.2950e-02,
          9.2063e-02,  2.8045e-02, -1.3415e-01, -1.3883e-01, -1.4583e-01,
          6.2003e-02, -4.8621e-02, -1.4843e-01, -8.8175e-02,  1.4155e-01,
         -5.0980e-03, -4.9955e-02,  1.2979e-02, -1.4618e-01, -1.5178e-01,
         -1.4092e-01,  1.8638e-03, -5.7703e-02, -8.1759e-02, -1.0486e-01,
         -1.4624e-01, -1.3986e-01, -8.7663e-02, -4.4511e-02,  1.5260e-01,
         -3.7176e-02,  1.3897e-01,  3.1065e-02, -2.0670e-02,  5.4618e-03,
         -1.5266e-01,  3.8588e-02, -1.4858e-01,  1.6997e-02, -1.5148e-01,
         -3.0123e-02,  6.6865e-02, -1.2441e-01, -1.3933e-01,  5.2836e-02,
          5.4147e-02, -1.3253e-01,  8.7744e-02,  1.4354e-01,  5.1691e-02,
          1.4128e-02,  7.4761e-02,  7.2047e-02, -1.5217e-01,  2.0672e-03,
         -1.2428e-01,  8.7654e-02,  4.4756e-02, -7.6879e-03,  5.7773e-02,
         -5.2255e-02,  3.4460e-02, -1.3198e-01,  4.9726e-02,  3.4745e-02,
          1.4057e-01, -1.1709e-01, -5.7597e-02,  8.1930e-02,  1.1020e-01,
          1.3154e-01,  5.4685e-02,  4.7736e-02, -9.1037e-02, -3.4651e-02,
          7.9740e-02,  4.8177e-02, -1.3311e-02,  1.0741e-01,  4.8962e-02,
         -1.4420e-01, -6.0037e-02, -4.9235e-03,  1.9883e-02,  5.4198e-02,
         -1.1221e-01, -9.3267e-02,  1.3227e-01, -7.9352e-02,  6.0575e-02,
          8.8799e-02,  6.9384e-02,  5.3170e-02,  1.4336e-03, -1.1737e-01,
         -5.8065e-02,  1.2469e-01,  1.4208e-01,  7.3095e-02, -4.5360e-02,
         -2.3959e-02, -7.4563e-02, -1.3644e-01, -1.5155e-01, -1.4230e-01,
          1.2712e-01, -9.8826e-02, -8.5846e-02,  1.3566e-02,  7.5732e-02,
          1.1803e-01, -1.0700e-01, -1.3935e-01,  4.3419e-02, -6.6423e-02,
         -1.6662e-02,  1.4869e-01, -7.4919e-02, -1.1748e-02, -1.3438e-01,
          8.1620e-02,  9.3228e-02,  1.0294e-01,  1.2233e-01,  1.4733e-02,
         -1.9626e-02,  1.3402e-01, -4.9755e-02,  2.6042e-02,  9.0184e-02,
          7.2142e-02,  3.5883e-02,  4.5195e-03,  9.5774e-02,  4.7803e-02,
         -8.7992e-02, -4.6938e-02, -7.5484e-02, -5.0584e-02, -7.9325e-02,
          1.0331e-01, -1.1269e-01, -1.2428e-02, -1.9223e-02,  4.1586e-03,
         -1.1827e-01,  3.6950e-02, -4.4994e-02,  9.3782e-02, -1.4631e-01,
          3.4696e-02,  5.5808e-02, -5.8851e-04, -1.1329e-01, -8.9611e-02,
         -1.0783e-01, -1.1147e-02, -1.1332e-01, -3.2303e-02,  1.2505e-01,
          3.1586e-02, -1.3046e-01, -7.3754e-02,  1.4007e-02,  1.0215e-01,
         -1.2777e-01, -2.4747e-02,  1.0160e-01, -1.1349e-01,  9.5123e-02,
          1.0795e-02,  1.1937e-01, -8.4398e-02, -9.4199e-03, -1.4818e-01,
         -1.2911e-01, -8.5997e-02,  8.5724e-02,  1.0140e-01,  2.3592e-02,
         -3.6523e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 9.7975e-02,  1.5186e-01, -6.9683e-02, -7.0263e-02, -3.3951e-03,
          5.8807e-02, -5.8377e-03, -9.2254e-02,  1.0636e-01, -8.7780e-02,
          7.6782e-02,  1.2898e-01, -1.8496e-02, -1.3985e-02, -8.5440e-02,
          1.8720e-02, -1.0502e-01, -1.5013e-01, -9.7237e-02,  9.1903e-02,
          8.5478e-02,  4.1265e-02,  9.9890e-02,  1.4194e-01,  1.1612e-01,
          3.4068e-02,  1.2282e-01,  1.4844e-02,  6.7838e-04,  3.8498e-02,
          4.5502e-02, -2.0974e-02,  1.4957e-01,  1.0653e-02, -1.4054e-01,
         -1.4075e-01, -5.3080e-02,  9.1202e-03,  1.6584e-02,  4.7898e-02,
          2.8325e-02, -8.8846e-02, -5.5261e-03,  8.4056e-02, -4.7899e-02,
          1.3419e-01, -4.4352e-02,  1.2789e-01,  1.2275e-01,  3.9822e-02,
          7.2935e-02, -1.2515e-01,  1.1153e-01,  1.1858e-01,  1.8433e-02,
          1.2603e-01, -1.3813e-01, -1.1088e-01, -3.6191e-02,  1.4593e-01,
          2.0913e-02,  1.0341e-01, -4.4974e-02,  1.3786e-01,  1.1297e-01,
         -4.2222e-02,  1.1519e-01,  9.2603e-03, -7.8168e-03,  1.4051e-01,
          1.1658e-02,  1.1757e-01, -4.9730e-02, -6.9390e-02, -1.3526e-01,
         -1.1951e-04, -1.3543e-01,  3.4742e-02, -2.4268e-02,  7.2950e-02,
          9.2063e-02,  2.8045e-02, -1.3415e-01, -1.3883e-01, -1.4583e-01,
          6.2003e-02, -4.8621e-02, -1.4843e-01, -8.8175e-02,  1.4155e-01,
         -5.0980e-03, -4.9955e-02,  1.2979e-02, -1.4618e-01, -1.5178e-01,
         -1.4092e-01,  1.8638e-03, -5.7703e-02, -8.1759e-02, -1.0486e-01,
         -1.4624e-01, -1.3986e-01, -8.7663e-02, -4.4511e-02,  1.5260e-01,
         -3.7176e-02,  1.3897e-01,  3.1065e-02, -2.0670e-02,  5.4618e-03,
         -1.5266e-01,  3.8588e-02, -1.4858e-01,  1.6997e-02, -1.5148e-01,
         -3.0123e-02,  6.6865e-02, -1.2441e-01, -1.3933e-01,  5.2836e-02,
          5.4147e-02, -1.3253e-01,  8.7744e-02,  1.4354e-01,  5.1691e-02,
          1.4128e-02,  7.4761e-02,  7.2047e-02, -1.5217e-01,  2.0672e-03,
         -1.2428e-01,  8.7654e-02,  4.4756e-02, -7.6879e-03,  5.7773e-02,
         -5.2255e-02,  3.4460e-02, -1.3198e-01,  4.9726e-02,  3.4745e-02,
          1.4057e-01, -1.1709e-01, -5.7597e-02,  8.1930e-02,  1.1020e-01,
          1.3154e-01,  5.4685e-02,  4.7736e-02, -9.1037e-02, -3.4651e-02,
          7.9740e-02,  4.8177e-02, -1.3311e-02,  1.0741e-01,  4.8962e-02,
         -1.4420e-01, -6.0037e-02, -4.9235e-03,  1.9883e-02,  5.4198e-02,
         -1.1221e-01, -9.3267e-02,  1.3227e-01, -7.9352e-02,  6.0575e-02,
          8.8799e-02,  6.9384e-02,  5.3170e-02,  1.4336e-03, -1.1737e-01,
         -5.8065e-02,  1.2469e-01,  1.4208e-01,  7.3095e-02, -4.5360e-02,
         -2.3959e-02, -7.4563e-02, -1.3644e-01, -1.5155e-01, -1.4230e-01,
          1.2712e-01, -9.8826e-02, -8.5846e-02,  1.3566e-02,  7.5732e-02,
          1.1803e-01, -1.0700e-01, -1.3935e-01,  4.3419e-02, -6.6423e-02,
         -1.6662e-02,  1.4869e-01, -7.4919e-02, -1.1748e-02, -1.3438e-01,
          8.1620e-02,  9.3228e-02,  1.0294e-01,  1.2233e-01,  1.4733e-02,
         -1.9626e-02,  1.3402e-01, -4.9755e-02,  2.6042e-02,  9.0184e-02,
          7.2142e-02,  3.5883e-02,  4.5195e-03,  9.5774e-02,  4.7803e-02,
         -8.7992e-02, -4.6938e-02, -7.5484e-02, -5.0584e-02, -7.9325e-02,
          1.0331e-01, -1.1269e-01, -1.2428e-02, -1.9223e-02,  4.1586e-03,
         -1.1827e-01,  3.6950e-02, -4.4994e-02,  9.3782e-02, -1.4631e-01,
          3.4696e-02,  5.5808e-02, -5.8851e-04, -1.1329e-01, -8.9611e-02,
         -1.0783e-01, -1.1147e-02, -1.1332e-01, -3.2303e-02,  1.2505e-01,
          3.1586e-02, -1.3046e-01, -7.3754e-02,  1.4007e-02,  1.0215e-01,
         -1.2777e-01, -2.4747e-02,  1.0160e-01, -1.1349e-01,  9.5123e-02,
          1.0795e-02,  1.1937e-01, -8.4398e-02, -9.4199e-03, -1.4818e-01,
         -1.2911e-01, -8.5997e-02,  8.5724e-02,  1.0140e-01,  2.3592e-02,
         -3.6523e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1246,  0.0551,  0.0148,  ..., -0.0494,  0.1189, -0.0318],
        [-0.0133, -0.0708, -0.0870,  ...,  0.0817,  0.0631, -0.0020],
        [ 0.0895, -0.0864, -0.0133,  ...,  0.0420, -0.1155, -0.1208],
        ...,
        [ 0.0313, -0.0923, -0.0312,  ...,  0.0677, -0.0400, -0.0261],
        [-0.0843, -0.0091, -0.0017,  ...,  0.0807,  0.0373,  0.0391],
        [-0.0463, -0.0404, -0.1087,  ..., -0.0128, -0.0989, -0.0068]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1246,  0.0551,  0.0148,  ..., -0.0494,  0.1189, -0.0318],
        [-0.0133, -0.0708, -0.0870,  ...,  0.0817,  0.0631, -0.0020],
        [ 0.0895, -0.0864, -0.0133,  ...,  0.0420, -0.1155, -0.1208],
        ...,
        [ 0.0313, -0.0923, -0.0312,  ...,  0.0677, -0.0400, -0.0261],
        [-0.0843, -0.0091, -0.0017,  ...,  0.0807,  0.0373,  0.0391],
        [-0.0463, -0.0404, -0.1087,  ..., -0.0128, -0.0989, -0.0068]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0899, -0.0640, -0.1725,  ...,  0.0850,  0.0751, -0.0507],
        [ 0.0345, -0.0786,  0.0593,  ..., -0.0765,  0.0945, -0.0197],
        [-0.0424, -0.0693,  0.1427,  ...,  0.0027,  0.0872,  0.0906],
        ...,
        [-0.1389, -0.0762,  0.0324,  ..., -0.1110,  0.0483, -0.1528],
        [-0.0640,  0.1441,  0.1093,  ..., -0.1187, -0.0286,  0.0990],
        [-0.1334, -0.0289, -0.0186,  ...,  0.0769, -0.1420,  0.1028]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0899, -0.0640, -0.1725,  ...,  0.0850,  0.0751, -0.0507],
        [ 0.0345, -0.0786,  0.0593,  ..., -0.0765,  0.0945, -0.0197],
        [-0.0424, -0.0693,  0.1427,  ...,  0.0027,  0.0872,  0.0906],
        ...,
        [-0.1389, -0.0762,  0.0324,  ..., -0.1110,  0.0483, -0.1528],
        [-0.0640,  0.1441,  0.1093,  ..., -0.1187, -0.0286,  0.0990],
        [-0.1334, -0.0289, -0.0186,  ...,  0.0769, -0.1420,  0.1028]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0448, -0.0481,  0.0765,  ..., -0.1055, -0.1447,  0.0139],
        [ 0.1574,  0.0941,  0.0941,  ..., -0.0560,  0.0639, -0.2137],
        [-0.1883, -0.0588, -0.0190,  ...,  0.0901,  0.2166, -0.0914],
        ...,
        [ 0.0767,  0.2248,  0.1125,  ..., -0.0540, -0.2255, -0.1117],
        [ 0.1914,  0.0892,  0.1872,  ...,  0.1549, -0.1806,  0.1114],
        [ 0.2040, -0.2140,  0.1707,  ..., -0.0143,  0.1571,  0.2304]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0448, -0.0481,  0.0765,  ..., -0.1055, -0.1447,  0.0139],
        [ 0.1574,  0.0941,  0.0941,  ..., -0.0560,  0.0639, -0.2137],
        [-0.1883, -0.0588, -0.0190,  ...,  0.0901,  0.2166, -0.0914],
        ...,
        [ 0.0767,  0.2248,  0.1125,  ..., -0.0540, -0.2255, -0.1117],
        [ 0.1914,  0.0892,  0.1872,  ...,  0.1549, -0.1806,  0.1114],
        [ 0.2040, -0.2140,  0.1707,  ..., -0.0143,  0.1571,  0.2304]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.1125],
        [-0.0278],
        [-0.1683],
        [-0.2099],
        [ 0.3612],
        [-0.3606],
        [-0.1731],
        [ 0.0184],
        [ 0.3046],
        [-0.1240],
        [ 0.3749],
        [-0.3892],
        [-0.3212],
        [ 0.1438],
        [-0.3323],
        [-0.3913],
        [ 0.0925],
        [-0.0686],
        [ 0.0440],
        [-0.0181],
        [ 0.2745],
        [ 0.2601],
        [ 0.1688],
        [ 0.2004],
        [ 0.3151],
        [ 0.2168],
        [-0.0599],
        [ 0.1086],
        [ 0.0252],
        [-0.2172],
        [ 0.2778],
        [ 0.3609]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.1125],
        [-0.0278],
        [-0.1683],
        [-0.2099],
        [ 0.3612],
        [-0.3606],
        [-0.1731],
        [ 0.0184],
        [ 0.3046],
        [-0.1240],
        [ 0.3749],
        [-0.3892],
        [-0.3212],
        [ 0.1438],
        [-0.3323],
        [-0.3913],
        [ 0.0925],
        [-0.0686],
        [ 0.0440],
        [-0.0181],
        [ 0.2745],
        [ 0.2601],
        [ 0.1688],
        [ 0.2004],
        [ 0.3151],
        [ 0.2168],
        [-0.0599],
        [ 0.1086],
        [ 0.0252],
        [-0.2172],
        [ 0.2778],
        [ 0.3609]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-20.3983, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.1678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(4.2775, device='cuda:0')



h[100].sum tensor(-0.4332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4446, device='cuda:0')



h[200].sum tensor(4.6128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(4.7342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2790.0264, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15457.2900, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.1364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1.5628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-23.8302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0161],
        [0.0197],
        [0.0284],
        ...,
        [0.0046],
        [0.0046],
        [0.0036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(406.7011, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0161],
        [0.0197],
        [0.0284],
        ...,
        [0.0046],
        [0.0046],
        [0.0036]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-244.3254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.6056, device='cuda:0')



h[100].sum tensor(-10.8727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7846, device='cuda:0')



h[200].sum tensor(6.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(12598.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0303,  ..., 0.0008, 0.0000, 0.0069],
        [0.0000, 0.0028, 0.0063,  ..., 0.0002, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(64575.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.8228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(288.0933, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(20.2687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-13.6398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2846],
        [0.1745],
        [0.1068],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(16719.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0161],
        [0.0197],
        [0.0284],
        ...,
        [0.0046],
        [0.0046],
        [0.0036]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 30001 
BatchSize 5 
EpochNum 50 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1506, -0.0676, -0.0042, -0.0609,  0.0462, -0.1294, -0.0435, -0.0639,
          0.0491, -0.0946, -0.0737, -0.0607, -0.1104, -0.0053, -0.0200, -0.0021,
         -0.1073, -0.1100,  0.0698,  0.0047,  0.1360, -0.0325,  0.0592,  0.0718,
         -0.1461,  0.0423, -0.0377,  0.1174,  0.0155,  0.1103,  0.1031, -0.0649,
          0.1201, -0.1090,  0.0824, -0.0485,  0.0080, -0.0918, -0.1396,  0.0683,
          0.0490, -0.0135, -0.0343, -0.0318,  0.1051,  0.0519,  0.1040,  0.0655,
         -0.1071, -0.0429,  0.0428, -0.0317, -0.0005, -0.1410, -0.1520,  0.0388,
         -0.0063,  0.0933,  0.0784, -0.0361,  0.1392, -0.0064, -0.0948,  0.0386,
         -0.0092, -0.0525,  0.0919,  0.0604,  0.0626, -0.0627, -0.0077, -0.1389,
          0.1160,  0.0094,  0.1133,  0.0542, -0.0929, -0.0411, -0.0347, -0.0741,
         -0.0263,  0.0455,  0.0128,  0.1384,  0.0159,  0.0978,  0.0634,  0.1278,
          0.0169, -0.0384,  0.0845, -0.1124, -0.0678, -0.0558,  0.0108, -0.1316,
         -0.1150,  0.1507,  0.0255, -0.0681, -0.1073, -0.1057, -0.0599,  0.0485,
          0.1456, -0.0398,  0.0242,  0.0310, -0.0761,  0.0311, -0.1183,  0.0360,
         -0.0341, -0.0524,  0.0974,  0.0775, -0.1250,  0.0013,  0.0311,  0.1485,
          0.1157,  0.0320, -0.0722, -0.1422,  0.1459,  0.1489,  0.0286, -0.0339,
         -0.1223,  0.1181, -0.0520,  0.0940, -0.1433, -0.1035, -0.0779, -0.0305,
          0.0443, -0.1044, -0.0855,  0.0316, -0.0698,  0.1253, -0.0897,  0.0272,
          0.1477,  0.0228,  0.0960, -0.1028, -0.1184,  0.1499,  0.0333,  0.0595,
         -0.1362,  0.1524,  0.0037,  0.0546, -0.1097, -0.0532, -0.0373,  0.0725,
         -0.1177,  0.0269, -0.1219,  0.1308,  0.1506,  0.1497, -0.0884, -0.0796,
          0.0981,  0.0190, -0.0583,  0.0657,  0.1314,  0.1244,  0.1082, -0.0701,
          0.0521, -0.1169,  0.1508,  0.0335, -0.1271, -0.0437,  0.0251, -0.1526,
         -0.0624,  0.1079,  0.0888, -0.0353,  0.0244, -0.0671, -0.1048,  0.0222,
         -0.0815,  0.0986,  0.0607,  0.1387,  0.0685, -0.0587, -0.0725,  0.0677,
          0.0601, -0.0590,  0.0613, -0.0407,  0.0284, -0.0621, -0.0457, -0.0735,
          0.1371, -0.0614,  0.0254, -0.0101,  0.1152, -0.1492,  0.1324, -0.0653,
         -0.0982, -0.0242, -0.0290, -0.1041,  0.1053,  0.0290,  0.0295,  0.0089,
         -0.1138, -0.1391,  0.0205,  0.1018, -0.1252, -0.0911, -0.0868,  0.0243,
          0.1230, -0.1410, -0.0461, -0.0840, -0.1486, -0.1140,  0.0113, -0.1186,
         -0.0946, -0.0080,  0.0011,  0.0247,  0.0633,  0.1077, -0.0889,  0.1223,
          0.0995,  0.0641,  0.0837, -0.1037, -0.0140, -0.0364,  0.0295,  0.0198]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0814,  0.0682,  0.0782,  ...,  0.0760,  0.0752,  0.0709],
        [-0.1185, -0.0472, -0.0232,  ...,  0.0413, -0.0529,  0.0222],
        [ 0.0358,  0.0302,  0.0180,  ...,  0.0831, -0.0146,  0.0390],
        ...,
        [ 0.0518, -0.0213,  0.0620,  ..., -0.0605, -0.0233,  0.0588],
        [ 0.0453, -0.1091,  0.0414,  ..., -0.0735, -0.0568, -0.0812],
        [ 0.1038,  0.1102,  0.0532,  ..., -0.0904,  0.1244,  0.0162]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1042,  0.1250, -0.1334,  ..., -0.0307,  0.0481, -0.1515],
        [ 0.0136, -0.1040, -0.0477,  ...,  0.0629, -0.0600, -0.1455],
        [ 0.1146, -0.0096, -0.0805,  ...,  0.1660,  0.0496, -0.0308],
        ...,
        [ 0.0924,  0.0763, -0.1171,  ...,  0.0046, -0.1153, -0.0738],
        [ 0.0870, -0.0604, -0.1653,  ..., -0.0624, -0.1240,  0.1209],
        [-0.1035,  0.0151,  0.1128,  ...,  0.0360,  0.0314,  0.0202]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0587,  0.2330, -0.0119,  ..., -0.0015, -0.2470, -0.2030],
        [-0.2134, -0.0196,  0.0614,  ..., -0.2085, -0.1663,  0.1505],
        [ 0.0260, -0.2179, -0.0341,  ...,  0.1838,  0.0199, -0.0119],
        ...,
        [-0.1913,  0.0042,  0.0046,  ..., -0.0111, -0.0020, -0.1071],
        [-0.1289, -0.1847,  0.2322,  ...,  0.2448, -0.2222,  0.0023],
        [-0.1558,  0.0157, -0.0037,  ..., -0.2050, -0.0843, -0.1374]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.4160],
        [-0.1184],
        [ 0.0520],
        [ 0.2473],
        [ 0.1697],
        [ 0.1497],
        [-0.1437],
        [ 0.2427],
        [ 0.2153],
        [-0.3389],
        [-0.1588],
        [ 0.3527],
        [-0.0569],
        [ 0.2257],
        [ 0.1586],
        [ 0.2842],
        [-0.0656],
        [-0.1728],
        [ 0.0012],
        [ 0.0754],
        [-0.0958],
        [-0.0613],
        [-0.2046],
        [ 0.0428],
        [ 0.0716],
        [ 0.2032],
        [-0.1509],
        [ 0.4168],
        [-0.3581],
        [ 0.1740],
        [ 0.3506],
        [-0.3057]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1506, -0.0676, -0.0042, -0.0609,  0.0462, -0.1294, -0.0435, -0.0639,
          0.0491, -0.0946, -0.0737, -0.0607, -0.1104, -0.0053, -0.0200, -0.0021,
         -0.1073, -0.1100,  0.0698,  0.0047,  0.1360, -0.0325,  0.0592,  0.0718,
         -0.1461,  0.0423, -0.0377,  0.1174,  0.0155,  0.1103,  0.1031, -0.0649,
          0.1201, -0.1090,  0.0824, -0.0485,  0.0080, -0.0918, -0.1396,  0.0683,
          0.0490, -0.0135, -0.0343, -0.0318,  0.1051,  0.0519,  0.1040,  0.0655,
         -0.1071, -0.0429,  0.0428, -0.0317, -0.0005, -0.1410, -0.1520,  0.0388,
         -0.0063,  0.0933,  0.0784, -0.0361,  0.1392, -0.0064, -0.0948,  0.0386,
         -0.0092, -0.0525,  0.0919,  0.0604,  0.0626, -0.0627, -0.0077, -0.1389,
          0.1160,  0.0094,  0.1133,  0.0542, -0.0929, -0.0411, -0.0347, -0.0741,
         -0.0263,  0.0455,  0.0128,  0.1384,  0.0159,  0.0978,  0.0634,  0.1278,
          0.0169, -0.0384,  0.0845, -0.1124, -0.0678, -0.0558,  0.0108, -0.1316,
         -0.1150,  0.1507,  0.0255, -0.0681, -0.1073, -0.1057, -0.0599,  0.0485,
          0.1456, -0.0398,  0.0242,  0.0310, -0.0761,  0.0311, -0.1183,  0.0360,
         -0.0341, -0.0524,  0.0974,  0.0775, -0.1250,  0.0013,  0.0311,  0.1485,
          0.1157,  0.0320, -0.0722, -0.1422,  0.1459,  0.1489,  0.0286, -0.0339,
         -0.1223,  0.1181, -0.0520,  0.0940, -0.1433, -0.1035, -0.0779, -0.0305,
          0.0443, -0.1044, -0.0855,  0.0316, -0.0698,  0.1253, -0.0897,  0.0272,
          0.1477,  0.0228,  0.0960, -0.1028, -0.1184,  0.1499,  0.0333,  0.0595,
         -0.1362,  0.1524,  0.0037,  0.0546, -0.1097, -0.0532, -0.0373,  0.0725,
         -0.1177,  0.0269, -0.1219,  0.1308,  0.1506,  0.1497, -0.0884, -0.0796,
          0.0981,  0.0190, -0.0583,  0.0657,  0.1314,  0.1244,  0.1082, -0.0701,
          0.0521, -0.1169,  0.1508,  0.0335, -0.1271, -0.0437,  0.0251, -0.1526,
         -0.0624,  0.1079,  0.0888, -0.0353,  0.0244, -0.0671, -0.1048,  0.0222,
         -0.0815,  0.0986,  0.0607,  0.1387,  0.0685, -0.0587, -0.0725,  0.0677,
          0.0601, -0.0590,  0.0613, -0.0407,  0.0284, -0.0621, -0.0457, -0.0735,
          0.1371, -0.0614,  0.0254, -0.0101,  0.1152, -0.1492,  0.1324, -0.0653,
         -0.0982, -0.0242, -0.0290, -0.1041,  0.1053,  0.0290,  0.0295,  0.0089,
         -0.1138, -0.1391,  0.0205,  0.1018, -0.1252, -0.0911, -0.0868,  0.0243,
          0.1230, -0.1410, -0.0461, -0.0840, -0.1486, -0.1140,  0.0113, -0.1186,
         -0.0946, -0.0080,  0.0011,  0.0247,  0.0633,  0.1077, -0.0889,  0.1223,
          0.0995,  0.0641,  0.0837, -0.1037, -0.0140, -0.0364,  0.0295,  0.0198]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0814,  0.0682,  0.0782,  ...,  0.0760,  0.0752,  0.0709],
        [-0.1185, -0.0472, -0.0232,  ...,  0.0413, -0.0529,  0.0222],
        [ 0.0358,  0.0302,  0.0180,  ...,  0.0831, -0.0146,  0.0390],
        ...,
        [ 0.0518, -0.0213,  0.0620,  ..., -0.0605, -0.0233,  0.0588],
        [ 0.0453, -0.1091,  0.0414,  ..., -0.0735, -0.0568, -0.0812],
        [ 0.1038,  0.1102,  0.0532,  ..., -0.0904,  0.1244,  0.0162]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1042,  0.1250, -0.1334,  ..., -0.0307,  0.0481, -0.1515],
        [ 0.0136, -0.1040, -0.0477,  ...,  0.0629, -0.0600, -0.1455],
        [ 0.1146, -0.0096, -0.0805,  ...,  0.1660,  0.0496, -0.0308],
        ...,
        [ 0.0924,  0.0763, -0.1171,  ...,  0.0046, -0.1153, -0.0738],
        [ 0.0870, -0.0604, -0.1653,  ..., -0.0624, -0.1240,  0.1209],
        [-0.1035,  0.0151,  0.1128,  ...,  0.0360,  0.0314,  0.0202]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0587,  0.2330, -0.0119,  ..., -0.0015, -0.2470, -0.2030],
        [-0.2134, -0.0196,  0.0614,  ..., -0.2085, -0.1663,  0.1505],
        [ 0.0260, -0.2179, -0.0341,  ...,  0.1838,  0.0199, -0.0119],
        ...,
        [-0.1913,  0.0042,  0.0046,  ..., -0.0111, -0.0020, -0.1071],
        [-0.1289, -0.1847,  0.2322,  ...,  0.2448, -0.2222,  0.0023],
        [-0.1558,  0.0157, -0.0037,  ..., -0.2050, -0.0843, -0.1374]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.4160],
        [-0.1184],
        [ 0.0520],
        [ 0.2473],
        [ 0.1697],
        [ 0.1497],
        [-0.1437],
        [ 0.2427],
        [ 0.2153],
        [-0.3389],
        [-0.1588],
        [ 0.3527],
        [-0.0569],
        [ 0.2257],
        [ 0.1586],
        [ 0.2842],
        [-0.0656],
        [-0.1728],
        [ 0.0012],
        [ 0.0754],
        [-0.0958],
        [-0.0613],
        [-0.2046],
        [ 0.0428],
        [ 0.0716],
        [ 0.2032],
        [-0.1509],
        [ 0.4168],
        [-0.3581],
        [ 0.1740],
        [ 0.3506],
        [-0.3057]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}]
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 77, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(5 * 6796, 1)
TypeError: slice indices must be integers or None or have an __index__ method

real	0m27.961s
user	0m13.026s
sys	0m10.347s
