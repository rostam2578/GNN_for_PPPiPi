0: gpu027.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-60b276b2-4ce7-6f0f-583c-12a5a65d2ee5)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Tue Aug  9 18:08:45 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |
| N/A   41C    P0    47W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b7b9c832880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m29.127s
user	0m2.928s
sys	0m2.145s
[18:09:16] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.8426],
        [ 1.8665],
        [ 1.0562],
        ...,
        [-0.2464],
        [-1.0217],
        [ 1.2669]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-49.6249, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-1.0041e-01,  4.2163e-03, -3.4067e-02, -8.4294e-02,  1.0953e-01,
         -1.2790e-01, -4.3901e-02, -6.1778e-02, -1.6466e-02,  9.9330e-02,
         -1.3630e-01,  6.0089e-02, -1.1365e-01,  3.8679e-02,  1.4761e-01,
         -1.4620e-01, -6.6787e-02, -8.0987e-03, -3.4737e-02,  4.6536e-02,
         -1.4985e-01, -9.7825e-02, -1.3110e-01, -1.3077e-01,  4.0589e-02,
          7.4486e-02, -8.2699e-02, -8.9649e-02,  4.2843e-02, -6.2941e-02,
          3.5872e-03,  1.1107e-01,  5.0080e-02,  4.5289e-02, -1.2599e-01,
         -3.4278e-02, -1.2310e-02,  4.2439e-02,  5.1524e-02, -5.3029e-02,
         -6.2573e-02,  1.5294e-02, -8.2516e-02,  1.4692e-01, -1.1065e-01,
         -9.6237e-02,  1.1853e-01,  1.3285e-02, -8.8842e-02, -1.2097e-02,
         -1.7902e-03, -1.4798e-01,  2.6998e-02, -9.5904e-02, -1.0227e-01,
         -3.6316e-02, -1.2334e-01, -6.1516e-02,  1.4449e-02, -1.0310e-01,
          4.8856e-02,  1.4992e-01,  3.7889e-02, -5.1000e-02,  1.0427e-01,
         -1.2961e-01, -1.2781e-01,  1.2736e-01, -3.3877e-02, -1.0566e-01,
          4.3977e-02,  8.6302e-02,  1.4248e-01, -1.3927e-01,  2.5135e-02,
          9.8731e-02,  3.8658e-02,  1.4803e-01,  9.2681e-02,  1.2476e-01,
          1.3762e-01, -3.9761e-02,  3.6347e-02, -2.1309e-02,  9.9380e-02,
          4.0861e-02, -5.1927e-02,  1.4424e-01,  9.6563e-02,  2.8651e-02,
         -4.4765e-02, -3.2944e-02, -7.0627e-02, -8.7882e-02,  5.5949e-02,
          1.3675e-01,  1.3870e-01, -1.1359e-01,  1.3553e-01,  1.6980e-02,
         -5.8668e-02,  5.6409e-02,  3.7886e-02, -1.0515e-01,  1.0718e-01,
          1.3749e-01,  1.3074e-01, -1.3348e-01, -1.1843e-01, -2.1226e-02,
          1.4040e-01,  2.1633e-02,  1.0440e-01,  1.1478e-01,  5.5853e-02,
         -8.9852e-02,  4.6972e-02,  4.9409e-02, -7.2020e-05,  1.0919e-01,
          8.3821e-02,  2.7493e-02,  6.4688e-02,  8.4332e-02,  2.2640e-02,
          9.5316e-02,  1.3805e-01, -1.3222e-02,  3.8919e-02, -8.7907e-02,
         -5.3350e-02, -1.0148e-01, -1.6872e-02,  2.2682e-02, -7.1272e-02,
          6.2827e-02, -8.9638e-02,  1.1405e-01, -6.8094e-02,  7.6832e-02,
          3.8219e-02, -3.2179e-02, -9.5582e-03,  1.0360e-01,  8.3710e-02,
         -5.3761e-02, -1.2667e-01, -5.8565e-03, -5.5742e-02,  6.2412e-02,
          7.3165e-03,  4.1498e-02, -1.1314e-02,  6.4457e-02,  4.7462e-02,
          1.2949e-01,  9.1775e-02,  4.8801e-02,  6.9107e-02, -5.5403e-02,
         -1.2926e-01,  2.7516e-02, -8.4562e-02, -9.4149e-02,  1.3348e-01,
          8.9275e-02,  1.4864e-01,  2.4767e-02, -1.6762e-02, -1.2403e-01,
          1.4668e-01,  6.2982e-02, -4.0290e-02, -1.1663e-01,  7.0821e-02,
          1.1329e-01, -3.0499e-02,  9.0845e-02,  2.5850e-02,  1.3534e-02,
          3.9978e-02,  9.0532e-02, -9.6406e-02,  9.4234e-02,  1.0921e-01,
         -6.3681e-02,  6.2844e-02, -2.6535e-02,  4.9126e-02, -1.4113e-01,
          2.4469e-02, -1.3371e-01, -2.3299e-02,  3.7494e-02, -2.6403e-02,
         -1.4819e-01, -8.3276e-02,  1.2315e-01,  1.1914e-01, -4.0231e-03,
         -2.1677e-02, -1.7562e-02,  1.3343e-01,  2.6535e-02, -8.8244e-02,
          9.1729e-02,  9.6997e-02,  9.8938e-02,  4.3817e-02, -1.3109e-01,
          1.1677e-01, -8.7437e-02,  1.1522e-01,  2.7718e-03, -6.0888e-02,
         -6.2157e-02, -1.5025e-01,  1.0549e-01, -2.7643e-02, -5.1993e-03,
         -3.0593e-02, -1.0092e-01, -8.7790e-02,  8.7417e-02,  8.5454e-02,
          1.1615e-01, -9.1951e-02, -4.2259e-02, -7.7287e-02,  4.8741e-03,
         -5.0507e-02, -3.7793e-02,  9.5146e-02,  8.4633e-02, -8.8991e-02,
          4.2106e-02,  7.3334e-02, -6.8329e-02,  1.4774e-01, -3.7821e-02,
          9.5448e-02,  3.3022e-02,  2.0642e-02,  5.2329e-02, -1.3098e-01,
         -3.5757e-02, -1.0020e-01, -7.8165e-02, -8.3858e-02, -5.1273e-02,
          9.8394e-02, -1.0163e-01,  5.4510e-02,  3.7627e-02, -9.2427e-02,
          9.3613e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-1.0041e-01,  4.2163e-03, -3.4067e-02, -8.4294e-02,  1.0953e-01,
         -1.2790e-01, -4.3901e-02, -6.1778e-02, -1.6466e-02,  9.9330e-02,
         -1.3630e-01,  6.0089e-02, -1.1365e-01,  3.8679e-02,  1.4761e-01,
         -1.4620e-01, -6.6787e-02, -8.0987e-03, -3.4737e-02,  4.6536e-02,
         -1.4985e-01, -9.7825e-02, -1.3110e-01, -1.3077e-01,  4.0589e-02,
          7.4486e-02, -8.2699e-02, -8.9649e-02,  4.2843e-02, -6.2941e-02,
          3.5872e-03,  1.1107e-01,  5.0080e-02,  4.5289e-02, -1.2599e-01,
         -3.4278e-02, -1.2310e-02,  4.2439e-02,  5.1524e-02, -5.3029e-02,
         -6.2573e-02,  1.5294e-02, -8.2516e-02,  1.4692e-01, -1.1065e-01,
         -9.6237e-02,  1.1853e-01,  1.3285e-02, -8.8842e-02, -1.2097e-02,
         -1.7902e-03, -1.4798e-01,  2.6998e-02, -9.5904e-02, -1.0227e-01,
         -3.6316e-02, -1.2334e-01, -6.1516e-02,  1.4449e-02, -1.0310e-01,
          4.8856e-02,  1.4992e-01,  3.7889e-02, -5.1000e-02,  1.0427e-01,
         -1.2961e-01, -1.2781e-01,  1.2736e-01, -3.3877e-02, -1.0566e-01,
          4.3977e-02,  8.6302e-02,  1.4248e-01, -1.3927e-01,  2.5135e-02,
          9.8731e-02,  3.8658e-02,  1.4803e-01,  9.2681e-02,  1.2476e-01,
          1.3762e-01, -3.9761e-02,  3.6347e-02, -2.1309e-02,  9.9380e-02,
          4.0861e-02, -5.1927e-02,  1.4424e-01,  9.6563e-02,  2.8651e-02,
         -4.4765e-02, -3.2944e-02, -7.0627e-02, -8.7882e-02,  5.5949e-02,
          1.3675e-01,  1.3870e-01, -1.1359e-01,  1.3553e-01,  1.6980e-02,
         -5.8668e-02,  5.6409e-02,  3.7886e-02, -1.0515e-01,  1.0718e-01,
          1.3749e-01,  1.3074e-01, -1.3348e-01, -1.1843e-01, -2.1226e-02,
          1.4040e-01,  2.1633e-02,  1.0440e-01,  1.1478e-01,  5.5853e-02,
         -8.9852e-02,  4.6972e-02,  4.9409e-02, -7.2020e-05,  1.0919e-01,
          8.3821e-02,  2.7493e-02,  6.4688e-02,  8.4332e-02,  2.2640e-02,
          9.5316e-02,  1.3805e-01, -1.3222e-02,  3.8919e-02, -8.7907e-02,
         -5.3350e-02, -1.0148e-01, -1.6872e-02,  2.2682e-02, -7.1272e-02,
          6.2827e-02, -8.9638e-02,  1.1405e-01, -6.8094e-02,  7.6832e-02,
          3.8219e-02, -3.2179e-02, -9.5582e-03,  1.0360e-01,  8.3710e-02,
         -5.3761e-02, -1.2667e-01, -5.8565e-03, -5.5742e-02,  6.2412e-02,
          7.3165e-03,  4.1498e-02, -1.1314e-02,  6.4457e-02,  4.7462e-02,
          1.2949e-01,  9.1775e-02,  4.8801e-02,  6.9107e-02, -5.5403e-02,
         -1.2926e-01,  2.7516e-02, -8.4562e-02, -9.4149e-02,  1.3348e-01,
          8.9275e-02,  1.4864e-01,  2.4767e-02, -1.6762e-02, -1.2403e-01,
          1.4668e-01,  6.2982e-02, -4.0290e-02, -1.1663e-01,  7.0821e-02,
          1.1329e-01, -3.0499e-02,  9.0845e-02,  2.5850e-02,  1.3534e-02,
          3.9978e-02,  9.0532e-02, -9.6406e-02,  9.4234e-02,  1.0921e-01,
         -6.3681e-02,  6.2844e-02, -2.6535e-02,  4.9126e-02, -1.4113e-01,
          2.4469e-02, -1.3371e-01, -2.3299e-02,  3.7494e-02, -2.6403e-02,
         -1.4819e-01, -8.3276e-02,  1.2315e-01,  1.1914e-01, -4.0231e-03,
         -2.1677e-02, -1.7562e-02,  1.3343e-01,  2.6535e-02, -8.8244e-02,
          9.1729e-02,  9.6997e-02,  9.8938e-02,  4.3817e-02, -1.3109e-01,
          1.1677e-01, -8.7437e-02,  1.1522e-01,  2.7718e-03, -6.0888e-02,
         -6.2157e-02, -1.5025e-01,  1.0549e-01, -2.7643e-02, -5.1993e-03,
         -3.0593e-02, -1.0092e-01, -8.7790e-02,  8.7417e-02,  8.5454e-02,
          1.1615e-01, -9.1951e-02, -4.2259e-02, -7.7287e-02,  4.8741e-03,
         -5.0507e-02, -3.7793e-02,  9.5146e-02,  8.4633e-02, -8.8991e-02,
          4.2106e-02,  7.3334e-02, -6.8329e-02,  1.4774e-01, -3.7821e-02,
          9.5448e-02,  3.3022e-02,  2.0642e-02,  5.2329e-02, -1.3098e-01,
         -3.5757e-02, -1.0020e-01, -7.8165e-02, -8.3858e-02, -5.1273e-02,
          9.8394e-02, -1.0163e-01,  5.4510e-02,  3.7627e-02, -9.2427e-02,
          9.3613e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0063, -0.0531,  0.0889,  ...,  0.0952, -0.0490,  0.0093],
        [-0.0197, -0.0223, -0.0414,  ...,  0.0324, -0.0846, -0.1169],
        [ 0.0159, -0.0583,  0.1055,  ..., -0.0427,  0.0804,  0.0431],
        ...,
        [-0.0376, -0.0734,  0.0268,  ..., -0.0645, -0.0317,  0.0396],
        [-0.0899,  0.1014,  0.0149,  ..., -0.0123, -0.0111, -0.0844],
        [ 0.1208, -0.0240, -0.0871,  ...,  0.1194, -0.0809,  0.0321]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0063, -0.0531,  0.0889,  ...,  0.0952, -0.0490,  0.0093],
        [-0.0197, -0.0223, -0.0414,  ...,  0.0324, -0.0846, -0.1169],
        [ 0.0159, -0.0583,  0.1055,  ..., -0.0427,  0.0804,  0.0431],
        ...,
        [-0.0376, -0.0734,  0.0268,  ..., -0.0645, -0.0317,  0.0396],
        [-0.0899,  0.1014,  0.0149,  ..., -0.0123, -0.0111, -0.0844],
        [ 0.1208, -0.0240, -0.0871,  ...,  0.1194, -0.0809,  0.0321]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0393, -0.1289,  0.1492,  ..., -0.0592, -0.0363, -0.0371],
        [ 0.1343,  0.1225, -0.0399,  ...,  0.0786,  0.1390,  0.0817],
        [-0.1421,  0.1237, -0.1076,  ...,  0.0950,  0.1629,  0.0776],
        ...,
        [-0.0435, -0.0140, -0.1525,  ...,  0.0014, -0.1544,  0.0806],
        [ 0.1195,  0.1372, -0.0264,  ...,  0.0825, -0.0442,  0.1211],
        [-0.0690, -0.1296, -0.0981,  ..., -0.1356,  0.1421,  0.1154]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0393, -0.1289,  0.1492,  ..., -0.0592, -0.0363, -0.0371],
        [ 0.1343,  0.1225, -0.0399,  ...,  0.0786,  0.1390,  0.0817],
        [-0.1421,  0.1237, -0.1076,  ...,  0.0950,  0.1629,  0.0776],
        ...,
        [-0.0435, -0.0140, -0.1525,  ...,  0.0014, -0.1544,  0.0806],
        [ 0.1195,  0.1372, -0.0264,  ...,  0.0825, -0.0442,  0.1211],
        [-0.0690, -0.1296, -0.0981,  ..., -0.1356,  0.1421,  0.1154]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.2327, -0.2156,  0.2329,  ...,  0.0451,  0.0257, -0.2384],
        [ 0.0732, -0.1558,  0.0891,  ...,  0.0349, -0.0793,  0.0545],
        [ 0.0669, -0.0579,  0.0466,  ..., -0.2413,  0.1112, -0.0905],
        ...,
        [-0.2274, -0.0743,  0.1210,  ..., -0.1627, -0.1350, -0.0491],
        [-0.1401,  0.1622,  0.0012,  ..., -0.0991,  0.1537,  0.1395],
        [ 0.2349, -0.1884, -0.0991,  ..., -0.1248, -0.1137,  0.0058]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.2327, -0.2156,  0.2329,  ...,  0.0451,  0.0257, -0.2384],
        [ 0.0732, -0.1558,  0.0891,  ...,  0.0349, -0.0793,  0.0545],
        [ 0.0669, -0.0579,  0.0466,  ..., -0.2413,  0.1112, -0.0905],
        ...,
        [-0.2274, -0.0743,  0.1210,  ..., -0.1627, -0.1350, -0.0491],
        [-0.1401,  0.1622,  0.0012,  ..., -0.0991,  0.1537,  0.1395],
        [ 0.2349, -0.1884, -0.0991,  ..., -0.1248, -0.1137,  0.0058]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0941],
        [-0.0867],
        [-0.3047],
        [-0.0687],
        [ 0.2496],
        [ 0.0233],
        [ 0.2552],
        [ 0.3847],
        [-0.0091],
        [ 0.2640],
        [ 0.3442],
        [ 0.3173],
        [-0.0388],
        [-0.0596],
        [ 0.2333],
        [-0.2907],
        [ 0.3726],
        [ 0.1292],
        [-0.1509],
        [-0.2038],
        [-0.2481],
        [-0.0609],
        [ 0.1529],
        [ 0.0920],
        [-0.2370],
        [-0.3176],
        [-0.2358],
        [-0.0254],
        [-0.0638],
        [-0.1114],
        [-0.1455],
        [ 0.3940]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0941],
        [-0.0867],
        [-0.3047],
        [-0.0687],
        [ 0.2496],
        [ 0.0233],
        [ 0.2552],
        [ 0.3847],
        [-0.0091],
        [ 0.2640],
        [ 0.3442],
        [ 0.3173],
        [-0.0388],
        [-0.0596],
        [ 0.2333],
        [-0.2907],
        [ 0.3726],
        [ 0.1292],
        [-0.1509],
        [-0.2038],
        [-0.2481],
        [-0.0609],
        [ 0.1529],
        [ 0.0920],
        [-0.2370],
        [-0.3176],
        [-0.2358],
        [-0.0254],
        [-0.0638],
        [-0.1114],
        [-0.1455],
        [ 0.3940]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1110,  0.0008,  0.0561, -0.1034,  0.0827,  0.0375, -0.0013,  0.1402,
         -0.1252, -0.1525, -0.1072,  0.0208, -0.1463,  0.0660, -0.0624, -0.1115,
         -0.1277,  0.0293,  0.1266,  0.0706,  0.1280, -0.0882, -0.0082,  0.0722,
         -0.0545, -0.0726,  0.1113,  0.0893,  0.1413,  0.1496, -0.1331, -0.0637,
          0.1474,  0.1045,  0.0478, -0.0443,  0.1382, -0.1368,  0.0842,  0.1111,
          0.0319,  0.0230, -0.1473, -0.0987,  0.0619,  0.0798,  0.0479,  0.0277,
         -0.0894,  0.0567, -0.1485,  0.0142,  0.0881,  0.0037,  0.1104, -0.0036,
         -0.1406,  0.1390, -0.1110,  0.0136, -0.0796,  0.0183,  0.1198,  0.0979,
         -0.1414,  0.1111,  0.0810, -0.0010,  0.0783,  0.0462,  0.0045, -0.1230,
         -0.0098, -0.0553,  0.0471, -0.1354, -0.0095, -0.1471, -0.1151,  0.0799,
          0.0853, -0.0705,  0.0551,  0.0625, -0.0596, -0.1142, -0.0994, -0.0281,
          0.0068,  0.0762, -0.1175, -0.0706,  0.0846, -0.1497,  0.1142,  0.0823,
         -0.0601,  0.1031,  0.0956,  0.0194,  0.0233, -0.0063, -0.1153,  0.1177,
          0.0611,  0.1033,  0.0040, -0.0036, -0.0733, -0.1315,  0.0661, -0.0082,
          0.0677, -0.0588,  0.0644,  0.1371, -0.0527,  0.0189,  0.0343,  0.1438,
         -0.1157, -0.0457,  0.0658, -0.0311,  0.1464, -0.0666,  0.1036, -0.0072,
         -0.0773, -0.1244, -0.0260, -0.0289, -0.1400, -0.1335,  0.1476, -0.0983,
          0.0711,  0.0542, -0.0974,  0.1351,  0.0020, -0.1142, -0.1403,  0.1417,
          0.1128,  0.0232,  0.0433,  0.1401, -0.1390, -0.0848,  0.0940,  0.1395,
          0.0867, -0.1056,  0.0329, -0.1489,  0.0904, -0.0462,  0.0266,  0.0261,
          0.0984, -0.1442, -0.1060,  0.0203,  0.0993, -0.1093, -0.0573, -0.0364,
          0.1123, -0.0312, -0.0215, -0.0985,  0.1507, -0.0472,  0.1351,  0.0340,
         -0.0823,  0.0658, -0.0656,  0.0083, -0.1138,  0.1152, -0.0352, -0.1231,
         -0.1310, -0.0330, -0.0726, -0.0547,  0.1506,  0.1461, -0.0736, -0.1096,
         -0.0867, -0.1274, -0.0395, -0.0386, -0.0753,  0.1192,  0.0215,  0.0587,
         -0.1274, -0.0590, -0.1008,  0.0654,  0.0908,  0.0282,  0.1323,  0.0987,
         -0.0820, -0.0606,  0.1367,  0.0979, -0.1087,  0.0314, -0.0736,  0.0974,
          0.0323,  0.0517, -0.0174, -0.1237, -0.1257, -0.1157,  0.0079,  0.0926,
         -0.0681,  0.0781,  0.0217, -0.1132, -0.1226,  0.0974, -0.0096,  0.0373,
         -0.1423, -0.0501,  0.0719, -0.0622,  0.1136,  0.1491,  0.0385,  0.0222,
         -0.1075,  0.0849,  0.1516, -0.1161, -0.1158,  0.0807, -0.1310,  0.0946,
         -0.0531, -0.0062, -0.0517,  0.1370, -0.0869,  0.1417,  0.0224, -0.0050]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1110,  0.0008,  0.0561, -0.1034,  0.0827,  0.0375, -0.0013,  0.1402,
         -0.1252, -0.1525, -0.1072,  0.0208, -0.1463,  0.0660, -0.0624, -0.1115,
         -0.1277,  0.0293,  0.1266,  0.0706,  0.1280, -0.0882, -0.0082,  0.0722,
         -0.0545, -0.0726,  0.1113,  0.0893,  0.1413,  0.1496, -0.1331, -0.0637,
          0.1474,  0.1045,  0.0478, -0.0443,  0.1382, -0.1368,  0.0842,  0.1111,
          0.0319,  0.0230, -0.1473, -0.0987,  0.0619,  0.0798,  0.0479,  0.0277,
         -0.0894,  0.0567, -0.1485,  0.0142,  0.0881,  0.0037,  0.1104, -0.0036,
         -0.1406,  0.1390, -0.1110,  0.0136, -0.0796,  0.0183,  0.1198,  0.0979,
         -0.1414,  0.1111,  0.0810, -0.0010,  0.0783,  0.0462,  0.0045, -0.1230,
         -0.0098, -0.0553,  0.0471, -0.1354, -0.0095, -0.1471, -0.1151,  0.0799,
          0.0853, -0.0705,  0.0551,  0.0625, -0.0596, -0.1142, -0.0994, -0.0281,
          0.0068,  0.0762, -0.1175, -0.0706,  0.0846, -0.1497,  0.1142,  0.0823,
         -0.0601,  0.1031,  0.0956,  0.0194,  0.0233, -0.0063, -0.1153,  0.1177,
          0.0611,  0.1033,  0.0040, -0.0036, -0.0733, -0.1315,  0.0661, -0.0082,
          0.0677, -0.0588,  0.0644,  0.1371, -0.0527,  0.0189,  0.0343,  0.1438,
         -0.1157, -0.0457,  0.0658, -0.0311,  0.1464, -0.0666,  0.1036, -0.0072,
         -0.0773, -0.1244, -0.0260, -0.0289, -0.1400, -0.1335,  0.1476, -0.0983,
          0.0711,  0.0542, -0.0974,  0.1351,  0.0020, -0.1142, -0.1403,  0.1417,
          0.1128,  0.0232,  0.0433,  0.1401, -0.1390, -0.0848,  0.0940,  0.1395,
          0.0867, -0.1056,  0.0329, -0.1489,  0.0904, -0.0462,  0.0266,  0.0261,
          0.0984, -0.1442, -0.1060,  0.0203,  0.0993, -0.1093, -0.0573, -0.0364,
          0.1123, -0.0312, -0.0215, -0.0985,  0.1507, -0.0472,  0.1351,  0.0340,
         -0.0823,  0.0658, -0.0656,  0.0083, -0.1138,  0.1152, -0.0352, -0.1231,
         -0.1310, -0.0330, -0.0726, -0.0547,  0.1506,  0.1461, -0.0736, -0.1096,
         -0.0867, -0.1274, -0.0395, -0.0386, -0.0753,  0.1192,  0.0215,  0.0587,
         -0.1274, -0.0590, -0.1008,  0.0654,  0.0908,  0.0282,  0.1323,  0.0987,
         -0.0820, -0.0606,  0.1367,  0.0979, -0.1087,  0.0314, -0.0736,  0.0974,
          0.0323,  0.0517, -0.0174, -0.1237, -0.1257, -0.1157,  0.0079,  0.0926,
         -0.0681,  0.0781,  0.0217, -0.1132, -0.1226,  0.0974, -0.0096,  0.0373,
         -0.1423, -0.0501,  0.0719, -0.0622,  0.1136,  0.1491,  0.0385,  0.0222,
         -0.1075,  0.0849,  0.1516, -0.1161, -0.1158,  0.0807, -0.1310,  0.0946,
         -0.0531, -0.0062, -0.0517,  0.1370, -0.0869,  0.1417,  0.0224, -0.0050]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0505, -0.0065,  0.0844,  ...,  0.0409, -0.0553, -0.0760],
        [-0.1061, -0.1200,  0.0866,  ..., -0.0445,  0.1133,  0.0541],
        [ 0.0333,  0.0127, -0.0789,  ...,  0.0900, -0.0924,  0.0846],
        ...,
        [ 0.0642,  0.0623,  0.0381,  ...,  0.0605,  0.0163,  0.0235],
        [ 0.0976, -0.0929, -0.0761,  ...,  0.1067, -0.0551, -0.0153],
        [ 0.0397, -0.0571, -0.0992,  ..., -0.0241,  0.0040, -0.0298]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0505, -0.0065,  0.0844,  ...,  0.0409, -0.0553, -0.0760],
        [-0.1061, -0.1200,  0.0866,  ..., -0.0445,  0.1133,  0.0541],
        [ 0.0333,  0.0127, -0.0789,  ...,  0.0900, -0.0924,  0.0846],
        ...,
        [ 0.0642,  0.0623,  0.0381,  ...,  0.0605,  0.0163,  0.0235],
        [ 0.0976, -0.0929, -0.0761,  ...,  0.1067, -0.0551, -0.0153],
        [ 0.0397, -0.0571, -0.0992,  ..., -0.0241,  0.0040, -0.0298]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1082,  0.0727,  0.0240,  ...,  0.1161, -0.0372,  0.0575],
        [ 0.0030, -0.0213,  0.0717,  ..., -0.0663,  0.1695, -0.1195],
        [ 0.0495, -0.1671, -0.1303,  ...,  0.1088,  0.0809, -0.1535],
        ...,
        [ 0.0603,  0.0369, -0.0811,  ...,  0.0580,  0.0908, -0.0391],
        [-0.0514, -0.0413,  0.0026,  ...,  0.1683, -0.1320, -0.1468],
        [-0.0107, -0.1716, -0.0569,  ..., -0.1363,  0.1504,  0.1481]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1082,  0.0727,  0.0240,  ...,  0.1161, -0.0372,  0.0575],
        [ 0.0030, -0.0213,  0.0717,  ..., -0.0663,  0.1695, -0.1195],
        [ 0.0495, -0.1671, -0.1303,  ...,  0.1088,  0.0809, -0.1535],
        ...,
        [ 0.0603,  0.0369, -0.0811,  ...,  0.0580,  0.0908, -0.0391],
        [-0.0514, -0.0413,  0.0026,  ...,  0.1683, -0.1320, -0.1468],
        [-0.0107, -0.1716, -0.0569,  ..., -0.1363,  0.1504,  0.1481]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1583, -0.1373,  0.0829,  ...,  0.2034, -0.0875, -0.0340],
        [-0.0422,  0.0772, -0.1753,  ..., -0.0607, -0.2328, -0.0847],
        [-0.1012,  0.2327, -0.0320,  ...,  0.1830, -0.1383, -0.1956],
        ...,
        [-0.1685,  0.2353,  0.1872,  ...,  0.1089, -0.0188, -0.0299],
        [-0.1224,  0.0411, -0.0320,  ..., -0.1981, -0.1229, -0.0517],
        [ 0.1127,  0.1413, -0.0923,  ..., -0.1105,  0.0680,  0.1630]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1583, -0.1373,  0.0829,  ...,  0.2034, -0.0875, -0.0340],
        [-0.0422,  0.0772, -0.1753,  ..., -0.0607, -0.2328, -0.0847],
        [-0.1012,  0.2327, -0.0320,  ...,  0.1830, -0.1383, -0.1956],
        ...,
        [-0.1685,  0.2353,  0.1872,  ...,  0.1089, -0.0188, -0.0299],
        [-0.1224,  0.0411, -0.0320,  ..., -0.1981, -0.1229, -0.0517],
        [ 0.1127,  0.1413, -0.0923,  ..., -0.1105,  0.0680,  0.1630]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2986],
        [ 0.4172],
        [ 0.1009],
        [ 0.2219],
        [ 0.1146],
        [ 0.4019],
        [ 0.2758],
        [-0.4123],
        [-0.4184],
        [-0.3374],
        [-0.0611],
        [ 0.1923],
        [ 0.4161],
        [ 0.3546],
        [ 0.2031],
        [ 0.1310],
        [-0.2562],
        [ 0.2271],
        [-0.2797],
        [ 0.1465],
        [ 0.0550],
        [ 0.0697],
        [ 0.1190],
        [ 0.4245],
        [ 0.2966],
        [-0.3196],
        [-0.1482],
        [ 0.3916],
        [-0.1002],
        [-0.3706],
        [ 0.2152],
        [-0.2035]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2986],
        [ 0.4172],
        [ 0.1009],
        [ 0.2219],
        [ 0.1146],
        [ 0.4019],
        [ 0.2758],
        [-0.4123],
        [-0.4184],
        [-0.3374],
        [-0.0611],
        [ 0.1923],
        [ 0.4161],
        [ 0.3546],
        [ 0.2031],
        [ 0.1310],
        [-0.2562],
        [ 0.2271],
        [-0.2797],
        [ 0.1465],
        [ 0.0550],
        [ 0.0697],
        [ 0.1190],
        [ 0.4245],
        [ 0.2966],
        [-0.3196],
        [-0.1482],
        [ 0.3916],
        [-0.1002],
        [-0.3706],
        [ 0.2152],
        [-0.2035]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(13.9668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.8650, device='cuda:0')



h[100].sum tensor(2.8168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8910, device='cuda:0')



h[200].sum tensor(-0.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3112.5186, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.8684e-05, 3.9753e-03,  ..., 0.0000e+00, 0.0000e+00,
         1.3527e-04],
        [0.0000e+00, 9.7487e-05, 2.0742e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.0578e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15087.3643, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-17.0140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(273.5298, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(21.8817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-31.8005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0840],
        [-0.1029],
        [-0.1483],
        ...,
        [-0.0237],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2120.1245, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0840],
        [-0.1029],
        [-0.1483],
        ...,
        [-0.0237],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(437.3452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3236, device='cuda:0')



h[100].sum tensor(12.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9352, device='cuda:0')



h[200].sum tensor(6.7809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(16808.6934, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0156, 0.0139, 0.0000,  ..., 0.0000, 0.0000, 0.0055],
        [0.0033, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(72522.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1262.8550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.8477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.0229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.0893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.3745],
        [0.2296],
        [0.1405],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(21997.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0840],
        [-0.1029],
        [-0.1483],
        ...,
        [-0.0237],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 30001 
BatchSize 5 
EpochNum 50 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0776,  0.1474, -0.0030,  0.1100, -0.0455, -0.1008, -0.0882,  0.0539,
          0.0320, -0.0275, -0.1210, -0.0432,  0.0027,  0.0679,  0.1482,  0.0288,
          0.1213,  0.1209,  0.0233, -0.1523,  0.1001,  0.1426,  0.0164,  0.1383,
         -0.0193,  0.0450,  0.1357, -0.0618,  0.1010,  0.1504, -0.0676, -0.0480,
         -0.1304,  0.1451,  0.1482, -0.1114,  0.1169,  0.1405, -0.0375, -0.0977,
         -0.1491,  0.1394,  0.1078, -0.0422,  0.1349, -0.0949, -0.0444,  0.0693,
          0.0560, -0.0962, -0.0923, -0.0171, -0.1423,  0.0436, -0.0292,  0.0075,
         -0.0123,  0.0435,  0.0311, -0.0578, -0.0709, -0.0959, -0.0102,  0.0927,
         -0.0192, -0.0900, -0.0841,  0.1366, -0.0885, -0.1302,  0.0924, -0.0841,
         -0.0743,  0.1277,  0.0106, -0.1070, -0.0658,  0.1264,  0.0255,  0.0478,
         -0.1178,  0.1270, -0.0771,  0.1105,  0.0718,  0.1115, -0.0885,  0.0824,
         -0.0815,  0.1256, -0.1456, -0.0287, -0.0291, -0.0934, -0.1411, -0.0829,
          0.0491, -0.0105, -0.0955, -0.1465, -0.0956,  0.0177, -0.0635, -0.1215,
          0.0506,  0.0228,  0.1270, -0.1174, -0.0607,  0.1031,  0.0103, -0.0700,
         -0.0278, -0.0065,  0.1021,  0.1526,  0.0136,  0.1400,  0.0876, -0.0320,
          0.0668,  0.1471,  0.1319, -0.0389,  0.0342,  0.1141,  0.0069,  0.0307,
         -0.1415, -0.0107,  0.1315, -0.1336, -0.0041, -0.1126, -0.1010, -0.0796,
          0.1212, -0.0075,  0.1106, -0.0656, -0.0498,  0.0336, -0.1366,  0.1109,
         -0.0347, -0.1085,  0.0739,  0.0080,  0.1244, -0.0911,  0.0415,  0.1318,
          0.1397,  0.1206,  0.0195, -0.0279,  0.1356,  0.0894, -0.1057,  0.0496,
          0.1110, -0.0031,  0.0351, -0.0381, -0.0722,  0.0192,  0.0828, -0.0354,
         -0.0015, -0.0433, -0.1192,  0.0182, -0.1300, -0.0994,  0.1231, -0.0273,
          0.0196,  0.0930,  0.1181, -0.0455,  0.1043, -0.0696,  0.0012,  0.1492,
         -0.0485,  0.0359, -0.0164, -0.1001, -0.1274,  0.0120,  0.0011,  0.0417,
          0.1071, -0.1033,  0.1527, -0.0697,  0.0741, -0.1128,  0.0741,  0.1030,
          0.0737, -0.1023, -0.1091, -0.0851,  0.0820, -0.1445,  0.0705, -0.0892,
          0.0550,  0.0495,  0.0805,  0.0611,  0.0338, -0.1373,  0.1381, -0.1033,
         -0.0076, -0.0110,  0.0975,  0.0784,  0.0985, -0.0657,  0.0995, -0.0894,
         -0.1495,  0.1302,  0.0018, -0.0767, -0.0667, -0.0991, -0.1232,  0.0960,
          0.1124,  0.0816,  0.0202, -0.0821,  0.1156, -0.1349, -0.0203,  0.0509,
          0.0558, -0.0828,  0.0525, -0.0067,  0.0022, -0.0170, -0.0030,  0.0997,
         -0.0877, -0.0485,  0.1189,  0.1131, -0.0938,  0.0851,  0.1217,  0.0846]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0144, -0.1180, -0.0347,  ..., -0.0617,  0.0358, -0.0861],
        [-0.0078, -0.0460, -0.0833,  ..., -0.0144, -0.1188,  0.0995],
        [-0.0502,  0.0234,  0.0868,  ...,  0.0897, -0.1137, -0.0125],
        ...,
        [ 0.1089, -0.0677, -0.0871,  ...,  0.0985,  0.1175,  0.0110],
        [ 0.1136,  0.0017,  0.0911,  ...,  0.1140,  0.0901, -0.1066],
        [ 0.1056,  0.0243, -0.0103,  ...,  0.1129,  0.1089,  0.0723]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0287,  0.1339, -0.0936,  ...,  0.1485, -0.0208,  0.1464],
        [-0.1560,  0.1279, -0.0354,  ...,  0.0768, -0.1709, -0.1443],
        [ 0.1106,  0.1599, -0.1178,  ..., -0.0427,  0.0268,  0.0164],
        ...,
        [-0.1070, -0.1098, -0.1494,  ...,  0.1595, -0.0258, -0.0750],
        [-0.0036, -0.1244,  0.0573,  ..., -0.0860, -0.0181, -0.1129],
        [-0.1170,  0.0136,  0.0453,  ..., -0.0920,  0.0932,  0.1633]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0399, -0.2169,  0.2222,  ...,  0.1736,  0.0413, -0.1242],
        [-0.1571,  0.2490,  0.0963,  ...,  0.1508,  0.1780, -0.1580],
        [ 0.1617,  0.1754,  0.2315,  ...,  0.0573, -0.0099, -0.0243],
        ...,
        [-0.0729,  0.2423, -0.1209,  ..., -0.0982,  0.1530, -0.1075],
        [-0.0177,  0.1863, -0.0575,  ..., -0.1206,  0.1376, -0.0020],
        [-0.0562, -0.0611, -0.0191,  ...,  0.1652,  0.0180,  0.0479]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2892],
        [ 0.0633],
        [-0.2567],
        [-0.2981],
        [-0.2431],
        [-0.1194],
        [-0.2902],
        [-0.0685],
        [ 0.0309],
        [ 0.4159],
        [-0.2191],
        [-0.3172],
        [ 0.3740],
        [-0.1039],
        [ 0.1915],
        [ 0.4242],
        [-0.3883],
        [ 0.2677],
        [-0.2482],
        [ 0.2496],
        [-0.1982],
        [-0.2359],
        [-0.1621],
        [-0.4073],
        [-0.1165],
        [-0.3785],
        [ 0.1971],
        [-0.0888],
        [-0.3267],
        [-0.0739],
        [-0.2717],
        [ 0.1239]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0776,  0.1474, -0.0030,  0.1100, -0.0455, -0.1008, -0.0882,  0.0539,
          0.0320, -0.0275, -0.1210, -0.0432,  0.0027,  0.0679,  0.1482,  0.0288,
          0.1213,  0.1209,  0.0233, -0.1523,  0.1001,  0.1426,  0.0164,  0.1383,
         -0.0193,  0.0450,  0.1357, -0.0618,  0.1010,  0.1504, -0.0676, -0.0480,
         -0.1304,  0.1451,  0.1482, -0.1114,  0.1169,  0.1405, -0.0375, -0.0977,
         -0.1491,  0.1394,  0.1078, -0.0422,  0.1349, -0.0949, -0.0444,  0.0693,
          0.0560, -0.0962, -0.0923, -0.0171, -0.1423,  0.0436, -0.0292,  0.0075,
         -0.0123,  0.0435,  0.0311, -0.0578, -0.0709, -0.0959, -0.0102,  0.0927,
         -0.0192, -0.0900, -0.0841,  0.1366, -0.0885, -0.1302,  0.0924, -0.0841,
         -0.0743,  0.1277,  0.0106, -0.1070, -0.0658,  0.1264,  0.0255,  0.0478,
         -0.1178,  0.1270, -0.0771,  0.1105,  0.0718,  0.1115, -0.0885,  0.0824,
         -0.0815,  0.1256, -0.1456, -0.0287, -0.0291, -0.0934, -0.1411, -0.0829,
          0.0491, -0.0105, -0.0955, -0.1465, -0.0956,  0.0177, -0.0635, -0.1215,
          0.0506,  0.0228,  0.1270, -0.1174, -0.0607,  0.1031,  0.0103, -0.0700,
         -0.0278, -0.0065,  0.1021,  0.1526,  0.0136,  0.1400,  0.0876, -0.0320,
          0.0668,  0.1471,  0.1319, -0.0389,  0.0342,  0.1141,  0.0069,  0.0307,
         -0.1415, -0.0107,  0.1315, -0.1336, -0.0041, -0.1126, -0.1010, -0.0796,
          0.1212, -0.0075,  0.1106, -0.0656, -0.0498,  0.0336, -0.1366,  0.1109,
         -0.0347, -0.1085,  0.0739,  0.0080,  0.1244, -0.0911,  0.0415,  0.1318,
          0.1397,  0.1206,  0.0195, -0.0279,  0.1356,  0.0894, -0.1057,  0.0496,
          0.1110, -0.0031,  0.0351, -0.0381, -0.0722,  0.0192,  0.0828, -0.0354,
         -0.0015, -0.0433, -0.1192,  0.0182, -0.1300, -0.0994,  0.1231, -0.0273,
          0.0196,  0.0930,  0.1181, -0.0455,  0.1043, -0.0696,  0.0012,  0.1492,
         -0.0485,  0.0359, -0.0164, -0.1001, -0.1274,  0.0120,  0.0011,  0.0417,
          0.1071, -0.1033,  0.1527, -0.0697,  0.0741, -0.1128,  0.0741,  0.1030,
          0.0737, -0.1023, -0.1091, -0.0851,  0.0820, -0.1445,  0.0705, -0.0892,
          0.0550,  0.0495,  0.0805,  0.0611,  0.0338, -0.1373,  0.1381, -0.1033,
         -0.0076, -0.0110,  0.0975,  0.0784,  0.0985, -0.0657,  0.0995, -0.0894,
         -0.1495,  0.1302,  0.0018, -0.0767, -0.0667, -0.0991, -0.1232,  0.0960,
          0.1124,  0.0816,  0.0202, -0.0821,  0.1156, -0.1349, -0.0203,  0.0509,
          0.0558, -0.0828,  0.0525, -0.0067,  0.0022, -0.0170, -0.0030,  0.0997,
         -0.0877, -0.0485,  0.1189,  0.1131, -0.0938,  0.0851,  0.1217,  0.0846]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0144, -0.1180, -0.0347,  ..., -0.0617,  0.0358, -0.0861],
        [-0.0078, -0.0460, -0.0833,  ..., -0.0144, -0.1188,  0.0995],
        [-0.0502,  0.0234,  0.0868,  ...,  0.0897, -0.1137, -0.0125],
        ...,
        [ 0.1089, -0.0677, -0.0871,  ...,  0.0985,  0.1175,  0.0110],
        [ 0.1136,  0.0017,  0.0911,  ...,  0.1140,  0.0901, -0.1066],
        [ 0.1056,  0.0243, -0.0103,  ...,  0.1129,  0.1089,  0.0723]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0287,  0.1339, -0.0936,  ...,  0.1485, -0.0208,  0.1464],
        [-0.1560,  0.1279, -0.0354,  ...,  0.0768, -0.1709, -0.1443],
        [ 0.1106,  0.1599, -0.1178,  ..., -0.0427,  0.0268,  0.0164],
        ...,
        [-0.1070, -0.1098, -0.1494,  ...,  0.1595, -0.0258, -0.0750],
        [-0.0036, -0.1244,  0.0573,  ..., -0.0860, -0.0181, -0.1129],
        [-0.1170,  0.0136,  0.0453,  ..., -0.0920,  0.0932,  0.1633]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0399, -0.2169,  0.2222,  ...,  0.1736,  0.0413, -0.1242],
        [-0.1571,  0.2490,  0.0963,  ...,  0.1508,  0.1780, -0.1580],
        [ 0.1617,  0.1754,  0.2315,  ...,  0.0573, -0.0099, -0.0243],
        ...,
        [-0.0729,  0.2423, -0.1209,  ..., -0.0982,  0.1530, -0.1075],
        [-0.0177,  0.1863, -0.0575,  ..., -0.1206,  0.1376, -0.0020],
        [-0.0562, -0.0611, -0.0191,  ...,  0.1652,  0.0180,  0.0479]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2892],
        [ 0.0633],
        [-0.2567],
        [-0.2981],
        [-0.2431],
        [-0.1194],
        [-0.2902],
        [-0.0685],
        [ 0.0309],
        [ 0.4159],
        [-0.2191],
        [-0.3172],
        [ 0.3740],
        [-0.1039],
        [ 0.1915],
        [ 0.4242],
        [-0.3883],
        [ 0.2677],
        [-0.2482],
        [ 0.2496],
        [-0.1982],
        [-0.2359],
        [-0.1621],
        [-0.4073],
        [-0.1165],
        [-0.3785],
        [ 0.1971],
        [-0.0888],
        [-0.3267],
        [-0.0739],
        [-0.2717],
        [ 0.1239]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': array(0.0001), 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': array(5.e-05), 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114,  0.0216, -0.0004,  ...,  0.0125,  0.0179,  0.0124],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(477.5527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.8179, device='cuda:0')



h[100].sum tensor(-27.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.6551, device='cuda:0')



h[200].sum tensor(21.5665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0817, 0.0000,  ..., 0.0472, 0.0675, 0.0469],
        [0.0000, 0.0672, 0.0000,  ..., 0.0388, 0.0555, 0.0386],
        [0.0000, 0.0157, 0.0000,  ..., 0.0091, 0.0130, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33101.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0374, 0.0920, 0.0837,  ..., 0.1245, 0.1195, 0.0000],
        [0.0321, 0.0788, 0.0717,  ..., 0.1067, 0.1024, 0.0000],
        [0.0258, 0.0633, 0.0576,  ..., 0.0857, 0.0822, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(143606.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(521.6234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.2270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-108.4431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(459.0871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(195.6844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7721e+00],
        [-1.9175e+00],
        [-2.1159e+00],
        ...,
        [-2.1289e-06],
        [-2.7970e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33026.3164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 85, in <module>
    optimizer.step()  # Update parameters based on gradients.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/adam.py", line 133, in step
    F.adam(params_with_grad,
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/_functional.py", line 83, in adam
    grad = grad.add(param, alpha=weight_decay)
TypeError: add(): argument 'alpha' must be Number, not ndarray

real	1m0.903s
user	0m11.409s
sys	0m10.748s
