0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:13:59 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b866969ffa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m52.610s
user	0m3.201s
sys	0m1.742s
[02:14:53] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.4297],
        [ 1.4514],
        [ 0.0657],
        ...,
        [-0.5653],
        [ 1.0548],
        [-1.5802]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-46.7562, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0321,  0.0188,  0.1001, -0.0188, -0.0570, -0.1038,  0.0578, -0.0414,
         -0.0667,  0.0083, -0.0391, -0.0173,  0.1138,  0.0988, -0.1413, -0.1185,
          0.0951, -0.1371, -0.0064, -0.0235, -0.0075,  0.1407,  0.1396, -0.1041,
          0.1400,  0.0909, -0.0002, -0.1001, -0.0331, -0.0167, -0.0265,  0.0009,
          0.1045, -0.1260, -0.0701,  0.0436, -0.0322,  0.0155,  0.0194, -0.0018,
          0.0920, -0.1403, -0.1421,  0.0465,  0.1306, -0.0358,  0.0342, -0.0873,
          0.0319, -0.0599,  0.1076, -0.0159,  0.0484,  0.0861,  0.1007,  0.1341,
          0.1066,  0.0222,  0.1454,  0.0930,  0.1232, -0.0342, -0.0690, -0.0706,
         -0.1270, -0.0611,  0.1524, -0.0871,  0.0612, -0.0846,  0.0160,  0.1161,
          0.0327, -0.1516,  0.1031,  0.0339,  0.0074,  0.0716,  0.0709, -0.0112,
          0.0065,  0.0943, -0.0468, -0.0310, -0.0612,  0.1410, -0.1157, -0.1026,
          0.1156, -0.1080, -0.0961, -0.0055, -0.0607, -0.0997, -0.1362,  0.0273,
         -0.0227,  0.1033, -0.0079, -0.0559, -0.0777,  0.0093, -0.1488, -0.0827,
          0.1430,  0.0487,  0.0782,  0.0971, -0.0581,  0.0048,  0.0643, -0.0619,
         -0.0027,  0.0777, -0.0970,  0.0426,  0.0754,  0.0635,  0.0818, -0.0352,
         -0.1043, -0.1194, -0.1188, -0.1037,  0.1339, -0.0249,  0.0308,  0.0672,
         -0.0570,  0.0827, -0.0179, -0.0066,  0.0986, -0.0705,  0.1059, -0.0113,
         -0.0949,  0.0269,  0.0170,  0.0547,  0.0190,  0.0358,  0.0698, -0.1409,
         -0.0828,  0.0580, -0.1213,  0.0676, -0.0834,  0.1426, -0.0695, -0.1272,
         -0.0309,  0.0616,  0.0300,  0.0428, -0.1057,  0.0726,  0.1112,  0.1125,
          0.1028, -0.0317,  0.0812,  0.0159,  0.0026,  0.1207, -0.0230, -0.0005,
          0.0447, -0.1464,  0.0991,  0.0879,  0.0014,  0.0930,  0.1454, -0.1270,
         -0.0537,  0.0189,  0.0208, -0.1457, -0.0842,  0.1402, -0.0992,  0.0889,
          0.0703,  0.1118, -0.0760, -0.0526, -0.1366,  0.1228, -0.0998, -0.0442,
         -0.0153,  0.1380,  0.0070, -0.0793,  0.0413, -0.1250,  0.0131,  0.0831,
          0.0366,  0.0273, -0.1284,  0.1114, -0.1069, -0.0076, -0.0822,  0.0153,
         -0.1373,  0.0339, -0.0129,  0.1468,  0.0138, -0.1184,  0.0446,  0.0409,
         -0.0712,  0.1265,  0.0115, -0.1337,  0.0440, -0.1500, -0.0684, -0.1194,
          0.0047,  0.0825, -0.0986, -0.1186, -0.0489,  0.0818, -0.1314, -0.1380,
          0.0673,  0.1036, -0.0634,  0.1303, -0.0463, -0.1172,  0.0962, -0.1471,
         -0.1121, -0.1108,  0.0777, -0.1059, -0.0509, -0.0546, -0.1088, -0.0405,
         -0.1434, -0.0534,  0.0732,  0.0861,  0.0400, -0.1223,  0.0753, -0.0507]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0321,  0.0188,  0.1001, -0.0188, -0.0570, -0.1038,  0.0578, -0.0414,
         -0.0667,  0.0083, -0.0391, -0.0173,  0.1138,  0.0988, -0.1413, -0.1185,
          0.0951, -0.1371, -0.0064, -0.0235, -0.0075,  0.1407,  0.1396, -0.1041,
          0.1400,  0.0909, -0.0002, -0.1001, -0.0331, -0.0167, -0.0265,  0.0009,
          0.1045, -0.1260, -0.0701,  0.0436, -0.0322,  0.0155,  0.0194, -0.0018,
          0.0920, -0.1403, -0.1421,  0.0465,  0.1306, -0.0358,  0.0342, -0.0873,
          0.0319, -0.0599,  0.1076, -0.0159,  0.0484,  0.0861,  0.1007,  0.1341,
          0.1066,  0.0222,  0.1454,  0.0930,  0.1232, -0.0342, -0.0690, -0.0706,
         -0.1270, -0.0611,  0.1524, -0.0871,  0.0612, -0.0846,  0.0160,  0.1161,
          0.0327, -0.1516,  0.1031,  0.0339,  0.0074,  0.0716,  0.0709, -0.0112,
          0.0065,  0.0943, -0.0468, -0.0310, -0.0612,  0.1410, -0.1157, -0.1026,
          0.1156, -0.1080, -0.0961, -0.0055, -0.0607, -0.0997, -0.1362,  0.0273,
         -0.0227,  0.1033, -0.0079, -0.0559, -0.0777,  0.0093, -0.1488, -0.0827,
          0.1430,  0.0487,  0.0782,  0.0971, -0.0581,  0.0048,  0.0643, -0.0619,
         -0.0027,  0.0777, -0.0970,  0.0426,  0.0754,  0.0635,  0.0818, -0.0352,
         -0.1043, -0.1194, -0.1188, -0.1037,  0.1339, -0.0249,  0.0308,  0.0672,
         -0.0570,  0.0827, -0.0179, -0.0066,  0.0986, -0.0705,  0.1059, -0.0113,
         -0.0949,  0.0269,  0.0170,  0.0547,  0.0190,  0.0358,  0.0698, -0.1409,
         -0.0828,  0.0580, -0.1213,  0.0676, -0.0834,  0.1426, -0.0695, -0.1272,
         -0.0309,  0.0616,  0.0300,  0.0428, -0.1057,  0.0726,  0.1112,  0.1125,
          0.1028, -0.0317,  0.0812,  0.0159,  0.0026,  0.1207, -0.0230, -0.0005,
          0.0447, -0.1464,  0.0991,  0.0879,  0.0014,  0.0930,  0.1454, -0.1270,
         -0.0537,  0.0189,  0.0208, -0.1457, -0.0842,  0.1402, -0.0992,  0.0889,
          0.0703,  0.1118, -0.0760, -0.0526, -0.1366,  0.1228, -0.0998, -0.0442,
         -0.0153,  0.1380,  0.0070, -0.0793,  0.0413, -0.1250,  0.0131,  0.0831,
          0.0366,  0.0273, -0.1284,  0.1114, -0.1069, -0.0076, -0.0822,  0.0153,
         -0.1373,  0.0339, -0.0129,  0.1468,  0.0138, -0.1184,  0.0446,  0.0409,
         -0.0712,  0.1265,  0.0115, -0.1337,  0.0440, -0.1500, -0.0684, -0.1194,
          0.0047,  0.0825, -0.0986, -0.1186, -0.0489,  0.0818, -0.1314, -0.1380,
          0.0673,  0.1036, -0.0634,  0.1303, -0.0463, -0.1172,  0.0962, -0.1471,
         -0.1121, -0.1108,  0.0777, -0.1059, -0.0509, -0.0546, -0.1088, -0.0405,
         -0.1434, -0.0534,  0.0732,  0.0861,  0.0400, -0.1223,  0.0753, -0.0507]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.1046, -0.0769, -0.0150,  ..., -0.1025,  0.0939, -0.0070],
        [-0.0161,  0.0595,  0.0986,  ...,  0.0618,  0.0070, -0.0701],
        [-0.0659,  0.1204,  0.0425,  ..., -0.0962, -0.0519,  0.0559],
        ...,
        [-0.1121, -0.0672, -0.0521,  ...,  0.1046,  0.1194,  0.0889],
        [-0.0185, -0.0816, -0.0303,  ...,  0.0086, -0.0041,  0.1166],
        [-0.0045,  0.0441,  0.0371,  ...,  0.0981, -0.0392, -0.0564]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1046, -0.0769, -0.0150,  ..., -0.1025,  0.0939, -0.0070],
        [-0.0161,  0.0595,  0.0986,  ...,  0.0618,  0.0070, -0.0701],
        [-0.0659,  0.1204,  0.0425,  ..., -0.0962, -0.0519,  0.0559],
        ...,
        [-0.1121, -0.0672, -0.0521,  ...,  0.1046,  0.1194,  0.0889],
        [-0.0185, -0.0816, -0.0303,  ...,  0.0086, -0.0041,  0.1166],
        [-0.0045,  0.0441,  0.0371,  ...,  0.0981, -0.0392, -0.0564]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0460, -0.0196, -0.1459,  ..., -0.1508, -0.0376, -0.1703],
        [ 0.0507,  0.0678, -0.0441,  ..., -0.1320, -0.0762, -0.0100],
        [ 0.0954,  0.0193, -0.1522,  ..., -0.1109, -0.0048,  0.0892],
        ...,
        [-0.0920,  0.1386, -0.0992,  ...,  0.0203,  0.0244,  0.1043],
        [-0.1474, -0.0773,  0.0042,  ...,  0.1039,  0.1634,  0.1610],
        [-0.0606,  0.0758, -0.1693,  ..., -0.1438,  0.1101, -0.1692]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0460, -0.0196, -0.1459,  ..., -0.1508, -0.0376, -0.1703],
        [ 0.0507,  0.0678, -0.0441,  ..., -0.1320, -0.0762, -0.0100],
        [ 0.0954,  0.0193, -0.1522,  ..., -0.1109, -0.0048,  0.0892],
        ...,
        [-0.0920,  0.1386, -0.0992,  ...,  0.0203,  0.0244,  0.1043],
        [-0.1474, -0.0773,  0.0042,  ...,  0.1039,  0.1634,  0.1610],
        [-0.0606,  0.0758, -0.1693,  ..., -0.1438,  0.1101, -0.1692]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1343,  0.2467, -0.2201,  ..., -0.2472, -0.0110,  0.1515],
        [ 0.2231,  0.1447, -0.0309,  ...,  0.1042, -0.0497,  0.1459],
        [-0.1224, -0.0464, -0.0486,  ..., -0.2175,  0.1705, -0.1203],
        ...,
        [ 0.2221,  0.0782, -0.1332,  ...,  0.1295,  0.2016, -0.0667],
        [ 0.0993, -0.0348,  0.2116,  ...,  0.0812, -0.0147, -0.2002],
        [-0.1507,  0.0461,  0.1118,  ...,  0.0608,  0.1927,  0.2345]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1343,  0.2467, -0.2201,  ..., -0.2472, -0.0110,  0.1515],
        [ 0.2231,  0.1447, -0.0309,  ...,  0.1042, -0.0497,  0.1459],
        [-0.1224, -0.0464, -0.0486,  ..., -0.2175,  0.1705, -0.1203],
        ...,
        [ 0.2221,  0.0782, -0.1332,  ...,  0.1295,  0.2016, -0.0667],
        [ 0.0993, -0.0348,  0.2116,  ...,  0.0812, -0.0147, -0.2002],
        [-0.1507,  0.0461,  0.1118,  ...,  0.0608,  0.1927,  0.2345]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2296],
        [-0.4115],
        [-0.2596],
        [-0.2683],
        [-0.1516],
        [ 0.0510],
        [ 0.3566],
        [ 0.3986],
        [-0.0567],
        [-0.0472],
        [ 0.1384],
        [-0.0166],
        [-0.2681],
        [-0.2275],
        [ 0.3485],
        [-0.3724],
        [-0.2264],
        [-0.0513],
        [ 0.3416],
        [-0.2265],
        [-0.1310],
        [-0.0864],
        [-0.2962],
        [-0.2596],
        [-0.2384],
        [-0.0297],
        [ 0.2759],
        [-0.0208],
        [-0.0947],
        [-0.3313],
        [ 0.2934],
        [ 0.3712]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2296],
        [-0.4115],
        [-0.2596],
        [-0.2683],
        [-0.1516],
        [ 0.0510],
        [ 0.3566],
        [ 0.3986],
        [-0.0567],
        [-0.0472],
        [ 0.1384],
        [-0.0166],
        [-0.2681],
        [-0.2275],
        [ 0.3485],
        [-0.3724],
        [-0.2264],
        [-0.0513],
        [ 0.3416],
        [-0.2265],
        [-0.1310],
        [-0.0864],
        [-0.2962],
        [-0.2596],
        [-0.2384],
        [-0.0297],
        [ 0.2759],
        [-0.0208],
        [-0.0947],
        [-0.3313],
        [ 0.2934],
        [ 0.3712]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0011,  0.0999,  0.0686,  0.0236, -0.1011,  0.0836, -0.0603,  0.0764,
          0.0671,  0.0026, -0.0343,  0.0622,  0.0790, -0.1193, -0.1239, -0.1308,
          0.1255,  0.1159, -0.0404, -0.1239, -0.0543,  0.0555, -0.1220, -0.0631,
         -0.0783, -0.0948, -0.0514,  0.1450,  0.1347, -0.1295, -0.0496,  0.1460,
         -0.1292,  0.0729, -0.1494,  0.0826, -0.0342,  0.1452, -0.1479, -0.1223,
          0.0593, -0.0472,  0.0292, -0.1146, -0.0363, -0.0289,  0.0019, -0.0801,
         -0.1109,  0.1311, -0.0460,  0.0061, -0.0209,  0.0774,  0.1045,  0.0962,
         -0.0821, -0.0209,  0.0569,  0.0377,  0.1439, -0.0929, -0.0004, -0.0132,
          0.0708,  0.0714,  0.0704,  0.0711, -0.0149, -0.1494,  0.0471, -0.0350,
          0.0554,  0.0864, -0.1145,  0.0346,  0.0129, -0.0486, -0.0733,  0.0079,
          0.1443,  0.1092, -0.0932,  0.1078, -0.0330,  0.0944,  0.1423, -0.1463,
         -0.1445, -0.0510,  0.1404, -0.0153, -0.0400, -0.1080, -0.0220,  0.1387,
         -0.0521,  0.1514, -0.0311,  0.1320,  0.0334,  0.0940, -0.0731,  0.0795,
          0.0993, -0.0567, -0.0223, -0.1500, -0.0328,  0.0852,  0.0342,  0.1493,
         -0.0110,  0.0195,  0.0671,  0.0203, -0.0658, -0.0393, -0.1454, -0.0780,
         -0.0345,  0.1367,  0.0234,  0.0412, -0.0240,  0.1251,  0.1410,  0.1270,
         -0.0757,  0.0957, -0.0405, -0.1319, -0.0391,  0.0922,  0.0042, -0.0281,
          0.0468, -0.0032, -0.1517, -0.0820, -0.0865,  0.0704,  0.1065, -0.0614,
         -0.1437, -0.0607,  0.0166,  0.0717, -0.1067,  0.0381,  0.0387,  0.1493,
          0.0757,  0.0419,  0.1434, -0.1225, -0.0343,  0.0292, -0.0669, -0.1317,
          0.1252,  0.0744, -0.0411, -0.0482,  0.0665, -0.1348,  0.0183, -0.1384,
         -0.1403, -0.1401, -0.0428, -0.0558, -0.1200, -0.0875, -0.0654,  0.0676,
         -0.1438,  0.1513, -0.1463,  0.1456,  0.0597, -0.0987,  0.0829, -0.0285,
         -0.0831,  0.1330, -0.1454,  0.0700, -0.0221, -0.0301,  0.0071,  0.0074,
         -0.0842, -0.0278, -0.0389,  0.0561, -0.0891,  0.0153,  0.0296, -0.0946,
          0.1118,  0.1162,  0.0736,  0.0300,  0.1086, -0.0363, -0.1381, -0.1193,
          0.0029,  0.0997, -0.0597,  0.0181,  0.0047, -0.0330,  0.1428, -0.0297,
         -0.0655,  0.1475, -0.1094, -0.0873, -0.0591, -0.0605,  0.0164, -0.0057,
          0.1156, -0.0873, -0.1183,  0.0534, -0.1230, -0.1367,  0.1323, -0.0194,
         -0.0690, -0.0803, -0.1469,  0.0171,  0.0964,  0.0117, -0.0256,  0.0793,
          0.0822,  0.0090, -0.0740,  0.1397, -0.1007,  0.0065,  0.1034, -0.1086,
          0.0530, -0.0993, -0.0752, -0.1224,  0.0911, -0.0444,  0.0022, -0.0975]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0011,  0.0999,  0.0686,  0.0236, -0.1011,  0.0836, -0.0603,  0.0764,
          0.0671,  0.0026, -0.0343,  0.0622,  0.0790, -0.1193, -0.1239, -0.1308,
          0.1255,  0.1159, -0.0404, -0.1239, -0.0543,  0.0555, -0.1220, -0.0631,
         -0.0783, -0.0948, -0.0514,  0.1450,  0.1347, -0.1295, -0.0496,  0.1460,
         -0.1292,  0.0729, -0.1494,  0.0826, -0.0342,  0.1452, -0.1479, -0.1223,
          0.0593, -0.0472,  0.0292, -0.1146, -0.0363, -0.0289,  0.0019, -0.0801,
         -0.1109,  0.1311, -0.0460,  0.0061, -0.0209,  0.0774,  0.1045,  0.0962,
         -0.0821, -0.0209,  0.0569,  0.0377,  0.1439, -0.0929, -0.0004, -0.0132,
          0.0708,  0.0714,  0.0704,  0.0711, -0.0149, -0.1494,  0.0471, -0.0350,
          0.0554,  0.0864, -0.1145,  0.0346,  0.0129, -0.0486, -0.0733,  0.0079,
          0.1443,  0.1092, -0.0932,  0.1078, -0.0330,  0.0944,  0.1423, -0.1463,
         -0.1445, -0.0510,  0.1404, -0.0153, -0.0400, -0.1080, -0.0220,  0.1387,
         -0.0521,  0.1514, -0.0311,  0.1320,  0.0334,  0.0940, -0.0731,  0.0795,
          0.0993, -0.0567, -0.0223, -0.1500, -0.0328,  0.0852,  0.0342,  0.1493,
         -0.0110,  0.0195,  0.0671,  0.0203, -0.0658, -0.0393, -0.1454, -0.0780,
         -0.0345,  0.1367,  0.0234,  0.0412, -0.0240,  0.1251,  0.1410,  0.1270,
         -0.0757,  0.0957, -0.0405, -0.1319, -0.0391,  0.0922,  0.0042, -0.0281,
          0.0468, -0.0032, -0.1517, -0.0820, -0.0865,  0.0704,  0.1065, -0.0614,
         -0.1437, -0.0607,  0.0166,  0.0717, -0.1067,  0.0381,  0.0387,  0.1493,
          0.0757,  0.0419,  0.1434, -0.1225, -0.0343,  0.0292, -0.0669, -0.1317,
          0.1252,  0.0744, -0.0411, -0.0482,  0.0665, -0.1348,  0.0183, -0.1384,
         -0.1403, -0.1401, -0.0428, -0.0558, -0.1200, -0.0875, -0.0654,  0.0676,
         -0.1438,  0.1513, -0.1463,  0.1456,  0.0597, -0.0987,  0.0829, -0.0285,
         -0.0831,  0.1330, -0.1454,  0.0700, -0.0221, -0.0301,  0.0071,  0.0074,
         -0.0842, -0.0278, -0.0389,  0.0561, -0.0891,  0.0153,  0.0296, -0.0946,
          0.1118,  0.1162,  0.0736,  0.0300,  0.1086, -0.0363, -0.1381, -0.1193,
          0.0029,  0.0997, -0.0597,  0.0181,  0.0047, -0.0330,  0.1428, -0.0297,
         -0.0655,  0.1475, -0.1094, -0.0873, -0.0591, -0.0605,  0.0164, -0.0057,
          0.1156, -0.0873, -0.1183,  0.0534, -0.1230, -0.1367,  0.1323, -0.0194,
         -0.0690, -0.0803, -0.1469,  0.0171,  0.0964,  0.0117, -0.0256,  0.0793,
          0.0822,  0.0090, -0.0740,  0.1397, -0.1007,  0.0065,  0.1034, -0.1086,
          0.0530, -0.0993, -0.0752, -0.1224,  0.0911, -0.0444,  0.0022, -0.0975]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0766, -0.0227, -0.0006,  ...,  0.0791, -0.0852, -0.0531],
        [ 0.0963, -0.0452,  0.1119,  ...,  0.1140,  0.1107,  0.0695],
        [ 0.0967,  0.0793,  0.0579,  ...,  0.0252, -0.1084, -0.0980],
        ...,
        [-0.0586, -0.0030,  0.0259,  ..., -0.0462,  0.0921, -0.0646],
        [ 0.0837, -0.1239,  0.0781,  ..., -0.0784,  0.1119,  0.0395],
        [ 0.0981, -0.0396,  0.1246,  ..., -0.0724,  0.0465, -0.0467]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0766, -0.0227, -0.0006,  ...,  0.0791, -0.0852, -0.0531],
        [ 0.0963, -0.0452,  0.1119,  ...,  0.1140,  0.1107,  0.0695],
        [ 0.0967,  0.0793,  0.0579,  ...,  0.0252, -0.1084, -0.0980],
        ...,
        [-0.0586, -0.0030,  0.0259,  ..., -0.0462,  0.0921, -0.0646],
        [ 0.0837, -0.1239,  0.0781,  ..., -0.0784,  0.1119,  0.0395],
        [ 0.0981, -0.0396,  0.1246,  ..., -0.0724,  0.0465, -0.0467]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0220, -0.0637,  0.1742,  ...,  0.1388,  0.0190,  0.0103],
        [-0.0905, -0.1005,  0.0483,  ...,  0.0566, -0.1014, -0.0929],
        [-0.1037, -0.1538, -0.1212,  ..., -0.0315, -0.1327, -0.1477],
        ...,
        [-0.1757, -0.0974, -0.0428,  ...,  0.0206,  0.0441,  0.1679],
        [ 0.0712,  0.1440, -0.0803,  ..., -0.1541,  0.0099,  0.0508],
        [-0.0389,  0.0836,  0.1579,  ..., -0.0756,  0.0159, -0.0876]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0220, -0.0637,  0.1742,  ...,  0.1388,  0.0190,  0.0103],
        [-0.0905, -0.1005,  0.0483,  ...,  0.0566, -0.1014, -0.0929],
        [-0.1037, -0.1538, -0.1212,  ..., -0.0315, -0.1327, -0.1477],
        ...,
        [-0.1757, -0.0974, -0.0428,  ...,  0.0206,  0.0441,  0.1679],
        [ 0.0712,  0.1440, -0.0803,  ..., -0.1541,  0.0099,  0.0508],
        [-0.0389,  0.0836,  0.1579,  ..., -0.0756,  0.0159, -0.0876]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0659,  0.1216, -0.1285,  ..., -0.0701,  0.1531,  0.2302],
        [-0.2209,  0.0436,  0.2465,  ..., -0.1806,  0.0271, -0.1972],
        [-0.1184, -0.1814, -0.1563,  ...,  0.0172, -0.1967, -0.2482],
        ...,
        [ 0.2300, -0.0846,  0.0440,  ...,  0.0866,  0.2160,  0.2307],
        [-0.1887,  0.1423,  0.0194,  ..., -0.1927, -0.0981, -0.1322],
        [ 0.0565,  0.0710,  0.0181,  ...,  0.0009,  0.1906,  0.1561]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0659,  0.1216, -0.1285,  ..., -0.0701,  0.1531,  0.2302],
        [-0.2209,  0.0436,  0.2465,  ..., -0.1806,  0.0271, -0.1972],
        [-0.1184, -0.1814, -0.1563,  ...,  0.0172, -0.1967, -0.2482],
        ...,
        [ 0.2300, -0.0846,  0.0440,  ...,  0.0866,  0.2160,  0.2307],
        [-0.1887,  0.1423,  0.0194,  ..., -0.1927, -0.0981, -0.1322],
        [ 0.0565,  0.0710,  0.0181,  ...,  0.0009,  0.1906,  0.1561]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.1255],
        [-0.2155],
        [-0.3353],
        [-0.3775],
        [-0.0426],
        [ 0.3810],
        [-0.1728],
        [-0.1653],
        [ 0.0910],
        [ 0.4045],
        [ 0.3199],
        [ 0.0736],
        [-0.1865],
        [ 0.2769],
        [-0.3377],
        [-0.0111],
        [-0.2553],
        [-0.0112],
        [-0.0972],
        [ 0.0520],
        [-0.0039],
        [ 0.1727],
        [-0.3366],
        [ 0.1565],
        [-0.0113],
        [ 0.4203],
        [ 0.3928],
        [ 0.1186],
        [-0.2333],
        [ 0.3142],
        [-0.2649],
        [ 0.2263]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.1255],
        [-0.2155],
        [-0.3353],
        [-0.3775],
        [-0.0426],
        [ 0.3810],
        [-0.1728],
        [-0.1653],
        [ 0.0910],
        [ 0.4045],
        [ 0.3199],
        [ 0.0736],
        [-0.1865],
        [ 0.2769],
        [-0.3377],
        [-0.0111],
        [-0.2553],
        [-0.0112],
        [-0.0972],
        [ 0.0520],
        [-0.0039],
        [ 0.1727],
        [-0.3366],
        [ 0.1565],
        [-0.0113],
        [ 0.4203],
        [ 0.3928],
        [ 0.1186],
        [-0.2333],
        [ 0.3142],
        [-0.2649],
        [ 0.2263]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-38.0560, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.4189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.4299, device='cuda:0')



h[100].sum tensor(-2.3522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.4141, device='cuda:0')



h[200].sum tensor(2.5256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(2.5920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2840.2258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0012,  ..., 0.0013, 0.0008, 0.0028],
        [0.0000, 0.0025, 0.0063,  ..., 0.0066, 0.0043, 0.0146],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(14567.9941, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.8897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(437.2379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(34.9779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(58.6224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4.6896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0570],
        [-0.0698],
        [-0.1006],
        ...,
        [-0.0161],
        [-0.0161],
        [-0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-1437.8787, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0570],
        [-0.0698],
        [-0.1006],
        ...,
        [-0.0161],
        [-0.0161],
        [-0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(34.8108, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.9682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(5.9199, device='cuda:0')



h[100].sum tensor(13.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0657, device='cuda:0')



h[200].sum tensor(0.5706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.5660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15031.8994, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0254, 0.0000, 0.0024,  ..., 0.0113, 0.0090, 0.0164],
        [0.0053, 0.0000, 0.0005,  ..., 0.0024, 0.0019, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(83197.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2051.2554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(144.3153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.4290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(407.7209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.6851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.3767],
        [0.2310],
        [0.1414],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(22129.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0570],
        [-0.0698],
        [-0.1006],
        ...,
        [-0.0161],
        [-0.0161],
        [-0.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 2 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0939, -0.0086, -0.0108,  0.0720, -0.0041,  0.0990,  0.1239,  0.0785,
          0.1433,  0.0766,  0.0814, -0.0248,  0.0195,  0.0744,  0.0569,  0.0222,
          0.0168,  0.0277,  0.1259,  0.1243, -0.1106,  0.1048, -0.1156,  0.0426,
         -0.1127, -0.0297,  0.1292, -0.0763,  0.1183, -0.0866,  0.1333, -0.0633,
          0.0369,  0.1288,  0.1215,  0.0613,  0.1099, -0.1323,  0.0674, -0.0146,
          0.1104,  0.1514, -0.0789,  0.1447, -0.1243,  0.0039, -0.1452, -0.0622,
          0.1456, -0.0314,  0.0226, -0.0058,  0.0859,  0.1357,  0.0191, -0.0150,
         -0.1084,  0.0229,  0.0781, -0.0034, -0.0372,  0.1338,  0.1519, -0.1408,
         -0.1328,  0.0892,  0.0981, -0.1078, -0.0073, -0.0798,  0.0154,  0.0515,
         -0.0045,  0.0537,  0.0699, -0.1153,  0.1259, -0.0673, -0.1104, -0.0016,
         -0.0255, -0.0575,  0.0521, -0.0524,  0.1046, -0.0041, -0.1237,  0.0998,
         -0.0990,  0.0083, -0.0221,  0.0272, -0.0702,  0.0118, -0.1172, -0.1277,
          0.0412,  0.0455,  0.0690, -0.0811, -0.0262,  0.0293, -0.0142, -0.1381,
          0.0767,  0.1395,  0.0296,  0.0386,  0.0725, -0.1319,  0.1197, -0.0063,
         -0.0160, -0.0154, -0.1300, -0.0678,  0.1408, -0.1455,  0.0953,  0.0654,
          0.0669, -0.0956,  0.1463,  0.0161,  0.0256,  0.0776, -0.0752, -0.0269,
         -0.1440,  0.1170,  0.0423, -0.1011, -0.0284,  0.0571,  0.0194, -0.1048,
         -0.0912, -0.0209,  0.0521,  0.0150, -0.1026, -0.0214,  0.0230, -0.0937,
          0.0934, -0.1445,  0.0797,  0.1136,  0.1182,  0.0207,  0.0301, -0.1249,
          0.0662,  0.0139,  0.1516,  0.0172,  0.1387,  0.0584,  0.0648,  0.0301,
         -0.0132,  0.1491, -0.0668,  0.0864,  0.1142,  0.1148, -0.0566,  0.0425,
          0.0339, -0.0860,  0.1096, -0.0011,  0.1395,  0.1483, -0.1179,  0.1515,
         -0.0445, -0.0703,  0.1051,  0.1284, -0.0725, -0.1292, -0.0637, -0.1388,
          0.1466, -0.0282, -0.0313, -0.0512, -0.1003, -0.0020,  0.0083,  0.1491,
          0.1025,  0.0058,  0.1527,  0.0593,  0.1482, -0.0978, -0.0668, -0.1142,
         -0.1288,  0.0909, -0.0732,  0.1484, -0.0420,  0.1180,  0.0487,  0.1057,
         -0.1275,  0.1343, -0.0301,  0.0367, -0.0525, -0.1023,  0.0691,  0.1233,
          0.0002, -0.0271, -0.0320,  0.1411, -0.0910, -0.1251,  0.0166, -0.0587,
          0.1112,  0.0888, -0.1216,  0.1043, -0.0807, -0.1207,  0.1146,  0.1446,
          0.0496,  0.1474, -0.1507, -0.1293,  0.0476,  0.0118,  0.0891, -0.0173,
          0.1176,  0.0971, -0.1510,  0.0528, -0.0988, -0.0996,  0.1023,  0.0762,
          0.1427, -0.0521,  0.0072, -0.0646,  0.1389,  0.1293,  0.0788, -0.0741]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0847, -0.0223, -0.1230,  ..., -0.0408,  0.1238, -0.0147],
        [-0.0411,  0.0128,  0.0454,  ..., -0.0688, -0.0687, -0.0239],
        [ 0.0369, -0.0386, -0.0731,  ...,  0.0700,  0.0672,  0.0395],
        ...,
        [-0.0803, -0.0456, -0.0529,  ...,  0.0369, -0.0830, -0.0600],
        [-0.0457,  0.0613, -0.0979,  ...,  0.0198, -0.0480,  0.1124],
        [-0.0241,  0.0568,  0.0506,  ..., -0.0060, -0.0263, -0.0117]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1236, -0.0451, -0.1495,  ...,  0.1432,  0.0354,  0.0784],
        [ 0.0486,  0.1040, -0.1709,  ...,  0.0033,  0.1693,  0.1069],
        [-0.1051, -0.1018,  0.1378,  ...,  0.1720, -0.0486,  0.1124],
        ...,
        [ 0.0388, -0.1099, -0.1666,  ..., -0.1548,  0.1612, -0.0835],
        [-0.0004, -0.1753, -0.0913,  ..., -0.0039, -0.0898, -0.0876],
        [-0.0926,  0.1259,  0.0386,  ...,  0.0700, -0.1113, -0.1658]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1126,  0.1402, -0.0591,  ..., -0.1199, -0.1631,  0.0362],
        [-0.1774, -0.2089,  0.2196,  ..., -0.0295,  0.1337, -0.2486],
        [ 0.2399, -0.1968,  0.2073,  ..., -0.0103, -0.2377,  0.0164],
        ...,
        [-0.1793, -0.1844,  0.1551,  ...,  0.2301,  0.1137,  0.0640],
        [ 0.0209, -0.1349, -0.0456,  ...,  0.1640, -0.1914, -0.1621],
        [-0.0739,  0.1828,  0.0845,  ...,  0.1864, -0.0613,  0.0206]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2201],
        [ 0.3729],
        [ 0.0335],
        [-0.0307],
        [-0.3885],
        [ 0.4153],
        [ 0.0370],
        [ 0.2257],
        [ 0.2611],
        [ 0.1161],
        [ 0.2442],
        [ 0.2577],
        [-0.3079],
        [ 0.1870],
        [-0.3520],
        [ 0.0697],
        [ 0.3448],
        [-0.1566],
        [ 0.1680],
        [-0.0340],
        [ 0.2239],
        [-0.4072],
        [ 0.3903],
        [-0.2679],
        [ 0.1876],
        [ 0.0407],
        [-0.2511],
        [-0.1533],
        [ 0.0339],
        [ 0.2406],
        [-0.3993],
        [ 0.2479]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0939, -0.0086, -0.0108,  0.0720, -0.0041,  0.0990,  0.1239,  0.0785,
          0.1433,  0.0766,  0.0814, -0.0248,  0.0195,  0.0744,  0.0569,  0.0222,
          0.0168,  0.0277,  0.1259,  0.1243, -0.1106,  0.1048, -0.1156,  0.0426,
         -0.1127, -0.0297,  0.1292, -0.0763,  0.1183, -0.0866,  0.1333, -0.0633,
          0.0369,  0.1288,  0.1215,  0.0613,  0.1099, -0.1323,  0.0674, -0.0146,
          0.1104,  0.1514, -0.0789,  0.1447, -0.1243,  0.0039, -0.1452, -0.0622,
          0.1456, -0.0314,  0.0226, -0.0058,  0.0859,  0.1357,  0.0191, -0.0150,
         -0.1084,  0.0229,  0.0781, -0.0034, -0.0372,  0.1338,  0.1519, -0.1408,
         -0.1328,  0.0892,  0.0981, -0.1078, -0.0073, -0.0798,  0.0154,  0.0515,
         -0.0045,  0.0537,  0.0699, -0.1153,  0.1259, -0.0673, -0.1104, -0.0016,
         -0.0255, -0.0575,  0.0521, -0.0524,  0.1046, -0.0041, -0.1237,  0.0998,
         -0.0990,  0.0083, -0.0221,  0.0272, -0.0702,  0.0118, -0.1172, -0.1277,
          0.0412,  0.0455,  0.0690, -0.0811, -0.0262,  0.0293, -0.0142, -0.1381,
          0.0767,  0.1395,  0.0296,  0.0386,  0.0725, -0.1319,  0.1197, -0.0063,
         -0.0160, -0.0154, -0.1300, -0.0678,  0.1408, -0.1455,  0.0953,  0.0654,
          0.0669, -0.0956,  0.1463,  0.0161,  0.0256,  0.0776, -0.0752, -0.0269,
         -0.1440,  0.1170,  0.0423, -0.1011, -0.0284,  0.0571,  0.0194, -0.1048,
         -0.0912, -0.0209,  0.0521,  0.0150, -0.1026, -0.0214,  0.0230, -0.0937,
          0.0934, -0.1445,  0.0797,  0.1136,  0.1182,  0.0207,  0.0301, -0.1249,
          0.0662,  0.0139,  0.1516,  0.0172,  0.1387,  0.0584,  0.0648,  0.0301,
         -0.0132,  0.1491, -0.0668,  0.0864,  0.1142,  0.1148, -0.0566,  0.0425,
          0.0339, -0.0860,  0.1096, -0.0011,  0.1395,  0.1483, -0.1179,  0.1515,
         -0.0445, -0.0703,  0.1051,  0.1284, -0.0725, -0.1292, -0.0637, -0.1388,
          0.1466, -0.0282, -0.0313, -0.0512, -0.1003, -0.0020,  0.0083,  0.1491,
          0.1025,  0.0058,  0.1527,  0.0593,  0.1482, -0.0978, -0.0668, -0.1142,
         -0.1288,  0.0909, -0.0732,  0.1484, -0.0420,  0.1180,  0.0487,  0.1057,
         -0.1275,  0.1343, -0.0301,  0.0367, -0.0525, -0.1023,  0.0691,  0.1233,
          0.0002, -0.0271, -0.0320,  0.1411, -0.0910, -0.1251,  0.0166, -0.0587,
          0.1112,  0.0888, -0.1216,  0.1043, -0.0807, -0.1207,  0.1146,  0.1446,
          0.0496,  0.1474, -0.1507, -0.1293,  0.0476,  0.0118,  0.0891, -0.0173,
          0.1176,  0.0971, -0.1510,  0.0528, -0.0988, -0.0996,  0.1023,  0.0762,
          0.1427, -0.0521,  0.0072, -0.0646,  0.1389,  0.1293,  0.0788, -0.0741]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0847, -0.0223, -0.1230,  ..., -0.0408,  0.1238, -0.0147],
        [-0.0411,  0.0128,  0.0454,  ..., -0.0688, -0.0687, -0.0239],
        [ 0.0369, -0.0386, -0.0731,  ...,  0.0700,  0.0672,  0.0395],
        ...,
        [-0.0803, -0.0456, -0.0529,  ...,  0.0369, -0.0830, -0.0600],
        [-0.0457,  0.0613, -0.0979,  ...,  0.0198, -0.0480,  0.1124],
        [-0.0241,  0.0568,  0.0506,  ..., -0.0060, -0.0263, -0.0117]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1236, -0.0451, -0.1495,  ...,  0.1432,  0.0354,  0.0784],
        [ 0.0486,  0.1040, -0.1709,  ...,  0.0033,  0.1693,  0.1069],
        [-0.1051, -0.1018,  0.1378,  ...,  0.1720, -0.0486,  0.1124],
        ...,
        [ 0.0388, -0.1099, -0.1666,  ..., -0.1548,  0.1612, -0.0835],
        [-0.0004, -0.1753, -0.0913,  ..., -0.0039, -0.0898, -0.0876],
        [-0.0926,  0.1259,  0.0386,  ...,  0.0700, -0.1113, -0.1658]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1126,  0.1402, -0.0591,  ..., -0.1199, -0.1631,  0.0362],
        [-0.1774, -0.2089,  0.2196,  ..., -0.0295,  0.1337, -0.2486],
        [ 0.2399, -0.1968,  0.2073,  ..., -0.0103, -0.2377,  0.0164],
        ...,
        [-0.1793, -0.1844,  0.1551,  ...,  0.2301,  0.1137,  0.0640],
        [ 0.0209, -0.1349, -0.0456,  ...,  0.1640, -0.1914, -0.1621],
        [-0.0739,  0.1828,  0.0845,  ...,  0.1864, -0.0613,  0.0206]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2201],
        [ 0.3729],
        [ 0.0335],
        [-0.0307],
        [-0.3885],
        [ 0.4153],
        [ 0.0370],
        [ 0.2257],
        [ 0.2611],
        [ 0.1161],
        [ 0.2442],
        [ 0.2577],
        [-0.3079],
        [ 0.1870],
        [-0.3520],
        [ 0.0697],
        [ 0.3448],
        [-0.1566],
        [ 0.1680],
        [-0.0340],
        [ 0.2239],
        [-0.4072],
        [ 0.3903],
        [-0.2679],
        [ 0.1876],
        [ 0.0407],
        [-0.2511],
        [-0.1533],
        [ 0.0339],
        [ 0.2406],
        [-0.3993],
        [ 0.2479]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138, -0.0013, -0.0016,  ...,  0.0190,  0.0116, -0.0109],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1077.6891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3153, device='cuda:0')



h[100].sum tensor(-7.6717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-37.6793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0520, 0.0000, 0.0000,  ..., 0.0717, 0.0437, 0.0000],
        [0.0428, 0.0000, 0.0000,  ..., 0.0590, 0.0359, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0138, 0.0084, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36331.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.1624, 0.0000,  ..., 0.4671, 0.0000, 0.0911],
        [0.0135, 0.1391, 0.0000,  ..., 0.4002, 0.0000, 0.0781],
        [0.0109, 0.1117, 0.0000,  ..., 0.3214, 0.0000, 0.0627],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172652.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(219.8650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(145.5870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-72.5843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(89.5442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[9.4045e-01],
        [1.0176e+00],
        [1.1229e+00],
        ...,
        [1.1298e-06],
        [1.4844e-07],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(17527.3145, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(79.8856, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365922.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        ...,
        [ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  0.0000e+00,  0.0000e+00,  ...,  1.0000e-04,
         -1.0000e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1051.8391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2100, device='cuda:0')



h[100].sum tensor(-7.0495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-34.7280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37136.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0024, 0.0000,  ..., 0.0022, 0.0000, 0.0002],
        [0.0000, 0.0024, 0.0000,  ..., 0.0022, 0.0000, 0.0002],
        [0.0000, 0.0025, 0.0000,  ..., 0.0026, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0021, 0.0000, 0.0002],
        [0.0000, 0.0024, 0.0000,  ..., 0.0021, 0.0000, 0.0002],
        [0.0000, 0.0024, 0.0000,  ..., 0.0021, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184928.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(120.5798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.6058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31.5115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-76.9547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.2477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0205],
        [-0.0207],
        [-0.0200],
        ...,
        [-0.0081],
        [-0.0080],
        [-0.0081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3177.8755, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365922.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365922.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00],
        [ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00],
        [ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00],
        ...,
        [ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00],
        [ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00],
        [ 5.9014e-05,  0.0000e+00,  0.0000e+00,  ...,  1.9173e-04,
         -1.9546e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(761.5331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8598, device='cuda:0')



h[100].sum tensor(-4.9925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-24.6693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0466, 0.0000, 0.0000,  ..., 0.0647, 0.0382, 0.0000],
        [0.0310, 0.0000, 0.0000,  ..., 0.0432, 0.0253, 0.0000],
        [0.0257, 0.0000, 0.0000,  ..., 0.0359, 0.0209, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30108.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.1451, 0.0000,  ..., 0.3880, 0.0000, 0.0828],
        [0.0046, 0.1172, 0.0000,  ..., 0.3118, 0.0000, 0.0666],
        [0.0034, 0.0899, 0.0000,  ..., 0.2375, 0.0000, 0.0508],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0035, 0.0000, 0.0011],
        [0.0000, 0.0039, 0.0000,  ..., 0.0035, 0.0000, 0.0011],
        [0.0000, 0.0039, 0.0000,  ..., 0.0035, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161169.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(79.7665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(158.3422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11.7219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-65.6340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.3566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0118],
        [ 0.0035],
        [-0.0093],
        ...,
        [-0.0183],
        [-0.0182],
        [-0.0182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-15525.0635, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365922.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365927.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.1128e-03, -4.4982e-04, -5.7277e-04,  ...,  7.3309e-03,
          4.0100e-03, -4.0183e-03],
        [ 1.2411e-02, -1.0905e-03, -1.3885e-03,  ...,  1.7382e-02,
          1.0102e-02, -9.7411e-03],
        [ 9.0783e-03, -7.9790e-04, -1.0160e-03,  ...,  1.2792e-02,
          7.3198e-03, -7.1276e-03],
        ...,
        [-1.1978e-05,  0.0000e+00,  0.0000e+00,  ...,  2.7375e-04,
         -2.6730e-04,  0.0000e+00],
        [-1.1978e-05,  0.0000e+00,  0.0000e+00,  ...,  2.7375e-04,
         -2.6730e-04,  0.0000e+00],
        [-1.1978e-05,  0.0000e+00,  0.0000e+00,  ...,  2.7375e-04,
         -2.6730e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(819.9954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9453, device='cuda:0')



h[100].sum tensor(-5.4608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-27.0654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0443, 0.0000, 0.0000,  ..., 0.0622, 0.0360, 0.0000],
        [0.0286, 0.0000, 0.0000,  ..., 0.0405, 0.0228, 0.0000],
        [0.0240, 0.0000, 0.0000,  ..., 0.0342, 0.0193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31517.2910, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0993, 0.0000,  ..., 0.2654, 0.0000, 0.0574],
        [0.0016, 0.0890, 0.0000,  ..., 0.2375, 0.0000, 0.0513],
        [0.0013, 0.0769, 0.0000,  ..., 0.2049, 0.0000, 0.0443],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0052, 0.0000, 0.0012],
        [0.0000, 0.0031, 0.0000,  ..., 0.0052, 0.0000, 0.0012],
        [0.0000, 0.0031, 0.0000,  ..., 0.0052, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167300.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(94.1857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(179.1752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-70.4577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.5527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0273],
        [-0.0281],
        [-0.0293],
        ...,
        [-0.0270],
        [-0.0269],
        [-0.0269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17291.5254, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365927.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365932.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00],
        [-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00],
        [-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00],
        ...,
        [-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00],
        [-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00],
        [-9.4021e-05,  0.0000e+00,  0.0000e+00,  ...,  3.3488e-04,
         -3.2153e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(756.8618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.1911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8802, device='cuda:0')



h[100].sum tensor(-5.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-25.0920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29539.5723, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0069, 0.0000, 0.0016],
        [0.0000, 0.0036, 0.0000,  ..., 0.0097, 0.0000, 0.0022],
        [0.0000, 0.0078, 0.0000,  ..., 0.0215, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0068, 0.0000, 0.0016],
        [0.0000, 0.0025, 0.0000,  ..., 0.0068, 0.0000, 0.0016],
        [0.0000, 0.0025, 0.0000,  ..., 0.0068, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160572.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(84.5610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.3221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43.0258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-70.4289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.3541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0207],
        [-0.0199],
        [-0.0174],
        ...,
        [-0.0273],
        [-0.0288],
        [-0.0291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14426.7695, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365932.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365939.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0007, -0.0010,  ...,  0.0124,  0.0069, -0.0068],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        ...,
        [-0.0002,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1089.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2613, device='cuda:0')



h[100].sum tensor(-7.3078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-36.4421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0322, 0.0000, 0.0000,  ..., 0.0464, 0.0264, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0136, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43423.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9649e-05, 8.9452e-02, 0.0000e+00,  ..., 2.4568e-01, 0.0000e+00,
         5.4029e-02],
        [0.0000e+00, 4.1231e-02, 0.0000e+00,  ..., 1.1472e-01, 0.0000e+00,
         2.5161e-02],
        [0.0000e+00, 1.2963e-02, 0.0000e+00,  ..., 3.7566e-02, 0.0000e+00,
         8.2278e-03],
        ...,
        [0.0000e+00, 2.1287e-03, 0.0000e+00,  ..., 7.9381e-03, 0.0000e+00,
         1.7355e-03],
        [0.0000e+00, 2.1287e-03, 0.0000e+00,  ..., 7.9383e-03, 0.0000e+00,
         1.7355e-03],
        [0.0000e+00, 2.1287e-03, 0.0000e+00,  ..., 7.9383e-03, 0.0000e+00,
         1.7355e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245849.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12.3712, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.7972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27.2569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-103.6013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.9586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0112],
        [ 0.0033],
        [-0.0074],
        ...,
        [-0.0309],
        [-0.0308],
        [-0.0308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9780.1367, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365939.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365950.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0004, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(983.7133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1131, device='cuda:0')



h[100].sum tensor(-6.4776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-32.4020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39504.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0164, 0.0000,  ..., 0.0498, 0.0000, 0.0100],
        [0.0000, 0.0048, 0.0000,  ..., 0.0173, 0.0000, 0.0029],
        [0.0000, 0.0018, 0.0000,  ..., 0.0087, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0086, 0.0000, 0.0010],
        [0.0000, 0.0017, 0.0000,  ..., 0.0086, 0.0000, 0.0010],
        [0.0000, 0.0017, 0.0000,  ..., 0.0086, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224647.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0.2257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.0291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53.4504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-101.7637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.0929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0204],
        [ 0.0054],
        [-0.0069],
        ...,
        [-0.0323],
        [-0.0322],
        [-0.0321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9535.4902, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365950.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365963.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0157, -0.0013, -0.0017,  ...,  0.0226,  0.0129, -0.0125],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0004,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0004,  0.0000],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0004,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0004,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(793.3771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.6230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8837, device='cuda:0')



h[100].sum tensor(-5.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-25.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0281, 0.0000, 0.0000,  ..., 0.0417, 0.0231, 0.0000],
        [0.0157, 0.0000, 0.0000,  ..., 0.0241, 0.0129, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33019.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2443e-01, 0.0000e+00,  ..., 3.4733e-01, 0.0000e+00,
         7.5714e-02],
        [0.0000e+00, 6.2762e-02, 0.0000e+00,  ..., 1.7829e-01, 0.0000e+00,
         3.7883e-02],
        [0.0000e+00, 2.1193e-02, 0.0000e+00,  ..., 6.5078e-02, 0.0000e+00,
         1.2464e-02],
        ...,
        [0.0000e+00, 1.3132e-03, 0.0000e+00,  ..., 9.2758e-03, 0.0000e+00,
         1.2913e-04],
        [0.0000e+00, 1.3133e-03, 0.0000e+00,  ..., 9.2760e-03, 0.0000e+00,
         1.2913e-04],
        [0.0000e+00, 1.3133e-03, 0.0000e+00,  ..., 9.2760e-03, 0.0000e+00,
         1.2913e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201245.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.9537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65.6617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-94.5194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.3716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0601],
        [ 0.0551],
        [ 0.0554],
        ...,
        [-0.0378],
        [-0.0376],
        [-0.0376]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-7658.6274, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365963.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365978.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0005, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(671.0255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.3650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7293, device='cuda:0')



h[100].sum tensor(-4.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-20.6483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0219, 0.0000, 0.0000,  ..., 0.0336, 0.0180, 0.0000],
        [0.0109, 0.0000, 0.0000,  ..., 0.0179, 0.0090, 0.0000],
        [0.0193, 0.0000, 0.0000,  ..., 0.0295, 0.0160, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29050.3535, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0559, 0.0000,  ..., 0.1654, 0.0000, 0.0338],
        [0.0000, 0.0507, 0.0000,  ..., 0.1499, 0.0000, 0.0304],
        [0.0000, 0.0621, 0.0000,  ..., 0.1801, 0.0000, 0.0373],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0097, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188028.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.6955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82.5251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-90.2457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2.6356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0566],
        [ 0.0610],
        [ 0.0661],
        ...,
        [-0.0482],
        [-0.0480],
        [-0.0480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14673.7363, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365978.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9999],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365993.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0006, -0.0005,  0.0000],
        [ 0.0059, -0.0005, -0.0007,  ...,  0.0094,  0.0048, -0.0050],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0006, -0.0005,  0.0000],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0006, -0.0005,  0.0000],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0006, -0.0005,  0.0000],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0006, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(624.0789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6547, device='cuda:0')



h[100].sum tensor(-3.7045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0068, device='cuda:0')



h[200].sum tensor(-18.7041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0000,  ..., 0.0112, 0.0048, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0149, 0.0071, 0.0000],
        [0.0288, 0.0000, 0.0000,  ..., 0.0450, 0.0238, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28436.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.0750, 0.0000, 0.0130],
        [0.0000, 0.0332, 0.0000,  ..., 0.1066, 0.0000, 0.0200],
        [0.0000, 0.0510, 0.0000,  ..., 0.1596, 0.0000, 0.0317],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0097, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193387.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.7289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(69.5028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-93.6200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-8.1715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0688],
        [ 0.0864],
        [ 0.1014],
        ...,
        [-0.0630],
        [-0.0627],
        [-0.0626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20358.0352, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9999],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365993.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0000],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366010.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000],
        [ 0.0205, -0.0017, -0.0022,  ...,  0.0298,  0.0171, -0.0164],
        [ 0.0208, -0.0017, -0.0022,  ...,  0.0302,  0.0173, -0.0166],
        ...,
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(876.9070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.2604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9434, device='cuda:0')



h[100].sum tensor(-5.3318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-27.0048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1308, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0000, 0.0000,  ..., 0.0410, 0.0221, 0.0000],
        [0.0369, 0.0000, 0.0000,  ..., 0.0560, 0.0307, 0.0000],
        [0.0785, 0.0000, 0.0000,  ..., 0.1143, 0.0653, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36355.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0832, 0.0000,  ..., 0.2458, 0.0000, 0.0515],
        [0.0000, 0.1310, 0.0000,  ..., 0.3789, 0.0000, 0.0816],
        [0.0000, 0.1995, 0.0000,  ..., 0.5689, 0.0000, 0.1247],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232227.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.5735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52.3541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-114.1669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1.5044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1616],
        [ 0.1762],
        [ 0.1922],
        ...,
        [-0.0752],
        [-0.0749],
        [-0.0747]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27060.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0000],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366010.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(575.3925, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0000],
        [0.9996],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366027.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0068, -0.0006, -0.0008,  ...,  0.0110,  0.0057, -0.0058],
        [ 0.0084, -0.0007, -0.0009,  ...,  0.0132,  0.0070, -0.0070],
        [ 0.0142, -0.0012, -0.0015,  ...,  0.0212,  0.0119, -0.0115],
        ...,
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0007, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.9404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.1884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3259, device='cuda:0')



h[100].sum tensor(-7.4996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-38.1039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0363, 0.0000, 0.0000,  ..., 0.0566, 0.0304, 0.0000],
        [0.0393, 0.0000, 0.0000,  ..., 0.0607, 0.0329, 0.0000],
        [0.0264, 0.0000, 0.0000,  ..., 0.0429, 0.0221, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46222.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0697, 0.0000,  ..., 0.2142, 0.0000, 0.0440],
        [0.0000, 0.0898, 0.0000,  ..., 0.2720, 0.0000, 0.0570],
        [0.0000, 0.1104, 0.0000,  ..., 0.3312, 0.0000, 0.0703],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(274625.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29.0399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-138.4447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.3084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1204],
        [ 0.1456],
        [ 0.1709],
        ...,
        [-0.0856],
        [-0.0852],
        [-0.0850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32300.3770, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0000],
        [0.9996],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366027.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0001],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366042.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        ...,
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1663.4700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.4905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8469, device='cuda:0')



h[100].sum tensor(-10.2894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0193, device='cuda:0')



h[200].sum tensor(-52.4434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0173, 0.0081, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59829.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0143, 0.0000, 0.0006],
        [0.0000, 0.0077, 0.0000,  ..., 0.0320, 0.0000, 0.0040],
        [0.0000, 0.0245, 0.0000,  ..., 0.0823, 0.0000, 0.0143],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344802.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.7427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9.9660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-169.8588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.3602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0744],
        [-0.0242],
        [ 0.0319],
        ...,
        [-0.0944],
        [-0.0940],
        [-0.0939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32307.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0001],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366042.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366054.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0204, -0.0016, -0.0022,  ...,  0.0301,  0.0171, -0.0164],
        [ 0.0301, -0.0024, -0.0031,  ...,  0.0434,  0.0252, -0.0239],
        [ 0.0213, -0.0017, -0.0022,  ...,  0.0313,  0.0178, -0.0171],
        ...,
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1258.1312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.0934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3509, device='cuda:0')



h[100].sum tensor(-7.4874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-38.2828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0920, 0.0000, 0.0000,  ..., 0.1346, 0.0771, 0.0000],
        [0.1080, 0.0000, 0.0000,  ..., 0.1567, 0.0904, 0.0000],
        [0.0890, 0.0000, 0.0000,  ..., 0.1303, 0.0745, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48838.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.1492e-01, 0.0000e+00,  ..., 6.1204e-01, 0.0000e+00,
         1.3593e-01],
        [0.0000e+00, 2.6220e-01, 0.0000e+00,  ..., 7.4303e-01, 0.0000e+00,
         1.6604e-01],
        [0.0000e+00, 2.5233e-01, 0.0000e+00,  ..., 7.1753e-01, 0.0000e+00,
         1.6005e-01],
        ...,
        [0.0000e+00, 2.7662e-04, 0.0000e+00,  ..., 9.3128e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.7664e-04, 0.0000e+00,  ..., 9.3127e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.7663e-04, 0.0000e+00,  ..., 9.3128e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300102.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.9203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-149.4465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6.1510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1305],
        [ 0.1560],
        [ 0.1749],
        ...,
        [-0.1034],
        [-0.1029],
        [-0.1027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42662.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366054.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0002],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366065.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0008, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1237.8303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.3039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2812, device='cuda:0')



h[100].sum tensor(-7.1912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-36.8854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48957.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0090, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310667.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.1818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-151.1155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(0.5365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1672],
        [-0.1602],
        [-0.1379],
        ...,
        [-0.0986],
        [-0.1089],
        [-0.1094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45825.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0002],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366065.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366076.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0007, -0.0009,  ...,  0.0138,  0.0073, -0.0072],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [ 0.0151, -0.0012, -0.0016,  ...,  0.0228,  0.0127, -0.0122],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1148.3176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1625, device='cuda:0')



h[100].sum tensor(-6.4239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-33.0553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0140, 0.0059, 0.0000],
        [0.0373, 0.0000, 0.0000,  ..., 0.0594, 0.0317, 0.0000],
        [0.0202, 0.0000, 0.0000,  ..., 0.0347, 0.0174, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46522.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0313, 0.0000,  ..., 0.1018, 0.0000, 0.0189],
        [0.0000, 0.0639, 0.0000,  ..., 0.1988, 0.0000, 0.0408],
        [0.0000, 0.0643, 0.0000,  ..., 0.2032, 0.0000, 0.0414],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301132.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.1994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-146.7956, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-12.2361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0079],
        [ 0.0335],
        [ 0.0583],
        ...,
        [-0.1149],
        [-0.1144],
        [-0.1142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43918.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366076.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366076.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0075, -0.0006, -0.0008,  ...,  0.0123,  0.0064, -0.0064],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [ 0.0075, -0.0006, -0.0008,  ...,  0.0123,  0.0064, -0.0064],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(918.0892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8963, device='cuda:0')



h[100].sum tensor(-4.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-25.4502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0128, 0.0051, 0.0000],
        [0.0269, 0.0000, 0.0000,  ..., 0.0450, 0.0231, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0128, 0.0051, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37814.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0208, 0.0000,  ..., 0.0714, 0.0000, 0.0120],
        [0.0000, 0.0357, 0.0000,  ..., 0.1164, 0.0000, 0.0221],
        [0.0000, 0.0208, 0.0000,  ..., 0.0714, 0.0000, 0.0119],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0088, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(256392.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.4783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-129.3511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-28.6946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1222],
        [-0.1101],
        [-0.1131],
        ...,
        [-0.1149],
        [-0.1144],
        [-0.1142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52901.8984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366076.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366087.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0171, -0.0013, -0.0018,  ...,  0.0256,  0.0144, -0.0138],
        [ 0.0157, -0.0012, -0.0016,  ...,  0.0238,  0.0133, -0.0128],
        [ 0.0247, -0.0019, -0.0025,  ...,  0.0361,  0.0207, -0.0196],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1099.1681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0771, device='cuda:0')



h[100].sum tensor(-5.8976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-30.4443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0681, 0.0000, 0.0000,  ..., 0.1022, 0.0575, 0.0000],
        [0.0893, 0.0000, 0.0000,  ..., 0.1315, 0.0751, 0.0000],
        [0.1083, 0.0000, 0.0000,  ..., 0.1577, 0.0909, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41860.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0167e-01, 0.0000e+00,  ..., 5.6990e-01, 0.0000e+00,
         1.2942e-01],
        [0.0000e+00, 2.4374e-01, 0.0000e+00,  ..., 6.8151e-01, 0.0000e+00,
         1.5624e-01],
        [0.0000e+00, 2.9151e-01, 0.0000e+00,  ..., 8.0815e-01, 0.0000e+00,
         1.8667e-01],
        ...,
        [0.0000e+00, 1.6631e-04, 0.0000e+00,  ..., 9.0616e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6632e-04, 0.0000e+00,  ..., 9.0615e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6630e-04, 0.0000e+00,  ..., 9.0615e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(272131.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.3259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-138.5084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.7299, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0133],
        [-0.0213],
        [-0.0284],
        ...,
        [-0.1181],
        [-0.1175],
        [-0.1174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50343.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9996],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366087.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0004],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366099., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0005, -0.0006,  ...,  0.0096,  0.0048, -0.0049],
        [ 0.0050, -0.0004, -0.0006,  ...,  0.0091,  0.0045, -0.0045],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1667.9150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.8022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6820, device='cuda:0')



h[100].sum tensor(-9.2651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0175, device='cuda:0')



h[200].sum tensor(-47.9817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0454, 0.0000, 0.0000,  ..., 0.0711, 0.0388, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0191, 0.0084, 0.0000],
        [0.0216, 0.0000, 0.0000,  ..., 0.0370, 0.0187, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62545.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1021e-01, 0.0000e+00,  ..., 3.2503e-01, 0.0000e+00,
         7.0839e-02],
        [0.0000e+00, 6.2909e-02, 0.0000e+00,  ..., 1.9607e-01, 0.0000e+00,
         4.0119e-02],
        [0.0000e+00, 6.9124e-02, 0.0000e+00,  ..., 2.1471e-01, 0.0000e+00,
         4.4362e-02],
        ...,
        [0.0000e+00, 5.6606e-05, 0.0000e+00,  ..., 9.9767e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.6612e-05, 0.0000e+00,  ..., 9.9766e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.6595e-05, 0.0000e+00,  ..., 9.9766e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385456.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.8824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-179.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.4398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0301],
        [ 0.0452],
        [ 0.0549],
        ...,
        [-0.1086],
        [-0.1079],
        [-0.1101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36524.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0004],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366099., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366110.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0009, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1334.0084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2663, device='cuda:0')



h[100].sum tensor(-6.9474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-36.0950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49431.7852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321375.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.8730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-154.6349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-4.2476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1576],
        [-0.1598],
        [-0.1556],
        ...,
        [-0.1159],
        [-0.1132],
        [-0.1055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26608.3535, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9997],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366110.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0005],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366122.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070, -0.0006, -0.0008,  ...,  0.0119,  0.0062, -0.0060],
        [ 0.0073, -0.0006, -0.0008,  ...,  0.0124,  0.0065, -0.0063],
        [ 0.0070, -0.0006, -0.0008,  ...,  0.0119,  0.0062, -0.0060],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1540.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.2053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4944, device='cuda:0')



h[100].sum tensor(-8.1059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-42.2504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0328, 0.0000, 0.0000,  ..., 0.0542, 0.0288, 0.0000],
        [0.0318, 0.0000, 0.0000,  ..., 0.0528, 0.0279, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0242, 0.0114, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57364.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0586, 0.0000,  ..., 0.1887, 0.0000, 0.0380],
        [0.0000, 0.0590, 0.0000,  ..., 0.1907, 0.0000, 0.0383],
        [0.0000, 0.0448, 0.0000,  ..., 0.1501, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363920.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.1343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.5318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.1865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0525],
        [ 0.0611],
        [ 0.0669],
        ...,
        [-0.1164],
        [-0.1152],
        [-0.1157]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31398.0332, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0005],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366122.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0005],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366122.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0004, -0.0006,  ...,  0.0092,  0.0046, -0.0046],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1138.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0389, device='cuda:0')



h[100].sum tensor(-5.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-29.1674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0212, 0.0000, 0.0000,  ..., 0.0356, 0.0183, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0121, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42629.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0740, 0.0000,  ..., 0.2239, 0.0000, 0.0476],
        [0.0000, 0.0364, 0.0000,  ..., 0.1180, 0.0000, 0.0226],
        [0.0000, 0.0115, 0.0000,  ..., 0.0449, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(286340.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.2030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-143.5900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.7617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0099],
        [ 0.0013],
        [-0.0192],
        ...,
        [-0.1205],
        [-0.1200],
        [-0.1198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36276.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0005],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366122.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366134.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1024.0726, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8869, device='cuda:0')



h[100].sum tensor(-4.7923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-25.0605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0238, 0.0116, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38090.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0208, 0.0000, 0.0010],
        [0.0000, 0.0111, 0.0000,  ..., 0.0437, 0.0000, 0.0065],
        [0.0000, 0.0364, 0.0000,  ..., 0.1141, 0.0000, 0.0227],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(262494.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.9957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-137.1052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-29.6573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0039],
        [-0.0131],
        [-0.0232],
        ...,
        [-0.1259],
        [-0.1254],
        [-0.1252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46324.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366134.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366147.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1389.7889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2890, device='cuda:0')



h[100].sum tensor(-6.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-36.6433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51915.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0169, 0.0000,  ..., 0.0614, 0.0000, 0.0107],
        [0.0000, 0.0041, 0.0000,  ..., 0.0240, 0.0000, 0.0023],
        [0.0000, 0.0004, 0.0000,  ..., 0.0133, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342033.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.3381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-4.8593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0404],
        [-0.0424],
        [-0.0301],
        ...,
        [-0.1305],
        [-0.1300],
        [-0.1298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-34768.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9995],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366147.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366160.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0004, -0.0005,  ...,  0.0081,  0.0040, -0.0039],
        [ 0.0055, -0.0004, -0.0006,  ...,  0.0101,  0.0051, -0.0050],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        ...,
        [ 0.0074, -0.0006, -0.0008,  ...,  0.0127,  0.0067, -0.0064],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1264.9348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1576, device='cuda:0')



h[100].sum tensor(-6.2023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-32.6457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0445, 0.0000, 0.0000,  ..., 0.0713, 0.0391, 0.0000],
        [0.0175, 0.0000, 0.0000,  ..., 0.0311, 0.0156, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0131, 0.0051, 0.0000],
        ...,
        [0.0580, 0.0000, 0.0000,  ..., 0.0885, 0.0497, 0.0000],
        [0.0510, 0.0000, 0.0000,  ..., 0.0788, 0.0440, 0.0000],
        [0.0368, 0.0000, 0.0000,  ..., 0.0577, 0.0316, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45956.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0967, 0.0000,  ..., 0.2912, 0.0000, 0.0645],
        [0.0000, 0.0631, 0.0000,  ..., 0.1967, 0.0000, 0.0420],
        [0.0000, 0.0307, 0.0000,  ..., 0.1041, 0.0000, 0.0202],
        ...,
        [0.0000, 0.2440, 0.0000,  ..., 0.6710, 0.0000, 0.1602],
        [0.0000, 0.1941, 0.0000,  ..., 0.5399, 0.0000, 0.1276],
        [0.0000, 0.1335, 0.0000,  ..., 0.3780, 0.0000, 0.0878]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310083.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37.8333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.8948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-160.1536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-22.8173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0160],
        [ 0.0112],
        [-0.0018],
        ...,
        [-0.2455],
        [-0.1863],
        [-0.1306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52775.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366160.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366173.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [ 0.0089, -0.0007, -0.0009,  ...,  0.0148,  0.0080, -0.0076],
        [ 0.0089, -0.0007, -0.0009,  ...,  0.0148,  0.0080, -0.0076],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0010, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1122.2698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9969, device='cuda:0')



h[100].sum tensor(-5.3325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-28.1598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0000,  ..., 0.0292, 0.0145, 0.0000],
        [0.0230, 0.0000, 0.0000,  ..., 0.0404, 0.0209, 0.0000],
        [0.0349, 0.0000, 0.0000,  ..., 0.0568, 0.0307, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42596.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0758, 0.0000,  ..., 0.2288, 0.0000, 0.0507],
        [0.0000, 0.1109, 0.0000,  ..., 0.3240, 0.0000, 0.0740],
        [0.0000, 0.1554, 0.0000,  ..., 0.4413, 0.0000, 0.1032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297858.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(61.1087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(305.7163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-158.1382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-34.3102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1940],
        [-0.2490],
        [-0.3024],
        ...,
        [-0.1520],
        [-0.1513],
        [-0.1511]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66081.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366173.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [ 0.0051, -0.0004, -0.0006,  ...,  0.0096,  0.0049, -0.0047],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1154.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0469, device='cuda:0')



h[100].sum tensor(-5.5128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-29.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0128, 0.0049, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0239, 0.0113, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45721.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0355, 0.0000, 0.0045],
        [0.0000, 0.0259, 0.0000,  ..., 0.0942, 0.0000, 0.0179],
        [0.0000, 0.0534, 0.0000,  ..., 0.1735, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324662.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(62.5600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.3316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.5929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-32.9310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0115],
        [-0.0073],
        [-0.0111],
        ...,
        [-0.1581],
        [-0.1574],
        [-0.1572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56027.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0163, -0.0012, -0.0016,  ...,  0.0250,  0.0142, -0.0132],
        [ 0.0169, -0.0012, -0.0016,  ...,  0.0259,  0.0147, -0.0137],
        [ 0.0141, -0.0010, -0.0014,  ...,  0.0220,  0.0123, -0.0116],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0011, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.1577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8918, device='cuda:0')



h[100].sum tensor(-4.7506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-25.1696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0611, 0.0000, 0.0000,  ..., 0.0947, 0.0533, 0.0000],
        [0.0661, 0.0000, 0.0000,  ..., 0.1016, 0.0575, 0.0000],
        [0.0825, 0.0000, 0.0000,  ..., 0.1243, 0.0711, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39807.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1584, 0.0000,  ..., 0.4534, 0.0000, 0.1061],
        [0.0000, 0.1732, 0.0000,  ..., 0.4936, 0.0000, 0.1161],
        [0.0000, 0.1893, 0.0000,  ..., 0.5361, 0.0000, 0.1267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(290288.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(72.4608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.9401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-156.0396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-44.6407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0276],
        [-0.0306],
        [-0.0284],
        ...,
        [-0.1583],
        [-0.1577],
        [-0.1574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64533.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366200.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [ 0.0055, -0.0004, -0.0006,  ...,  0.0103,  0.0053, -0.0051],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        ...,
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1150.8528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0290, device='cuda:0')



h[100].sum tensor(-5.4612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-29.0299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0244, 0.0000, 0.0000,  ..., 0.0444, 0.0231, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0118, 0.0043, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0135, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44365.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0575, 0.0000,  ..., 0.1882, 0.0000, 0.0399],
        [0.0000, 0.0337, 0.0000,  ..., 0.1174, 0.0000, 0.0235],
        [0.0000, 0.0186, 0.0000,  ..., 0.0730, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(317572.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(89.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.6626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.7365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.8702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0344],
        [ 0.0338],
        [ 0.0301],
        ...,
        [-0.1635],
        [-0.1628],
        [-0.1626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62735.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366200.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366214.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        ...,
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1327.8303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2201, device='cuda:0')



h[100].sum tensor(-6.4476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-34.3869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0205, 0.0093, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0159, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49982.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0197, 0.0000, 0.0016],
        [0.0000, 0.0169, 0.0000,  ..., 0.0637, 0.0000, 0.0118],
        [0.0000, 0.0634, 0.0000,  ..., 0.1961, 0.0000, 0.0435],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0139, 0.0000, 0.0005],
        [0.0000, 0.0127, 0.0000,  ..., 0.0491, 0.0000, 0.0087],
        [0.0000, 0.0378, 0.0000,  ..., 0.1243, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349958.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(89.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(337.3667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-180.6411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.8616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0894],
        [-0.1218],
        [-0.1882],
        ...,
        [-0.1046],
        [-0.0608],
        [-0.0250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50998.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9996],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366214.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9997],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366227.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        ...,
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0012,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.5760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8304, device='cuda:0')



h[100].sum tensor(-4.3834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-23.4558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38788.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0141, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296379.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(124.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.7932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-160.0590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-50.8428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0898],
        [-0.1464],
        [-0.1900],
        ...,
        [-0.1677],
        [-0.1670],
        [-0.1668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64527.0117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9997],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366227.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(937.9753, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9997],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366239.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1182.4160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0444, device='cuda:0')



h[100].sum tensor(-5.4603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-29.3154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45429.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0146, 0.0000, 0.0001],
        [0.0000, 0.0001, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334163.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(108.8412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.7108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-174.6278, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-41.1734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0296],
        [-0.0650],
        [-0.1036],
        ...,
        [-0.1717],
        [-0.1709],
        [-0.1706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55270.0742, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0006],
        [0.9997],
        ...,
        [0.9997],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366239.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9997],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366252.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [ 0.0060, -0.0005, -0.0006,  ...,  0.0112,  0.0059, -0.0055],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1312.5726, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1613, device='cuda:0')



h[100].sum tensor(-6.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-33.4130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0223, 0.0104, 0.0000],
        [0.0284, 0.0000, 0.0000,  ..., 0.0491, 0.0264, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50753.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9866e-02, 0.0000e+00,  ..., 7.4446e-02, 0.0000e+00,
         1.3518e-02],
        [0.0000e+00, 5.5059e-02, 0.0000e+00,  ..., 1.7611e-01, 0.0000e+00,
         3.7707e-02],
        [0.0000e+00, 1.0775e-01, 0.0000e+00,  ..., 3.2052e-01, 0.0000e+00,
         7.3530e-02],
        ...,
        [0.0000e+00, 1.3928e-04, 0.0000e+00,  ..., 1.1870e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3929e-04, 0.0000e+00,  ..., 1.1870e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3924e-04, 0.0000e+00,  ..., 1.1870e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367239.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(106.4573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.9464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.3276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-31.9380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0251],
        [ 0.0370],
        [ 0.0315],
        ...,
        [-0.1662],
        [-0.1764],
        [-0.1775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61199.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9997],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366252.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366265.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(992.9861, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8260, device='cuda:0')



h[100].sum tensor(-4.3325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-23.4164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1145, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39730.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0119, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312621.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(111.4175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.5324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.0674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-62.4982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2568],
        [-0.2711],
        [-0.2820],
        ...,
        [-0.1870],
        [-0.1862],
        [-0.1859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67060.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366265.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366277.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0256, -0.0016, -0.0023,  ...,  0.0384,  0.0223, -0.0203],
        [ 0.0274, -0.0017, -0.0024,  ...,  0.0409,  0.0238, -0.0217],
        [ 0.0216, -0.0014, -0.0019,  ...,  0.0329,  0.0189, -0.0173],
        ...,
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1584.6523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4676, device='cuda:0')



h[100].sum tensor(-7.7320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-41.9307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1071, 0.0000, 0.0000,  ..., 0.1598, 0.0928, 0.0000],
        [0.1105, 0.0000, 0.0000,  ..., 0.1646, 0.0957, 0.0000],
        [0.1215, 0.0000, 0.0000,  ..., 0.1798, 0.1048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61152.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0749e-01, 0.0000e+00,  ..., 8.3737e-01, 0.0000e+00,
         2.0558e-01],
        [0.0000e+00, 3.3643e-01, 0.0000e+00,  ..., 9.1148e-01, 0.0000e+00,
         2.2475e-01],
        [0.0000e+00, 3.5455e-01, 0.0000e+00,  ..., 9.5705e-01, 0.0000e+00,
         2.3666e-01],
        ...,
        [0.0000e+00, 4.8510e-04, 0.0000e+00,  ..., 1.1734e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.8511e-04, 0.0000e+00,  ..., 1.1734e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.8503e-04, 0.0000e+00,  ..., 1.1734e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431023.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(107.5559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.2392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.5707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-16.5323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1666],
        [-0.1962],
        [-0.2024],
        ...,
        [-0.1945],
        [-0.1936],
        [-0.1934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74506., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366277.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366277.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0004, -0.0006,  ...,  0.0110,  0.0058, -0.0054],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        ...,
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1006.7012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.2542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8510, device='cuda:0')



h[100].sum tensor(-4.4039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-23.8822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0125, 0.0047, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0143, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40586.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.0652, 0.0000, 0.0104],
        [0.0000, 0.0136, 0.0000,  ..., 0.0534, 0.0000, 0.0078],
        [0.0000, 0.0046, 0.0000,  ..., 0.0248, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319903.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(100.1061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.7925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-170.1113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-65.5660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1689],
        [-0.1745],
        [-0.1721],
        ...,
        [-0.1945],
        [-0.1936],
        [-0.1934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71754.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0006],
        [0.9996],
        ...,
        [0.9998],
        [0.9995],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366277.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9996],
        ...,
        [0.9999],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366289.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0116, -0.0008, -0.0011,  ...,  0.0191,  0.0107, -0.0098],
        [ 0.0135, -0.0009, -0.0012,  ...,  0.0217,  0.0122, -0.0112],
        [ 0.0403, -0.0025, -0.0034,  ...,  0.0588,  0.0345, -0.0314],
        ...,
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1414.0493, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2690, device='cuda:0')



h[100].sum tensor(-6.6640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-36.2609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0963, 0.0000, 0.0000,  ..., 0.1452, 0.0840, 0.0000],
        [0.1099, 0.0000, 0.0000,  ..., 0.1640, 0.0953, 0.0000],
        [0.0647, 0.0000, 0.0000,  ..., 0.1015, 0.0578, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55778.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.8058e-01, 0.0000e+00,  ..., 7.6945e-01, 0.0000e+00,
         1.8765e-01],
        [0.0000e+00, 3.0183e-01, 0.0000e+00,  ..., 8.2401e-01, 0.0000e+00,
         2.0173e-01],
        [0.0000e+00, 2.4822e-01, 0.0000e+00,  ..., 6.8531e-01, 0.0000e+00,
         1.6605e-01],
        ...,
        [0.0000e+00, 7.3753e-04, 0.0000e+00,  ..., 1.2326e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3757e-04, 0.0000e+00,  ..., 1.2326e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3747e-04, 0.0000e+00,  ..., 1.2326e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402330.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(89.1788, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.2162, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-200.4883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-32.3776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1422],
        [-0.1502],
        [-0.1336],
        ...,
        [-0.1971],
        [-0.1963],
        [-0.1960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70259.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9996],
        ...,
        [0.9999],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366289.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9996],
        ...,
        [0.9999],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366302.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        ...,
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0011, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1368.8297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2353, device='cuda:0')



h[100].sum tensor(-6.3099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-34.4503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0149, 0.0000, 0.0000,  ..., 0.0290, 0.0145, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52903.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0310, 0.0000, 0.0030],
        [0.0000, 0.0127, 0.0000,  ..., 0.0497, 0.0000, 0.0071],
        [0.0000, 0.0379, 0.0000,  ..., 0.1273, 0.0000, 0.0244],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0130, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387555.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(77.8446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.6078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.5481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-42.6951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [ 0.0059],
        [ 0.0550],
        ...,
        [-0.1996],
        [-0.1987],
        [-0.1985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62316.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9996],
        ...,
        [0.9999],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366302.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366314.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9045e-03, -4.1886e-04, -5.9076e-04,  ...,  1.1277e-02,
          5.9936e-03, -5.5057e-03],
        [ 3.2901e-03, -2.6950e-04, -3.8010e-04,  ...,  7.6645e-03,
          3.8244e-03, -3.5424e-03],
        [ 1.0622e-02, -6.8836e-04, -9.7086e-04,  ...,  1.7794e-02,
          9.9076e-03, -9.0481e-03],
        ...,
        [-1.4272e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1472e-03,
         -8.9470e-05,  0.0000e+00],
        [-1.4272e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1472e-03,
         -8.9470e-05,  0.0000e+00],
        [-1.4272e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1472e-03,
         -8.9470e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1037.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8468, device='cuda:0')



h[100].sum tensor(-4.3387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-23.7683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0117, 0.0000, 0.0000,  ..., 0.0247, 0.0119, 0.0000],
        [0.0343, 0.0000, 0.0000,  ..., 0.0599, 0.0329, 0.0000],
        [0.0199, 0.0000, 0.0000,  ..., 0.0379, 0.0198, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41785.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0512, 0.0000,  ..., 0.1640, 0.0000, 0.0330],
        [0.0000, 0.0763, 0.0000,  ..., 0.2374, 0.0000, 0.0505],
        [0.0000, 0.0685, 0.0000,  ..., 0.2157, 0.0000, 0.0451],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328900.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(99.0302, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.0563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-171.8296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-67.0789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0595],
        [ 0.0670],
        [ 0.0644],
        ...,
        [-0.1912],
        [-0.1806],
        [-0.1717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78421.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366314.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366326.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00],
        [-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00],
        [-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00],
        ...,
        [-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00],
        [-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00],
        [-1.4419e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1597e-03,
         -7.7362e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1804.6223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6542, device='cuda:0')



h[100].sum tensor(-8.5296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0173, device='cuda:0')



h[200].sum tensor(-46.8859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0024, 0.0000, 0.0000,  ..., 0.0100, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63425.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0188, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0220, 0.0000, 0.0006],
        [0.0000, 0.0071, 0.0000,  ..., 0.0336, 0.0000, 0.0025],
        [0.0000, 0.0121, 0.0000,  ..., 0.0524, 0.0000, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427982.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(84.6653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(393.6024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-213.3219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-21.6262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2383],
        [-0.1917],
        [-0.1229],
        ...,
        [-0.1529],
        [-0.1092],
        [-0.0716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69093.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366326.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366338.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00],
        [-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00],
        [-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00],
        ...,
        [-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00],
        [-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00],
        [-1.4531e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1729e-03,
         -7.1674e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1309.9761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1348, device='cuda:0')



h[100].sum tensor(-5.6773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-31.3139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0128, 0.0000, 0.0000,  ..., 0.0245, 0.0118, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49138.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0482, 0.0000,  ..., 0.1457, 0.0000, 0.0294],
        [0.0000, 0.0267, 0.0000,  ..., 0.0846, 0.0000, 0.0146],
        [0.0000, 0.0156, 0.0000,  ..., 0.0543, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364823.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(93.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.6197, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.5298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-60.9292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0367],
        [ 0.0328],
        [ 0.0332],
        ...,
        [-0.2080],
        [-0.2071],
        [-0.2068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72894.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0000],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366338.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(501.1705, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4939e-03, -2.6973e-04, -3.8529e-04,  ...,  8.0346e-03,
          4.0397e-03, -3.7048e-03],
        [ 3.4939e-03, -2.6973e-04, -3.8529e-04,  ...,  8.0346e-03,
          4.0397e-03, -3.7048e-03],
        [-1.4589e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1896e-03,
         -7.0602e-05,  0.0000e+00],
        ...,
        [-1.4589e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1896e-03,
         -7.0602e-05,  0.0000e+00],
        [-1.4589e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1896e-03,
         -7.0602e-05,  0.0000e+00],
        [-1.4589e-03,  0.0000e+00,  0.0000e+00,  ...,  1.1896e-03,
         -7.0602e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1221.1003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0128, device='cuda:0')



h[100].sum tensor(-5.1081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-28.2706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0172, 0.0073, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0172, 0.0073, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0172, 0.0073, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46098.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0287, 0.0000,  ..., 0.1085, 0.0000, 0.0164],
        [0.0000, 0.0222, 0.0000,  ..., 0.0864, 0.0000, 0.0117],
        [0.0000, 0.0170, 0.0000,  ..., 0.0679, 0.0000, 0.0080],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347905.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(91.5804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.6426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-176.2906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-76.2803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0432],
        [-0.0072],
        [-0.0788],
        ...,
        [-0.2124],
        [-0.2114],
        [-0.2111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72595.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366361.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4689e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2122e-03,
         -6.4270e-05,  0.0000e+00],
        [ 2.0942e-02, -1.2009e-03, -1.7229e-03,  ...,  3.2190e-02,
          1.8537e-02, -1.6743e-02],
        [-1.4689e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2122e-03,
         -6.4270e-05,  0.0000e+00],
        ...,
        [-1.4689e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2122e-03,
         -6.4270e-05,  0.0000e+00],
        [-1.4689e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2122e-03,
         -6.4270e-05,  0.0000e+00],
        [-1.4689e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2122e-03,
         -6.4270e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1594.9280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3824, device='cuda:0')



h[100].sum tensor(-7.0729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-39.2792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0323, 0.0000, 0.0000,  ..., 0.0535, 0.0291, 0.0000],
        [0.0409, 0.0000, 0.0000,  ..., 0.0655, 0.0363, 0.0000],
        [0.0883, 0.0000, 0.0000,  ..., 0.1351, 0.0779, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58435.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0895, 0.0000,  ..., 0.2587, 0.0000, 0.0563],
        [0.0000, 0.1194, 0.0000,  ..., 0.3401, 0.0000, 0.0764],
        [0.0000, 0.1472, 0.0000,  ..., 0.4138, 0.0000, 0.0950],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412524.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(73.7611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.0482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-199.2653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-55.5233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0416],
        [-0.0430],
        [-0.0516],
        ...,
        [-0.2150],
        [-0.2140],
        [-0.2137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71496.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366361.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366373.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5082e-02, -1.9278e-03, -2.7780e-03,  ...,  5.1792e-02,
          3.0304e-02, -2.7284e-02],
        [ 3.3088e-02, -1.8226e-03, -2.6265e-03,  ...,  4.9035e-02,
          2.8648e-02, -2.5796e-02],
        [ 3.3493e-02, -1.8440e-03, -2.6573e-03,  ...,  4.9595e-02,
          2.8984e-02, -2.6098e-02],
        ...,
        [-1.4836e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2389e-03,
         -5.2277e-05,  0.0000e+00],
        [-1.4836e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2389e-03,
         -5.2277e-05,  0.0000e+00],
        [-1.4836e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2389e-03,
         -5.2277e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1408.6774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1853, device='cuda:0')



h[100].sum tensor(-5.9492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-33.1525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1323, 0.0000, 0.0000,  ..., 0.1960, 0.1145, 0.0000],
        [0.1446, 0.0000, 0.0000,  ..., 0.2131, 0.1248, 0.0000],
        [0.1416, 0.0000, 0.0000,  ..., 0.2089, 0.1222, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51921.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3929, 0.0000,  ..., 1.0679, 0.0000, 0.2593],
        [0.0000, 0.4066, 0.0000,  ..., 1.1030, 0.0000, 0.2684],
        [0.0000, 0.3838, 0.0000,  ..., 1.0446, 0.0000, 0.2533],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0155, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381644.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.1119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-184.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-74.9143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1717],
        [-0.1702],
        [-0.1539],
        ...,
        [-0.2166],
        [-0.2157],
        [-0.2154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65167.5898, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0001],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366373.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366385.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00],
        [-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00],
        [-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00],
        ...,
        [-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00],
        [-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00],
        [-1.4919e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2654e-03,
         -3.9502e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1576.6478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3651, device='cuda:0')



h[100].sum tensor(-6.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-37.6901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56204.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0518, 0.0000, 0.0054],
        [0.0000, 0.0030, 0.0000,  ..., 0.0239, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400337.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15.9892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(359.3364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-191.3938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-63.3133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0128],
        [-0.0381],
        [-0.0970],
        ...,
        [-0.2175],
        [-0.2165],
        [-0.2162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73802.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366385.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366397.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [ 8.3073e-03, -5.0016e-04, -7.2729e-04,  ...,  1.4851e-02,
          8.1171e-03, -7.2979e-03],
        ...,
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1194.7571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9233, device='cuda:0')



h[100].sum tensor(-4.5824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-25.7123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0000, 0.0000,  ..., 0.0288, 0.0142, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0187, 0.0081, 0.0000],
        [0.0258, 0.0000, 0.0000,  ..., 0.0450, 0.0239, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44648.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0686, 0.0000,  ..., 0.2117, 0.0000, 0.0425],
        [0.0000, 0.0641, 0.0000,  ..., 0.1999, 0.0000, 0.0394],
        [0.0000, 0.0906, 0.0000,  ..., 0.2723, 0.0000, 0.0573],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347860.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.1039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-166.6687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-93.0213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0426],
        [ 0.0404],
        [ 0.0330],
        ...,
        [-0.2202],
        [-0.2193],
        [-0.2190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72806.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366397.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366397.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0633e-02, -6.1879e-04, -8.9978e-04,  ...,  1.8067e-02,
          1.0048e-02, -9.0288e-03],
        [ 4.3803e-03, -2.9986e-04, -4.3603e-04,  ...,  9.4200e-03,
          4.8563e-03, -4.3753e-03],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        ...,
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2902e-03,
         -2.5376e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1695.6067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4524, device='cuda:0')



h[100].sum tensor(-7.2783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-40.8393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.0000, 0.0000,  ..., 0.0770, 0.0430, 0.0000],
        [0.0281, 0.0000, 0.0000,  ..., 0.0482, 0.0258, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0133, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59348.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1330, 0.0000,  ..., 0.3973, 0.0000, 0.0865],
        [0.0000, 0.0981, 0.0000,  ..., 0.2979, 0.0000, 0.0627],
        [0.0000, 0.0602, 0.0000,  ..., 0.1924, 0.0000, 0.0369],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414701.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(369.8414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-195.0612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-61.5116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0380],
        [-0.0049],
        [ 0.0236],
        ...,
        [-0.2202],
        [-0.2193],
        [-0.2190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62174.7461, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366397.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366409.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00],
        ...,
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00],
        [-1.4986e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3103e-03,
         -1.3991e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.5035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.1775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9370, device='cuda:0')



h[100].sum tensor(-4.6467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-26.1633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44026.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.8424e-02, 0.0000e+00,  ..., 1.5041e-01, 0.0000e+00,
         2.9338e-02],
        [0.0000e+00, 1.3164e-02, 0.0000e+00,  ..., 5.2541e-02, 0.0000e+00,
         6.3387e-03],
        [0.0000e+00, 2.8083e-03, 0.0000e+00,  ..., 2.4204e-02, 0.0000e+00,
         8.0550e-06],
        ...,
        [0.0000e+00, 6.9155e-04, 0.0000e+00,  ..., 1.5789e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.9147e-04, 0.0000e+00,  ..., 1.5788e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.9125e-04, 0.0000e+00,  ..., 1.5787e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338099.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.3175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-164.7147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-99.8856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0344],
        [ 0.0391],
        [ 0.0471],
        ...,
        [-0.2250],
        [-0.2242],
        [-0.2240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73366.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9995],
        ...,
        [1.0002],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366409.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366422.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3301e-03,
         -7.0063e-07,  0.0000e+00],
        [-1.5012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3301e-03,
         -7.0063e-07,  0.0000e+00],
        [ 5.3695e-03, -3.3887e-04, -4.9733e-04,  ...,  1.0834e-02,
          5.7058e-03, -5.1002e-03],
        ...,
        [-1.5012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3301e-03,
         -7.0063e-07,  0.0000e+00],
        [-1.5012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3301e-03,
         -7.0063e-07,  0.0000e+00],
        [-1.5012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3301e-03,
         -7.0063e-07,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1375.0320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1135, device='cuda:0')



h[100].sum tensor(-5.4477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-30.7800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0000,  ..., 0.0237, 0.0110, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0148, 0.0057, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0263, 0.0126, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48408.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0350, 0.0000,  ..., 0.1217, 0.0000, 0.0197],
        [0.0000, 0.0352, 0.0000,  ..., 0.1261, 0.0000, 0.0199],
        [0.0000, 0.0513, 0.0000,  ..., 0.1766, 0.0000, 0.0312],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0154, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359547.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.6469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-173.2677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-92.4852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0782],
        [ 0.0901],
        [ 0.1021],
        ...,
        [-0.2321],
        [-0.2311],
        [-0.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78406.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366422.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366435.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00],
        [-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00],
        [-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00],
        ...,
        [-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00],
        [-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00],
        [-1.5024e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3447e-03,
          1.2892e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1270.7385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9912, device='cuda:0')



h[100].sum tensor(-4.8502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-27.4999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3817e-03, 5.1600e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3849e-03, 5.1630e-05,
         0.0000e+00],
        [8.8748e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9736e-02, 8.6711e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4019e-03, 5.1793e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4015e-03, 5.1790e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4013e-03, 5.1788e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45343.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0260, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0413, 0.0000, 0.0036],
        [0.0000, 0.0276, 0.0000,  ..., 0.0972, 0.0000, 0.0147],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349275.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.4751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.6627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.4306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0442],
        [-0.0733],
        [-0.0799],
        ...,
        [-0.2380],
        [-0.2369],
        [-0.2366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76744.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0006],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366435.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366448.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5025e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3558e-03,
          2.2778e-05,  0.0000e+00],
        [ 1.2317e-02, -6.5868e-04, -9.7592e-04,  ...,  2.0474e-02,
          1.1503e-02, -1.0231e-02],
        [-1.5025e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3558e-03,
          2.2778e-05,  0.0000e+00],
        ...,
        [-1.5025e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3558e-03,
          2.2778e-05,  0.0000e+00],
        [-1.5025e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3558e-03,
          2.2778e-05,  0.0000e+00],
        [-1.5025e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3558e-03,
          2.2778e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1252.4792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.7943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9703, device='cuda:0')



h[100].sum tensor(-4.7082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-26.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6708e-02, 0.0000e+00, 0.0000e+00,  ..., 3.2699e-02, 1.6467e-02,
         0.0000e+00],
        [9.7949e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1061e-02, 9.4773e-03,
         0.0000e+00],
        [5.8761e-02, 0.0000e+00, 0.0000e+00,  ..., 9.5033e-02, 5.3897e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4476e-03, 9.1518e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4472e-03, 9.1512e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4469e-03, 9.1508e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46459.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.0504e-02, 0.0000e+00,  ..., 1.7084e-01, 0.0000e+00,
         3.0725e-02],
        [0.0000e+00, 5.2810e-02, 0.0000e+00,  ..., 1.7265e-01, 0.0000e+00,
         3.2139e-02],
        [0.0000e+00, 9.7084e-02, 0.0000e+00,  ..., 2.9620e-01, 0.0000e+00,
         6.2261e-02],
        ...,
        [0.0000e+00, 2.3691e-04, 0.0000e+00,  ..., 1.4753e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3682e-04, 0.0000e+00,  ..., 1.4752e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3666e-04, 0.0000e+00,  ..., 1.4751e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364725.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.6623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-170.3208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.8099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0759],
        [ 0.0563],
        [ 0.0330],
        ...,
        [-0.2435],
        [-0.2425],
        [-0.2421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75785.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366448.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(517.8226, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366462.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5067e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3614e-03,
          3.8107e-05,  0.0000e+00],
        [-1.5067e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3614e-03,
          3.8107e-05,  0.0000e+00],
        [ 6.9811e-03, -3.9759e-04, -5.9194e-04,  ...,  1.3104e-02,
          7.0891e-03, -6.2752e-03],
        ...,
        [-1.5067e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3614e-03,
          3.8107e-05,  0.0000e+00],
        [-1.5067e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3614e-03,
          3.8107e-05,  0.0000e+00],
        [-1.5067e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3614e-03,
          3.8107e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2549.6841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.3101, device='cuda:0')



h[100].sum tensor(-11.3928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0241, device='cuda:0')



h[200].sum tensor(-65.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0002, 0.0000],
        [0.0101, 0.0000, 0.0000,  ..., 0.0236, 0.0111, 0.0000],
        [0.0260, 0.0000, 0.0000,  ..., 0.0477, 0.0255, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86845.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6893e-02, 0.0000e+00,  ..., 7.1353e-02, 0.0000e+00,
         8.4995e-03],
        [0.0000e+00, 4.6109e-02, 0.0000e+00,  ..., 1.6052e-01, 0.0000e+00,
         2.8221e-02],
        [0.0000e+00, 8.6939e-02, 0.0000e+00,  ..., 2.7974e-01, 0.0000e+00,
         5.6199e-02],
        ...,
        [0.0000e+00, 1.3572e-04, 0.0000e+00,  ..., 1.4995e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3563e-04, 0.0000e+00,  ..., 1.4993e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3549e-04, 0.0000e+00,  ..., 1.4992e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572429.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(492.6949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.0411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-11.7294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0545],
        [ 0.0614],
        [ 0.0678],
        ...,
        [-0.2480],
        [-0.2470],
        [-0.2466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61567.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366462.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366476.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00],
        [-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00],
        [-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00],
        ...,
        [-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00],
        [-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00],
        [-1.4974e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3592e-03,
          3.9211e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1515.3234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2097, device='cuda:0')



h[100].sum tensor(-5.9056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-33.8363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56449.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0408, 0.0000, 0.0019],
        [0.0000, 0.0052, 0.0000,  ..., 0.0340, 0.0000, 0.0010],
        [0.0000, 0.0016, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0153, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426571.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.9297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-188.7727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-76.5098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0004],
        [-0.0234],
        [-0.0561],
        ...,
        [-0.2522],
        [-0.2511],
        [-0.2508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88854.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366476.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366489.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5622e-02, -7.7342e-04, -1.1629e-03,  ...,  2.5015e-02,
          1.4233e-02, -1.2608e-02],
        [ 7.0989e-03, -3.8801e-04, -5.8341e-04,  ...,  1.3226e-02,
          7.1538e-03, -6.3253e-03],
        [ 7.0874e-03, -3.8749e-04, -5.8263e-04,  ...,  1.3210e-02,
          7.1443e-03, -6.3168e-03],
        ...,
        [-1.4818e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3564e-03,
          2.7062e-05,  0.0000e+00],
        [-1.4818e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3564e-03,
          2.7062e-05,  0.0000e+00],
        [-1.4818e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3564e-03,
          2.7062e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1437.8198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1276, device='cuda:0')



h[100].sum tensor(-5.4277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-31.2076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0392, 0.0000, 0.0000,  ..., 0.0679, 0.0376, 0.0000],
        [0.0408, 0.0000, 0.0000,  ..., 0.0701, 0.0389, 0.0000],
        [0.0126, 0.0000, 0.0000,  ..., 0.0270, 0.0131, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51712.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 8.7203e-02, 0.0000e+00,  ..., 2.7603e-01, 0.0000e+00,
         5.5933e-02],
        [0.0000e+00, 7.8178e-02, 0.0000e+00,  ..., 2.4954e-01, 0.0000e+00,
         4.9723e-02],
        [0.0000e+00, 4.6460e-02, 0.0000e+00,  ..., 1.5918e-01, 0.0000e+00,
         2.8096e-02],
        ...,
        [0.0000e+00, 2.4427e-04, 0.0000e+00,  ..., 1.5168e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.4417e-04, 0.0000e+00,  ..., 1.5167e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.4399e-04, 0.0000e+00,  ..., 1.5166e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393676.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.6103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-179.6758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-95.1210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0899],
        [ 0.0840],
        [ 0.0740],
        ...,
        [-0.2568],
        [-0.2558],
        [-0.2557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78607.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [0.9996],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366489.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366502.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6091e-02, -7.8013e-04, -1.1789e-03,  ...,  2.5640e-02,
          1.4607e-02, -1.2927e-02],
        [ 1.0700e-02, -5.4063e-04, -8.1699e-04,  ...,  1.8183e-02,
          1.0129e-02, -8.9589e-03],
        [ 1.5732e-02, -7.6416e-04, -1.1548e-03,  ...,  2.5143e-02,
          1.4308e-02, -1.2663e-02],
        ...,
        [-1.4711e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3481e-03,
          2.1182e-05,  0.0000e+00],
        [-1.4711e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3481e-03,
          2.1182e-05,  0.0000e+00],
        [-1.4711e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3481e-03,
          2.1182e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1419.2859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0990, device='cuda:0')



h[100].sum tensor(-5.2295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-30.1745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.7711e-02, 0.0000e+00, 0.0000e+00,  ..., 6.5701e-02, 3.6294e-02,
         0.0000e+00],
        [4.3009e-02, 0.0000e+00, 0.0000e+00,  ..., 7.3037e-02, 4.0697e-02,
         0.0000e+00],
        [2.6069e-02, 0.0000e+00, 0.0000e+00,  ..., 4.7563e-02, 2.5404e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4192e-03, 8.5153e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4188e-03, 8.5146e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4185e-03, 8.5142e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49853.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1926e-01, 0.0000e+00,  ..., 3.7089e-01, 0.0000e+00,
         7.7508e-02],
        [0.0000e+00, 1.1672e-01, 0.0000e+00,  ..., 3.6167e-01, 0.0000e+00,
         7.5751e-02],
        [0.0000e+00, 9.5974e-02, 0.0000e+00,  ..., 3.0285e-01, 0.0000e+00,
         6.1662e-02],
        ...,
        [0.0000e+00, 3.5570e-04, 0.0000e+00,  ..., 1.5626e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5559e-04, 0.0000e+00,  ..., 1.5625e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5539e-04, 0.0000e+00,  ..., 1.5623e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381216., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.2158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-175.7393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-99.9683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0704],
        [ 0.0765],
        [ 0.0818],
        ...,
        [-0.2617],
        [-0.2606],
        [-0.2603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82053.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366502.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366516.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00],
        [-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00],
        [-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00],
        ...,
        [-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00],
        [-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00],
        [-1.4605e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3349e-03,
          2.3481e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1311.2697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9432, device='cuda:0')



h[100].sum tensor(-4.5444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-26.3142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3429e-03, 9.3979e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3465e-03, 9.4041e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3429e-03, 9.3978e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3672e-03, 9.4406e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3668e-03, 9.4399e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3665e-03, 9.4394e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45992.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0166, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0166, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0166, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0166, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363072.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.6184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.2048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.2396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2847],
        [-0.2692],
        [-0.2359],
        ...,
        [-0.2644],
        [-0.2633],
        [-0.2629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88464.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366516.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366529.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00],
        [-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00],
        [-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00],
        ...,
        [-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00],
        [-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00],
        [-1.4547e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
          3.2449e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1528.5033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1593, device='cuda:0')



h[100].sum tensor(-5.4880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-31.8905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1607, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49785.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0177, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0382, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0178, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372357.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.0106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.6497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-89.0395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0662],
        [-0.0881],
        [-0.0859],
        ...,
        [-0.2626],
        [-0.2616],
        [-0.2612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83273.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366529.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366543.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00],
        [-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00],
        [-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00],
        ...,
        [-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00],
        [-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00],
        [-1.4515e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3078e-03,
          3.9099e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1947.6802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5681, device='cuda:0')



h[100].sum tensor(-7.4575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-43.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64389.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.0675, 0.0000, 0.0077],
        [0.0000, 0.0097, 0.0000,  ..., 0.0510, 0.0000, 0.0044],
        [0.0000, 0.0066, 0.0000,  ..., 0.0397, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0187, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455856.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(397.9513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-198.8116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-55.4625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0726],
        [ 0.0461],
        [ 0.0099],
        ...,
        [-0.2587],
        [-0.2577],
        [-0.2574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61632.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0008],
        [0.9997],
        ...,
        [1.0003],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366543.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9997],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366557.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2276e-02, -9.7961e-04, -1.5114e-03,  ...,  3.4091e-02,
          1.9718e-02, -1.7355e-02],
        [ 5.1854e-03, -2.7359e-04, -4.2210e-04,  ...,  1.0455e-02,
          5.5265e-03, -4.8470e-03],
        [ 1.9318e-02, -8.5742e-04, -1.3229e-03,  ...,  3.0000e-02,
          1.7262e-02, -1.5190e-02],
        ...,
        [-1.4374e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2961e-03,
          2.7223e-05,  0.0000e+00],
        [-1.4374e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2961e-03,
          2.7223e-05,  0.0000e+00],
        [-1.4374e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2961e-03,
          2.7223e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2370.4604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.9899, device='cuda:0')



h[100].sum tensor(-9.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0208, device='cuda:0')



h[200].sum tensor(-55.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.1022e-02, 0.0000e+00, 0.0000e+00,  ..., 5.4056e-02, 2.9451e-02,
         0.0000e+00],
        [8.1253e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2552e-01, 7.2360e-02,
         0.0000e+00],
        [4.9563e-02, 0.0000e+00, 0.0000e+00,  ..., 8.1687e-02, 4.6042e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.2130e-03, 1.0950e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.2125e-03, 1.0949e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.2122e-03, 1.0948e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80799.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2866e-01, 0.0000e+00,  ..., 3.8823e-01, 0.0000e+00,
         8.4325e-02],
        [0.0000e+00, 1.9853e-01, 0.0000e+00,  ..., 5.7830e-01, 0.0000e+00,
         1.3130e-01],
        [0.0000e+00, 2.0408e-01, 0.0000e+00,  ..., 5.9501e-01, 0.0000e+00,
         1.3512e-01],
        ...,
        [0.0000e+00, 4.5818e-04, 0.0000e+00,  ..., 1.8674e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1861e-03, 0.0000e+00,  ..., 3.2891e-02, 0.0000e+00,
         2.2639e-03],
        [0.0000e+00, 2.0144e-02, 0.0000e+00,  ..., 7.5451e-02, 0.0000e+00,
         1.1879e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562131., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.3000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-231.7384, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-18.9240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0484],
        [ 0.0402],
        [ 0.0352],
        ...,
        [-0.1677],
        [-0.0998],
        [-0.0418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65608.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9997],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366557.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366570.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00],
        [-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00],
        [-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00],
        ...,
        [-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00],
        [-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00],
        [-1.4185e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2843e-03,
          9.5026e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1856.6431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4515, device='cuda:0')



h[100].sum tensor(-6.8632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-40.3098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1404e-03, 3.8033e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1441e-03, 3.8060e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1410e-03, 3.8037e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1664e-03, 3.8226e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1660e-03, 3.8223e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1657e-03, 3.8220e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60749.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0183, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0183, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0183, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0183, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444394.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.6574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.0165, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-69.5415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1480],
        [-0.2267],
        [-0.2907],
        ...,
        [-0.2769],
        [-0.2758],
        [-0.2755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71523.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366570.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366584.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00],
        [-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00],
        [-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00],
        ...,
        [-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00],
        [-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00],
        [-1.4012e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2742e-03,
         -1.0311e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1453.6748, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0367, device='cuda:0')



h[100].sum tensor(-4.8343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-28.4953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50026.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0179, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391272.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.4908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-174.5571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-98.9654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3888],
        [-0.3713],
        [-0.3381],
        ...,
        [-0.2828],
        [-0.2817],
        [-0.2813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85726.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366584.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(569.3707, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366598.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        ...,
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1667.9711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2464, device='cuda:0')



h[100].sum tensor(-5.8067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-34.3497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0053, 0.0000, 0.0000,  ..., 0.0143, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54059.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1486e-03, 0.0000e+00,  ..., 2.1329e-02, 0.0000e+00,
         3.3260e-05],
        [0.0000e+00, 8.5363e-03, 0.0000e+00,  ..., 4.4613e-02, 0.0000e+00,
         4.4345e-03],
        [0.0000e+00, 2.4029e-02, 0.0000e+00,  ..., 9.3824e-02, 0.0000e+00,
         1.3801e-02],
        ...,
        [0.0000e+00, 2.8979e-04, 0.0000e+00,  ..., 1.7899e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8966e-04, 0.0000e+00,  ..., 1.7897e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8943e-04, 0.0000e+00,  ..., 1.7895e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403141.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.5593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.9240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-87.3557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1109],
        [-0.0422],
        [ 0.0181],
        ...,
        [-0.2869],
        [-0.2858],
        [-0.2854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94405.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366598.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366598.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4477e-02, -6.1965e-04, -9.7166e-04,  ...,  2.3201e-02,
          1.3144e-02, -1.1559e-02],
        [ 1.9578e-02, -8.1888e-04, -1.2841e-03,  ...,  3.0252e-02,
          1.7378e-02, -1.5275e-02],
        [ 8.9719e-03, -4.0468e-04, -6.3456e-04,  ...,  1.5591e-02,
          8.5752e-03, -7.5488e-03],
        ...,
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00],
        [-1.3907e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2668e-03,
         -2.4928e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1236.8604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.5016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7943, device='cuda:0')



h[100].sum tensor(-3.7095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-21.9436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0893, 0.0000, 0.0000,  ..., 0.1362, 0.0786, 0.0000],
        [0.0737, 0.0000, 0.0000,  ..., 0.1147, 0.0657, 0.0000],
        [0.0619, 0.0000, 0.0000,  ..., 0.0964, 0.0548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43481.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0229e-01, 0.0000e+00,  ..., 8.4353e-01, 0.0000e+00,
         1.9911e-01],
        [0.0000e+00, 2.7298e-01, 0.0000e+00,  ..., 7.6551e-01, 0.0000e+00,
         1.7962e-01],
        [0.0000e+00, 2.1419e-01, 0.0000e+00,  ..., 6.0579e-01, 0.0000e+00,
         1.4053e-01],
        ...,
        [0.0000e+00, 2.8979e-04, 0.0000e+00,  ..., 1.7899e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8966e-04, 0.0000e+00,  ..., 1.7897e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8943e-04, 0.0000e+00,  ..., 1.7895e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(362753.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.0648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-162.3059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.3610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1256],
        [-0.1196],
        [-0.1023],
        ...,
        [-0.2870],
        [-0.2858],
        [-0.2854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86508.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0009],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366598.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366612.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6856e-03, -1.9442e-04, -3.0656e-04,  ...,  8.2768e-03,
          4.1772e-03, -3.6906e-03],
        [ 1.1060e-02, -4.7699e-04, -7.5211e-04,  ...,  1.8469e-02,
          1.0297e-02, -9.0545e-03],
        [-1.3880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2638e-03,
         -3.3317e-05,  0.0000e+00],
        ...,
        [-1.3880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2638e-03,
         -3.3317e-05,  0.0000e+00],
        [-1.3880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2638e-03,
         -3.3317e-05,  0.0000e+00],
        [-1.3880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2638e-03,
         -3.3317e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2100.6191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7037, device='cuda:0')



h[100].sum tensor(-7.8048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0178, device='cuda:0')



h[200].sum tensor(-46.3362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0000, 0.0000,  ..., 0.0667, 0.0369, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0321, 0.0161, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0280, 0.0137, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63723.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.8183e-02, 0.0000e+00,  ..., 2.5306e-01, 0.0000e+00,
         5.0190e-02],
        [0.0000e+00, 5.8358e-02, 0.0000e+00,  ..., 1.9758e-01, 0.0000e+00,
         3.6937e-02],
        [0.0000e+00, 4.3426e-02, 0.0000e+00,  ..., 1.5179e-01, 0.0000e+00,
         2.7058e-02],
        ...,
        [0.0000e+00, 1.2694e-04, 0.0000e+00,  ..., 1.8152e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2682e-04, 0.0000e+00,  ..., 1.8150e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2662e-04, 0.0000e+00,  ..., 1.8148e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441377.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.9540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-202.1149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-57.4613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0590],
        [ 0.0594],
        [ 0.0441],
        ...,
        [-0.2901],
        [-0.2836],
        [-0.2697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105236.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366612.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366626.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00],
        [-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00],
        [-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00],
        ...,
        [-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00],
        [-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00],
        [-1.4007e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2641e-03,
         -3.5840e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1382.0918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.2432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9185, device='cuda:0')



h[100].sum tensor(-4.2484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-25.3134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45683.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7344e-03, 0.0000e+00,  ..., 2.6306e-02, 0.0000e+00,
         4.7086e-04],
        [0.0000e+00, 2.1158e-04, 0.0000e+00,  ..., 1.9716e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.6487e-06, 0.0000e+00,  ..., 1.8885e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8891e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8890e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8888e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370793.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.4400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-165.2148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-94.5121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1168],
        [-0.1525],
        [-0.1779],
        ...,
        [-0.2936],
        [-0.2924],
        [-0.2921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88280.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0004],
        [0.9999],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366626.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0005],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366640.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4116e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2689e-03,
         -3.7501e-05,  0.0000e+00],
        [ 2.9192e-02, -1.1295e-03, -1.8011e-03,  ...,  4.3596e-02,
          2.5378e-02, -2.2211e-02],
        [ 1.3007e-02, -5.3211e-04, -8.4853e-04,  ...,  2.1210e-02,
          1.1936e-02, -1.0464e-02],
        ...,
        [ 9.6029e-03, -4.0650e-04, -6.4822e-04,  ...,  1.6502e-02,
          9.1095e-03, -7.9939e-03],
        [ 6.3198e-03, -2.8533e-04, -4.5500e-04,  ...,  1.1962e-02,
          6.3831e-03, -5.6111e-03],
        [ 9.6029e-03, -4.0650e-04, -6.4822e-04,  ...,  1.6502e-02,
          9.1095e-03, -7.9939e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1596.6838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1395, device='cuda:0')



h[100].sum tensor(-5.1900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-31.0354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0970, 0.0000, 0.0000,  ..., 0.1470, 0.0851, 0.0000],
        [0.0352, 0.0000, 0.0000,  ..., 0.0597, 0.0327, 0.0000],
        [0.0979, 0.0000, 0.0000,  ..., 0.1483, 0.0858, 0.0000],
        ...,
        [0.0140, 0.0000, 0.0000,  ..., 0.0284, 0.0139, 0.0000],
        [0.0409, 0.0000, 0.0000,  ..., 0.0696, 0.0386, 0.0000],
        [0.0316, 0.0000, 0.0000,  ..., 0.0567, 0.0308, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49648.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2330, 0.0000,  ..., 0.6669, 0.0000, 0.1541],
        [0.0000, 0.1751, 0.0000,  ..., 0.5131, 0.0000, 0.1158],
        [0.0000, 0.2394, 0.0000,  ..., 0.6852, 0.0000, 0.1585],
        ...,
        [0.0000, 0.0429, 0.0000,  ..., 0.1517, 0.0000, 0.0277],
        [0.0000, 0.0717, 0.0000,  ..., 0.2351, 0.0000, 0.0470],
        [0.0000, 0.0790, 0.0000,  ..., 0.2564, 0.0000, 0.0518]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382968.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.7961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.1967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-77.2326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0081],
        [ 0.0149],
        [ 0.0068],
        ...,
        [-0.0286],
        [ 0.0093],
        [ 0.0207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92253.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0005],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366640.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366655.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00],
        [-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00],
        [-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00],
        ...,
        [-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00],
        [-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00],
        [-1.4186e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2755e-03,
         -4.0241e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1409.9192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9274, device='cuda:0')



h[100].sum tensor(-4.2498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-25.5053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46753.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0227, 0.0000,  ..., 0.0905, 0.0000, 0.0147],
        [0.0000, 0.0068, 0.0000,  ..., 0.0447, 0.0000, 0.0041],
        [0.0000, 0.0017, 0.0000,  ..., 0.0306, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380703.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.7925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-166.2701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-83.5832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0403],
        [ 0.0279],
        [ 0.0351],
        ...,
        [-0.2980],
        [-0.2968],
        [-0.2964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91943.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9998],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366655.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9999],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366669.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00],
        [-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00],
        [-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00],
        ...,
        [-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00],
        [-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00],
        [-1.4159e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2834e-03,
         -4.5918e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1552.8517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0729, device='cuda:0')



h[100].sum tensor(-4.8925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-29.4693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51417.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0584, 0.0000, 0.0084],
        [0.0000, 0.0025, 0.0000,  ..., 0.0299, 0.0000, 0.0016],
        [0.0000, 0.0028, 0.0000,  ..., 0.0328, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405715.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.4215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-175.2410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-79.9737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0861],
        [-0.0831],
        [-0.0466],
        ...,
        [-0.3041],
        [-0.3031],
        [-0.3029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78800.5703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [0.9999],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366669.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366683.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00],
        [-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00],
        [-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00],
        ...,
        [-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00],
        [-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00],
        [-1.3995e-03,  0.0000e+00,  0.0000e+00,  ...,  1.2944e-03,
         -5.7239e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1224.4098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.9427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7443, device='cuda:0')



h[100].sum tensor(-3.3698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0078, device='cuda:0')



h[200].sum tensor(-20.3716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0112, 0.0036, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0173, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40305.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0618, 0.0000, 0.0068],
        [0.0000, 0.0197, 0.0000,  ..., 0.0926, 0.0000, 0.0123],
        [0.0000, 0.0283, 0.0000,  ..., 0.1197, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(345828.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(306.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-157.2993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.8890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0413],
        [ 0.0705],
        [ 0.0874],
        ...,
        [-0.3130],
        [-0.3117],
        [-0.3113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119251.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0005],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366683.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366697.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0621e-02, -4.1020e-04, -6.6957e-04,  ...,  1.7926e-02,
          9.9154e-03, -8.6720e-03],
        [ 3.5707e-03, -1.6941e-04, -2.7653e-04,  ...,  8.1697e-03,
          4.0568e-03, -3.5815e-03],
        [ 5.6609e-03, -2.4079e-04, -3.9304e-04,  ...,  1.1062e-02,
          5.7935e-03, -5.0905e-03],
        ...,
        [-1.3899e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3053e-03,
         -6.5058e-05,  0.0000e+00],
        [-1.3899e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3053e-03,
         -6.5058e-05,  0.0000e+00],
        [-1.3899e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3053e-03,
         -6.5058e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1554.7206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0844, device='cuda:0')



h[100].sum tensor(-4.8974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-29.7147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0000, 0.0000,  ..., 0.0327, 0.0163, 0.0000],
        [0.0393, 0.0000, 0.0000,  ..., 0.0674, 0.0371, 0.0000],
        [0.0171, 0.0000, 0.0000,  ..., 0.0328, 0.0164, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49538.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0556, 0.0000,  ..., 0.1987, 0.0000, 0.0359],
        [0.0000, 0.0755, 0.0000,  ..., 0.2547, 0.0000, 0.0491],
        [0.0000, 0.0606, 0.0000,  ..., 0.2092, 0.0000, 0.0393],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388154., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(347.2545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-176.7610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-100.8121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0917],
        [ 0.0960],
        [ 0.0925],
        ...,
        [-0.3183],
        [-0.3170],
        [-0.3165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126923.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366697.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366710.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00],
        [-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00],
        [-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00],
        ...,
        [-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00],
        [-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00],
        [-1.3768e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3132e-03,
         -7.3806e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1503.7061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0295, device='cuda:0')



h[100].sum tensor(-4.6155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-28.1069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50557.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0494, 0.0000, 0.0045],
        [0.0000, 0.0011, 0.0000,  ..., 0.0252, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(404041.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.4686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-178.2775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.3485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0395],
        [-0.1107],
        [-0.1901],
        ...,
        [-0.3221],
        [-0.3207],
        [-0.3203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102919.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366710.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(500.2720, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0012],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366724.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6389e-02, -5.8287e-04, -9.6297e-04,  ...,  2.5894e-02,
          1.4682e-02, -1.2787e-02],
        [ 2.2327e-02, -7.7774e-04, -1.2849e-03,  ...,  3.4111e-02,
          1.9616e-02, -1.7062e-02],
        [ 2.9170e-02, -1.0023e-03, -1.6560e-03,  ...,  4.3581e-02,
          2.5303e-02, -2.1989e-02],
        ...,
        [-1.3706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3150e-03,
         -7.6550e-05,  0.0000e+00],
        [-1.3706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3150e-03,
         -7.6550e-05,  0.0000e+00],
        [-1.3706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3150e-03,
         -7.6550e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1920.8523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4292, device='cuda:0')



h[100].sum tensor(-6.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-39.2564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0743, 0.0000, 0.0000,  ..., 0.1157, 0.0660, 0.0000],
        [0.0818, 0.0000, 0.0000,  ..., 0.1261, 0.0723, 0.0000],
        [0.0983, 0.0000, 0.0000,  ..., 0.1489, 0.0859, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59993.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1783, 0.0000,  ..., 0.5266, 0.0000, 0.1164],
        [0.0000, 0.2155, 0.0000,  ..., 0.6266, 0.0000, 0.1409],
        [0.0000, 0.2476, 0.0000,  ..., 0.7138, 0.0000, 0.1619],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439530.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.5243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-94.0170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0116],
        [ 0.0197],
        [ 0.0295],
        ...,
        [-0.3241],
        [-0.3228],
        [-0.3223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100678.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0012],
        [0.9999],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366724.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0000],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366738., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00],
        [-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00],
        [-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00],
        ...,
        [-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00],
        [-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00],
        [-1.3677e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3130e-03,
         -7.6169e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1634.2279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1199, device='cuda:0')



h[100].sum tensor(-4.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-30.5340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0126, 0.0000, 0.0000,  ..., 0.0265, 0.0126, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52807.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0371, 0.0000,  ..., 0.1461, 0.0000, 0.0227],
        [0.0000, 0.0153, 0.0000,  ..., 0.0759, 0.0000, 0.0088],
        [0.0000, 0.0050, 0.0000,  ..., 0.0422, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411804.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.9382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-176.9292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-110.8945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0639],
        [ 0.0138],
        [-0.0519],
        ...,
        [-0.3245],
        [-0.3232],
        [-0.3228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94081.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0000],
        ...,
        [1.0006],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366738., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0000],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366751.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3693e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3105e-03,
         -7.3593e-05,  0.0000e+00],
        [ 8.5136e-03, -3.1152e-04, -5.2106e-04,  ...,  1.4989e-02,
          8.1401e-03, -7.0961e-03],
        [ 5.7774e-03, -2.2527e-04, -3.7680e-04,  ...,  1.1202e-02,
          5.8661e-03, -5.1315e-03],
        ...,
        [-1.3693e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3105e-03,
         -7.3593e-05,  0.0000e+00],
        [-1.3693e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3105e-03,
         -7.3593e-05,  0.0000e+00],
        [-1.3693e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3105e-03,
         -7.3593e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1661.1074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1229, device='cuda:0')



h[100].sum tensor(-4.9651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-30.5709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1556, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0000, 0.0000,  ..., 0.0376, 0.0193, 0.0000],
        [0.0183, 0.0000, 0.0000,  ..., 0.0344, 0.0174, 0.0000],
        [0.0434, 0.0000, 0.0000,  ..., 0.0730, 0.0404, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54558.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0557, 0.0000,  ..., 0.1932, 0.0000, 0.0352],
        [0.0000, 0.0596, 0.0000,  ..., 0.2068, 0.0000, 0.0376],
        [0.0000, 0.0879, 0.0000,  ..., 0.2889, 0.0000, 0.0563],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423859.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(363.7042, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-176.9151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-104.7165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0549],
        [ 0.0612],
        [ 0.0705],
        ...,
        [-0.3243],
        [-0.3230],
        [-0.3226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87251.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0000],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366751.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366765.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3672e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3090e-03,
         -7.1965e-05,  0.0000e+00],
        [ 3.3067e-03, -1.4435e-04, -2.4296e-04,  ...,  7.7781e-03,
          3.8126e-03, -3.3512e-03],
        [ 3.3067e-03, -1.4435e-04, -2.4296e-04,  ...,  7.7781e-03,
          3.8126e-03, -3.3512e-03],
        ...,
        [-1.3672e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3090e-03,
         -7.1965e-05,  0.0000e+00],
        [-1.3672e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3090e-03,
         -7.1965e-05,  0.0000e+00],
        [-1.3672e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3090e-03,
         -7.1965e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1738.4589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1863, device='cuda:0')



h[100].sum tensor(-5.1806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-32.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0000,  ..., 0.0170, 0.0069, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0170, 0.0069, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0170, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53731.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0722, 0.0000, 0.0052],
        [0.0000, 0.0201, 0.0000,  ..., 0.1018, 0.0000, 0.0113],
        [0.0000, 0.0434, 0.0000,  ..., 0.1671, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413521.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.5822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.7658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-105.1260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0672],
        [ 0.0037],
        [ 0.0441],
        ...,
        [-0.3218],
        [-0.3205],
        [-0.3201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77656.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366765.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366779.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00],
        [-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00],
        [-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00],
        ...,
        [-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00],
        [-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00],
        [-1.3569e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3111e-03,
         -7.1114e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1497.7841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9095, device='cuda:0')



h[100].sum tensor(-4.0050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-24.8425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45830.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368542.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.4529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-156.9428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.7950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4363],
        [-0.4238],
        [-0.3997],
        ...,
        [-0.3243],
        [-0.3230],
        [-0.3226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109916.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366779.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366793.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00],
        [-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00],
        [-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00],
        ...,
        [-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00],
        [-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00],
        [-1.3487e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3141e-03,
         -7.0265e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1676.0564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0853, device='cuda:0')



h[100].sum tensor(-4.7263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-29.4259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0317, 0.0157, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55338.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0456, 0.0000,  ..., 0.1712, 0.0000, 0.0278],
        [0.0000, 0.0151, 0.0000,  ..., 0.0790, 0.0000, 0.0085],
        [0.0000, 0.0034, 0.0000,  ..., 0.0415, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440094.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.1692, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-174.6147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-102.3813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0435],
        [-0.0216],
        [-0.1033],
        ...,
        [-0.3289],
        [-0.3275],
        [-0.3270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93968.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366793.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366806.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00],
        [-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00],
        [-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00],
        ...,
        [-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00],
        [-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00],
        [-1.3496e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3178e-03,
         -6.7661e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1567.0992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9672, device='cuda:0')



h[100].sum tensor(-4.1956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-26.2189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0180, 0.0076, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50040.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0243, 0.0000,  ..., 0.1034, 0.0000, 0.0142],
        [0.0000, 0.0051, 0.0000,  ..., 0.0424, 0.0000, 0.0023],
        [0.0000, 0.0003, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405466.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.3107, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-164.6436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.7786, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0646],
        [-0.1564],
        [-0.2502],
        ...,
        [-0.3347],
        [-0.3334],
        [-0.3329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90858.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366806.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366820.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00],
        [-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00],
        [-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00],
        ...,
        [-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00],
        [-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00],
        [-1.3474e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -7.0148e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8571, device='cuda:0')



h[100].sum tensor(-3.6947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-23.1752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1188, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0182, 0.0000, 0.0000,  ..., 0.0342, 0.0172, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46127.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0343, 0.0000, 0.0014],
        [0.0000, 0.0177, 0.0000,  ..., 0.0806, 0.0000, 0.0107],
        [0.0000, 0.0559, 0.0000,  ..., 0.1956, 0.0000, 0.0346],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382342.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.1450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-159.4534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.4454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1765],
        [-0.0860],
        [-0.0111],
        ...,
        [-0.3412],
        [-0.3398],
        [-0.3394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100128.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0001],
        ...,
        [1.0007],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366820.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366833.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7043e-03, -1.6835e-04, -2.9271e-04,  ...,  9.7062e-03,
          4.9589e-03, -4.3078e-03],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3305e-03,
         -7.0239e-05,  0.0000e+00],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3305e-03,
         -7.0239e-05,  0.0000e+00],
        ...,
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3305e-03,
         -7.0239e-05,  0.0000e+00],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3305e-03,
         -7.0239e-05,  0.0000e+00],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3305e-03,
         -7.0239e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1462.0763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8518, device='cuda:0')



h[100].sum tensor(-3.6805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-23.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0000, 0.0000,  ..., 0.0329, 0.0164, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0137, 0.0050, 0.0000],
        [0.0303, 0.0000, 0.0000,  ..., 0.0510, 0.0273, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46094.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0750, 0.0000,  ..., 0.2520, 0.0000, 0.0471],
        [0.0000, 0.0617, 0.0000,  ..., 0.2114, 0.0000, 0.0385],
        [0.0000, 0.0958, 0.0000,  ..., 0.3018, 0.0000, 0.0609],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381690.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.9044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-161.4236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-131.1478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0634],
        [ 0.0492],
        [ 0.0338],
        ...,
        [-0.3461],
        [-0.3447],
        [-0.3445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102999.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366833.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366847.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        ...,
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1508.3984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8985, device='cuda:0')



h[100].sum tensor(-3.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-24.3021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48000.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0496, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392340.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.9193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-168.3179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-123.2589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3275],
        [-0.2276],
        [-0.1063],
        ...,
        [-0.3526],
        [-0.3512],
        [-0.3507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119345.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366847.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366847.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [ 1.1352e-02, -3.4591e-04, -6.0546e-04,  ...,  1.8919e-02,
          1.0488e-02, -9.0290e-03],
        [ 5.0704e-03, -1.7486e-04, -3.0607e-04,  ...,  1.0226e-02,
          5.2685e-03, -4.5643e-03],
        ...,
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00],
        [-1.3517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3386e-03,
         -6.7712e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1712.8608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0939, device='cuda:0')



h[100].sum tensor(-4.7264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-29.8692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0000, 0.0000,  ..., 0.0302, 0.0148, 0.0000],
        [0.0164, 0.0000, 0.0000,  ..., 0.0337, 0.0168, 0.0000],
        [0.0684, 0.0000, 0.0000,  ..., 0.1075, 0.0610, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50423.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0477, 0.0000,  ..., 0.1769, 0.0000, 0.0297],
        [0.0000, 0.0876, 0.0000,  ..., 0.2894, 0.0000, 0.0554],
        [0.0000, 0.1699, 0.0000,  ..., 0.5120, 0.0000, 0.1092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396596.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.1933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0439],
        [ 0.0699],
        [ 0.0790],
        ...,
        [-0.3526],
        [-0.3512],
        [-0.3507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103581.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366847.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366861.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00],
        [-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00],
        [-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00],
        ...,
        [-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00],
        [-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00],
        [-1.3588e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3459e-03,
         -6.3008e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1912.1818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2927, device='cuda:0')



h[100].sum tensor(-5.5205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-35.0187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60681.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461195.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.5576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-193.6250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-98.0895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2877],
        [-0.3325],
        [-0.3707],
        ...,
        [-0.3576],
        [-0.3562],
        [-0.3557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98312.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0002],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366861.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0003],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366875.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00],
        [-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00],
        [-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00],
        ...,
        [-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00],
        [-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00],
        [-1.3655e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3520e-03,
         -5.8553e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2152.2334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5108, device='cuda:0')



h[100].sum tensor(-6.4560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-41.1077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0118, 0.0037, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0181, 0.0075, 0.0000],
        [0.0032, 0.0000, 0.0000,  ..., 0.0118, 0.0038, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63995.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0151, 0.0000,  ..., 0.0927, 0.0000, 0.0089],
        [0.0000, 0.0182, 0.0000,  ..., 0.1031, 0.0000, 0.0108],
        [0.0000, 0.0175, 0.0000,  ..., 0.1000, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471439.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.7973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-201.1104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-86.8776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1166],
        [ 0.1163],
        [ 0.1148],
        ...,
        [-0.3616],
        [-0.3602],
        [-0.3597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109016.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0003],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366875.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0003],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366888.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00],
        [-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00],
        [-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00],
        ...,
        [-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00],
        [-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00],
        [-1.3704e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3585e-03,
         -5.7772e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1656.4802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0167, device='cuda:0')



h[100].sum tensor(-4.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-27.3646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51276.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0342, 0.0000,  ..., 0.1376, 0.0000, 0.0219],
        [0.0000, 0.0190, 0.0000,  ..., 0.0922, 0.0000, 0.0120],
        [0.0000, 0.0119, 0.0000,  ..., 0.0725, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414510.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.7261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-177.7364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-116.0024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0485],
        [ 0.0426],
        [ 0.0407],
        ...,
        [-0.3640],
        [-0.3625],
        [-0.3610]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108738.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0003],
        ...,
        [1.0008],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366888.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0017],
        [1.0003],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366902.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4522e-02, -8.9595e-04, -1.6120e-03,  ...,  5.1028e-02,
          2.9765e-02, -2.5362e-02],
        [ 2.4402e-02, -6.4332e-04, -1.1575e-03,  ...,  3.7023e-02,
          2.1357e-02, -1.8211e-02],
        [ 1.2227e-02, -3.3942e-04, -6.1068e-04,  ...,  2.0176e-02,
          1.1241e-02, -9.6080e-03],
        ...,
        [-1.3700e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3604e-03,
         -5.6241e-05,  0.0000e+00],
        [-1.3700e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3604e-03,
         -5.6241e-05,  0.0000e+00],
        [-1.3700e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3604e-03,
         -5.6241e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1649.4739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9882, device='cuda:0')



h[100].sum tensor(-4.1892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-26.8763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1045, 0.0000, 0.0000,  ..., 0.1577, 0.0912, 0.0000],
        [0.0957, 0.0000, 0.0000,  ..., 0.1454, 0.0838, 0.0000],
        [0.0782, 0.0000, 0.0000,  ..., 0.1212, 0.0693, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49091.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2552, 0.0000,  ..., 0.7378, 0.0000, 0.1660],
        [0.0000, 0.2215, 0.0000,  ..., 0.6486, 0.0000, 0.1441],
        [0.0000, 0.1764, 0.0000,  ..., 0.5312, 0.0000, 0.1148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398434.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.8026, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-175.4041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.7256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0334],
        [ 0.0512],
        [ 0.0716],
        ...,
        [-0.3710],
        [-0.3696],
        [-0.3692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117821.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0017],
        [1.0003],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366902.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0004],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366915.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8946e-02, -9.8446e-04, -1.7838e-03,  ...,  5.7155e-02,
          3.3448e-02, -2.8449e-02],
        [ 1.0909e-02, -2.9990e-04, -5.4341e-04,  ...,  1.8358e-02,
          1.0153e-02, -8.6665e-03],
        [-1.3734e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3611e-03,
         -5.1928e-05,  0.0000e+00],
        ...,
        [-1.3734e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3611e-03,
         -5.1928e-05,  0.0000e+00],
        [-1.3734e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3611e-03,
         -5.1928e-05,  0.0000e+00],
        [-1.3734e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3611e-03,
         -5.1928e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1648.8121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9900, device='cuda:0')



h[100].sum tensor(-4.1333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-26.6186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0622, 0.0000, 0.0000,  ..., 0.0992, 0.0561, 0.0000],
        [0.0763, 0.0000, 0.0000,  ..., 0.1167, 0.0667, 0.0000],
        [0.0294, 0.0000, 0.0000,  ..., 0.0499, 0.0266, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50298.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1817, 0.0000,  ..., 0.5462, 0.0000, 0.1185],
        [0.0000, 0.1823, 0.0000,  ..., 0.5462, 0.0000, 0.1188],
        [0.0000, 0.1307, 0.0000,  ..., 0.4076, 0.0000, 0.0853],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410628.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.1761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-179.4790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.7392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0259],
        [ 0.0397],
        [ 0.0538],
        ...,
        [-0.3761],
        [-0.3747],
        [-0.3742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112356.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0004],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366915.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0004],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366929.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00],
        [-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00],
        [-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00],
        ...,
        [-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00],
        [-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00],
        [-1.3723e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3568e-03,
         -4.6355e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2194.0762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5344, device='cuda:0')



h[100].sum tensor(-6.3128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0160, device='cuda:0')



h[200].sum tensor(-40.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64719.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486765.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.8456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-207.8533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-85.9529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4640],
        [-0.4858],
        [-0.4955],
        ...,
        [-0.3778],
        [-0.3763],
        [-0.3758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98541.6953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0004],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366929.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0004],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366942.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3240e-02, -3.4070e-04, -6.2625e-04,  ...,  2.1540e-02,
          1.2071e-02, -1.0265e-02],
        [ 2.9058e-02, -7.0994e-04, -1.3050e-03,  ...,  4.3421e-02,
          2.5209e-02, -2.1390e-02],
        [ 2.2945e-02, -5.6725e-04, -1.0427e-03,  ...,  3.4965e-02,
          2.0132e-02, -1.7091e-02],
        ...,
        [-1.3546e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3496e-03,
         -5.0780e-05,  0.0000e+00],
        [-1.3546e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3496e-03,
         -5.0780e-05,  0.0000e+00],
        [-1.3546e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3496e-03,
         -5.0780e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2060.2778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3545, device='cuda:0')



h[100].sum tensor(-5.6730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-36.8132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0694, 0.0000, 0.0000,  ..., 0.1089, 0.0619, 0.0000],
        [0.0741, 0.0000, 0.0000,  ..., 0.1154, 0.0658, 0.0000],
        [0.0928, 0.0000, 0.0000,  ..., 0.1414, 0.0814, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61550.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1483, 0.0000,  ..., 0.4517, 0.0000, 0.0963],
        [0.0000, 0.1831, 0.0000,  ..., 0.5470, 0.0000, 0.1188],
        [0.0000, 0.2084, 0.0000,  ..., 0.6150, 0.0000, 0.1352],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465515.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-201.9387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-97.8826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0113],
        [-0.0017],
        [-0.0180],
        ...,
        [-0.3840],
        [-0.3825],
        [-0.3821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98269.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0004],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366942.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366955.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3404e-03, -1.3005e-04, -2.4079e-04,  ...,  9.2244e-03,
          4.6869e-03, -4.0019e-03],
        [ 1.7523e-02, -4.3092e-04, -7.9787e-04,  ...,  2.7461e-02,
          1.5636e-02, -1.3260e-02],
        [-1.3578e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3420e-03,
         -4.5707e-05,  0.0000e+00],
        ...,
        [-1.3578e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3420e-03,
         -4.5707e-05,  0.0000e+00],
        [-1.3578e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3420e-03,
         -4.5707e-05,  0.0000e+00],
        [-1.3578e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3420e-03,
         -4.5707e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2466.5188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7413, device='cuda:0')



h[100].sum tensor(-7.2273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0182, device='cuda:0')



h[200].sum tensor(-47.0791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0552, 0.0000, 0.0000,  ..., 0.0892, 0.0502, 0.0000],
        [0.0208, 0.0000, 0.0000,  ..., 0.0398, 0.0205, 0.0000],
        [0.0354, 0.0000, 0.0000,  ..., 0.0618, 0.0337, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70457.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1418, 0.0000,  ..., 0.4401, 0.0000, 0.0920],
        [0.0000, 0.1068, 0.0000,  ..., 0.3457, 0.0000, 0.0694],
        [0.0000, 0.0925, 0.0000,  ..., 0.3059, 0.0000, 0.0602],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505395.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.6002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.9290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-76.6624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0341],
        [ 0.0322],
        [ 0.0275],
        ...,
        [-0.3867],
        [-0.3852],
        [-0.3848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96609.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0018],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366955.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0019],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366968.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3541e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3352e-03,
         -4.2885e-05,  0.0000e+00],
        [-1.3541e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3352e-03,
         -4.2885e-05,  0.0000e+00],
        [ 4.7126e-03, -1.3534e-04, -2.5245e-04,  ...,  9.7269e-03,
          4.9953e-03, -4.2543e-03],
        ...,
        [-1.3541e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3352e-03,
         -4.2885e-05,  0.0000e+00],
        [-1.3541e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3352e-03,
         -4.2885e-05,  0.0000e+00],
        [-1.3541e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3352e-03,
         -4.2885e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2381.2246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6304, device='cuda:0')



h[100].sum tensor(-6.7794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0170, device='cuda:0')



h[200].sum tensor(-44.3312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0138, 0.0050, 0.0000],
        [0.0178, 0.0000, 0.0000,  ..., 0.0337, 0.0170, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69859.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0567, 0.0000, 0.0054],
        [0.0000, 0.0251, 0.0000,  ..., 0.1155, 0.0000, 0.0165],
        [0.0000, 0.0516, 0.0000,  ..., 0.1947, 0.0000, 0.0336],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509897.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.0363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.1703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-78.1796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0908],
        [ 0.0014],
        [ 0.0582],
        ...,
        [-0.3896],
        [-0.3882],
        [-0.3877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101385.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0019],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366968.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(1068.8383, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366981.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00],
        [-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00],
        [-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00],
        ...,
        [-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00],
        [-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00],
        [-1.3556e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3282e-03,
         -3.9178e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1847.3484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0907, device='cuda:0')



h[100].sum tensor(-4.5233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-29.6917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57459.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0380, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0002],
        [0.0000, 0.0057, 0.0000,  ..., 0.0542, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464676.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(385.3108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.2753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-108.1048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0138],
        [-0.0194],
        [-0.0049],
        ...,
        [-0.3922],
        [-0.3907],
        [-0.3903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98741.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366981.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366995.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00],
        [-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00],
        [-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00],
        ...,
        [-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00],
        [-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00],
        [-1.3629e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3229e-03,
         -3.2221e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1723.3535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9694, device='cuda:0')



h[100].sum tensor(-3.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-25.8467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49610.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0634, 0.0000, 0.0080],
        [0.0000, 0.0031, 0.0000,  ..., 0.0480, 0.0000, 0.0035],
        [0.0000, 0.0137, 0.0000,  ..., 0.0803, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(404375.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2.4318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.7260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-179.5158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-121.6970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0306],
        [-0.0211],
        [ 0.0043],
        ...,
        [-0.3914],
        [-0.3893],
        [-0.3839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113980.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366995.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367008.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2466e-03, -1.3790e-04, -2.6309e-04,  ...,  1.0482e-02,
          5.4783e-03, -4.6246e-03],
        [ 3.8907e-03, -1.0967e-04, -2.0922e-04,  ...,  8.6065e-03,
          4.3521e-03, -3.6778e-03],
        [ 1.0513e-02, -2.4758e-04, -4.7231e-04,  ...,  1.7768e-02,
          9.8529e-03, -8.3024e-03],
        ...,
        [-1.3761e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
         -2.2453e-05,  0.0000e+00],
        [-1.3761e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
         -2.2453e-05,  0.0000e+00],
        [-1.3761e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3203e-03,
         -2.2453e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1589.6913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8135, device='cuda:0')



h[100].sum tensor(-3.2946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-21.7939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1128, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0000, 0.0000,  ..., 0.0326, 0.0164, 0.0000],
        [0.0377, 0.0000, 0.0000,  ..., 0.0651, 0.0358, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0326, 0.0163, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46234.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0490, 0.0000,  ..., 0.1910, 0.0000, 0.0335],
        [0.0000, 0.0729, 0.0000,  ..., 0.2594, 0.0000, 0.0485],
        [0.0000, 0.0536, 0.0000,  ..., 0.2060, 0.0000, 0.0362],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394340.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(21.3708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.7211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-172.6140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-129.2468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0910],
        [ 0.0985],
        [ 0.0856],
        ...,
        [-0.3868],
        [-0.3779],
        [-0.3517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104142.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367008.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367022.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3928e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3184e-03,
         -1.2590e-05,  0.0000e+00],
        [-1.3928e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3184e-03,
         -1.2590e-05,  0.0000e+00],
        [ 5.8777e-03, -1.4792e-04, -2.8436e-04,  ...,  1.1377e-02,
          6.0270e-03, -5.0701e-03],
        ...,
        [-1.3928e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3184e-03,
         -1.2590e-05,  0.0000e+00],
        [-1.3928e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3184e-03,
         -1.2590e-05,  0.0000e+00],
        [-1.3928e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3184e-03,
         -1.2590e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2085.7297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3015, device='cuda:0')



h[100].sum tensor(-5.1927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-34.4828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0200, 0.0000, 0.0000,  ..., 0.0348, 0.0177, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0154, 0.0060, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0227, 0.0104, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59065.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0590, 0.0000,  ..., 0.2071, 0.0000, 0.0414],
        [0.0000, 0.0333, 0.0000,  ..., 0.1413, 0.0000, 0.0248],
        [0.0000, 0.0410, 0.0000,  ..., 0.1672, 0.0000, 0.0294],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456476.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(54.1569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-199.2534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-95.6726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0439],
        [ 0.0606],
        [ 0.0810],
        ...,
        [-0.3954],
        [-0.3940],
        [-0.3936]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107473.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367022.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367035.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4059e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3158e-03,
         -5.5906e-06,  0.0000e+00],
        [-1.4059e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3158e-03,
         -5.5906e-06,  0.0000e+00],
        [ 6.1854e-03, -1.5087e-04, -2.9229e-04,  ...,  1.1819e-02,
          6.3011e-03, -5.2865e-03],
        ...,
        [-1.4059e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3158e-03,
         -5.5906e-06,  0.0000e+00],
        [-1.4059e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3158e-03,
         -5.5906e-06,  0.0000e+00],
        [-1.4059e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3158e-03,
         -5.5906e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2193.5659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3851, device='cuda:0')



h[100].sum tensor(-5.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-37.1596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0158, 0.0063, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0139, 0.0052, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64094.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.4863e-03, 0.0000e+00,  ..., 4.4152e-02, 0.0000e+00,
         3.2814e-03],
        [0.0000e+00, 9.5396e-03, 0.0000e+00,  ..., 7.7095e-02, 0.0000e+00,
         9.7221e-03],
        [0.0000e+00, 1.7187e-02, 0.0000e+00,  ..., 1.0350e-01, 0.0000e+00,
         1.4911e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5906e-02, 0.0000e+00,
         1.4392e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5906e-02, 0.0000e+00,
         1.4398e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5905e-02, 0.0000e+00,
         1.4372e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491522.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(78.6879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.4534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-211.2113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-84.4608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1401],
        [-0.0155],
        [ 0.0742],
        ...,
        [-0.4020],
        [-0.4006],
        [-0.4002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101839.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367035.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367047.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00],
        [-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00],
        [-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00],
        ...,
        [-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00],
        [-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00],
        [-1.4001e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3160e-03,
         -7.4278e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2482.9790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6466, device='cuda:0')



h[100].sum tensor(-6.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0172, device='cuda:0')



h[200].sum tensor(-44.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69517.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4830e-02, 0.0000e+00,
         2.2865e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4862e-02, 0.0000e+00,
         2.2963e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4934e-02, 0.0000e+00,
         3.1072e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5100e-02, 0.0000e+00,
         1.7822e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5100e-02, 0.0000e+00,
         1.7831e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5099e-02, 0.0000e+00,
         1.7807e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508002., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(66.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.4843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13.2073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.6798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-75.7379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5413],
        [-0.5296],
        [-0.5026],
        ...,
        [-0.4093],
        [-0.4086],
        [-0.4084]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102446.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0007],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367047.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0008],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367060.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8485e-03, -1.1832e-04, -2.3286e-04,  ...,  9.9545e-03,
          5.1771e-03, -4.3349e-03],
        [-1.3945e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3163e-03,
         -9.2981e-06,  0.0000e+00],
        [-1.3945e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3163e-03,
         -9.2981e-06,  0.0000e+00],
        ...,
        [-1.3945e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3163e-03,
         -9.2981e-06,  0.0000e+00],
        [-1.3945e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3163e-03,
         -9.2981e-06,  0.0000e+00],
        [-1.3945e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3163e-03,
         -9.2981e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1867.4137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0766, device='cuda:0')



h[100].sum tensor(-4.2774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-28.7373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0341, 0.0173, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0139, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53230.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.2754e-02, 0.0000e+00,  ..., 1.9942e-01, 0.0000e+00,
         3.6957e-02],
        [0.0000e+00, 2.4836e-02, 0.0000e+00,  ..., 1.1600e-01, 0.0000e+00,
         1.8938e-02],
        [0.0000e+00, 7.1964e-03, 0.0000e+00,  ..., 5.9972e-02, 0.0000e+00,
         7.4229e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4225e-02, 0.0000e+00,
         3.3974e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4225e-02, 0.0000e+00,
         3.3988e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4225e-02, 0.0000e+00,
         3.3965e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429050.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(69.9019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.5664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41.0780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-197.2197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-115.8962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0569],
        [-0.0164],
        [-0.1181],
        ...,
        [-0.4215],
        [-0.4200],
        [-0.4195]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124883.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0008],
        ...,
        [1.0008],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367060.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367073.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4885e-02, -6.7146e-04, -1.3320e-03,  ...,  5.1527e-02,
          3.0141e-02, -2.5160e-02],
        [-1.3999e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3169e-03,
         -5.2102e-06,  0.0000e+00],
        [ 2.0105e-02, -3.9795e-04, -7.8944e-04,  ...,  3.1074e-02,
          1.7861e-02, -1.4911e-02],
        ...,
        [-1.3999e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3169e-03,
         -5.2102e-06,  0.0000e+00],
        [-1.3999e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3169e-03,
         -5.2102e-06,  0.0000e+00],
        [-1.3999e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3169e-03,
         -5.2102e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2031.2667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2110, device='cuda:0')



h[100].sum tensor(-4.8539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-32.7385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0597, 0.0000, 0.0000,  ..., 0.0937, 0.0531, 0.0000],
        [0.0875, 0.0000, 0.0000,  ..., 0.1341, 0.0773, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0296, 0.0146, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57959.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0126e-01, 0.0000e+00,  ..., 5.8628e-01, 0.0000e+00,
         1.3228e-01],
        [0.0000e+00, 1.6091e-01, 0.0000e+00,  ..., 4.7829e-01, 0.0000e+00,
         1.0670e-01],
        [0.0000e+00, 7.0523e-02, 0.0000e+00,  ..., 2.3531e-01, 0.0000e+00,
         4.9347e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4383e-02, 0.0000e+00,
         1.2956e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4384e-02, 0.0000e+00,
         1.2958e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4383e-02, 0.0000e+00,
         1.2955e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459173.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(63.3749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.7175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(62.6000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-207.7906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-102.3745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0005],
        [-0.0080],
        [-0.0405],
        ...,
        [-0.4253],
        [-0.4237],
        [-0.4232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132530.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0020],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367073.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367085.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4050e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3222e-03,
         -1.0082e-06,  0.0000e+00],
        [-1.4050e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3222e-03,
         -1.0082e-06,  0.0000e+00],
        [ 8.7670e-03, -1.8376e-04, -3.6749e-04,  ...,  1.5399e-02,
          8.4509e-03, -7.0436e-03],
        ...,
        [-1.4050e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3222e-03,
         -1.0082e-06,  0.0000e+00],
        [-1.4050e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3222e-03,
         -1.0082e-06,  0.0000e+00],
        [-1.4050e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3222e-03,
         -1.0082e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2265.3599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4074, device='cuda:0')



h[100].sum tensor(-5.6648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-38.3578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0194, 0.0085, 0.0000],
        [0.0326, 0.0000, 0.0000,  ..., 0.0543, 0.0294, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66313.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7656e-02, 0.0000e+00,  ..., 9.0693e-02, 0.0000e+00,
         1.4505e-02],
        [0.0000e+00, 6.3375e-02, 0.0000e+00,  ..., 2.1966e-01, 0.0000e+00,
         4.4355e-02],
        [0.0000e+00, 1.4505e-01, 0.0000e+00,  ..., 4.3932e-01, 0.0000e+00,
         9.6438e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4826e-02, 0.0000e+00,
         1.2589e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4826e-02, 0.0000e+00,
         1.2591e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4826e-02, 0.0000e+00,
         1.2588e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501224.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38.4484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(420.5701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61.2585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.5397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-82.1378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0316],
        [-0.0263],
        [-0.0384],
        ...,
        [-0.4273],
        [-0.4257],
        [-0.4252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121107.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367085.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367098.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00],
        [-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00],
        [-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00],
        ...,
        [-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00],
        [-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00],
        [-1.4155e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3265e-03,
          5.0688e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1615.7681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7931, device='cuda:0')



h[100].sum tensor(-3.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-21.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3103e-03, 2.0292e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3173e-03, 2.0319e-05,
         0.0000e+00],
        [2.0642e-02, 0.0000e+00, 0.0000e+00,  ..., 3.5848e-02, 1.8352e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3639e-03, 2.0497e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3640e-03, 2.0497e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3639e-03, 2.0497e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46045.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.5209e-03, 0.0000e+00,  ..., 3.4779e-02, 0.0000e+00,
         2.3054e-03],
        [0.0000e+00, 1.1198e-02, 0.0000e+00,  ..., 6.6263e-02, 0.0000e+00,
         9.5899e-03],
        [0.0000e+00, 4.4205e-02, 0.0000e+00,  ..., 1.6610e-01, 0.0000e+00,
         3.2834e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5642e-02, 0.0000e+00,
         1.4565e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5642e-02, 0.0000e+00,
         1.4567e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5642e-02, 0.0000e+00,
         1.4563e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398142.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(22.9191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.9234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66.6181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.5418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.6342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3354],
        [-0.1877],
        [-0.0500],
        ...,
        [-0.3757],
        [-0.4125],
        [-0.4215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129798.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367098.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(528.4738, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367110.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4287e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3326e-03,
          1.4716e-05,  0.0000e+00],
        [-1.4287e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3326e-03,
          1.4716e-05,  0.0000e+00],
        [ 4.7431e-03, -1.0622e-04, -2.1591e-04,  ...,  9.8759e-03,
          5.1444e-03, -4.2622e-03],
        ...,
        [-1.4287e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3326e-03,
          1.4716e-05,  0.0000e+00],
        [-1.4287e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3326e-03,
          1.4716e-05,  0.0000e+00],
        [-1.4287e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3326e-03,
          1.4716e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1692.7551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.7855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8435, device='cuda:0')



h[100].sum tensor(-3.3104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-22.5919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3349e-03, 5.8916e-05,
         0.0000e+00],
        [4.7543e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3905e-02, 5.2008e-03,
         0.0000e+00],
        [3.6196e-03, 0.0000e+00, 0.0000e+00,  ..., 1.2335e-02, 4.2579e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3894e-03, 5.9518e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3894e-03, 5.9518e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3894e-03, 5.9518e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47679.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0381, 0.0000, 0.0025],
        [0.0000, 0.0055, 0.0000,  ..., 0.0643, 0.0000, 0.0075],
        [0.0000, 0.0084, 0.0000,  ..., 0.0778, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407360.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7.5114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.9517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61.4562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-184.3386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.9647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2634],
        [-0.1608],
        [-0.0395],
        ...,
        [-0.4151],
        [-0.3959],
        [-0.3690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118934.7891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367110.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367123.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2596e-03, -1.1240e-04, -2.3038e-04,  ...,  1.0607e-02,
          5.5801e-03, -4.6160e-03],
        [-1.4337e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3408e-03,
          1.6428e-05,  0.0000e+00],
        [ 5.2596e-03, -1.1240e-04, -2.3038e-04,  ...,  1.0607e-02,
          5.5801e-03, -4.6160e-03],
        ...,
        [-1.4337e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3408e-03,
          1.6428e-05,  0.0000e+00],
        [-1.4337e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3408e-03,
          1.6428e-05,  0.0000e+00],
        [-1.4337e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3408e-03,
          1.6428e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2465.5537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5908, device='cuda:0')



h[100].sum tensor(-6.1449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-42.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7743e-02, 0.0000e+00, 0.0000e+00,  ..., 3.3903e-02, 1.7199e-02,
         0.0000e+00],
        [2.8419e-02, 0.0000e+00, 0.0000e+00,  ..., 5.2674e-02, 2.8466e-02,
         0.0000e+00],
        [8.9390e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9739e-02, 8.6906e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4234e-03, 6.6448e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4234e-03, 6.6449e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4234e-03, 6.6449e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70583.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0568, 0.0000,  ..., 0.2190, 0.0000, 0.0403],
        [0.0000, 0.0560, 0.0000,  ..., 0.2189, 0.0000, 0.0395],
        [0.0000, 0.0303, 0.0000,  ..., 0.1430, 0.0000, 0.0238],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525098.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1.9438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.7892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40.1549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-228.6759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-68.1762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1377],
        [ 0.1279],
        [ 0.0917],
        ...,
        [-0.4275],
        [-0.4259],
        [-0.4254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84656.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367123.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0010],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367135.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00],
        [-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00],
        [-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00],
        ...,
        [-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00],
        [-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00],
        [-1.4135e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3570e-03,
         -4.8493e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2171.7700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2886, device='cuda:0')



h[100].sum tensor(-5.0023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-34.4089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0133, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60128.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.7594e-04, 0.0000e+00,  ..., 4.7634e-02, 0.0000e+00,
         3.3621e-03],
        [0.0000e+00, 2.7084e-03, 0.0000e+00,  ..., 5.3931e-02, 0.0000e+00,
         5.1330e-03],
        [0.0000e+00, 1.3528e-02, 0.0000e+00,  ..., 8.6436e-02, 0.0000e+00,
         1.2203e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9244e-02, 0.0000e+00,
         7.2157e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5823e-02, 0.0000e+00,
         7.4413e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5823e-02, 0.0000e+00,
         7.4392e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466528.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5.8843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.8252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57.1883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.8536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-98.9755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0518],
        [-0.0481],
        [-0.0167],
        ...,
        [-0.4000],
        [-0.4266],
        [-0.4357]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123695.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0010],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367135.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0010],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367146.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3780e-03,
         -3.4223e-05,  0.0000e+00],
        [ 7.9991e-03, -1.5002e-04, -3.1271e-04,  ...,  1.4377e-02,
          7.7698e-03, -6.4562e-03],
        [ 6.6741e-03, -1.2885e-04, -2.6858e-04,  ...,  1.2543e-02,
          6.6686e-03, -5.5452e-03],
        ...,
        [-1.3909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3780e-03,
         -3.4223e-05,  0.0000e+00],
        [-1.3909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3780e-03,
         -3.4223e-05,  0.0000e+00],
        [-1.3909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3780e-03,
         -3.4223e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1612.6362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7430, device='cuda:0')



h[100].sum tensor(-2.8757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0077, device='cuda:0')



h[200].sum tensor(-19.8594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0080, 0.0000, 0.0000,  ..., 0.0185, 0.0078, 0.0000],
        [0.0196, 0.0000, 0.0000,  ..., 0.0365, 0.0185, 0.0000],
        [0.0497, 0.0000, 0.0000,  ..., 0.0821, 0.0458, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45294.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0443, 0.0000,  ..., 0.1704, 0.0000, 0.0312],
        [0.0000, 0.0688, 0.0000,  ..., 0.2387, 0.0000, 0.0463],
        [0.0000, 0.1123, 0.0000,  ..., 0.3587, 0.0000, 0.0732],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394040.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12.4600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.4001, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49.9325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.2214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.8662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0005],
        [ 0.0074],
        [ 0.0095],
        ...,
        [-0.4467],
        [-0.4450],
        [-0.4431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145027.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0010],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367146.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0010],
        ...,
        [1.0007],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367158.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0648e-02, -1.8750e-04, -3.9418e-04,  ...,  1.8056e-02,
          9.9488e-03, -8.2622e-03],
        [ 1.9042e-02, -3.1828e-04, -6.6912e-04,  ...,  2.9676e-02,
          1.6925e-02, -1.4025e-02],
        [ 1.9879e-02, -3.3133e-04, -6.9655e-04,  ...,  3.0835e-02,
          1.7621e-02, -1.4600e-02],
        ...,
        [-1.3855e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3958e-03,
         -5.2988e-05,  0.0000e+00],
        [-1.3855e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3958e-03,
         -5.2988e-05,  0.0000e+00],
        [-1.3855e-03,  0.0000e+00,  0.0000e+00,  ...,  1.3958e-03,
         -5.2988e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1860.2495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9718, device='cuda:0')



h[100].sum tensor(-3.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-25.6913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0580, 0.0000, 0.0000,  ..., 0.0936, 0.0526, 0.0000],
        [0.0792, 0.0000, 0.0000,  ..., 0.1230, 0.0702, 0.0000],
        [0.0694, 0.0000, 0.0000,  ..., 0.1093, 0.0621, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52656.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1800, 0.0000,  ..., 0.5407, 0.0000, 0.1145],
        [0.0000, 0.1933, 0.0000,  ..., 0.5727, 0.0000, 0.1233],
        [0.0000, 0.1615, 0.0000,  ..., 0.4868, 0.0000, 0.1036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0256, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435783.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7.3190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.7017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43.8296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-195.1295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-129.9481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0051],
        [-0.0023],
        [-0.0095],
        ...,
        [-0.4495],
        [-0.4479],
        [-0.4474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141256.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0010],
        ...,
        [1.0007],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367158.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367170.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4149e-02, -2.3616e-04, -5.0076e-04,  ...,  2.2933e-02,
          1.2856e-02, -1.0657e-02],
        [ 7.7308e-03, -1.3864e-04, -2.9399e-04,  ...,  1.4047e-02,
          7.5217e-03, -6.2564e-03],
        [ 3.0895e-02, -4.9061e-04, -1.0403e-03,  ...,  4.6121e-02,
          2.6777e-02, -2.2139e-02],
        ...,
        [-1.3939e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4122e-03,
         -6.3305e-05,  0.0000e+00],
        [-1.3939e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4122e-03,
         -6.3305e-05,  0.0000e+00],
        [-1.3939e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4122e-03,
         -6.3305e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1937.3558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0209, device='cuda:0')



h[100].sum tensor(-3.8949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-27.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0403, 0.0000, 0.0000,  ..., 0.0692, 0.0379, 0.0000],
        [0.0753, 0.0000, 0.0000,  ..., 0.1176, 0.0670, 0.0000],
        [0.0314, 0.0000, 0.0000,  ..., 0.0549, 0.0294, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53187.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0890, 0.0000,  ..., 0.2950, 0.0000, 0.0579],
        [0.0000, 0.1284, 0.0000,  ..., 0.3983, 0.0000, 0.0827],
        [0.0000, 0.1010, 0.0000,  ..., 0.3260, 0.0000, 0.0656],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431639.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0.5022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.6291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37.4846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-192.5491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-132.9752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0854],
        [-0.0524],
        [-0.0309],
        ...,
        [-0.4508],
        [-0.4492],
        [-0.4487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126139.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367170.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0011],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367182.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00],
        [-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00],
        [-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00],
        ...,
        [-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00],
        [-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00],
        [-1.4167e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4287e-03,
         -6.4641e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.3459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9314, device='cuda:0')



h[100].sum tensor(-3.5496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-24.8081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0000,  ..., 0.0158, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50788.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0157, 0.0000,  ..., 0.0982, 0.0000, 0.0120],
        [0.0000, 0.0037, 0.0000,  ..., 0.0540, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419711.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.2933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39.6727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.3483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.5349, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0127],
        [-0.0573],
        [-0.1269],
        ...,
        [-0.4480],
        [-0.4464],
        [-0.4459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113407., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0011],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367182.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0012],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367194.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6188e-03, -7.3155e-05, -1.5786e-04,  ...,  8.4621e-03,
          4.1523e-03, -3.4639e-03],
        [ 3.6188e-03, -7.3155e-05, -1.5786e-04,  ...,  8.4621e-03,
          4.1523e-03, -3.4639e-03],
        [-1.4458e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4462e-03,
         -6.0092e-05,  0.0000e+00],
        ...,
        [-1.4458e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4462e-03,
         -6.0092e-05,  0.0000e+00],
        [-1.4458e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4462e-03,
         -6.0092e-05,  0.0000e+00],
        [-1.4458e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4462e-03,
         -6.0092e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2841.3115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8368, device='cuda:0')



h[100].sum tensor(-6.9489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0192, device='cuda:0')



h[200].sum tensor(-48.7596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.0000, 0.0000,  ..., 0.0505, 0.0266, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0322, 0.0157, 0.0000],
        [0.0323, 0.0000, 0.0000,  ..., 0.0586, 0.0315, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79278.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0759, 0.0000,  ..., 0.2820, 0.0000, 0.0496],
        [0.0000, 0.0674, 0.0000,  ..., 0.2573, 0.0000, 0.0444],
        [0.0000, 0.0817, 0.0000,  ..., 0.2953, 0.0000, 0.0536],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0297, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0297, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0297, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570912.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.7537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45.6929, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.4399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-71.2388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1394],
        [ 0.1370],
        [ 0.1265],
        ...,
        [-0.4347],
        [-0.4332],
        [-0.4327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107628.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0012],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367194.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0012],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367206.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00],
        [-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00],
        [-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00],
        ...,
        [-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00],
        [-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00],
        [-1.4678e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4608e-03,
         -5.7007e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2231.4656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2452, device='cuda:0')



h[100].sum tensor(-4.6929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-33.0623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0210, 0.0000, 0.0000,  ..., 0.0370, 0.0186, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60339.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0640, 0.0000, 0.0060],
        [0.0000, 0.0184, 0.0000,  ..., 0.1016, 0.0000, 0.0149],
        [0.0000, 0.0548, 0.0000,  ..., 0.2079, 0.0000, 0.0389],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470048.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(385.3440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26.9117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-196.3219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.5105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0853],
        [ 0.0847],
        [ 0.0906],
        ...,
        [-0.3837],
        [-0.3972],
        [-0.4096]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91652.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0012],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367206.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0024],
        [1.0012],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367218.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4767e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4755e-03,
         -5.8985e-05,  0.0000e+00],
        [-1.4767e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4755e-03,
         -5.8985e-05,  0.0000e+00],
        [-1.4767e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4755e-03,
         -5.8985e-05,  0.0000e+00],
        ...,
        [ 1.2202e-02, -1.8766e-04, -4.1222e-04,  ...,  2.0429e-02,
          1.1321e-02, -9.3301e-03],
        [-1.4767e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4755e-03,
         -5.8985e-05,  0.0000e+00],
        [-1.4767e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4755e-03,
         -5.8985e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2513.0454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4910, device='cuda:0')



h[100].sum tensor(-5.6591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-40.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0179, 0.0000, 0.0000,  ..., 0.0369, 0.0184, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0329, 0.0161, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70586.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0420, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0636, 0.0000,  ..., 0.2463, 0.0000, 0.0439],
        [0.0000, 0.0442, 0.0000,  ..., 0.1878, 0.0000, 0.0322],
        [0.0000, 0.0126, 0.0000,  ..., 0.0833, 0.0000, 0.0104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532926.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(429.4504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-217.7868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-99.2129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1147],
        [-0.2128],
        [-0.2747],
        ...,
        [ 0.0729],
        [ 0.0025],
        [-0.1228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91578.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0024],
        [1.0012],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367218.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(534., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367229.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5934e-03, -6.7679e-05, -1.5001e-04,  ...,  8.5065e-03,
          4.1454e-03, -3.4487e-03],
        [ 5.0238e-03, -8.6797e-05, -1.9238e-04,  ...,  1.0488e-02,
          5.3354e-03, -4.4229e-03],
        [-1.4706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4901e-03,
         -6.7149e-05,  0.0000e+00],
        ...,
        [-1.4706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4901e-03,
         -6.7149e-05,  0.0000e+00],
        [-1.4706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4901e-03,
         -6.7149e-05,  0.0000e+00],
        [-1.4706e-03,  0.0000e+00,  0.0000e+00,  ...,  1.4901e-03,
         -6.7149e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2227.8298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2521, device='cuda:0')



h[100].sum tensor(-4.6356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-32.9220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0444, 0.0000, 0.0000,  ..., 0.0756, 0.0415, 0.0000],
        [0.0177, 0.0000, 0.0000,  ..., 0.0346, 0.0170, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0150, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59860.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0920, 0.0000,  ..., 0.3232, 0.0000, 0.0617],
        [0.0000, 0.0578, 0.0000,  ..., 0.2255, 0.0000, 0.0406],
        [0.0000, 0.0259, 0.0000,  ..., 0.1295, 0.0000, 0.0199],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470814.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.8272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32.7182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-198.6555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-132.1589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0967],
        [ 0.0712],
        [ 0.0058],
        ...,
        [-0.4541],
        [-0.4525],
        [-0.4520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105359.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367229.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0026],
        [1.0013],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367240.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4444e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5101e-03,
         -8.1315e-05,  0.0000e+00],
        [-1.4444e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5101e-03,
         -8.1315e-05,  0.0000e+00],
        [ 3.6301e-03, -6.6046e-05, -1.4773e-04,  ...,  8.5397e-03,
          4.1388e-03, -3.4501e-03],
        ...,
        [-1.4444e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5101e-03,
         -8.1315e-05,  0.0000e+00],
        [-1.4444e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5101e-03,
         -8.1315e-05,  0.0000e+00],
        [-1.4444e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5101e-03,
         -8.1315e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2584.4573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5728, device='cuda:0')



h[100].sum tensor(-5.8707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-41.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0118, 0.0033, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0357, 0.0175, 0.0000],
        [0.0326, 0.0000, 0.0000,  ..., 0.0572, 0.0305, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72043.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0215, 0.0000,  ..., 0.1207, 0.0000, 0.0168],
        [0.0000, 0.0533, 0.0000,  ..., 0.2163, 0.0000, 0.0367],
        [0.0000, 0.0806, 0.0000,  ..., 0.2917, 0.0000, 0.0535],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535301.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.8138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29.1525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.5076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-115.6849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0394],
        [ 0.0367],
        [ 0.0702],
        ...,
        [-0.4748],
        [-0.4731],
        [-0.4725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112402.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0026],
        [1.0013],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367240.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0026],
        [1.0014],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367251.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00],
        [-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00],
        [-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00],
        ...,
        [-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00],
        [-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00],
        [-1.4227e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5257e-03,
         -9.0454e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1958.2742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9914, device='cuda:0')



h[100].sum tensor(-3.6628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-26.2247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53799.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438748.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.4277, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20.9821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-192.3186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.7277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5888],
        [-0.5334],
        [-0.4525],
        ...,
        [-0.4829],
        [-0.4812],
        [-0.4807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147212.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0026],
        [1.0014],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367251.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0027],
        [1.0014],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367262.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5374e-03,
         -9.4745e-05,  0.0000e+00],
        [-1.4189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5374e-03,
         -9.4745e-05,  0.0000e+00],
        [ 4.7459e-03, -7.6073e-05, -1.7333e-04,  ...,  1.0076e-02,
          5.0307e-03, -4.1781e-03],
        ...,
        [-1.4189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5374e-03,
         -9.4745e-05,  0.0000e+00],
        [-1.4189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5374e-03,
         -9.4745e-05,  0.0000e+00],
        [-1.4189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5374e-03,
         -9.4745e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2201.7466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2222, device='cuda:0')



h[100].sum tensor(-4.4643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-32.0928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0147, 0.0050, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0189, 0.0076, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60662.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0470, 0.0000, 0.0028],
        [0.0000, 0.0143, 0.0000,  ..., 0.0941, 0.0000, 0.0110],
        [0.0000, 0.0274, 0.0000,  ..., 0.1378, 0.0000, 0.0196],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474818.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.1870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7.7859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-205.2467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.9341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3503],
        [-0.2034],
        [-0.0830],
        ...,
        [-0.4866],
        [-0.4848],
        [-0.4843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124813.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0027],
        [1.0014],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367262.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0028],
        [1.0015],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367273.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00],
        [-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00],
        [-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00],
        ...,
        [-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00],
        [-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00],
        [-1.4368e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5426e-03,
         -8.9609e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2117.2378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1282, device='cuda:0')



h[100].sum tensor(-4.1070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-29.6443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0222, 0.0094, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56083.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0367, 0.0000, 0.0007],
        [0.0000, 0.0061, 0.0000,  ..., 0.0626, 0.0000, 0.0051],
        [0.0000, 0.0236, 0.0000,  ..., 0.1273, 0.0000, 0.0167],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447961.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.0309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-193.4052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.2725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3524],
        [-0.2122],
        [-0.0779],
        ...,
        [-0.4771],
        [-0.4733],
        [-0.4718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125351.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0028],
        [1.0015],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367273.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0028],
        [1.0015],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367284.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.1495e-02, -3.8552e-04, -8.9506e-04,  ...,  4.7205e-02,
          2.7331e-02, -2.2277e-02],
        [ 6.7941e-03, -9.6571e-05, -2.2421e-04,  ...,  1.2983e-02,
          6.7855e-03, -5.5802e-03],
        [ 2.5323e-02, -3.1332e-04, -7.2744e-04,  ...,  3.8654e-02,
          2.2197e-02, -1.8105e-02],
        ...,
        [-1.4613e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5455e-03,
         -8.1037e-05,  0.0000e+00],
        [-1.4613e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5455e-03,
         -8.1037e-05,  0.0000e+00],
        [-1.4613e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5455e-03,
         -8.1037e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2862.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8022, device='cuda:0')



h[100].sum tensor(-6.5646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0188, device='cuda:0')



h[200].sum tensor(-47.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0533, 0.0000, 0.0000,  ..., 0.0882, 0.0489, 0.0000],
        [0.1273, 0.0000, 0.0000,  ..., 0.1907, 0.1104, 0.0000],
        [0.0786, 0.0000, 0.0000,  ..., 0.1232, 0.0699, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78232.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1950, 0.0000,  ..., 0.5856, 0.0000, 0.1220],
        [0.0000, 0.2706, 0.0000,  ..., 0.7802, 0.0000, 0.1690],
        [0.0000, 0.2458, 0.0000,  ..., 0.7163, 0.0000, 0.1538],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563486.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(478.9780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.7222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-109.9487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0368],
        [ 0.0233],
        [ 0.0116],
        ...,
        [-0.4659],
        [-0.4594],
        [-0.4538]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96163.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0028],
        [1.0015],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367284.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0029],
        [1.0016],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367296.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4809e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5476e-03,
         -7.1975e-05,  0.0000e+00],
        [ 1.7561e-02, -2.1682e-04, -5.0822e-04,  ...,  2.7934e-02,
          1.5770e-02, -1.2854e-02],
        [-1.4809e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5476e-03,
         -7.1975e-05,  0.0000e+00],
        ...,
        [-1.4809e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5476e-03,
         -7.1975e-05,  0.0000e+00],
        [-1.4809e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5476e-03,
         -7.1975e-05,  0.0000e+00],
        [-1.4809e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5476e-03,
         -7.1975e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2036.3311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0308, device='cuda:0')



h[100].sum tensor(-3.6967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-26.9017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0176, 0.0000, 0.0000,  ..., 0.0326, 0.0158, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0278, 0.0129, 0.0000],
        [0.0634, 0.0000, 0.0000,  ..., 0.1023, 0.0574, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54811.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0320, 0.0000,  ..., 0.1440, 0.0000, 0.0228],
        [0.0000, 0.0435, 0.0000,  ..., 0.1761, 0.0000, 0.0298],
        [0.0000, 0.0812, 0.0000,  ..., 0.2793, 0.0000, 0.0529],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447440.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.1861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-184.4636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.9328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1686],
        [-0.0947],
        [-0.0354],
        ...,
        [-0.4606],
        [-0.4591],
        [-0.4587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105670.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0029],
        [1.0016],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367296.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0029],
        [1.0016],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367307.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00],
        [-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00],
        [-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00],
        ...,
        [-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00],
        [-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00],
        [-1.4949e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5531e-03,
         -6.3742e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2079.8311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0486, device='cuda:0')



h[100].sum tensor(-3.8000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-27.7667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54874.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0327, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448456.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.5130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.1334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3115],
        [-0.3359],
        [-0.3298],
        ...,
        [-0.4369],
        [-0.4503],
        [-0.4528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94131.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0029],
        [1.0016],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367307.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0030],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367318.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4992e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5596e-03,
         -6.0326e-05,  0.0000e+00],
        [-1.4992e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5596e-03,
         -6.0326e-05,  0.0000e+00],
        [ 3.5780e-03, -5.4739e-05, -1.3081e-04,  ...,  8.5964e-03,
          4.1647e-03, -3.4179e-03],
        ...,
        [-1.4992e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5596e-03,
         -6.0326e-05,  0.0000e+00],
        [-1.4992e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5596e-03,
         -6.0326e-05,  0.0000e+00],
        [-1.4992e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5596e-03,
         -6.0326e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1977.1498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9683, device='cuda:0')



h[100].sum tensor(-3.4276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-25.1486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0191, 0.0076, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0245, 0.0108, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51455.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0611, 0.0000, 0.0039],
        [0.0000, 0.0191, 0.0000,  ..., 0.1172, 0.0000, 0.0133],
        [0.0000, 0.0301, 0.0000,  ..., 0.1546, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427451.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(376.1378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-177.1293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.9461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0887],
        [ 0.0332],
        [ 0.1104],
        ...,
        [-0.4578],
        [-0.4564],
        [-0.4559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115916.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0030],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367318.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0030],
        [1.0017],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367330.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5020e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5670e-03,
         -5.7399e-05,  0.0000e+00],
        [-1.5020e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5670e-03,
         -5.7399e-05,  0.0000e+00],
        [ 4.6128e-03, -6.4131e-05, -1.5476e-04,  ...,  1.0042e-02,
          5.0315e-03, -4.1105e-03],
        ...,
        [-1.5020e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5670e-03,
         -5.7399e-05,  0.0000e+00],
        [ 1.4877e-02, -1.7178e-04, -4.1454e-04,  ...,  2.4269e-02,
          1.3573e-02, -1.1010e-02],
        [-1.5020e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5670e-03,
         -5.7399e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2359.7808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3181, device='cuda:0')



h[100].sum tensor(-4.6771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-34.4580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0046, 0.0000, 0.0000,  ..., 0.0148, 0.0050, 0.0000],
        [0.0174, 0.0000, 0.0000,  ..., 0.0346, 0.0169, 0.0000],
        ...,
        [0.0151, 0.0000, 0.0000,  ..., 0.0294, 0.0138, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0251, 0.0112, 0.0000],
        [0.0542, 0.0000, 0.0000,  ..., 0.0900, 0.0500, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64014.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0123, 0.0000,  ..., 0.0908, 0.0000, 0.0095],
        [0.0000, 0.0266, 0.0000,  ..., 0.1326, 0.0000, 0.0176],
        [0.0000, 0.0497, 0.0000,  ..., 0.2036, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0273, 0.0000,  ..., 0.1323, 0.0000, 0.0194],
        [0.0000, 0.0374, 0.0000,  ..., 0.1609, 0.0000, 0.0255],
        [0.0000, 0.0706, 0.0000,  ..., 0.2529, 0.0000, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502188.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(428.2384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-202.3991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.5828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1163],
        [ 0.1323],
        [ 0.1477],
        ...,
        [-0.1500],
        [-0.0541],
        [-0.0089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78778.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0030],
        [1.0017],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367330.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(566.8060, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0031],
        [1.0018],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367340.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00],
        [-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00],
        [-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00],
        ...,
        [-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00],
        [-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00],
        [-1.4916e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5797e-03,
         -6.5945e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2105.6819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0853, device='cuda:0')



h[100].sum tensor(-3.8406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-28.4119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58502.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.2080e-05, 0.0000e+00,  ..., 3.8758e-02, 0.0000e+00,
         6.9318e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1408e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0995e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1333e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1333e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.1332e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474962.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.1024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.4091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.9759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3382],
        [-0.4626],
        [-0.5480],
        ...,
        [-0.4854],
        [-0.4838],
        [-0.4833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117573.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0031],
        [1.0018],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367340.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0032],
        [1.0018],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367350.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00],
        [-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00],
        [-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00],
        ...,
        [-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00],
        [-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00],
        [-1.4745e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5948e-03,
         -7.9531e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1932.2742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9228, device='cuda:0')



h[100].sum tensor(-3.2732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-24.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0149, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0235, 0.0102, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53284.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0727, 0.0000, 0.0045],
        [0.0000, 0.0040, 0.0000,  ..., 0.0646, 0.0000, 0.0035],
        [0.0000, 0.0190, 0.0000,  ..., 0.1068, 0.0000, 0.0128],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448147.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.6677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.6731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.8788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3913],
        [-0.3986],
        [-0.3599],
        ...,
        [-0.5054],
        [-0.5037],
        [-0.5031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126283.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0032],
        [1.0018],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367350.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0033],
        [1.0019],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367361.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00],
        [-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00],
        [-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00],
        ...,
        [-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00],
        [-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00],
        [-1.4625e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.6828e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2296.6042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2589, device='cuda:0')



h[100].sum tensor(-4.4467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-33.1696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60585.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0489, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0298, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478591., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(430.1741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.3405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.4398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3883],
        [-0.2994],
        [-0.1700],
        ...,
        [-0.5158],
        [-0.5141],
        [-0.5136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108984.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0033],
        [1.0019],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367361.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0033],
        [1.0020],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367372.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00],
        [-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00],
        [-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00],
        ...,
        [-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00],
        [-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00],
        [-1.4517e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6097e-03,
         -9.6360e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2029.9958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0209, device='cuda:0')



h[100].sum tensor(-3.5654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-26.7063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0490, 0.0000, 0.0000,  ..., 0.0804, 0.0441, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0185, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54016.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1265, 0.0000,  ..., 0.3935, 0.0000, 0.0763],
        [0.0000, 0.0740, 0.0000,  ..., 0.2528, 0.0000, 0.0447],
        [0.0000, 0.0511, 0.0000,  ..., 0.1899, 0.0000, 0.0310],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446470.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.8131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-193.5952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.2085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0237],
        [ 0.0225],
        [ 0.0180],
        ...,
        [-0.5231],
        [-0.5214],
        [-0.5208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137746.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0033],
        [1.0020],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367372.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0034],
        [1.0020],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367383.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00],
        [-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00],
        [-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00],
        ...,
        [-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00],
        [-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00],
        [-1.4506e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6098e-03,
         -9.8055e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2051.0044, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0336, device='cuda:0')



h[100].sum tensor(-3.5854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-26.9682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55868.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0465, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0333, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0292, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461229.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.6093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-197.2533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.9926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3824],
        [-0.5215],
        [-0.6221],
        ...,
        [-0.5272],
        [-0.5255],
        [-0.5249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133350.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0034],
        [1.0020],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367383.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0035],
        [1.0021],
        ...,
        [1.0008],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367395.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00],
        [-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00],
        [-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00],
        ...,
        [-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00],
        [-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00],
        [-1.4606e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6064e-03,
         -9.3558e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2106.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0672, device='cuda:0')



h[100].sum tensor(-3.7011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-27.9545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55421.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0834, 0.0000, 0.0071],
        [0.0000, 0.0075, 0.0000,  ..., 0.0687, 0.0000, 0.0047],
        [0.0000, 0.0033, 0.0000,  ..., 0.0535, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(454192.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.9325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-195.0784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.1719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0070],
        [-0.0057],
        [-0.0273],
        ...,
        [-0.5289],
        [-0.5273],
        [-0.5270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125728.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0035],
        [1.0021],
        ...,
        [1.0008],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367395.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0035],
        [1.0022],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367407.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00],
        [-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00],
        [-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00],
        ...,
        [-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00],
        [-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00],
        [-1.4749e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6018e-03,
         -8.4817e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2101.3889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.2373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0400, device='cuda:0')



h[100].sum tensor(-3.5953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-27.2697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54383.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0316, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448454.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.2470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.6177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.3277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4955],
        [-0.5162],
        [-0.5257],
        ...,
        [-0.5286],
        [-0.5269],
        [-0.5263]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142155.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0035],
        [1.0022],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367407.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0036],
        [1.0022],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367420.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00],
        [-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00],
        [-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00],
        ...,
        [-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00],
        [-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00],
        [-1.4880e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5988e-03,
         -7.3509e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2121.8723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0381, device='cuda:0')



h[100].sum tensor(-3.5750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-27.2292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55799.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0003],
        [0.0000, 0.0067, 0.0000,  ..., 0.0688, 0.0000, 0.0054],
        [0.0000, 0.0169, 0.0000,  ..., 0.1017, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(462111.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.3215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.3413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.1912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0377],
        [ 0.0161],
        [ 0.0605],
        ...,
        [-0.5233],
        [-0.5217],
        [-0.5212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111018.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0036],
        [1.0022],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367420.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0037],
        [1.0023],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367432.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00],
        [-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00],
        [-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00],
        ...,
        [-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00],
        [-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00],
        [-1.4884e-03,  0.0000e+00,  0.0000e+00,  ...,  1.5996e-03,
         -6.9468e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2135.2158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0487, device='cuda:0')



h[100].sum tensor(-3.5468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-27.1283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55103.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0398, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0434, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0466, 0.0000, 0.0012],
        [0.0000, 0.0064, 0.0000,  ..., 0.0711, 0.0000, 0.0056],
        [0.0000, 0.0095, 0.0000,  ..., 0.0834, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452939.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.9565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.0303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.7586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3136],
        [-0.2659],
        [-0.1876],
        ...,
        [-0.3333],
        [-0.2247],
        [-0.1566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119529.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0037],
        [1.0023],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367432.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0038],
        [1.0024],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367443.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4806e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6023e-03,
         -7.1901e-05,  0.0000e+00],
        [-1.4806e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6023e-03,
         -7.1901e-05,  0.0000e+00],
        [ 1.3903e-02, -1.2120e-04, -3.2416e-04,  ...,  2.2930e-02,
          1.2734e-02, -1.0190e-02],
        ...,
        [-1.4806e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6023e-03,
         -7.1901e-05,  0.0000e+00],
        [-1.4806e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6023e-03,
         -7.1901e-05,  0.0000e+00],
        [-1.4806e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6023e-03,
         -7.1901e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3004.2622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8250, device='cuda:0')



h[100].sum tensor(-6.1831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0190, device='cuda:0')



h[200].sum tensor(-47.4927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0251, 0.0000, 0.0000,  ..., 0.0453, 0.0232, 0.0000],
        [0.0251, 0.0000, 0.0000,  ..., 0.0453, 0.0232, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76010.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0268, 0.0000,  ..., 0.1362, 0.0000, 0.0186],
        [0.0000, 0.0619, 0.0000,  ..., 0.2308, 0.0000, 0.0399],
        [0.0000, 0.0810, 0.0000,  ..., 0.2822, 0.0000, 0.0515],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558107.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.8653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-228.5578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-129.5899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1358],
        [ 0.1314],
        [ 0.1298],
        ...,
        [-0.5261],
        [-0.5243],
        [-0.5238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101703.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0038],
        [1.0024],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367443.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(574.0447, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0039],
        [1.0024],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367455.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6726e-03, -3.9255e-05, -1.0614e-04,  ...,  8.7232e-03,
          4.1913e-03, -3.3948e-03],
        [ 2.1072e-02, -1.7230e-04, -4.6586e-04,  ...,  3.2840e-02,
          1.8671e-02, -1.4901e-02],
        [ 3.3505e-02, -2.6738e-04, -7.2293e-04,  ...,  5.0075e-02,
          2.9018e-02, -2.3123e-02],
        ...,
        [-1.4610e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6076e-03,
         -8.0875e-05,  0.0000e+00],
        [-1.4610e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6076e-03,
         -8.0875e-05,  0.0000e+00],
        [-1.4610e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6076e-03,
         -8.0875e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2542.1226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4021, device='cuda:0')



h[100].sum tensor(-4.7168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-36.3832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0681, 0.0000, 0.0000,  ..., 0.1089, 0.0612, 0.0000],
        [0.0771, 0.0000, 0.0000,  ..., 0.1214, 0.0687, 0.0000],
        [0.1041, 0.0000, 0.0000,  ..., 0.1589, 0.0912, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67363.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2233, 0.0000,  ..., 0.6606, 0.0000, 0.1365],
        [0.0000, 0.2437, 0.0000,  ..., 0.7138, 0.0000, 0.1488],
        [0.0000, 0.2762, 0.0000,  ..., 0.7977, 0.0000, 0.1686],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522817.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.2450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-214.4856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.4353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1053],
        [ 0.1037],
        [ 0.0985],
        ...,
        [-0.5372],
        [-0.5355],
        [-0.5350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114488.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0039],
        [1.0024],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367455.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0039],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367467.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1483e-02, -9.5885e-05, -2.6209e-04,  ...,  1.9521e-02,
          1.0661e-02, -8.5302e-03],
        [-1.4386e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6140e-03,
         -8.8715e-05,  0.0000e+00],
        [ 3.3957e-02, -2.6267e-04, -7.1795e-04,  ...,  5.0667e-02,
          2.9359e-02, -2.3367e-02],
        ...,
        [-1.4386e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6140e-03,
         -8.8715e-05,  0.0000e+00],
        [-1.4386e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6140e-03,
         -8.8715e-05,  0.0000e+00],
        [-1.4386e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6140e-03,
         -8.8715e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2942.6677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7493, device='cuda:0')



h[100].sum tensor(-5.9067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0182, device='cuda:0')



h[200].sum tensor(-45.7546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0000, 0.0000,  ..., 0.0211, 0.0087, 0.0000],
        [0.0722, 0.0000, 0.0000,  ..., 0.1146, 0.0645, 0.0000],
        [0.0683, 0.0000, 0.0000,  ..., 0.1072, 0.0602, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72103.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.8223e-02, 0.0000e+00,  ..., 2.1643e-01, 0.0000e+00,
         3.6886e-02],
        [0.0000e+00, 1.4638e-01, 0.0000e+00,  ..., 4.5185e-01, 0.0000e+00,
         8.9619e-02],
        [0.0000e+00, 1.8803e-01, 0.0000e+00,  ..., 5.6154e-01, 0.0000e+00,
         1.1464e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.3214e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0814e-04, 0.0000e+00,  ..., 4.6508e-02, 0.0000e+00,
         1.1230e-03],
        [0.0000e+00, 4.8353e-03, 0.0000e+00,  ..., 6.5971e-02, 0.0000e+00,
         4.0104e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522815.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(482.7726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.9274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.1578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0319],
        [ 0.0671],
        [ 0.0736],
        ...,
        [-0.4916],
        [-0.4055],
        [-0.2851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131954.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0039],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367467.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0040],
        [1.0025],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367478.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4210e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6208e-03,
         -9.2017e-05,  0.0000e+00],
        [-1.4210e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6208e-03,
         -9.2017e-05,  0.0000e+00],
        [ 1.1131e-02, -9.0380e-05, -2.4977e-04,  ...,  1.9013e-02,
          1.0349e-02, -8.2728e-03],
        ...,
        [-1.4210e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6208e-03,
         -9.2017e-05,  0.0000e+00],
        [-1.4210e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6208e-03,
         -9.2017e-05,  0.0000e+00],
        [-1.4210e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6208e-03,
         -9.2017e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1910.3844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.0744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8195, device='cuda:0')



h[100].sum tensor(-2.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-21.3366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0304, 0.0141, 0.0000],
        [0.0117, 0.0000, 0.0000,  ..., 0.0286, 0.0130, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49379.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0683, 0.0000, 0.0052],
        [0.0000, 0.0301, 0.0000,  ..., 0.1445, 0.0000, 0.0189],
        [0.0000, 0.0398, 0.0000,  ..., 0.1754, 0.0000, 0.0241],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429092., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.6357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-187.9379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.3718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1920],
        [-0.0580],
        [ 0.0334],
        ...,
        [-0.5536],
        [-0.5517],
        [-0.5512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152800.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0040],
        [1.0025],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367478.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0041],
        [1.0026],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367490.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1233e-02, -1.5823e-04, -4.4215e-04,  ...,  3.3009e-02,
          1.8750e-02, -1.4905e-02],
        [ 2.1396e-02, -1.5937e-04, -4.4534e-04,  ...,  3.3235e-02,
          1.8885e-02, -1.5012e-02],
        [-1.4171e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6263e-03,
         -8.8936e-05,  0.0000e+00],
        ...,
        [-1.4171e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6263e-03,
         -8.8936e-05,  0.0000e+00],
        [-1.4171e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6263e-03,
         -8.8936e-05,  0.0000e+00],
        [-1.4171e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6263e-03,
         -8.8936e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2488.9915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3453, device='cuda:0')



h[100].sum tensor(-4.4612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0140, device='cuda:0')



h[200].sum tensor(-34.8523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0874, 0.0000, 0.0000,  ..., 0.1355, 0.0771, 0.0000],
        [0.0629, 0.0000, 0.0000,  ..., 0.0995, 0.0556, 0.0000],
        [0.0560, 0.0000, 0.0000,  ..., 0.0921, 0.0510, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64455.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1922, 0.0000,  ..., 0.5692, 0.0000, 0.1152],
        [0.0000, 0.1928, 0.0000,  ..., 0.5726, 0.0000, 0.1154],
        [0.0000, 0.1815, 0.0000,  ..., 0.5462, 0.0000, 0.1083],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505887.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.6581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.9124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.5316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0228],
        [ 0.0273],
        [ 0.0303],
        ...,
        [-0.5585],
        [-0.5566],
        [-0.5560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139724.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0041],
        [1.0026],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367490.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0041],
        [1.0026],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367503., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4654e-03, -3.3125e-05, -9.3601e-05,  ...,  8.4009e-03,
          3.9851e-03, -3.2116e-03],
        [ 5.2576e-03, -4.5271e-05, -1.2792e-04,  ...,  1.0884e-02,
          5.4758e-03, -4.3893e-03],
        [-1.4221e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6288e-03,
         -8.0165e-05,  0.0000e+00],
        ...,
        [-1.4221e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6288e-03,
         -8.0165e-05,  0.0000e+00],
        [-1.4221e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6288e-03,
         -8.0165e-05,  0.0000e+00],
        [-1.4221e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6288e-03,
         -8.0165e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2046.5962, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9226, device='cuda:0')



h[100].sum tensor(-3.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-24.2035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0381, 0.0000, 0.0000,  ..., 0.0673, 0.0362, 0.0000],
        [0.0115, 0.0000, 0.0000,  ..., 0.0265, 0.0118, 0.0000],
        [0.0053, 0.0000, 0.0000,  ..., 0.0158, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52716.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0982, 0.0000,  ..., 0.3287, 0.0000, 0.0580],
        [0.0000, 0.0490, 0.0000,  ..., 0.1951, 0.0000, 0.0293],
        [0.0000, 0.0189, 0.0000,  ..., 0.1075, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449368.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.4881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-195.7880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.1433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0502],
        [ 0.0102],
        [-0.0785],
        ...,
        [-0.5614],
        [-0.5595],
        [-0.5589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141267.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0041],
        [1.0026],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367503., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0027],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367515.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00],
        [-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00],
        [-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00],
        ...,
        [-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00],
        [-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00],
        [-1.4312e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6319e-03,
         -7.2360e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2146.9385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0170, device='cuda:0')



h[100].sum tensor(-3.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-26.1185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0182, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55653.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0734, 0.0000, 0.0049],
        [0.0000, 0.0175, 0.0000,  ..., 0.1004, 0.0000, 0.0104],
        [0.0000, 0.0335, 0.0000,  ..., 0.1519, 0.0000, 0.0198],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470650.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.9957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-199.3033, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.8411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0145],
        [ 0.0349],
        [ 0.0593],
        ...,
        [-0.5620],
        [-0.5602],
        [-0.5596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133072.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0027],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367515.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0028],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367527.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00],
        [-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00],
        [-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00],
        ...,
        [-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00],
        [-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00],
        [-1.4354e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6363e-03,
         -6.8033e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2151.3979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9877, device='cuda:0')



h[100].sum tensor(-3.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-25.8388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54662.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0445, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0378, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460004.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-196.3434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1429],
        [-0.2524],
        [-0.3753],
        ...,
        [-0.5566],
        [-0.5572],
        [-0.5579]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129454.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0028],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367527.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0028],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367539.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7477e-03, -6.9126e-05, -2.0210e-04,  ...,  1.7141e-02,
          9.2396e-03, -7.3163e-03],
        [ 1.1204e-02, -7.8125e-05, -2.2842e-04,  ...,  1.9158e-02,
          1.0451e-02, -8.2687e-03],
        [ 1.1491e-02, -7.9902e-05, -2.3361e-04,  ...,  1.9557e-02,
          1.0690e-02, -8.4569e-03],
        ...,
        [-1.4357e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6434e-03,
         -6.3315e-05,  0.0000e+00],
        [-1.4357e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6434e-03,
         -6.3315e-05,  0.0000e+00],
        [-1.4357e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6434e-03,
         -6.3315e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2203.4487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0312, device='cuda:0')



h[100].sum tensor(-3.3610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-26.7101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0352, 0.0000, 0.0000,  ..., 0.0634, 0.0338, 0.0000],
        [0.0420, 0.0000, 0.0000,  ..., 0.0728, 0.0395, 0.0000],
        [0.0546, 0.0000, 0.0000,  ..., 0.0903, 0.0500, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56146.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1654, 0.0000,  ..., 0.5072, 0.0000, 0.0942],
        [0.0000, 0.1491, 0.0000,  ..., 0.4662, 0.0000, 0.0845],
        [0.0000, 0.1307, 0.0000,  ..., 0.4173, 0.0000, 0.0739],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471783.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.9583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-199.3694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.5551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0887],
        [ 0.0949],
        [ 0.0923],
        ...,
        [-0.5643],
        [-0.5624],
        [-0.5618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120732., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0042],
        [1.0028],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367539.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0043],
        [1.0028],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367550.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0427e-02, -7.1042e-05, -2.1012e-04,  ...,  1.8087e-02,
          9.7970e-03, -7.7450e-03],
        [ 3.5699e-03, -2.9956e-05, -8.8603e-05,  ...,  8.5852e-03,
          4.0930e-03, -3.2658e-03],
        [ 5.4274e-03, -4.1086e-05, -1.2152e-04,  ...,  1.1159e-02,
          5.6382e-03, -4.4792e-03],
        ...,
        [-1.4298e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6570e-03,
         -6.5845e-05,  0.0000e+00],
        [-1.4298e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6570e-03,
         -6.5845e-05,  0.0000e+00],
        [-1.4298e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6570e-03,
         -6.5845e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2105.5122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9283, device='cuda:0')



h[100].sum tensor(-3.0263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-24.1540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0000, 0.0000,  ..., 0.0339, 0.0161, 0.0000],
        [0.0383, 0.0000, 0.0000,  ..., 0.0677, 0.0364, 0.0000],
        [0.0168, 0.0000, 0.0000,  ..., 0.0340, 0.0163, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52187.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0543, 0.0000,  ..., 0.2131, 0.0000, 0.0285],
        [0.0000, 0.0748, 0.0000,  ..., 0.2680, 0.0000, 0.0403],
        [0.0000, 0.0494, 0.0000,  ..., 0.1977, 0.0000, 0.0261],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447935.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.5652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-192.8938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.8364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0976],
        [ 0.0986],
        [ 0.0493],
        ...,
        [-0.5704],
        [-0.5685],
        [-0.5679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150190.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0043],
        [1.0028],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367550.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0043],
        [1.0029],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367562., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.0403e-03, -3.1725e-05, -9.4934e-05,  ...,  9.2420e-03,
          4.4767e-03, -3.5632e-03],
        [ 1.0926e-02, -7.1712e-05, -2.1459e-04,  ...,  1.8784e-02,
          1.0205e-02, -8.0544e-03],
        [-1.4230e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -6.7732e-05,  0.0000e+00],
        ...,
        [-1.4230e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -6.7732e-05,  0.0000e+00],
        [-1.4230e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -6.7732e-05,  0.0000e+00],
        [-1.4230e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -6.7732e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2800.5911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5508, device='cuda:0')



h[100].sum tensor(-4.9871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-39.9763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0372, 0.0000, 0.0000,  ..., 0.0662, 0.0354, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0339, 0.0161, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0301, 0.0139, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75568.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0750, 0.0000,  ..., 0.2670, 0.0000, 0.0394],
        [0.0000, 0.0538, 0.0000,  ..., 0.2102, 0.0000, 0.0274],
        [0.0000, 0.0357, 0.0000,  ..., 0.1585, 0.0000, 0.0179],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583610., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(499.9104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.5476, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.5587, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0902],
        [ 0.0631],
        [-0.0160],
        ...,
        [-0.5762],
        [-0.5741],
        [-0.5734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122445.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0043],
        [1.0029],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367562., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(521.4277, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0043],
        [1.0029],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367573.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3785e-02, -1.4182e-04, -4.2941e-04,  ...,  3.6610e-02,
          2.0898e-02, -1.6414e-02],
        [ 4.8954e-02, -2.8345e-04, -8.5824e-04,  ...,  7.1490e-02,
          4.1835e-02, -3.2805e-02],
        [ 2.3934e-02, -1.4266e-04, -4.3196e-04,  ...,  3.6818e-02,
          2.1022e-02, -1.6511e-02],
        ...,
        [-1.4187e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6843e-03,
         -6.7213e-05,  0.0000e+00],
        [-1.4187e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6843e-03,
         -6.7213e-05,  0.0000e+00],
        [-1.4187e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6843e-03,
         -6.7213e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2093.3972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9050, device='cuda:0')



h[100].sum tensor(-2.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-23.3867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1176, 0.0000, 0.0000,  ..., 0.1775, 0.1022, 0.0000],
        [0.1092, 0.0000, 0.0000,  ..., 0.1660, 0.0953, 0.0000],
        [0.1193, 0.0000, 0.0000,  ..., 0.1800, 0.1037, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51754.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2525, 0.0000,  ..., 0.7178, 0.0000, 0.1419],
        [0.0000, 0.2694, 0.0000,  ..., 0.7618, 0.0000, 0.1515],
        [0.0000, 0.2685, 0.0000,  ..., 0.7602, 0.0000, 0.1509],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448940.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.9864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-196.2924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.2534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0492],
        [ 0.0467],
        [ 0.0508],
        ...,
        [-0.5819],
        [-0.5800],
        [-0.5794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151110.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0043],
        [1.0029],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367573.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0044],
        [1.0030],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367585.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4197e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -6.3568e-05,  0.0000e+00],
        [ 4.7781e-03, -3.3789e-05, -1.0353e-04,  ...,  1.0282e-02,
          5.0923e-03, -4.0304e-03],
        [-1.4197e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -6.3568e-05,  0.0000e+00],
        ...,
        [-1.4197e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -6.3568e-05,  0.0000e+00],
        [-1.4197e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -6.3568e-05,  0.0000e+00],
        [-1.4197e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -6.3568e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2192.2085, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9808, device='cuda:0')



h[100].sum tensor(-3.1354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-25.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0000, 0.0000,  ..., 0.0516, 0.0266, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0138, 0.0042, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0154, 0.0051, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53782.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0471, 0.0000,  ..., 0.1920, 0.0000, 0.0228],
        [0.0000, 0.0185, 0.0000,  ..., 0.1117, 0.0000, 0.0076],
        [0.0000, 0.0156, 0.0000,  ..., 0.1023, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456648.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.7082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-201.5491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.8221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1962],
        [-0.2760],
        [-0.3149],
        ...,
        [-0.5857],
        [-0.5838],
        [-0.5832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154446.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0044],
        [1.0030],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367585.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0044],
        [1.0030],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367597., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4268e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6965e-03,
         -5.6322e-05,  0.0000e+00],
        [ 1.4519e-02, -8.4215e-05, -2.6114e-04,  ...,  2.3797e-02,
          1.3211e-02, -1.0355e-02],
        [ 2.2424e-02, -1.2597e-04, -3.9060e-04,  ...,  3.4754e-02,
          1.9788e-02, -1.5488e-02],
        ...,
        [-1.4268e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6965e-03,
         -5.6322e-05,  0.0000e+00],
        [-1.4268e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6965e-03,
         -5.6322e-05,  0.0000e+00],
        [-1.4268e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6965e-03,
         -5.6322e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2413.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1649, device='cuda:0')



h[100].sum tensor(-3.7046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-30.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0260, 0.0000, 0.0000,  ..., 0.0487, 0.0250, 0.0000],
        [0.0390, 0.0000, 0.0000,  ..., 0.0669, 0.0359, 0.0000],
        [0.0605, 0.0000, 0.0000,  ..., 0.0986, 0.0548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59394.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1409, 0.0000,  ..., 0.4362, 0.0000, 0.0766],
        [0.0000, 0.1697, 0.0000,  ..., 0.5111, 0.0000, 0.0932],
        [0.0000, 0.2041, 0.0000,  ..., 0.6017, 0.0000, 0.1130],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0350, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0350, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0350, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487174.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(435.0952, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-213.0222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.5826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0135],
        [-0.0047],
        [ 0.0027],
        ...,
        [-0.5875],
        [-0.5854],
        [-0.5846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133970.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0044],
        [1.0030],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367597., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0045],
        [1.0031],
        ...,
        [1.0009],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367608.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4329e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -4.8133e-05,  0.0000e+00],
        [-1.4329e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -4.8133e-05,  0.0000e+00],
        [ 5.2052e-03, -3.3955e-05, -1.0657e-04,  ...,  1.0894e-02,
          5.4755e-03, -4.3045e-03],
        ...,
        [-1.4329e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -4.8133e-05,  0.0000e+00],
        [-1.4329e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -4.8133e-05,  0.0000e+00],
        [-1.4329e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6930e-03,
         -4.8133e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2236.7893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9956, device='cuda:0')



h[100].sum tensor(-3.1375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-25.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0160, 0.0055, 0.0000],
        [0.0335, 0.0000, 0.0000,  ..., 0.0572, 0.0302, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55195.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0000,  ..., 0.0952, 0.0000, 0.0071],
        [0.0000, 0.0426, 0.0000,  ..., 0.1758, 0.0000, 0.0215],
        [0.0000, 0.0990, 0.0000,  ..., 0.3261, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472082.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.6320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.3693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.5906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [ 0.0508],
        [ 0.0867],
        ...,
        [-0.5883],
        [-0.5864],
        [-0.5858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135942.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0045],
        [1.0031],
        ...,
        [1.0009],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367608.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0045],
        [1.0031],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367620.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00],
        [-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00],
        [-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00],
        ...,
        [-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00],
        [-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00],
        [-1.4301e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6906e-03,
         -4.4716e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1925.9670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.7513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7028, device='cuda:0')



h[100].sum tensor(-2.2247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0073, device='cuda:0')



h[200].sum tensor(-18.2247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0000, 0.0000,  ..., 0.0487, 0.0251, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48269.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0569, 0.0000,  ..., 0.2112, 0.0000, 0.0297],
        [0.0000, 0.0161, 0.0000,  ..., 0.0940, 0.0000, 0.0079],
        [0.0000, 0.0043, 0.0000,  ..., 0.0571, 0.0000, 0.0022],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443855.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.2845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-191.9411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.2767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0454],
        [-0.2092],
        [-0.4017],
        ...,
        [-0.5939],
        [-0.5919],
        [-0.5914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138299.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0045],
        [1.0031],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367620.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0046],
        [1.0032],
        ...,
        [1.0009],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367631.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1609e-02, -6.2432e-05, -2.0079e-04,  ...,  1.9748e-02,
          1.0789e-02, -8.4183e-03],
        [ 2.4856e-02, -1.2595e-04, -4.0506e-04,  ...,  3.8110e-02,
          2.1813e-02, -1.6983e-02],
        [ 3.6668e-02, -1.8258e-04, -5.8721e-04,  ...,  5.4484e-02,
          3.1643e-02, -2.4619e-02],
        ...,
        [-1.4123e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6980e-03,
         -4.6621e-05,  0.0000e+00],
        [-1.4123e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6980e-03,
         -4.6621e-05,  0.0000e+00],
        [-1.4123e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6980e-03,
         -4.6621e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2026.4989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7867, device='cuda:0')



h[100].sum tensor(-2.4536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-20.1877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0797, 0.0000, 0.0000,  ..., 0.1251, 0.0708, 0.0000],
        [0.1158, 0.0000, 0.0000,  ..., 0.1752, 0.1009, 0.0000],
        [0.1737, 0.0000, 0.0000,  ..., 0.2555, 0.1491, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48842.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2745, 0.0000,  ..., 0.7769, 0.0000, 0.1519],
        [0.0000, 0.3974, 0.0000,  ..., 1.0860, 0.0000, 0.2229],
        [0.0000, 0.5251, 0.0000,  ..., 1.4067, 0.0000, 0.2966],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439118.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.9376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-194.3815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.0759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0326],
        [-0.0046],
        [-0.0401],
        ...,
        [-0.6012],
        [-0.5992],
        [-0.5986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159697.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0046],
        [1.0032],
        ...,
        [1.0009],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367631.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0046],
        [1.0032],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367643.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00],
        ...,
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.8743e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2294.1274, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0201, device='cuda:0')



h[100].sum tensor(-3.1402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-25.9508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0171, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0196, 0.0076, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56671.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0837, 0.0000, 0.0027],
        [0.0000, 0.0040, 0.0000,  ..., 0.0695, 0.0000, 0.0006],
        [0.0000, 0.0190, 0.0000,  ..., 0.1101, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483663.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.9906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.8342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.7451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3219],
        [-0.2662],
        [-0.1555],
        ...,
        [-0.6073],
        [-0.6053],
        [-0.6047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152868.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0046],
        [1.0032],
        ...,
        [1.0008],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367643.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367654.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4823e-03, -2.1881e-05, -7.2144e-05,  ...,  8.4629e-03,
          4.0068e-03, -3.1406e-03],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        ...,
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2712.7349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3511, device='cuda:0')



h[100].sum tensor(-4.2374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-35.1726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0142, 0.0000, 0.0000,  ..., 0.0305, 0.0141, 0.0000],
        [0.0269, 0.0000, 0.0000,  ..., 0.0500, 0.0258, 0.0000],
        [0.0476, 0.0000, 0.0000,  ..., 0.0787, 0.0430, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66460.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1001, 0.0000,  ..., 0.3284, 0.0000, 0.0516],
        [0.0000, 0.1376, 0.0000,  ..., 0.4257, 0.0000, 0.0729],
        [0.0000, 0.1796, 0.0000,  ..., 0.5328, 0.0000, 0.0971],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527977.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(469.4453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.1187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.8217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0471],
        [ 0.0427],
        [ 0.0385],
        ...,
        [-0.6121],
        [-0.6100],
        [-0.6094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149103.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367654.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367654.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        ...,
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00],
        [-1.3900e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7086e-03,
         -4.8118e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2099.3484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8265, device='cuda:0')



h[100].sum tensor(-2.5642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-21.2839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0176, 0.0064, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0123, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52099.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0885, 0.0000, 0.0008],
        [0.0000, 0.0044, 0.0000,  ..., 0.0729, 0.0000, 0.0004],
        [0.0000, 0.0009, 0.0000,  ..., 0.0545, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463809.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.5542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.2013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.2612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4455],
        [-0.4869],
        [-0.5456],
        ...,
        [-0.6130],
        [-0.6109],
        [-0.6103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145711.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367654.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367666.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7378e-03, -2.6625e-05, -8.8898e-05,  ...,  1.0199e-02,
          5.0547e-03, -3.9439e-03],
        [ 4.5775e-03, -2.5929e-05, -8.6572e-05,  ...,  9.9765e-03,
          4.9212e-03, -3.8407e-03],
        [ 1.0705e-02, -5.2554e-05, -1.7547e-04,  ...,  1.8472e-02,
          1.0021e-02, -7.7846e-03],
        ...,
        [-1.3894e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.5455e-05,  0.0000e+00],
        [-1.3894e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.5455e-05,  0.0000e+00],
        [-1.3894e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7038e-03,
         -4.5455e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2417.7676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0927, device='cuda:0')



h[100].sum tensor(-3.3723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-28.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0000, 0.0000,  ..., 0.0346, 0.0166, 0.0000],
        [0.0360, 0.0000, 0.0000,  ..., 0.0644, 0.0344, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0347, 0.0166, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58067.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0584, 0.0000,  ..., 0.2204, 0.0000, 0.0280],
        [0.0000, 0.0882, 0.0000,  ..., 0.3019, 0.0000, 0.0442],
        [0.0000, 0.0776, 0.0000,  ..., 0.2741, 0.0000, 0.0381],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485894.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.7925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-216.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.5692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0945],
        [ 0.1104],
        [ 0.1172],
        ...,
        [-0.6169],
        [-0.6148],
        [-0.6142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142022.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0047],
        [1.0033],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367666.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(456.7468, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0048],
        [1.0034],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367678.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00],
        [-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00],
        [-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00],
        ...,
        [-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00],
        [-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00],
        [-1.3927e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6959e-03,
         -4.1348e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2348.2314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0350, device='cuda:0')



h[100].sum tensor(-3.1278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-26.1927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56633.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0359, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0372, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0366, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0366, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0366, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483499.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(428.7477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-214.3499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.9473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4925],
        [-0.5024],
        [-0.4973],
        ...,
        [-0.6208],
        [-0.6190],
        [-0.6187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165855.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0048],
        [1.0034],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367678.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0048],
        [1.0035],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367691., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00],
        [-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00],
        [-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00],
        ...,
        [-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00],
        [-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00],
        [-1.4042e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6850e-03,
         -3.0016e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2131.1184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8168, device='cuda:0')



h[100].sum tensor(-2.4853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-20.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0121, 0.0032, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0176, 0.0064, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0122, 0.0032, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52671.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0901, 0.0000, 0.0016],
        [0.0000, 0.0129, 0.0000,  ..., 0.1049, 0.0000, 0.0030],
        [0.0000, 0.0080, 0.0000,  ..., 0.0869, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474855.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.3754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-206.7753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.2263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1134],
        [-0.0402],
        [-0.0146],
        ...,
        [-0.6241],
        [-0.6220],
        [-0.6214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140948.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0048],
        [1.0035],
        ...,
        [1.0008],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367691., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0049],
        [1.0035],
        ...,
        [1.0007],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367703.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4095e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -2.1715e-05,  0.0000e+00],
        [-1.4095e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -2.1715e-05,  0.0000e+00],
        [ 1.1384e-02, -5.0305e-05, -1.7454e-04,  ...,  1.9415e-02,
          1.0633e-02, -8.2006e-03],
        ...,
        [-1.4095e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -2.1715e-05,  0.0000e+00],
        [-1.4095e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -2.1715e-05,  0.0000e+00],
        [-1.4095e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6713e-03,
         -2.1715e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2705.7524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2990, device='cuda:0')



h[100].sum tensor(-3.9550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-33.4145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0000, 0.0000,  ..., 0.0164, 0.0058, 0.0000],
        [0.0337, 0.0000, 0.0000,  ..., 0.0593, 0.0315, 0.0000],
        [0.0530, 0.0000, 0.0000,  ..., 0.0861, 0.0476, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64247.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0517, 0.0000,  ..., 0.2048, 0.0000, 0.0260],
        [0.0000, 0.1275, 0.0000,  ..., 0.4057, 0.0000, 0.0685],
        [0.0000, 0.1931, 0.0000,  ..., 0.5768, 0.0000, 0.1059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518877.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-229.7870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.5990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1024],
        [ 0.1441],
        [ 0.1601],
        ...,
        [-0.6253],
        [-0.6233],
        [-0.6228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134475.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0049],
        [1.0035],
        ...,
        [1.0007],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367703.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0050],
        [1.0036],
        ...,
        [1.0007],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367715.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1213e-03, -3.2365e-05, -1.1377e-04,  ...,  1.3467e-02,
          7.0622e-03, -5.4494e-03],
        [-1.3935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6580e-03,
         -2.8694e-05,  0.0000e+00],
        [-1.3935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6580e-03,
         -2.8694e-05,  0.0000e+00],
        ...,
        [-1.3935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6580e-03,
         -2.8694e-05,  0.0000e+00],
        [-1.3935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6580e-03,
         -2.8694e-05,  0.0000e+00],
        [-1.3935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6580e-03,
         -2.8694e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2171.3184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8301, device='cuda:0')



h[100].sum tensor(-2.4912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-21.1414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0244, 0.0000, 0.0000,  ..., 0.0444, 0.0226, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0185, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51530.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0896, 0.0000,  ..., 0.3031, 0.0000, 0.0466],
        [0.0000, 0.0382, 0.0000,  ..., 0.1634, 0.0000, 0.0182],
        [0.0000, 0.0083, 0.0000,  ..., 0.0792, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461531., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.6700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-205.3897, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.8792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1124],
        [ 0.0574],
        [-0.0441],
        ...,
        [-0.6333],
        [-0.6313],
        [-0.6306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148922.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0050],
        [1.0036],
        ...,
        [1.0007],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367715.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0051],
        [1.0037],
        ...,
        [1.0007],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367727.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3737e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6460e-03,
         -3.9611e-05,  0.0000e+00],
        [ 2.1399e-02, -8.3658e-05, -2.9796e-04,  ...,  3.3228e-02,
          1.8923e-02, -1.4551e-02],
        [ 2.5306e-02, -9.8012e-05, -3.4908e-04,  ...,  3.8646e-02,
          2.2177e-02, -1.7048e-02],
        ...,
        [-1.3737e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6460e-03,
         -3.9611e-05,  0.0000e+00],
        [-1.3737e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6460e-03,
         -3.9611e-05,  0.0000e+00],
        [-1.3737e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6460e-03,
         -3.9611e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2225.6855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8596, device='cuda:0')



h[100].sum tensor(-2.5866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-22.0486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0000, 0.0000,  ..., 0.0383, 0.0190, 0.0000],
        [0.0477, 0.0000, 0.0000,  ..., 0.0766, 0.0419, 0.0000],
        [0.1139, 0.0000, 0.0000,  ..., 0.1723, 0.0993, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53320.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0728, 0.0000,  ..., 0.2516, 0.0000, 0.0367],
        [0.0000, 0.1421, 0.0000,  ..., 0.4333, 0.0000, 0.0754],
        [0.0000, 0.2260, 0.0000,  ..., 0.6517, 0.0000, 0.1224],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471971.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.6509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-209.1107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.6774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0847],
        [ 0.1192],
        [ 0.1387],
        ...,
        [-0.6398],
        [-0.6377],
        [-0.6371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169647.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0051],
        [1.0037],
        ...,
        [1.0007],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367727.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0038],
        ...,
        [1.0006],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367739.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00],
        [-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00],
        [-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00],
        ...,
        [-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00],
        [-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00],
        [-1.3564e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6331e-03,
         -4.9134e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2745.4031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3069, device='cuda:0')



h[100].sum tensor(-3.8843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-33.2585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62620.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0122, 0.0000,  ..., 0.0879, 0.0000, 0.0041],
        [0.0000, 0.0013, 0.0000,  ..., 0.0539, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0403, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0385, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0385, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0385, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504798., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.1675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-226.6656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.7034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0585],
        [-0.1944],
        [-0.3437],
        ...,
        [-0.6415],
        [-0.6396],
        [-0.6392]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143664.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0038],
        ...,
        [1.0006],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367739.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0039],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367751.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00],
        [-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00],
        [-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00],
        ...,
        [-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00],
        [-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00],
        [-1.3489e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6220e-03,
         -5.4210e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2338.0300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9376, device='cuda:0')



h[100].sum tensor(-2.7743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-23.8612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54746.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0379, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0458, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0560, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474859.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.6342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.9159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.7750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6525],
        [-0.5984],
        [-0.4813],
        ...,
        [-0.6482],
        [-0.6461],
        [-0.6455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147958.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0039],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367751.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0053],
        [1.0040],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367764.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6953e-03, -2.3332e-05, -8.6497e-05,  ...,  1.1382e-02,
          5.8119e-03, -4.4797e-03],
        [ 1.5281e-02, -5.5084e-05, -2.0421e-04,  ...,  2.4674e-02,
          1.3793e-02, -1.0576e-02],
        [ 1.9663e-02, -6.9602e-05, -2.5803e-04,  ...,  3.0752e-02,
          1.7441e-02, -1.3364e-02],
        ...,
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6153e-03,
         -5.2446e-05,  0.0000e+00],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6153e-03,
         -5.2446e-05,  0.0000e+00],
        [-1.3481e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6153e-03,
         -5.2446e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3536.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.9124, device='cuda:0')



h[100].sum tensor(-5.8023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0199, device='cuda:0')



h[200].sum tensor(-50.1296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0000, 0.0000,  ..., 0.0502, 0.0262, 0.0000],
        [0.0653, 0.0000, 0.0000,  ..., 0.1045, 0.0586, 0.0000],
        [0.0799, 0.0000, 0.0000,  ..., 0.1248, 0.0708, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84564.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1107, 0.0000,  ..., 0.3543, 0.0000, 0.0548],
        [0.0000, 0.1877, 0.0000,  ..., 0.5535, 0.0000, 0.0974],
        [0.0000, 0.2423, 0.0000,  ..., 0.6934, 0.0000, 0.1278],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620210.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.3529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.4291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-130.4855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0856],
        [ 0.0694],
        [ 0.0522],
        ...,
        [-0.6456],
        [-0.6469],
        [-0.6471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155624.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0053],
        [1.0040],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367764.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0054],
        [1.0041],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367776.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8489e-02, -6.3481e-05, -2.3856e-04,  ...,  2.9130e-02,
          1.6478e-02, -1.2602e-02],
        [ 3.3457e-02, -1.1137e-04, -4.1852e-04,  ...,  4.9889e-02,
          2.8942e-02, -2.2108e-02],
        [ 1.7478e-02, -6.0248e-05, -2.2641e-04,  ...,  2.7729e-02,
          1.5636e-02, -1.1960e-02],
        ...,
        [-1.3534e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6130e-03,
         -4.4416e-05,  0.0000e+00],
        [-1.3534e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6130e-03,
         -4.4416e-05,  0.0000e+00],
        [-1.3534e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6130e-03,
         -4.4416e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2329.7104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9098, device='cuda:0')



h[100].sum tensor(-2.6582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-23.0688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0921, 0.0000, 0.0000,  ..., 0.1417, 0.0810, 0.0000],
        [0.0955, 0.0000, 0.0000,  ..., 0.1465, 0.0839, 0.0000],
        [0.1013, 0.0000, 0.0000,  ..., 0.1545, 0.0887, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53870.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1967, 0.0000,  ..., 0.5739, 0.0000, 0.1028],
        [0.0000, 0.2311, 0.0000,  ..., 0.6647, 0.0000, 0.1215],
        [0.0000, 0.2363, 0.0000,  ..., 0.6794, 0.0000, 0.1241],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470015.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.6208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-207.1559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.2229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0560],
        [ 0.0927],
        [ 0.1127],
        ...,
        [-0.6515],
        [-0.6495],
        [-0.6489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148108.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0054],
        [1.0041],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367776.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0055],
        [1.0042],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367789., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3866e-02, -7.7932e-05, -2.9690e-04,  ...,  3.6601e-02,
          2.0974e-02, -1.5998e-02],
        [ 4.7708e-02, -1.5159e-04, -5.7751e-04,  ...,  6.9668e-02,
          4.0830e-02, -3.1118e-02],
        [ 4.6653e-02, -1.4833e-04, -5.6510e-04,  ...,  6.8205e-02,
          3.9951e-02, -3.0449e-02],
        ...,
        [-1.3596e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6131e-03,
         -3.4765e-05,  0.0000e+00],
        [-1.3596e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6131e-03,
         -3.4765e-05,  0.0000e+00],
        [-1.3596e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6131e-03,
         -3.4765e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2373.6504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9343, device='cuda:0')



h[100].sum tensor(-2.7312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-23.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1190, 0.0000, 0.0000,  ..., 0.1791, 0.1035, 0.0000],
        [0.1596, 0.0000, 0.0000,  ..., 0.2354, 0.1373, 0.0000],
        [0.1672, 0.0000, 0.0000,  ..., 0.2460, 0.1437, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56117.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2931, 0.0000,  ..., 0.8165, 0.0000, 0.1569],
        [0.0000, 0.3837, 0.0000,  ..., 1.0481, 0.0000, 0.2072],
        [0.0000, 0.3971, 0.0000,  ..., 1.0836, 0.0000, 0.2144],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488132.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.6130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-211.2249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.5645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0553],
        [ 0.0787],
        [ 0.0981],
        ...,
        [-0.6517],
        [-0.6495],
        [-0.6483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137563.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0055],
        [1.0042],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367789., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(534.7798, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0056],
        [1.0042],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367801.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0413e-03, -3.1033e-05, -1.1987e-04,  ...,  1.6048e-02,
          8.6413e-03, -6.5893e-03],
        [ 2.3680e-02, -7.4695e-05, -2.8853e-04,  ...,  3.6354e-02,
          2.0835e-02, -1.5860e-02],
        [ 2.0099e-02, -6.4014e-05, -2.4727e-04,  ...,  3.1387e-02,
          1.7852e-02, -1.3592e-02],
        ...,
        [-1.3635e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6158e-03,
         -2.5419e-05,  0.0000e+00],
        [-1.3635e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6158e-03,
         -2.5419e-05,  0.0000e+00],
        [-1.3635e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6158e-03,
         -2.5419e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2850.8335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3482, device='cuda:0')



h[100].sum tensor(-3.9111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-34.2509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0646, 0.0000, 0.0000,  ..., 0.1037, 0.0583, 0.0000],
        [0.0642, 0.0000, 0.0000,  ..., 0.1032, 0.0580, 0.0000],
        [0.0671, 0.0000, 0.0000,  ..., 0.1072, 0.0604, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66165.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1344, 0.0000,  ..., 0.4181, 0.0000, 0.0684],
        [0.0000, 0.1517, 0.0000,  ..., 0.4640, 0.0000, 0.0778],
        [0.0000, 0.1502, 0.0000,  ..., 0.4605, 0.0000, 0.0769],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537540.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.0243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-231.7516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.1019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1397],
        [ 0.1477],
        [ 0.1504],
        ...,
        [-0.6566],
        [-0.6552],
        [-0.6550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133861.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0056],
        [1.0042],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367801.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0058],
        [1.0043],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367813.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2642e-02, -4.0309e-05, -1.5789e-04,  ...,  2.1041e-02,
          1.1646e-02, -8.8545e-03],
        [ 1.1255e-02, -3.6317e-05, -1.4225e-04,  ...,  1.9118e-02,
          1.0491e-02, -7.9778e-03],
        [ 1.2384e-02, -3.9566e-05, -1.5498e-04,  ...,  2.0683e-02,
          1.1431e-02, -8.6915e-03],
        ...,
        [-1.3612e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6170e-03,
         -1.7975e-05,  0.0000e+00],
        [ 1.4372e-02, -4.5289e-05, -1.7740e-04,  ...,  2.3441e-02,
          1.3088e-02, -9.9486e-03],
        [-1.3612e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6170e-03,
         -1.7975e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2689.7014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2236, device='cuda:0')



h[100].sum tensor(-3.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-30.8190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1696, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0430, 0.0000, 0.0000,  ..., 0.0737, 0.0403, 0.0000],
        [0.0444, 0.0000, 0.0000,  ..., 0.0756, 0.0414, 0.0000],
        [0.0428, 0.0000, 0.0000,  ..., 0.0734, 0.0401, 0.0000],
        ...,
        [0.0147, 0.0000, 0.0000,  ..., 0.0288, 0.0133, 0.0000],
        [0.0117, 0.0000, 0.0000,  ..., 0.0248, 0.0109, 0.0000],
        [0.0527, 0.0000, 0.0000,  ..., 0.0874, 0.0485, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62219.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1097, 0.0000,  ..., 0.3602, 0.0000, 0.0539],
        [0.0000, 0.1097, 0.0000,  ..., 0.3593, 0.0000, 0.0541],
        [0.0000, 0.1077, 0.0000,  ..., 0.3542, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0277, 0.0000,  ..., 0.1370, 0.0000, 0.0108],
        [0.0000, 0.0380, 0.0000,  ..., 0.1646, 0.0000, 0.0156],
        [0.0000, 0.0713, 0.0000,  ..., 0.2537, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517225.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.4471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-226.8507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.8636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1877],
        [ 0.1874],
        [ 0.1860],
        ...,
        [-0.2100],
        [-0.1503],
        [-0.1273]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124978.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0058],
        [1.0043],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367813.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0059],
        [1.0044],
        ...,
        [1.0006],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367826.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00],
        [-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00],
        [-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00],
        ...,
        [-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00],
        [-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00],
        [-1.3505e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6197e-03,
         -1.0055e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2903.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3934, device='cuda:0')



h[100].sum tensor(-4.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-35.5708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1931, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67325.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0712, 0.0000, 0.0002],
        [0.0000, 0.0003, 0.0000,  ..., 0.0528, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0462, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540082.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.4635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.6305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.2426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0725],
        [-0.2016],
        [-0.3390],
        ...,
        [-0.6788],
        [-0.6765],
        [-0.6758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134285.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0059],
        [1.0044],
        ...,
        [1.0006],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367826.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0060],
        [1.0045],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367838.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3274e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6290e-03,
         -1.6191e-05,  0.0000e+00],
        [-1.3274e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6290e-03,
         -1.6191e-05,  0.0000e+00],
        [ 1.1118e-02, -3.3345e-05, -1.3435e-04,  ...,  1.8891e-02,
          1.0349e-02, -7.8447e-03],
        ...,
        [-1.3274e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6290e-03,
         -1.6191e-05,  0.0000e+00],
        [-1.3274e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6290e-03,
         -1.6191e-05,  0.0000e+00],
        [-1.3274e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6290e-03,
         -1.6191e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2807.0767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3164, device='cuda:0')



h[100].sum tensor(-3.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-33.4273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0201, 0.0000, 0.0000,  ..., 0.0381, 0.0189, 0.0000],
        [0.0274, 0.0000, 0.0000,  ..., 0.0501, 0.0261, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0298, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63421.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7204e-02, 0.0000e+00,  ..., 1.0178e-01, 0.0000e+00,
         6.5123e-03],
        [0.0000e+00, 5.9148e-02, 0.0000e+00,  ..., 2.1839e-01, 0.0000e+00,
         2.6197e-02],
        [0.0000e+00, 9.5405e-02, 0.0000e+00,  ..., 3.1601e-01, 0.0000e+00,
         4.5217e-02],
        ...,
        [0.0000e+00, 2.4489e-03, 0.0000e+00,  ..., 5.7169e-02, 0.0000e+00,
         1.0918e-04],
        [0.0000e+00, 1.4080e-02, 0.0000e+00,  ..., 9.4778e-02, 0.0000e+00,
         4.2587e-03],
        [0.0000e+00, 4.4119e-02, 0.0000e+00,  ..., 1.8308e-01, 0.0000e+00,
         1.7045e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517639.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.1939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.9676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0929],
        [ 0.0034],
        [ 0.0762],
        ...,
        [-0.4386],
        [-0.2393],
        [-0.0592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152933.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0060],
        [1.0045],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367838.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0061],
        [1.0046],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367850.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0033e-02, -2.9335e-05, -1.1990e-04,  ...,  1.7383e-02,
          9.4400e-03, -7.1444e-03],
        [ 2.8885e-02, -7.8051e-05, -3.1901e-04,  ...,  4.3530e-02,
          2.5141e-02, -1.9009e-02],
        [ 3.7858e-03, -1.3191e-05, -5.3914e-05,  ...,  8.7178e-03,
          4.2368e-03, -3.2126e-03],
        ...,
        [-1.3189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6377e-03,
         -1.4764e-05,  0.0000e+00],
        [-1.3189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6377e-03,
         -1.4764e-05,  0.0000e+00],
        [-1.3189e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6377e-03,
         -1.4764e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2350.3208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9212, device='cuda:0')



h[100].sum tensor(-2.6100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-23.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1022, 0.0000, 0.0000,  ..., 0.1556, 0.0894, 0.0000],
        [0.0490, 0.0000, 0.0000,  ..., 0.0820, 0.0452, 0.0000],
        [0.0552, 0.0000, 0.0000,  ..., 0.0905, 0.0503, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52892.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2155, 0.0000,  ..., 0.6224, 0.0000, 0.1109],
        [0.0000, 0.1647, 0.0000,  ..., 0.4947, 0.0000, 0.0825],
        [0.0000, 0.1354, 0.0000,  ..., 0.4188, 0.0000, 0.0665],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467632.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.3557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.7421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.0303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0867],
        [ 0.0885],
        [ 0.0801],
        ...,
        [-0.6929],
        [-0.6906],
        [-0.6899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187023.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0061],
        [1.0046],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367850.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0062],
        [1.0047],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367862.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6391e-03, -2.4834e-05, -1.0298e-04,  ...,  1.5468e-02,
          8.2939e-03, -6.2626e-03],
        [ 1.8670e-02, -4.9832e-05, -2.0663e-04,  ...,  2.9381e-02,
          1.6649e-02, -1.2566e-02],
        [ 8.7046e-03, -2.4997e-05, -1.0365e-04,  ...,  1.5558e-02,
          8.3485e-03, -6.3037e-03],
        ...,
        [-1.3263e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6445e-03,
         -6.8434e-06,  0.0000e+00],
        [-1.3263e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6445e-03,
         -6.8434e-06,  0.0000e+00],
        [-1.3263e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6445e-03,
         -6.8434e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2949.7053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4106, device='cuda:0')



h[100].sum tensor(-4.0141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-35.9615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0621, 0.0000, 0.0000,  ..., 0.0983, 0.0550, 0.0000],
        [0.0609, 0.0000, 0.0000,  ..., 0.0985, 0.0551, 0.0000],
        [0.0616, 0.0000, 0.0000,  ..., 0.0976, 0.0546, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70028.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2468, 0.0000,  ..., 0.7001, 0.0000, 0.1287],
        [0.0000, 0.2714, 0.0000,  ..., 0.7645, 0.0000, 0.1419],
        [0.0000, 0.2597, 0.0000,  ..., 0.7336, 0.0000, 0.1356],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556178.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(481.8979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.4880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.8325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0464],
        [ 0.0470],
        ...,
        [-0.6943],
        [-0.6921],
        [-0.6914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159238.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0062],
        [1.0047],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367862.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0063],
        [1.0048],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367875., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        ...,
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2450.0945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9758, device='cuda:0')



h[100].sum tensor(-2.7411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-24.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6311e-03, 2.4843e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6520e-03, 2.4921e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6536e-03, 2.4927e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7430e-03, 2.5262e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7427e-03, 2.5261e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7428e-03, 2.5261e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57219.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0395, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498617., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.0262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-219.3606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.7280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3793],
        [-0.5360],
        [-0.6627],
        ...,
        [-0.6925],
        [-0.6902],
        [-0.6896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146374.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0063],
        [1.0048],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367875., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0063],
        [1.0048],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367875., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        ...,
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00],
        [-1.3381e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6525e-03,
          6.1909e-07,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2350.6863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8817, device='cuda:0')



h[100].sum tensor(-2.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-22.5183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6311e-03, 2.4843e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6520e-03, 2.4921e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6536e-03, 2.4927e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7430e-03, 2.5262e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7427e-03, 2.5261e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7428e-03, 2.5261e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53957.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0395, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480149.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(412.0360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-212.9862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-221.9565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7833],
        [-0.8266],
        [-0.8599],
        ...,
        [-0.6925],
        [-0.6902],
        [-0.6896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155057.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0063],
        [1.0048],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367875., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0065],
        [1.0049],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367887.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2784e-02, -3.2743e-05, -1.3980e-04,  ...,  2.1276e-02,
          1.1794e-02, -8.8584e-03],
        [ 2.2405e-02, -5.5027e-05, -2.3494e-04,  ...,  3.4624e-02,
          1.9809e-02, -1.4887e-02],
        [ 1.4845e-02, -3.7517e-05, -1.6018e-04,  ...,  2.4136e-02,
          1.3511e-02, -1.0150e-02],
        ...,
        [-1.3522e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6635e-03,
          1.5340e-05,  0.0000e+00],
        [-1.3522e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6635e-03,
          1.5340e-05,  0.0000e+00],
        [-1.3522e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6635e-03,
          1.5340e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3891.2876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1826, device='cuda:0')



h[100].sum tensor(-6.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0228, device='cuda:0')



h[200].sum tensor(-55.3373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3025, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.5128e-02, 0.0000e+00, 0.0000e+00,  ..., 6.1058e-02, 3.2721e-02,
         0.0000e+00],
        [5.2089e-02, 0.0000e+00, 0.0000e+00,  ..., 8.6517e-02, 4.7997e-02,
         0.0000e+00],
        [1.0473e-01, 0.0000e+00, 0.0000e+00,  ..., 1.5956e-01, 9.1860e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7891e-03, 6.2604e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7888e-03, 6.2601e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7888e-03, 6.2601e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98578.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1352, 0.0000,  ..., 0.4269, 0.0000, 0.0667],
        [0.0000, 0.1964, 0.0000,  ..., 0.5843, 0.0000, 0.1003],
        [0.0000, 0.2841, 0.0000,  ..., 0.8060, 0.0000, 0.1489],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(746213.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.8224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.7447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-114.4036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1670],
        [ 0.1621],
        [ 0.1582],
        ...,
        [-0.6862],
        [-0.6841],
        [-0.6834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113488.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0065],
        [1.0049],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367887.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0066],
        [1.0050],
        ...,
        [1.0006],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367900., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1068e-02, -2.7678e-05, -1.1993e-04,  ...,  1.8888e-02,
          1.0330e-02, -7.7584e-03],
        [ 4.2888e-03, -1.2550e-05, -5.4381e-05,  ...,  9.4851e-03,
          4.6838e-03, -3.5180e-03],
        [ 2.0240e-02, -4.8146e-05, -2.0862e-04,  ...,  3.1610e-02,
          1.7969e-02, -1.3496e-02],
        ...,
        [-1.3352e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6842e-03,
         -3.5123e-07,  0.0000e+00],
        [-1.3352e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6842e-03,
         -3.5123e-07,  0.0000e+00],
        [-1.3352e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6842e-03,
         -3.5123e-07,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2892.3279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3052, device='cuda:0')



h[100].sum tensor(-3.6680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-33.3174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1809, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0000, 0.0000,  ..., 0.0454, 0.0232, 0.0000],
        [0.0446, 0.0000, 0.0000,  ..., 0.0761, 0.0416, 0.0000],
        [0.0240, 0.0000, 0.0000,  ..., 0.0457, 0.0234, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70171.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1045, 0.0000,  ..., 0.3484, 0.0000, 0.0477],
        [0.0000, 0.1039, 0.0000,  ..., 0.3463, 0.0000, 0.0476],
        [0.0000, 0.0842, 0.0000,  ..., 0.2941, 0.0000, 0.0373],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575947.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(474.2620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.0702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1422],
        [ 0.1446],
        [ 0.1405],
        ...,
        [-0.6867],
        [-0.6845],
        [-0.6839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115133.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0066],
        [1.0050],
        ...,
        [1.0006],
        [0.9999],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367900., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(513.1136, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0068],
        [1.0051],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367912.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8919e-03, -1.9774e-05, -8.6972e-05,  ...,  1.4463e-02,
          7.6399e-03, -5.7444e-03],
        [ 8.0868e-03, -2.0193e-05, -8.8815e-05,  ...,  1.4733e-02,
          7.8021e-03, -5.8661e-03],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7052e-03,
         -1.9528e-05,  0.0000e+00],
        ...,
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7052e-03,
         -1.9528e-05,  0.0000e+00],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7052e-03,
         -1.9528e-05,  0.0000e+00],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7052e-03,
         -1.9528e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2866.9956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2857, device='cuda:0')



h[100].sum tensor(-3.5529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-32.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0457, 0.0000, 0.0000,  ..., 0.0775, 0.0424, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0304, 0.0141, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0200, 0.0079, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68158.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0908, 0.0000,  ..., 0.3028, 0.0000, 0.0403],
        [0.0000, 0.0463, 0.0000,  ..., 0.1871, 0.0000, 0.0170],
        [0.0000, 0.0208, 0.0000,  ..., 0.1173, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561600.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.7468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.5474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.2798, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0627],
        [-0.2148],
        [-0.4058],
        ...,
        [-0.6919],
        [-0.6898],
        [-0.6891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132787.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0068],
        [1.0051],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367912.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0069],
        [1.0052],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367924.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2982e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7276e-03,
         -2.6220e-05,  0.0000e+00],
        [ 2.6875e-02, -5.8317e-05, -2.6038e-04,  ...,  4.0789e-02,
          2.3423e-02, -1.7561e-02],
        [-1.2982e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7276e-03,
         -2.6220e-05,  0.0000e+00],
        ...,
        [-1.2982e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7276e-03,
         -2.6220e-05,  0.0000e+00],
        [-1.2982e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7276e-03,
         -2.6220e-05,  0.0000e+00],
        [-1.2982e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7276e-03,
         -2.6220e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2841.7644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2499, device='cuda:0')



h[100].sum tensor(-3.4549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-31.6734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0590, 0.0000, 0.0000,  ..., 0.0959, 0.0533, 0.0000],
        [0.0205, 0.0000, 0.0000,  ..., 0.0391, 0.0192, 0.0000],
        [0.0618, 0.0000, 0.0000,  ..., 0.0999, 0.0557, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66568.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0933, 0.0000,  ..., 0.3064, 0.0000, 0.0415],
        [0.0000, 0.0726, 0.0000,  ..., 0.2534, 0.0000, 0.0306],
        [0.0000, 0.0965, 0.0000,  ..., 0.3147, 0.0000, 0.0432],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540350.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.8294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.1375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.5555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1382],
        [-0.1530],
        [-0.1786],
        ...,
        [-0.6896],
        [-0.6877],
        [-0.6880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146422.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0069],
        [1.0052],
        ...,
        [1.0006],
        [0.9998],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367924.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0070],
        [1.0053],
        ...,
        [1.0005],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367936.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9560e-03, -1.4469e-05, -6.5591e-05,  ...,  1.1814e-02,
          6.0166e-03, -4.5172e-03],
        [ 2.0371e-02, -4.3201e-05, -1.9584e-04,  ...,  3.1798e-02,
          1.8013e-02, -1.3488e-02],
        [ 2.5422e-02, -5.3270e-05, -2.4149e-04,  ...,  3.8801e-02,
          2.2218e-02, -1.6631e-02],
        ...,
        [-1.3027e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7501e-03,
         -2.4721e-05,  0.0000e+00],
        [-1.3027e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7501e-03,
         -2.4721e-05,  0.0000e+00],
        [-1.3027e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7501e-03,
         -2.4721e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2622.9333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0626, device='cuda:0')



h[100].sum tensor(-2.9174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-26.8696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0290, 0.0000, 0.0000,  ..., 0.0509, 0.0263, 0.0000],
        [0.0694, 0.0000, 0.0000,  ..., 0.1106, 0.0620, 0.0000],
        [0.1183, 0.0000, 0.0000,  ..., 0.1783, 0.1027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60710.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1155, 0.0000,  ..., 0.3643, 0.0000, 0.0529],
        [0.0000, 0.2035, 0.0000,  ..., 0.5871, 0.0000, 0.0997],
        [0.0000, 0.3003, 0.0000,  ..., 0.8294, 0.0000, 0.1517],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514575.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(436.1817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.2855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.1651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0200],
        [-0.0141],
        [-0.0501],
        ...,
        [-0.6996],
        [-0.6975],
        [-0.6968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128928.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0070],
        [1.0053],
        ...,
        [1.0005],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367936.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0071],
        [1.0054],
        ...,
        [1.0005],
        [0.9997],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367949.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7505e-03, -2.1248e-05, -9.7810e-05,  ...,  1.7122e-02,
          9.1991e-03, -6.8795e-03],
        [-1.3204e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7720e-03,
         -1.5863e-05,  0.0000e+00],
        [-1.3204e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7720e-03,
         -1.5863e-05,  0.0000e+00],
        ...,
        [-1.3204e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7720e-03,
         -1.5863e-05,  0.0000e+00],
        [-1.3204e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7720e-03,
         -1.5863e-05,  0.0000e+00],
        [-1.3204e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7720e-03,
         -1.5863e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2576.5889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0344, device='cuda:0')



h[100].sum tensor(-2.7894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-25.8109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0000, 0.0000,  ..., 0.0382, 0.0186, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0226, 0.0093, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58428.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0610, 0.0000,  ..., 0.2285, 0.0000, 0.0252],
        [0.0000, 0.0290, 0.0000,  ..., 0.1463, 0.0000, 0.0103],
        [0.0000, 0.0065, 0.0000,  ..., 0.0825, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500249.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.8561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.9815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.3451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0625e-04],
        [-7.1945e-02],
        [-1.5769e-01],
        ...,
        [-7.0066e-01],
        [-6.9857e-01],
        [-6.9797e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145429.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0071],
        [1.0054],
        ...,
        [1.0005],
        [0.9997],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367949.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0072],
        [1.0055],
        ...,
        [1.0005],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367961.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0689e-02, -2.2227e-05, -1.0391e-04,  ...,  1.8476e-02,
          1.0012e-02, -7.4647e-03],
        [ 3.9015e-03, -9.6861e-06, -4.5282e-05,  ...,  9.0638e-03,
          4.3610e-03, -3.2530e-03],
        [ 5.4466e-03, -1.2541e-05, -5.8628e-05,  ...,  1.1206e-02,
          5.6473e-03, -4.2117e-03],
        ...,
        [-1.3408e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7943e-03,
         -3.2641e-06,  0.0000e+00],
        [-1.3408e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7943e-03,
         -3.2641e-06,  0.0000e+00],
        [-1.3408e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7943e-03,
         -3.2641e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2478.3115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9453, device='cuda:0')



h[100].sum tensor(-2.5525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-23.7292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0000, 0.0000,  ..., 0.0349, 0.0166, 0.0000],
        [0.0387, 0.0000, 0.0000,  ..., 0.0684, 0.0367, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0350, 0.0167, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56172.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0536, 0.0000,  ..., 0.2196, 0.0000, 0.0206],
        [0.0000, 0.0748, 0.0000,  ..., 0.2756, 0.0000, 0.0317],
        [0.0000, 0.0495, 0.0000,  ..., 0.2082, 0.0000, 0.0187],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495027.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.5955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.5770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.0177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0832],
        [ 0.1221],
        [ 0.1067],
        ...,
        [-0.7030],
        [-0.7009],
        [-0.7003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151100.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0072],
        [1.0055],
        ...,
        [1.0005],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367961.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0073],
        [1.0056],
        ...,
        [1.0004],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367973.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6478e-03, -1.0693e-05, -5.0774e-05,  ...,  1.0155e-02,
          5.0215e-03, -3.7258e-03],
        [-1.3646e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8160e-03,
          1.4631e-05,  0.0000e+00],
        [-1.3646e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8160e-03,
          1.4631e-05,  0.0000e+00],
        ...,
        [-1.3646e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8160e-03,
          1.4631e-05,  0.0000e+00],
        [-1.3646e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8160e-03,
          1.4631e-05,  0.0000e+00],
        [-1.3646e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8160e-03,
          1.4631e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2560.3579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0211, device='cuda:0')



h[100].sum tensor(-2.7377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-25.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7656e-02, 0.0000e+00, 0.0000e+00,  ..., 3.5579e-02, 1.7043e-02,
         0.0000e+00],
        [4.6771e-03, 0.0000e+00, 0.0000e+00,  ..., 1.5708e-02, 5.0974e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.3195e-03, 5.8972e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4187e-03, 5.9771e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4183e-03, 5.9768e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4181e-03, 5.9766e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59108.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0504, 0.0000,  ..., 0.2136, 0.0000, 0.0202],
        [0.0000, 0.0234, 0.0000,  ..., 0.1351, 0.0000, 0.0080],
        [0.0000, 0.0060, 0.0000,  ..., 0.0790, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511820.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.7178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.0009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-217.6621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1223],
        [ 0.0343],
        [-0.0879],
        ...,
        [-0.6921],
        [-0.7001],
        [-0.7017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139399.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0073],
        [1.0056],
        ...,
        [1.0004],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367973.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0074],
        [1.0057],
        ...,
        [1.0004],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367986., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3472e-03, -9.8015e-06, -4.7279e-05,  ...,  9.7819e-03,
          4.7867e-03, -3.5441e-03],
        [-1.3800e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8370e-03,
          1.6155e-05,  0.0000e+00],
        [-1.3800e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8370e-03,
          1.6155e-05,  0.0000e+00],
        ...,
        [-1.3800e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8370e-03,
          1.6155e-05,  0.0000e+00],
        [-1.3800e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8370e-03,
          1.6155e-05,  0.0000e+00],
        [-1.3800e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8370e-03,
          1.6155e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2580.2625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0318, device='cuda:0')



h[100].sum tensor(-2.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-26.2121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0175e-02, 0.0000e+00, 0.0000e+00,  ..., 2.7255e-02, 1.2001e-02,
         0.0000e+00],
        [7.6936e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1928e-02, 8.7877e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4047e-03, 6.5118e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.5053e-03, 6.6004e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.5049e-03, 6.6000e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.5047e-03, 6.5998e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61738.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.2488e-02, 0.0000e+00,  ..., 1.7287e-01, 0.0000e+00,
         1.0984e-02],
        [0.0000e+00, 2.0936e-02, 0.0000e+00,  ..., 1.3669e-01, 0.0000e+00,
         6.4776e-03],
        [0.0000e+00, 4.7208e-03, 0.0000e+00,  ..., 7.9881e-02, 0.0000e+00,
         8.4027e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3183e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3181e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3179e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535691.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(444.5172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-231.1549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.1467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1113],
        [ 0.0249],
        [-0.1155],
        ...,
        [-0.7187],
        [-0.7165],
        [-0.7159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143293.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0074],
        [1.0057],
        ...,
        [1.0004],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367986., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0075],
        [1.0058],
        ...,
        [1.0004],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367997.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3947e-02, -4.1701e-05, -2.0436e-04,  ...,  3.6999e-02,
          2.1094e-02, -1.5651e-02],
        [ 3.9339e-02, -6.7040e-05, -3.2855e-04,  ...,  5.8353e-02,
          3.3916e-02, -2.5162e-02],
        [ 2.7533e-02, -4.7604e-05, -2.3329e-04,  ...,  4.1974e-02,
          2.4081e-02, -1.7867e-02],
        ...,
        [-1.3831e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8578e-03,
         -6.5350e-06,  0.0000e+00],
        [-1.3831e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8578e-03,
         -6.5350e-06,  0.0000e+00],
        [-1.3831e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8578e-03,
         -6.5350e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2259.3005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7835, device='cuda:0')



h[100].sum tensor(-2.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-19.7617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1042, 0.0000, 0.0000,  ..., 0.1597, 0.0914, 0.0000],
        [0.1355, 0.0000, 0.0000,  ..., 0.2032, 0.1175, 0.0000],
        [0.1437, 0.0000, 0.0000,  ..., 0.2147, 0.1244, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52117.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2527, 0.0000,  ..., 0.7344, 0.0000, 0.1327],
        [0.0000, 0.3227, 0.0000,  ..., 0.9146, 0.0000, 0.1705],
        [0.0000, 0.3327, 0.0000,  ..., 0.9420, 0.0000, 0.1756],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483437.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.4773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-216.2515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.5390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1333],
        [ 0.1452],
        [ 0.1578],
        ...,
        [-0.7361],
        [-0.7337],
        [-0.7330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175999.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0075],
        [1.0058],
        ...,
        [1.0004],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367997.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0076],
        [1.0059],
        ...,
        [1.0003],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368008.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0687e-02, -1.9110e-05, -9.5164e-05,  ...,  1.8617e-02,
          1.0028e-02, -7.4469e-03],
        [ 4.4464e-03, -9.2301e-06, -4.5964e-05,  ...,  9.9593e-03,
          4.8292e-03, -3.5968e-03],
        [ 4.8570e-03, -9.8801e-06, -4.9201e-05,  ...,  1.0529e-02,
          5.1713e-03, -3.8501e-03],
        ...,
        [-1.3832e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8711e-03,
         -2.7434e-05,  0.0000e+00],
        [-1.3832e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8711e-03,
         -2.7434e-05,  0.0000e+00],
        [-1.3832e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8711e-03,
         -2.7434e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2676.4912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1382, device='cuda:0')



h[100].sum tensor(-3.0283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-28.6849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0000, 0.0000,  ..., 0.0352, 0.0165, 0.0000],
        [0.0364, 0.0000, 0.0000,  ..., 0.0658, 0.0349, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0354, 0.0166, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63237.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0482, 0.0000,  ..., 0.2146, 0.0000, 0.0207],
        [0.0000, 0.0667, 0.0000,  ..., 0.2646, 0.0000, 0.0305],
        [0.0000, 0.0432, 0.0000,  ..., 0.1992, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540684.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4.3034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.4619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.9545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0695],
        [ 0.0593],
        [-0.0491],
        ...,
        [-0.7477],
        [-0.7452],
        [-0.7444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154970.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0076],
        [1.0059],
        ...,
        [1.0003],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368008.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0077],
        [1.0060],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368020.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8149e-02, -2.9718e-05, -1.5040e-04,  ...,  2.8957e-02,
          1.6208e-02, -1.2026e-02],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        ...,
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2768.1001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2140, device='cuda:0')



h[100].sum tensor(-3.2064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-30.5154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0680, 0.0000, 0.0000,  ..., 0.1075, 0.0599, 0.0000],
        [0.0449, 0.0000, 0.0000,  ..., 0.0756, 0.0407, 0.0000],
        [0.0252, 0.0000, 0.0000,  ..., 0.0482, 0.0243, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64693.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2070, 0.0000,  ..., 0.6214, 0.0000, 0.1070],
        [0.0000, 0.1708, 0.0000,  ..., 0.5297, 0.0000, 0.0872],
        [0.0000, 0.1346, 0.0000,  ..., 0.4386, 0.0000, 0.0671],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545341.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.8395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14.3822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.5593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.4255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1252],
        [ 0.1370],
        [ 0.1484],
        ...,
        [-0.7556],
        [-0.7531],
        [-0.7524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155737.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0077],
        [1.0060],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368020.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0077],
        [1.0060],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368020.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        ...,
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00],
        [-1.3748e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8715e-03,
         -5.4870e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2655.9033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1369, device='cuda:0')



h[100].sum tensor(-2.9586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-28.1570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0164, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62165.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0482, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0657, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.1156, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535280.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(451.6993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21.7563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.3916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.6687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4182],
        [-0.2412],
        [-0.0696],
        ...,
        [-0.7556],
        [-0.7531],
        [-0.7524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181633.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0077],
        [1.0060],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368020.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0078],
        [1.0061],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368031.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0463e-03, -1.0842e-05, -5.5768e-05,  ...,  1.2147e-02,
          6.0891e-03, -4.5573e-03],
        [-1.3642e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8675e-03,
         -8.2940e-05,  0.0000e+00],
        [ 6.0463e-03, -1.0842e-05, -5.5768e-05,  ...,  1.2147e-02,
          6.0891e-03, -4.5573e-03],
        ...,
        [-1.3642e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8675e-03,
         -8.2940e-05,  0.0000e+00],
        [-1.3642e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8675e-03,
         -8.2940e-05,  0.0000e+00],
        [-1.3642e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8675e-03,
         -8.2940e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3895.0103, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1173, device='cuda:0')



h[100].sum tensor(-5.6478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0221, device='cuda:0')



h[200].sum tensor(-54.0048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0000, 0.0000,  ..., 0.0289, 0.0127, 0.0000],
        [0.0263, 0.0000, 0.0000,  ..., 0.0517, 0.0262, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0160, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97521.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0415, 0.0000,  ..., 0.1934, 0.0000, 0.0162],
        [0.0000, 0.0494, 0.0000,  ..., 0.2155, 0.0000, 0.0201],
        [0.0000, 0.0255, 0.0000,  ..., 0.1474, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734463.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(597.7982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33.9421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.0079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.5725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1082],
        [ 0.1042],
        [ 0.0454],
        ...,
        [-0.7622],
        [-0.7597],
        [-0.7590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192603.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0078],
        [1.0061],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368031.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0080],
        [1.0062],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368042.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3970e-03, -8.0990e-06, -4.2351e-05,  ...,  9.8414e-03,
          4.6918e-03, -3.5371e-03],
        [ 4.3970e-03, -8.0990e-06, -4.2351e-05,  ...,  9.8414e-03,
          4.6918e-03, -3.5371e-03],
        [-1.3626e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8503e-03,
         -1.0628e-04,  0.0000e+00],
        ...,
        [-1.3626e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8503e-03,
         -1.0628e-04,  0.0000e+00],
        [-1.3626e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8503e-03,
         -1.0628e-04,  0.0000e+00],
        [-1.3626e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8503e-03,
         -1.0628e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2721.8174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1669, device='cuda:0')



h[100].sum tensor(-3.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-29.1104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0000,  ..., 0.0330, 0.0150, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0276, 0.0118, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0221, 0.0086, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62478.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0451, 0.0000,  ..., 0.2059, 0.0000, 0.0165],
        [0.0000, 0.0357, 0.0000,  ..., 0.1808, 0.0000, 0.0117],
        [0.0000, 0.0232, 0.0000,  ..., 0.1452, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534488.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.5973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.7140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.8601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1438],
        [ 0.1056],
        [ 0.0269],
        ...,
        [-0.7645],
        [-0.7619],
        [-0.7611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169798.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0080],
        [1.0062],
        ...,
        [1.0003],
        [0.9995],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368042.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0081],
        [1.0063],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368054.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3649e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8208e-03,
         -1.2296e-04,  0.0000e+00],
        [ 1.3600e-02, -2.0221e-05, -1.0751e-04,  ...,  2.2587e-02,
          1.2347e-02, -9.1778e-03],
        [-1.3649e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8208e-03,
         -1.2296e-04,  0.0000e+00],
        ...,
        [-1.3649e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8208e-03,
         -1.2296e-04,  0.0000e+00],
        [-1.3649e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8208e-03,
         -1.2296e-04,  0.0000e+00],
        [-1.3649e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8208e-03,
         -1.2296e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2505.3494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9626, device='cuda:0')



h[100].sum tensor(-2.4986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-24.1196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0261, 0.0000, 0.0000,  ..., 0.0511, 0.0258, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0244, 0.0100, 0.0000],
        [0.0327, 0.0000, 0.0000,  ..., 0.0604, 0.0314, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57669.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0583, 0.0000,  ..., 0.2372, 0.0000, 0.0231],
        [0.0000, 0.0511, 0.0000,  ..., 0.2178, 0.0000, 0.0196],
        [0.0000, 0.0716, 0.0000,  ..., 0.2729, 0.0000, 0.0302],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512427.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.1501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5.9064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.8471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.6757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1632],
        [ 0.1640],
        [ 0.1646],
        ...,
        [-0.7632],
        [-0.7605],
        [-0.7593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165826.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0081],
        [1.0063],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368054.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0082],
        [1.0064],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368066.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2307e-02, -1.7743e-05, -9.5922e-05,  ...,  2.0760e-02,
          1.1249e-02, -8.3708e-03],
        [ 1.0703e-02, -1.5661e-05, -8.4665e-05,  ...,  1.8534e-02,
          9.9124e-03, -7.3885e-03],
        [-1.3616e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7909e-03,
         -1.4208e-04,  0.0000e+00],
        ...,
        [-1.3616e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7909e-03,
         -1.4208e-04,  0.0000e+00],
        [-1.3616e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7909e-03,
         -1.4208e-04,  0.0000e+00],
        [-1.3616e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7909e-03,
         -1.4208e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2756.1074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1651, device='cuda:0')



h[100].sum tensor(-2.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-28.8365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0498, 0.0000, 0.0000,  ..., 0.0838, 0.0455, 0.0000],
        [0.0210, 0.0000, 0.0000,  ..., 0.0401, 0.0195, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0241, 0.0100, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61119.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0879, 0.0000,  ..., 0.3051, 0.0000, 0.0395],
        [0.0000, 0.0550, 0.0000,  ..., 0.2187, 0.0000, 0.0224],
        [0.0000, 0.0282, 0.0000,  ..., 0.1457, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517215.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(445.7023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-228.0436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.3963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1177],
        [-0.1467],
        [-0.1957],
        ...,
        [-0.7619],
        [-0.7595],
        [-0.7588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172568.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0082],
        [1.0064],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368066.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0084],
        [1.0065],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368078.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1001e-03, -6.8043e-06, -3.7412e-05,  ...,  9.3395e-03,
          4.3927e-03, -3.3377e-03],
        [ 1.1959e-02, -1.6602e-05, -9.1281e-05,  ...,  2.0247e-02,
          1.0943e-02, -8.1437e-03],
        [ 5.8195e-03, -8.9477e-06, -4.9197e-05,  ...,  1.1726e-02,
          5.8257e-03, -4.3891e-03],
        ...,
        [-1.3581e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7641e-03,
         -1.5654e-04,  0.0000e+00],
        [-1.3581e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7641e-03,
         -1.5654e-04,  0.0000e+00],
        [-1.3581e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7641e-03,
         -1.5654e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2472.5208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8955, device='cuda:0')



h[100].sum tensor(-2.3019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-22.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0332, 0.0000, 0.0000,  ..., 0.0607, 0.0316, 0.0000],
        [0.0182, 0.0000, 0.0000,  ..., 0.0400, 0.0191, 0.0000],
        [0.0339, 0.0000, 0.0000,  ..., 0.0617, 0.0322, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56257.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0620, 0.0000,  ..., 0.2416, 0.0000, 0.0231],
        [0.0000, 0.0641, 0.0000,  ..., 0.2498, 0.0000, 0.0235],
        [0.0000, 0.0826, 0.0000,  ..., 0.2982, 0.0000, 0.0333],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504434.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(423.2044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.5050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.7143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1835],
        [-0.0830],
        [-0.0249],
        ...,
        [-0.5527],
        [-0.6497],
        [-0.7160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145468.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0084],
        [1.0065],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368078.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0085],
        [1.0065],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368090.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        ...,
        [-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0014,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2351.4800, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7818, device='cuda:0')



h[100].sum tensor(-1.9928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-19.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53690.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0454, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0483, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492394.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(412.5972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-208.0517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.7623, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6837],
        [-0.5551],
        [-0.3881],
        ...,
        [-0.7492],
        [-0.7430],
        [-0.7383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143237.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0085],
        [1.0065],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368090.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0086],
        [1.0066],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368101.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2478.8696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8752, device='cuda:0')



h[100].sum tensor(-2.2276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-21.9174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55788.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0460, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500140.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.5654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-211.2485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.3628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6696],
        [-0.7557],
        [-0.8232],
        ...,
        [-0.7557],
        [-0.7537],
        [-0.7533]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142094.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0086],
        [1.0066],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368101.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0087],
        [1.0067],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368113.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3429e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7094e-03,
         -1.8965e-04,  0.0000e+00],
        [ 1.1351e-02, -1.3996e-05, -8.1025e-05,  ...,  1.9331e-02,
          1.0394e-02, -7.7284e-03],
        [ 1.7588e-02, -2.0872e-05, -1.2083e-04,  ...,  2.7989e-02,
          1.5594e-02, -1.1525e-02],
        ...,
        [-1.3429e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7094e-03,
         -1.8965e-04,  0.0000e+00],
        [-1.3429e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7094e-03,
         -1.8965e-04,  0.0000e+00],
        [-1.3429e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7094e-03,
         -1.8965e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2361.2822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7792, device='cuda:0')



h[100].sum tensor(-1.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0081, device='cuda:0')



h[200].sum tensor(-19.4400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.0566, 0.0291, 0.0000],
        [0.0268, 0.0000, 0.0000,  ..., 0.0479, 0.0243, 0.0000],
        [0.0257, 0.0000, 0.0000,  ..., 0.0463, 0.0233, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53094.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0638, 0.0000,  ..., 0.2392, 0.0000, 0.0226],
        [0.0000, 0.0773, 0.0000,  ..., 0.2729, 0.0000, 0.0300],
        [0.0000, 0.0879, 0.0000,  ..., 0.3011, 0.0000, 0.0353],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489259.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.9586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-206.9358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.1007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1245],
        [ 0.1604],
        [ 0.1723],
        ...,
        [-0.7644],
        [-0.7623],
        [-0.7617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151613.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0087],
        [1.0067],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368113.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0088],
        [1.0068],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368125.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2564.6069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9428, device='cuda:0')



h[100].sum tensor(-2.3828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-23.6716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60315.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0526, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0758, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533787.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(448.1744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.2579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.1334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1780],
        [-0.1670],
        [-0.1258],
        ...,
        [-0.7730],
        [-0.7708],
        [-0.7702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149389.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0088],
        [1.0068],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368125.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(1027.4065, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0089],
        [1.0069],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368137.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6305e-02, -1.7911e-05, -1.0740e-04,  ...,  2.6197e-02,
          1.4534e-02, -1.0716e-02],
        [-1.3465e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6854e-03,
         -1.9085e-04,  0.0000e+00],
        [-1.3465e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6854e-03,
         -1.9085e-04,  0.0000e+00],
        ...,
        [-1.3465e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6854e-03,
         -1.9085e-04,  0.0000e+00],
        [-1.3465e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6854e-03,
         -1.9085e-04,  0.0000e+00],
        [-1.3465e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6854e-03,
         -1.9085e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2581.8213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9591, device='cuda:0')



h[100].sum tensor(-2.4156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-24.1133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0525, 0.0000, 0.0000,  ..., 0.0834, 0.0457, 0.0000],
        [0.0296, 0.0000, 0.0000,  ..., 0.0517, 0.0266, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59705.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1339, 0.0000,  ..., 0.4081, 0.0000, 0.0617],
        [0.0000, 0.0906, 0.0000,  ..., 0.2991, 0.0000, 0.0389],
        [0.0000, 0.0422, 0.0000,  ..., 0.1739, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530417.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.3519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.1964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.8568, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [ 0.0733],
        [ 0.0922],
        ...,
        [-0.7839],
        [-0.7816],
        [-0.7810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148347.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0089],
        [1.0069],
        ...,
        [1.0002],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368137.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0090],
        [1.0070],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368149.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8724e-02, -1.9530e-05, -1.1922e-04,  ...,  2.9550e-02,
          1.6552e-02, -1.2168e-02],
        [ 7.6692e-03, -8.7734e-06, -5.3555e-05,  ...,  1.4198e-02,
          7.3293e-03, -5.4661e-03],
        [ 1.0228e-02, -1.1263e-05, -6.8755e-05,  ...,  1.7752e-02,
          9.4642e-03, -7.0174e-03],
        ...,
        [-1.3476e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6768e-03,
         -1.9304e-04,  0.0000e+00],
        [-1.3476e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6768e-03,
         -1.9304e-04,  0.0000e+00],
        [-1.3476e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6768e-03,
         -1.9304e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2905.2798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2419, device='cuda:0')



h[100].sum tensor(-3.0809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-30.9034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0526, 0.0000, 0.0000,  ..., 0.0873, 0.0476, 0.0000],
        [0.0530, 0.0000, 0.0000,  ..., 0.0879, 0.0480, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0327, 0.0152, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67042.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2053, 0.0000,  ..., 0.5984, 0.0000, 0.0977],
        [0.0000, 0.1635, 0.0000,  ..., 0.4926, 0.0000, 0.0755],
        [0.0000, 0.1025, 0.0000,  ..., 0.3348, 0.0000, 0.0435],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0622, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0676, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563128., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(482.5161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.2894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.5608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2277],
        [ 0.2186],
        [ 0.2076],
        ...,
        [-0.6898],
        [-0.6112],
        [-0.5606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147122.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0090],
        [1.0070],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368149.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0091],
        [1.0071],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368160.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2598.8696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9976, device='cuda:0')



h[100].sum tensor(-2.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-24.7423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58365.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.4839e-02, 0.0000e+00,  ..., 1.2306e-01, 0.0000e+00,
         7.5379e-03],
        [0.0000e+00, 3.9166e-03, 0.0000e+00,  ..., 6.8293e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.9638e-05, 0.0000e+00,  ..., 5.0245e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5687e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5685e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5684e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516517.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(451.1050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-229.5205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.1329, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0550],
        [-0.1910],
        [-0.3120],
        ...,
        [-0.8041],
        [-0.8016],
        [-0.8009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181502.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0091],
        [1.0071],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368160.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0092],
        [1.0071],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368172.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2452e-03, -9.4513e-06, -5.9818e-05,  ...,  1.6363e-02,
          8.6074e-03, -6.3902e-03],
        [ 6.3991e-03, -6.9081e-06, -4.3722e-05,  ...,  1.2412e-02,
          6.2340e-03, -4.6707e-03],
        [ 4.0594e-03, -4.8173e-06, -3.0489e-05,  ...,  9.1633e-03,
          4.2828e-03, -3.2571e-03],
        ...,
        [-1.3316e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6788e-03,
         -2.1290e-04,  0.0000e+00],
        [-1.3316e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6788e-03,
         -2.1290e-04,  0.0000e+00],
        [-1.3316e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6788e-03,
         -2.1290e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2443.4951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8739, device='cuda:0')



h[100].sum tensor(-2.1286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-21.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.0000, 0.0000,  ..., 0.0580, 0.0299, 0.0000],
        [0.0215, 0.0000, 0.0000,  ..., 0.0441, 0.0216, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0367, 0.0171, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54866.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0740, 0.0000,  ..., 0.2605, 0.0000, 0.0273],
        [0.0000, 0.0733, 0.0000,  ..., 0.2617, 0.0000, 0.0261],
        [0.0000, 0.0660, 0.0000,  ..., 0.2438, 0.0000, 0.0220],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505294.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(436.2609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.0396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.6758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1031],
        [ 0.1381],
        [ 0.1452],
        ...,
        [-0.8118],
        [-0.8092],
        [-0.8085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178418.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0092],
        [1.0071],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368172.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0093],
        [1.0072],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368184.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6882e-03,
         -2.1922e-04,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6882e-03,
         -2.1922e-04,  0.0000e+00],
        [ 1.3906e-02, -1.3041e-05, -8.4063e-05,  ...,  2.2836e-02,
          1.2483e-02, -9.1887e-03],
        ...,
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6882e-03,
         -2.1922e-04,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6882e-03,
         -2.1922e-04,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  1.6882e-03,
         -2.1922e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3057.5437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3534, device='cuda:0')



h[100].sum tensor(-3.3311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-33.9033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0282, 0.0126, 0.0000],
        [0.0482, 0.0000, 0.0000,  ..., 0.0774, 0.0420, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70966.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.1015, 0.0000, 0.0047],
        [0.0000, 0.0751, 0.0000,  ..., 0.2532, 0.0000, 0.0320],
        [0.0000, 0.1758, 0.0000,  ..., 0.5081, 0.0000, 0.0830],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590587.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(500.7129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.5628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.7153, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [ 0.0204],
        [ 0.0839],
        ...,
        [-0.8180],
        [-0.8155],
        [-0.8148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175435.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0093],
        [1.0072],
        ...,
        [1.0003],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368184.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0094],
        [1.0073],
        ...,
        [1.0003],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368196.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0017, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2613.3848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0088, device='cuda:0')



h[100].sum tensor(-2.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-24.5825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59293.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0460, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533556.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.3162, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.4024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.9493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8757],
        [-0.7746],
        [-0.6273],
        ...,
        [-0.8216],
        [-0.8186],
        [-0.8176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158333.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0094],
        [1.0073],
        ...,
        [1.0003],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368196.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0095],
        [1.0074],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368208.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6730e-02, -1.4178e-05, -9.4860e-05,  ...,  2.6791e-02,
          1.4838e-02, -1.0859e-02],
        [-1.3351e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7211e-03,
         -2.1689e-04,  0.0000e+00],
        [-1.3351e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7211e-03,
         -2.1689e-04,  0.0000e+00],
        ...,
        [-1.3351e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7211e-03,
         -2.1689e-04,  0.0000e+00],
        [-1.3351e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7211e-03,
         -2.1689e-04,  0.0000e+00],
        [-1.3351e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7211e-03,
         -2.1689e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2283.0952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.5157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7063, device='cuda:0')



h[100].sum tensor(-1.7040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0074, device='cuda:0')



h[200].sum tensor(-17.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0470, 0.0000, 0.0000,  ..., 0.0778, 0.0419, 0.0000],
        [0.0304, 0.0000, 0.0000,  ..., 0.0529, 0.0271, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51453.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1324, 0.0000,  ..., 0.4062, 0.0000, 0.0596],
        [0.0000, 0.0835, 0.0000,  ..., 0.2802, 0.0000, 0.0349],
        [0.0000, 0.0235, 0.0000,  ..., 0.1202, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493660.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-217.6838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.0461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0878],
        [-0.0084],
        [-0.2025],
        ...,
        [-0.8257],
        [-0.8232],
        [-0.8225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185118.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0095],
        [1.0074],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368208.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [ 4.7573e-03, -4.5883e-06, -3.1283e-05,  ...,  1.0218e-02,
          4.8834e-03, -3.6652e-03],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        ...,
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3998.8865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1034, device='cuda:0')



h[100].sum tensor(-5.0065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0219, device='cuda:0')



h[200].sum tensor(-51.7078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0346, 0.0000, 0.0000,  ..., 0.0625, 0.0325, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0140, 0.0040, 0.0000],
        [0.0346, 0.0000, 0.0000,  ..., 0.0607, 0.0316, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90566.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1225, 0.0000,  ..., 0.3891, 0.0000, 0.0540],
        [0.0000, 0.1084, 0.0000,  ..., 0.3515, 0.0000, 0.0472],
        [0.0000, 0.1588, 0.0000,  ..., 0.4791, 0.0000, 0.0733],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682487.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(566.3047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.0211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.0331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1300],
        [ 0.1291],
        [ 0.1284],
        ...,
        [-0.8241],
        [-0.8219],
        [-0.8216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146655.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1958e-02, -1.7510e-05, -1.1939e-04,  ...,  3.4087e-02,
          1.9217e-02, -1.3988e-02],
        [ 2.3897e-02, -1.8967e-05, -1.2932e-04,  ...,  3.6778e-02,
          2.0833e-02, -1.5152e-02],
        [ 2.2182e-02, -1.7679e-05, -1.2054e-04,  ...,  3.4398e-02,
          1.9404e-02, -1.4122e-02],
        ...,
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2926.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2206, device='cuda:0')



h[100].sum tensor(-2.9115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-30.0702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0833, 0.0000, 0.0000,  ..., 0.1301, 0.0731, 0.0000],
        [0.0972, 0.0000, 0.0000,  ..., 0.1494, 0.0847, 0.0000],
        [0.1013, 0.0000, 0.0000,  ..., 0.1551, 0.0881, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67438.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2905, 0.0000,  ..., 0.8099, 0.0000, 0.1427],
        [0.0000, 0.2984, 0.0000,  ..., 0.8295, 0.0000, 0.1468],
        [0.0000, 0.2827, 0.0000,  ..., 0.7904, 0.0000, 0.1383],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576532.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(470.8673, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.8383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.2139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1528],
        [ 0.1532],
        [ 0.1550],
        ...,
        [-0.8258],
        [-0.8233],
        [-0.8227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140429.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5307e-02, -2.0027e-05, -1.3654e-04,  ...,  3.8735e-02,
          2.2008e-02, -1.5998e-02],
        [ 1.7271e-02, -1.3989e-05, -9.5378e-05,  ...,  2.7583e-02,
          1.5311e-02, -1.1175e-02],
        [ 4.8777e-03, -4.6786e-06, -3.1899e-05,  ...,  1.0385e-02,
          4.9837e-03, -3.7374e-03],
        ...,
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00],
        [-1.3500e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7433e-03,
         -2.0605e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2774.7881, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0984, device='cuda:0')



h[100].sum tensor(-2.6141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-26.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0917, 0.0000, 0.0000,  ..., 0.1418, 0.0801, 0.0000],
        [0.0490, 0.0000, 0.0000,  ..., 0.0826, 0.0445, 0.0000],
        [0.0412, 0.0000, 0.0000,  ..., 0.0718, 0.0381, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62495.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2331, 0.0000,  ..., 0.6673, 0.0000, 0.1122],
        [0.0000, 0.1733, 0.0000,  ..., 0.5199, 0.0000, 0.0802],
        [0.0000, 0.1358, 0.0000,  ..., 0.4269, 0.0000, 0.0601],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549958.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.2110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.2247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1739],
        [ 0.1849],
        [ 0.1912],
        ...,
        [-0.8258],
        [-0.8233],
        [-0.8227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145652.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0096],
        [1.0075],
        ...,
        [1.0004],
        [0.9996],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368220.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(427.5880, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0097],
        [1.0076],
        ...,
        [1.0004],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368232.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3512e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7688e-03,
         -1.9933e-04,  0.0000e+00],
        [-1.3512e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7688e-03,
         -1.9933e-04,  0.0000e+00],
        [ 5.8454e-03, -5.1736e-06, -3.5952e-05,  ...,  1.1755e-02,
          5.7970e-03, -4.3118e-03],
        ...,
        [-1.3512e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7688e-03,
         -1.9933e-04,  0.0000e+00],
        [-1.3512e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7688e-03,
         -1.9933e-04,  0.0000e+00],
        [-1.3512e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7688e-03,
         -1.9933e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2648.6226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9769, device='cuda:0')



h[100].sum tensor(-2.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-24.0253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0172, 0.0059, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0276, 0.0119, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61367.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0839, 0.0000, 0.0000],
        [0.0000, 0.0174, 0.0000,  ..., 0.1193, 0.0000, 0.0039],
        [0.0000, 0.0365, 0.0000,  ..., 0.1719, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551801.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(441.8604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.2023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.3424, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1123],
        [ 0.0054],
        [ 0.0960],
        ...,
        [-0.8274],
        [-0.8250],
        [-0.8243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150565.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0097],
        [1.0076],
        ...,
        [1.0004],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368232.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0098],
        [1.0077],
        ...,
        [1.0005],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368244.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8768e-02, -1.3830e-05, -9.7973e-05,  ...,  2.9692e-02,
          1.6554e-02, -1.2029e-02],
        [ 1.7304e-02, -1.2824e-05, -9.0845e-05,  ...,  2.7662e-02,
          1.5335e-02, -1.1153e-02],
        [ 3.3823e-03, -3.2499e-06, -2.3023e-05,  ...,  8.3475e-03,
          3.7377e-03, -2.8266e-03],
        ...,
        [-1.3436e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7910e-03,
         -1.9914e-04,  0.0000e+00],
        [-1.3436e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7910e-03,
         -1.9914e-04,  0.0000e+00],
        [-1.3436e-03,  0.0000e+00,  0.0000e+00,  ...,  1.7910e-03,
         -1.9914e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2934.1265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1916, device='cuda:0')



h[100].sum tensor(-2.8232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-29.4462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0858, 0.0000, 0.0000,  ..., 0.1338, 0.0752, 0.0000],
        [0.0618, 0.0000, 0.0000,  ..., 0.1005, 0.0552, 0.0000],
        [0.0799, 0.0000, 0.0000,  ..., 0.1256, 0.0703, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64699.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2477, 0.0000,  ..., 0.7055, 0.0000, 0.1200],
        [0.0000, 0.2298, 0.0000,  ..., 0.6619, 0.0000, 0.1105],
        [0.0000, 0.2304, 0.0000,  ..., 0.6637, 0.0000, 0.1107],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0473, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0473, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0473, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559293.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(454.2755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.5176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.0638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1249],
        [ 0.1272],
        [ 0.1298],
        ...,
        [-0.8309],
        [-0.8284],
        [-0.8277]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155502.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0098],
        [1.0077],
        ...,
        [1.0005],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368244.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0099],
        [1.0079],
        ...,
        [1.0005],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368256.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3319e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8126e-03,
         -1.9926e-04,  0.0000e+00],
        [ 6.2571e-03, -4.9906e-06, -3.6048e-05,  ...,  1.2339e-02,
          6.1209e-03, -4.5311e-03],
        [-1.3319e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8126e-03,
         -1.9926e-04,  0.0000e+00],
        ...,
        [-1.3319e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8126e-03,
         -1.9926e-04,  0.0000e+00],
        [-1.3319e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8126e-03,
         -1.9926e-04,  0.0000e+00],
        [-1.3319e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8126e-03,
         -1.9926e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2806.9177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0882, device='cuda:0')



h[100].sum tensor(-2.5432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-26.6572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0000, 0.0000,  ..., 0.0179, 0.0062, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0160, 0.0050, 0.0000],
        [0.0267, 0.0000, 0.0000,  ..., 0.0518, 0.0259, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64265.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.1079, 0.0000, 0.0029],
        [0.0000, 0.0244, 0.0000,  ..., 0.1407, 0.0000, 0.0058],
        [0.0000, 0.0484, 0.0000,  ..., 0.2059, 0.0000, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566868.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.2255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.8524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.7181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0864],
        [ 0.0335],
        [ 0.0989],
        ...,
        [-0.8426],
        [-0.8402],
        [-0.8395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166780.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0099],
        [1.0079],
        ...,
        [1.0005],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368256.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0100],
        [1.0080],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368267.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0018, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2938.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1805, device='cuda:0')



h[100].sum tensor(-2.7575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-29.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62687.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0500, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545179.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(447.7494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.3989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.7023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6128],
        [-0.6936],
        [-0.7101],
        ...,
        [-0.8499],
        [-0.8477],
        [-0.8475]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178545.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0100],
        [1.0080],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368267.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0102],
        [1.0081],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368278.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8499e-03,
         -1.9581e-04,  0.0000e+00],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8499e-03,
         -1.9581e-04,  0.0000e+00],
        [ 4.6518e-03, -3.5831e-06, -2.6922e-05,  ...,  1.0119e-02,
          4.7686e-03, -3.5480e-03],
        ...,
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8499e-03,
         -1.9581e-04,  0.0000e+00],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8499e-03,
         -1.9581e-04,  0.0000e+00],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8499e-03,
         -1.9581e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2668.3630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9495, device='cuda:0')



h[100].sum tensor(-2.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-23.4191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0158, 0.0048, 0.0000],
        [0.0175, 0.0000, 0.0000,  ..., 0.0354, 0.0164, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59848.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0445, 0.0000,  ..., 0.1879, 0.0000, 0.0159],
        [0.0000, 0.0540, 0.0000,  ..., 0.2152, 0.0000, 0.0197],
        [0.0000, 0.0713, 0.0000,  ..., 0.2618, 0.0000, 0.0279],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0634, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.1023, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543936.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.0765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.1807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.3711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0102],
        [ 0.0303],
        [ 0.0499],
        ...,
        [-0.7133],
        [-0.5271],
        [-0.2975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170835.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0102],
        [1.0081],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368278.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0103],
        [1.0082],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368289.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9994e-03, -3.6237e-06, -2.7777e-05,  ...,  1.0612e-02,
          5.0659e-03, -3.7490e-03],
        [ 4.6346e-03, -3.4142e-06, -2.6171e-05,  ...,  1.0106e-02,
          4.7622e-03, -3.5323e-03],
        [-1.3105e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8603e-03,
         -1.8799e-04,  0.0000e+00],
        ...,
        [-1.3105e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8603e-03,
         -1.8799e-04,  0.0000e+00],
        [-1.3105e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8603e-03,
         -1.8799e-04,  0.0000e+00],
        [-1.3105e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8603e-03,
         -1.8799e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2772.2866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0145, device='cuda:0')



h[100].sum tensor(-2.3626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-25.1348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0280, 0.0000, 0.0000,  ..., 0.0536, 0.0269, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0304, 0.0133, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0158, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61187.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0542, 0.0000,  ..., 0.2209, 0.0000, 0.0190],
        [0.0000, 0.0360, 0.0000,  ..., 0.1721, 0.0000, 0.0106],
        [0.0000, 0.0155, 0.0000,  ..., 0.1122, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549187.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.3616, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.0502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.9279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0664],
        [ 0.0100],
        [-0.1164],
        ...,
        [-0.8619],
        [-0.8594],
        [-0.8591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191438.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0103],
        [1.0082],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368289.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0104],
        [1.0083],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368301.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2707.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9523, device='cuda:0')



h[100].sum tensor(-2.1945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-23.4638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59455.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.4457e-04, 0.0000e+00,  ..., 6.9004e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1126e-04, 0.0000e+00,  ..., 6.6133e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1554e-05, 0.0000e+00,  ..., 6.8720e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6316e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6315e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6315e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541276.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.1329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.1604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.6839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1620],
        [-0.1847],
        [-0.1735],
        ...,
        [-0.8676],
        [-0.8650],
        [-0.8644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181755.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0104],
        [1.0083],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368301.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0105],
        [1.0084],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368313.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3481.2263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5882, device='cuda:0')



h[100].sum tensor(-3.5599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-38.2520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77582.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0460, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634559.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(504.5776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.0180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.0143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8751],
        [-0.9654],
        [-1.0272],
        ...,
        [-0.8647],
        [-0.8625],
        [-0.8623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174400.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0105],
        [1.0084],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368313.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0106],
        [1.0085],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368325.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7453e-03, -3.5420e-06, -2.8866e-05,  ...,  1.1691e-02,
          5.7485e-03, -4.1870e-03],
        [-1.3335e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8712e-03,
         -1.4778e-04,  0.0000e+00],
        [-1.3335e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8712e-03,
         -1.4778e-04,  0.0000e+00],
        ...,
        [-1.3335e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8712e-03,
         -1.4778e-04,  0.0000e+00],
        [-1.3335e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8712e-03,
         -1.4778e-04,  0.0000e+00],
        [-1.3335e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8712e-03,
         -1.4778e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2701.9541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9065, device='cuda:0')



h[100].sum tensor(-2.0589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-22.2351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0000, 0.0000,  ..., 0.0282, 0.0121, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0175, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57742.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0377, 0.0000,  ..., 0.1786, 0.0000, 0.0119],
        [0.0000, 0.0164, 0.0000,  ..., 0.1161, 0.0000, 0.0045],
        [0.0000, 0.0017, 0.0000,  ..., 0.0665, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532701.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.4849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-229.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.0203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0746],
        [-0.3003],
        [-0.5527],
        ...,
        [-0.8621],
        [-0.8595],
        [-0.8588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166802.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0106],
        [1.0085],
        ...,
        [1.0005],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368325.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0107],
        [1.0086],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368336.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8747e-03,
         -1.5444e-04,  0.0000e+00],
        [ 1.1597e-02, -6.1755e-06, -5.1388e-05,  ...,  1.9813e-02,
          1.0617e-02, -7.6365e-03],
        [ 9.8771e-03, -5.3543e-06, -4.4554e-05,  ...,  1.7428e-02,
          9.1845e-03, -6.6210e-03],
        ...,
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8747e-03,
         -1.5444e-04,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8747e-03,
         -1.5444e-04,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8747e-03,
         -1.5444e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2867.2195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0255, device='cuda:0')



h[100].sum tensor(-2.3139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-25.1136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0117, 0.0000, 0.0000,  ..., 0.0256, 0.0107, 0.0000],
        [0.0193, 0.0000, 0.0000,  ..., 0.0381, 0.0180, 0.0000],
        [0.0636, 0.0000, 0.0000,  ..., 0.1033, 0.0569, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59779.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.9977e-02, 0.0000e+00,  ..., 1.5214e-01, 0.0000e+00,
         1.0802e-02],
        [0.0000e+00, 6.3567e-02, 0.0000e+00,  ..., 2.4145e-01, 0.0000e+00,
         2.5653e-02],
        [0.0000e+00, 1.2995e-01, 0.0000e+00,  ..., 4.0997e-01, 0.0000e+00,
         5.9457e-02],
        ...,
        [0.0000e+00, 9.6048e-03, 0.0000e+00,  ..., 9.9301e-02, 0.0000e+00,
         1.6028e-05],
        [0.0000e+00, 6.2410e-03, 0.0000e+00,  ..., 8.6624e-02, 0.0000e+00,
         1.6027e-05],
        [0.0000e+00, 8.1114e-04, 0.0000e+00,  ..., 6.0825e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537121.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.0299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.1407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.1364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0211],
        [ 0.0525],
        [ 0.1007],
        ...,
        [-0.3434],
        [-0.4490],
        [-0.6072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166494.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0107],
        [1.0086],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368336.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(519.2958, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0108],
        [1.0087],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368346.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2463.1333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.7443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6894, device='cuda:0')



h[100].sum tensor(-1.5446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0072, device='cuda:0')



h[200].sum tensor(-16.8482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53095.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0550, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0562, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0671, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515056.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(399.5868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-219.0869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-275.5913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2749],
        [-0.3034],
        [-0.2629],
        ...,
        [-0.8734],
        [-0.8708],
        [-0.8700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167090.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0108],
        [1.0087],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368346.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0109],
        [1.0088],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368357.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3264e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8853e-03,
         -1.7083e-04,  0.0000e+00],
        [ 4.7246e-03, -2.6308e-06, -2.2838e-05,  ...,  1.0282e-02,
          4.8715e-03, -3.5633e-03],
        [ 4.8728e-03, -2.6952e-06, -2.3397e-05,  ...,  1.0488e-02,
          4.9950e-03, -3.6506e-03],
        ...,
        [-1.3264e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8853e-03,
         -1.7083e-04,  0.0000e+00],
        [-1.3264e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8853e-03,
         -1.7083e-04,  0.0000e+00],
        [-1.3264e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8853e-03,
         -1.7083e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2955.0342, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0830, device='cuda:0')



h[100].sum tensor(-2.4087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-26.4076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0160, 0.0049, 0.0000],
        [0.0177, 0.0000, 0.0000,  ..., 0.0358, 0.0166, 0.0000],
        [0.0432, 0.0000, 0.0000,  ..., 0.0750, 0.0397, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63819.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.1350, 0.0000, 0.0068],
        [0.0000, 0.0543, 0.0000,  ..., 0.2228, 0.0000, 0.0197],
        [0.0000, 0.0900, 0.0000,  ..., 0.3166, 0.0000, 0.0372],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560666.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(447.5144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.6522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.9541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0178],
        [ 0.1032],
        [ 0.1500],
        ...,
        [-0.8856],
        [-0.8829],
        [-0.8822]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172357., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0109],
        [1.0088],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368357.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0110],
        [1.0089],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368368.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3018.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1354, device='cuda:0')



h[100].sum tensor(-2.4907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-27.4437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65794.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0476, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579666.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.9699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.4832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.3904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7090],
        [-0.8778],
        [-0.9916],
        ...,
        [-0.8955],
        [-0.8928],
        [-0.8920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185920.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0110],
        [1.0089],
        ...,
        [1.0006],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368368.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0111],
        [1.0090],
        ...,
        [1.0007],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368379.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3218e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8903e-03,
         -1.8169e-04,  0.0000e+00],
        [ 1.0651e-02, -4.7326e-06, -4.2899e-05,  ...,  1.8511e-02,
          9.7992e-03, -7.0302e-03],
        [-1.3218e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8903e-03,
         -1.8169e-04,  0.0000e+00],
        ...,
        [-1.3218e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8903e-03,
         -1.8169e-04,  0.0000e+00],
        [-1.3218e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8903e-03,
         -1.8169e-04,  0.0000e+00],
        [-1.3218e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8903e-03,
         -1.8169e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2874.4758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0096, device='cuda:0')



h[100].sum tensor(-2.2217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-24.6048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0000, 0.0000,  ..., 0.0243, 0.0099, 0.0000],
        [0.0180, 0.0000, 0.0000,  ..., 0.0345, 0.0160, 0.0000],
        [0.0868, 0.0000, 0.0000,  ..., 0.1355, 0.0761, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60743.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0837, 0.0000,  ..., 0.2915, 0.0000, 0.0357],
        [0.0000, 0.1344, 0.0000,  ..., 0.4199, 0.0000, 0.0620],
        [0.0000, 0.2247, 0.0000,  ..., 0.6492, 0.0000, 0.1085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546447.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.9118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.9874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.1827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1148],
        [ 0.1155],
        [ 0.1189],
        ...,
        [-0.9048],
        [-0.9019],
        [-0.9011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187819.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0111],
        [1.0090],
        ...,
        [1.0007],
        [0.9998],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368379.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0112],
        [1.0091],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368390.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5857e-02, -2.1535e-05, -1.9954e-04,  ...,  8.1259e-02,
          4.7476e-02, -3.3519e-02],
        [ 2.1366e-02, -8.5430e-06, -7.9159e-05,  ...,  3.3376e-02,
          1.8720e-02, -1.3297e-02],
        [ 1.0586e-02, -4.4823e-06, -4.1533e-05,  ...,  1.8410e-02,
          9.7328e-03, -6.9765e-03],
        ...,
        [-1.3140e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8906e-03,
         -1.8775e-04,  0.0000e+00],
        [-1.3140e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8906e-03,
         -1.8775e-04,  0.0000e+00],
        [-1.3140e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8906e-03,
         -1.8775e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2994.4978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0894, device='cuda:0')



h[100].sum tensor(-2.4040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-26.7584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0916, 0.0000, 0.0000,  ..., 0.1421, 0.0800, 0.0000],
        [0.1187, 0.0000, 0.0000,  ..., 0.1797, 0.1026, 0.0000],
        [0.0667, 0.0000, 0.0000,  ..., 0.1077, 0.0593, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65095.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3033, 0.0000,  ..., 0.8442, 0.0000, 0.1489],
        [0.0000, 0.3101, 0.0000,  ..., 0.8621, 0.0000, 0.1521],
        [0.0000, 0.2284, 0.0000,  ..., 0.6614, 0.0000, 0.1092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571481.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.7262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.8415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.3502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1112],
        [ 0.1183],
        [ 0.1276],
        ...,
        [-0.9103],
        [-0.9074],
        [-0.9066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182021.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0112],
        [1.0091],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368390.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0113],
        [1.0091],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368402.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0847e-02, -1.1542e-05, -1.0935e-04,  ...,  4.6544e-02,
          2.6633e-02, -1.8829e-02],
        [ 2.1787e-02, -8.2907e-06, -7.8546e-05,  ...,  3.3965e-02,
          1.9078e-02, -1.3525e-02],
        [-1.3156e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8883e-03,
         -1.8606e-04,  0.0000e+00],
        ...,
        [-1.3156e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8883e-03,
         -1.8606e-04,  0.0000e+00],
        [-1.3156e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8883e-03,
         -1.8606e-04,  0.0000e+00],
        [ 1.0667e-02, -4.3000e-06, -4.0738e-05,  ...,  1.8525e-02,
          9.8054e-03, -7.0148e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3658.5322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6230, device='cuda:0')



h[100].sum tensor(-3.5182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-39.3595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1171, 0.0000, 0.0000,  ..., 0.1776, 0.1013, 0.0000],
        [0.0555, 0.0000, 0.0000,  ..., 0.0902, 0.0490, 0.0000],
        [0.0297, 0.0000, 0.0000,  ..., 0.0526, 0.0266, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0109, 0.0000, 0.0000,  ..., 0.0248, 0.0101, 0.0000],
        [0.0195, 0.0000, 0.0000,  ..., 0.0386, 0.0181, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81481.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0964e-01, 0.0000e+00,  ..., 8.5767e-01, 0.0000e+00,
         1.5256e-01],
        [0.0000e+00, 1.8539e-01, 0.0000e+00,  ..., 5.4657e-01, 0.0000e+00,
         8.8053e-02],
        [0.0000e+00, 9.1962e-02, 0.0000e+00,  ..., 3.1028e-01, 0.0000e+00,
         3.9770e-02],
        ...,
        [0.0000e+00, 7.2639e-03, 0.0000e+00,  ..., 8.7708e-02, 0.0000e+00,
         5.1359e-04],
        [0.0000e+00, 3.0592e-02, 0.0000e+00,  ..., 1.5358e-01, 0.0000e+00,
         1.0351e-02],
        [0.0000e+00, 6.3039e-02, 0.0000e+00,  ..., 2.4014e-01, 0.0000e+00,
         2.4461e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661979.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.8303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.0636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-221.1490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0689],
        [ 0.0090],
        [-0.1459],
        ...,
        [-0.3518],
        [-0.1935],
        [-0.0362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169289.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0113],
        [1.0091],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368402.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0115],
        [1.0092],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368413.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.6558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3887, device='cuda:0')



h[100].sum tensor(-2.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-33.7215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69893.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0874, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0880, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0876, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580936., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(477.7206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.9758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.7331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1250],
        [-0.0796],
        [-0.0352],
        ...,
        [-0.9143],
        [-0.9114],
        [-0.9106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183065.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0115],
        [1.0092],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368413.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0115],
        [1.0092],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368413.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3178e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8855e-03,
         -1.7947e-04,  0.0000e+00],
        [ 1.4581e-02, -5.4335e-06, -5.2645e-05,  ...,  2.3962e-02,
          1.3079e-02, -9.2937e-03],
        [-1.3178e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8855e-03,
         -1.7947e-04,  0.0000e+00],
        ...,
        [-1.3178e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8855e-03,
         -1.7947e-04,  0.0000e+00],
        [-1.3178e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8855e-03,
         -1.7947e-04,  0.0000e+00],
        [-1.3178e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8855e-03,
         -1.7947e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3575.0952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4924, device='cuda:0')



h[100].sum tensor(-3.3206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-37.3390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0528, 0.0000, 0.0000,  ..., 0.0883, 0.0477, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0258, 0.0108, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0300, 0.0132, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78843.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0691, 0.0000,  ..., 0.2553, 0.0000, 0.0282],
        [0.0000, 0.0362, 0.0000,  ..., 0.1698, 0.0000, 0.0114],
        [0.0000, 0.0264, 0.0000,  ..., 0.1435, 0.0000, 0.0084],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644176.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.4595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.8487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.9286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2651],
        [-0.3498],
        [-0.4777],
        ...,
        [-0.9136],
        [-0.9107],
        [-0.9098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156640.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0115],
        [1.0092],
        ...,
        [1.0007],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368413.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0115],
        [1.0093],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368425.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5210e-03, -2.2265e-06, -2.2068e-05,  ...,  1.1387e-02,
          5.5457e-03, -3.9944e-03],
        [ 1.2193e-02, -4.3971e-06, -4.3582e-05,  ...,  2.0652e-02,
          1.1110e-02, -7.8885e-03],
        [-1.3232e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8836e-03,
         -1.6200e-04,  0.0000e+00],
        ...,
        [-1.3232e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8836e-03,
         -1.6200e-04,  0.0000e+00],
        [-1.3232e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8836e-03,
         -1.6200e-04,  0.0000e+00],
        [-1.3232e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8836e-03,
         -1.6200e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2697.9668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8201, device='cuda:0')



h[100].sum tensor(-1.7650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-19.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0307, 0.0000, 0.0000,  ..., 0.0558, 0.0285, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0327, 0.0148, 0.0000],
        [0.0443, 0.0000, 0.0000,  ..., 0.0766, 0.0408, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56661.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1132, 0.0000,  ..., 0.3738, 0.0000, 0.0500],
        [0.0000, 0.0719, 0.0000,  ..., 0.2671, 0.0000, 0.0289],
        [0.0000, 0.0687, 0.0000,  ..., 0.2584, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531069.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.0784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.1431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1561],
        [ 0.1365],
        [ 0.0918],
        ...,
        [-0.9160],
        [-0.9132],
        [-0.9124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184862.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0115],
        [1.0093],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368425.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0116],
        [1.0094],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368437.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000],
        ...,
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000],
        [-0.0013,  0.0000,  0.0000,  ...,  0.0019, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3204.9194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1920, device='cuda:0')



h[100].sum tensor(-2.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-29.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70291.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0495, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604938.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(476.0070, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.0268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.4847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0992],
        [-1.0891],
        [-1.0601],
        ...,
        [-0.8706],
        [-0.9014],
        [-0.9111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129750.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0116],
        [1.0094],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368437.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(482.5723, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0117],
        [1.0095],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368448.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1641e-02, -3.8149e-06, -3.9598e-05,  ...,  1.9870e-02,
          1.0664e-02, -7.5371e-03],
        [ 1.1904e-02, -3.8925e-06, -4.0403e-05,  ...,  2.0236e-02,
          1.0884e-02, -7.6904e-03],
        [ 1.6232e-02, -5.1666e-06, -5.3629e-05,  ...,  2.6244e-02,
          1.4492e-02, -1.0208e-02],
        ...,
        [-1.3166e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8815e-03,
         -1.4007e-04,  0.0000e+00],
        [-1.3166e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8815e-03,
         -1.4007e-04,  0.0000e+00],
        [-1.3166e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8815e-03,
         -1.4007e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3054.7231, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0898, device='cuda:0')



h[100].sum tensor(-2.3089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-26.3633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0537, 0.0000, 0.0000,  ..., 0.0895, 0.0486, 0.0000],
        [0.0509, 0.0000, 0.0000,  ..., 0.0857, 0.0463, 0.0000],
        [0.0562, 0.0000, 0.0000,  ..., 0.0912, 0.0498, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63594.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1222, 0.0000,  ..., 0.3943, 0.0000, 0.0552],
        [0.0000, 0.1470, 0.0000,  ..., 0.4583, 0.0000, 0.0679],
        [0.0000, 0.1593, 0.0000,  ..., 0.4875, 0.0000, 0.0744],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561247.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.9231, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.3952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.2654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1439],
        [ 0.1553],
        [ 0.1558],
        ...,
        [-0.9249],
        [-0.9222],
        [-0.9214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181182.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0117],
        [1.0095],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368448.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0118],
        [1.0096],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368459.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0000e-02, -5.9640e-06, -6.3376e-05,  ...,  3.1457e-02,
          1.7627e-02, -1.2372e-02],
        [-1.3063e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8815e-03,
         -1.3509e-04,  0.0000e+00],
        [-1.3063e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8815e-03,
         -1.3509e-04,  0.0000e+00],
        ...,
        [ 7.7398e-03, -2.5322e-06, -2.6908e-05,  ...,  1.4439e-02,
          7.4063e-03, -5.2530e-03],
        [ 1.8696e-02, -5.5990e-06, -5.9497e-05,  ...,  2.9647e-02,
          1.6540e-02, -1.1615e-02],
        [ 1.2881e-02, -3.9713e-06, -4.2200e-05,  ...,  2.1575e-02,
          1.1692e-02, -8.2382e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3209.8875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1992, device='cuda:0')



h[100].sum tensor(-2.5506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-29.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0753, 0.0000, 0.0000,  ..., 0.1176, 0.0657, 0.0000],
        [0.0475, 0.0000, 0.0000,  ..., 0.0790, 0.0425, 0.0000],
        [0.0206, 0.0000, 0.0000,  ..., 0.0399, 0.0191, 0.0000],
        ...,
        [0.0349, 0.0000, 0.0000,  ..., 0.0600, 0.0311, 0.0000],
        [0.0608, 0.0000, 0.0000,  ..., 0.0996, 0.0546, 0.0000],
        [0.0724, 0.0000, 0.0000,  ..., 0.1157, 0.0643, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66647.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2733, 0.0000,  ..., 0.7674, 0.0000, 0.1331],
        [0.0000, 0.2053, 0.0000,  ..., 0.5997, 0.0000, 0.0978],
        [0.0000, 0.1634, 0.0000,  ..., 0.4953, 0.0000, 0.0762],
        ...,
        [0.0000, 0.0965, 0.0000,  ..., 0.3283, 0.0000, 0.0414],
        [0.0000, 0.1400, 0.0000,  ..., 0.4410, 0.0000, 0.0633],
        [0.0000, 0.1444, 0.0000,  ..., 0.4508, 0.0000, 0.0659]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576857.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.9202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.0582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-264.1548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0974],
        [0.1018],
        [0.1055],
        ...,
        [0.0015],
        [0.1147],
        [0.1025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189621.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0118],
        [1.0096],
        ...,
        [1.0008],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368459.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0118],
        [1.0097],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368470.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7157e-02, -7.5700e-06, -8.2375e-05,  ...,  4.1377e-02,
          2.3590e-02, -1.6495e-02],
        [ 2.9437e-02, -8.1767e-06, -8.8977e-05,  ...,  4.4542e-02,
          2.5491e-02, -1.7817e-02],
        [ 2.6939e-02, -7.5121e-06, -8.1744e-05,  ...,  4.1075e-02,
          2.3409e-02, -1.6369e-02],
        ...,
        [-1.2976e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8838e-03,
         -1.2654e-04,  0.0000e+00],
        [-1.2976e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8838e-03,
         -1.2654e-04,  0.0000e+00],
        [-1.2976e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8838e-03,
         -1.2654e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2955.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0013, device='cuda:0')



h[100].sum tensor(-2.1030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-24.2610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0960, 0.0000, 0.0000,  ..., 0.1480, 0.0838, 0.0000],
        [0.0991, 0.0000, 0.0000,  ..., 0.1524, 0.0865, 0.0000],
        [0.0910, 0.0000, 0.0000,  ..., 0.1412, 0.0797, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61808.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2418, 0.0000,  ..., 0.6862, 0.0000, 0.1166],
        [0.0000, 0.2331, 0.0000,  ..., 0.6642, 0.0000, 0.1122],
        [0.0000, 0.2038, 0.0000,  ..., 0.5912, 0.0000, 0.0971],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557917.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.6828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.2889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.7138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1188],
        [ 0.1009],
        [ 0.0599],
        ...,
        [-0.9412],
        [-0.9384],
        [-0.9376]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172919.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0118],
        [1.0097],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368470.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0119],
        [1.0097],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368482., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0588e-03, -1.8583e-06, -2.0713e-05,  ...,  1.2092e-02,
          6.0126e-03, -4.2548e-03],
        [ 1.0193e-02, -2.9033e-06, -3.2362e-05,  ...,  1.7830e-02,
          9.4583e-03, -6.6474e-03],
        [ 2.3068e-02, -6.1576e-06, -6.8636e-05,  ...,  3.5699e-02,
          2.0189e-02, -1.4099e-02],
        ...,
        [-1.2931e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8887e-03,
         -1.1477e-04,  0.0000e+00],
        [-1.2931e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8887e-03,
         -1.1477e-04,  0.0000e+00],
        [-1.2931e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8887e-03,
         -1.1477e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3097.9700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1116, device='cuda:0')



h[100].sum tensor(-2.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-26.9198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0000, 0.0000,  ..., 0.0446, 0.0218, 0.0000],
        [0.0560, 0.0000, 0.0000,  ..., 0.0926, 0.0506, 0.0000],
        [0.0723, 0.0000, 0.0000,  ..., 0.1153, 0.0642, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65426.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0881, 0.0000,  ..., 0.3091, 0.0000, 0.0360],
        [0.0000, 0.1546, 0.0000,  ..., 0.4763, 0.0000, 0.0701],
        [0.0000, 0.2009, 0.0000,  ..., 0.5913, 0.0000, 0.0940],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577271.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.2629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.0954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-273.9816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1355],
        [ 0.1475],
        [ 0.1429],
        ...,
        [-0.9491],
        [-0.9462],
        [-0.9452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179399.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0119],
        [1.0097],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368482., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0119],
        [1.0098],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368493.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7824e-03, -2.6588e-06, -3.0365e-05,  ...,  1.7263e-02,
          9.1295e-03, -6.3990e-03],
        [ 1.9346e-02, -4.9546e-06, -5.6585e-05,  ...,  3.0535e-02,
          1.7100e-02, -1.1925e-02],
        [ 3.8496e-03, -1.2345e-06, -1.4098e-05,  ...,  9.0292e-03,
          4.1850e-03, -2.9711e-03],
        ...,
        [-1.2926e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8929e-03,
         -1.0057e-04,  0.0000e+00],
        [-1.2926e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8929e-03,
         -1.0057e-04,  0.0000e+00],
        [-1.2926e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8929e-03,
         -1.0057e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2967.6812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0005, device='cuda:0')



h[100].sum tensor(-2.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-24.3609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0795, 0.0000, 0.0000,  ..., 0.1252, 0.0702, 0.0000],
        [0.0411, 0.0000, 0.0000,  ..., 0.0720, 0.0382, 0.0000],
        [0.0383, 0.0000, 0.0000,  ..., 0.0681, 0.0359, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62397.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1655, 0.0000,  ..., 0.5024, 0.0000, 0.0760],
        [0.0000, 0.1354, 0.0000,  ..., 0.4304, 0.0000, 0.0598],
        [0.0000, 0.1108, 0.0000,  ..., 0.3705, 0.0000, 0.0468],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566618.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.0649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.7417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.3793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1741],
        [ 0.1776],
        [ 0.1786],
        ...,
        [-0.9571],
        [-0.9542],
        [-0.9534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190272.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0119],
        [1.0098],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368493.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0120],
        [1.0099],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368504.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5962e-02, -6.2120e-06, -7.2713e-05,  ...,  3.9721e-02,
          2.2631e-02, -1.5722e-02],
        [ 2.6948e-02, -6.4367e-06, -7.5343e-05,  ...,  4.1089e-02,
          2.3452e-02, -1.6291e-02],
        [ 1.1406e-02, -2.8944e-06, -3.3880e-05,  ...,  1.9520e-02,
          1.0500e-02, -7.3256e-03],
        ...,
        [-1.2932e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8970e-03,
         -8.3325e-05,  0.0000e+00],
        [-1.2932e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8970e-03,
         -8.3325e-05,  0.0000e+00],
        [-1.2932e-03,  0.0000e+00,  0.0000e+00,  ...,  1.8970e-03,
         -8.3325e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3081.0967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0931, device='cuda:0')



h[100].sum tensor(-2.2445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-26.2978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0789, 0.0000, 0.0000,  ..., 0.1243, 0.0697, 0.0000],
        [0.0795, 0.0000, 0.0000,  ..., 0.1252, 0.0703, 0.0000],
        [0.0757, 0.0000, 0.0000,  ..., 0.1200, 0.0671, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65100.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1736, 0.0000,  ..., 0.5173, 0.0000, 0.0813],
        [0.0000, 0.1754, 0.0000,  ..., 0.5227, 0.0000, 0.0823],
        [0.0000, 0.1510, 0.0000,  ..., 0.4627, 0.0000, 0.0698],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581435.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.3043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.5507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-277.2831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1293],
        [ 0.1461],
        [ 0.1567],
        ...,
        [-0.9614],
        [-0.9560],
        [-0.9435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163402.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0120],
        [1.0099],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368504.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0120],
        [1.0099],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368516.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9502e-03, -1.3499e-06, -1.6200e-05,  ...,  1.0568e-02,
          5.1259e-03, -3.5943e-03],
        [ 4.5443e-03, -1.2621e-06, -1.5146e-05,  ...,  1.0005e-02,
          4.7876e-03, -3.3605e-03],
        [ 1.0785e-02, -2.6120e-06, -3.1345e-05,  ...,  1.8665e-02,
          9.9885e-03, -6.9548e-03],
        ...,
        [-1.2909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9074e-03,
         -7.5010e-05,  0.0000e+00],
        [-1.2909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9074e-03,
         -7.5010e-05,  0.0000e+00],
        [-1.2909e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9074e-03,
         -7.5010e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2703.3506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7863, device='cuda:0')



h[100].sum tensor(-1.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-18.8877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0175, 0.0000, 0.0000,  ..., 0.0355, 0.0166, 0.0000],
        [0.0371, 0.0000, 0.0000,  ..., 0.0664, 0.0349, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0358, 0.0166, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56503.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0510, 0.0000,  ..., 0.2143, 0.0000, 0.0177],
        [0.0000, 0.0764, 0.0000,  ..., 0.2809, 0.0000, 0.0300],
        [0.0000, 0.0567, 0.0000,  ..., 0.2301, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541230., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.8813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.9367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.8678, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1007],
        [ 0.1246],
        [ 0.0708],
        ...,
        [-0.9672],
        [-0.9643],
        [-0.9634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204902.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0120],
        [1.0099],
        ...,
        [1.0008],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368516.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0121],
        [1.0101],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368528.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00],
        [-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00],
        [-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00],
        ...,
        [-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00],
        [-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00],
        [-1.2935e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9158e-03,
         -6.9947e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3284.2432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2309, device='cuda:0')



h[100].sum tensor(-2.5109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-29.7265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69132.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.0035e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.2743e-05, 0.0000e+00,  ..., 6.3213e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.7605e-04, 0.0000e+00,  ..., 6.5743e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.5670e-03, 0.0000e+00,  ..., 7.2633e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6496e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.1651e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601698.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(485.3676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.2560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.4574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6341],
        [-0.5529],
        [-0.4757],
        ...,
        [-0.7684],
        [-0.8827],
        [-0.9428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173032.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0121],
        [1.0101],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368528.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0122],
        [1.0102],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368540.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1267e-02, -2.4444e-06, -3.0860e-05,  ...,  1.9356e-02,
          1.0405e-02, -7.2118e-03],
        [-1.2962e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9230e-03,
         -6.3398e-05,  0.0000e+00],
        [-1.2962e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9230e-03,
         -6.3398e-05,  0.0000e+00],
        ...,
        [-1.2962e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9230e-03,
         -6.3398e-05,  0.0000e+00],
        [-1.2962e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9230e-03,
         -6.3398e-05,  0.0000e+00],
        [-1.2962e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9230e-03,
         -6.3398e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2853.0574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8971, device='cuda:0')



h[100].sum tensor(-1.7961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-21.3762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0000, 0.0000,  ..., 0.0350, 0.0162, 0.0000],
        [0.0114, 0.0000, 0.0000,  ..., 0.0254, 0.0105, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60679.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0886, 0.0000,  ..., 0.3039, 0.0000, 0.0377],
        [0.0000, 0.0613, 0.0000,  ..., 0.2315, 0.0000, 0.0254],
        [0.0000, 0.0376, 0.0000,  ..., 0.1703, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565736.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(451.3819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.1328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.8933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0743],
        [ 0.0684],
        [ 0.0683],
        ...,
        [-0.9787],
        [-0.9758],
        [-0.9750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206105.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0122],
        [1.0102],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368540.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0124],
        [1.0103],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368551.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9690e-03, -1.3413e-06, -1.7378e-05,  ...,  1.2021e-02,
          6.0061e-03, -4.1685e-03],
        [-1.3038e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9286e-03,
         -5.4357e-05,  0.0000e+00],
        [-1.3038e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9286e-03,
         -5.4357e-05,  0.0000e+00],
        ...,
        [-1.3038e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9286e-03,
         -5.4357e-05,  0.0000e+00],
        [-1.3038e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9286e-03,
         -5.4357e-05,  0.0000e+00],
        [-1.3038e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9286e-03,
         -5.4357e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3125.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0983, device='cuda:0')



h[100].sum tensor(-2.2038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-26.3654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0000, 0.0000,  ..., 0.0240, 0.0097, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0180, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68830.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.1413, 0.0000, 0.0052],
        [0.0000, 0.0114, 0.0000,  ..., 0.1101, 0.0000, 0.0026],
        [0.0000, 0.0009, 0.0000,  ..., 0.0808, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621277.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(483.0446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.7551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.1998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5073],
        [-0.5263],
        [-0.4864],
        ...,
        [-0.9837],
        [-0.9808],
        [-0.9799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175371.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0124],
        [1.0103],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368551.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(503.8808, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0125],
        [1.0104],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368563.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0635e-02, -2.0873e-06, -2.7760e-05,  ...,  1.8515e-02,
          9.9078e-03, -6.8357e-03],
        [ 1.9433e-02, -3.6246e-06, -4.8203e-05,  ...,  3.0723e-02,
          1.7238e-02, -1.1870e-02],
        [ 5.3892e-03, -1.1707e-06, -1.5569e-05,  ...,  1.1235e-02,
          5.5367e-03, -3.8339e-03],
        ...,
        [-1.3106e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9380e-03,
         -4.6114e-05,  0.0000e+00],
        [-1.3106e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9380e-03,
         -4.6114e-05,  0.0000e+00],
        [-1.3106e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9380e-03,
         -4.6114e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3133.9724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0886, device='cuda:0')



h[100].sum tensor(-2.1908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-26.3475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0897, 0.0000, 0.0000,  ..., 0.1396, 0.0790, 0.0000],
        [0.0608, 0.0000, 0.0000,  ..., 0.0995, 0.0549, 0.0000],
        [0.0331, 0.0000, 0.0000,  ..., 0.0575, 0.0297, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67725.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2071, 0.0000,  ..., 0.6041, 0.0000, 0.0974],
        [0.0000, 0.1707, 0.0000,  ..., 0.5145, 0.0000, 0.0786],
        [0.0000, 0.1095, 0.0000,  ..., 0.3600, 0.0000, 0.0479],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608846.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(477.2006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.1971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-274.8593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1472],
        [ 0.1419],
        [ 0.1126],
        ...,
        [-0.9888],
        [-0.9859],
        [-0.9850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156661.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0125],
        [1.0104],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368563.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0126],
        [1.0105],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368575.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2958e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9627e-03,
         -6.9417e-05,  0.0000e+00],
        [ 1.0030e-02, -1.8740e-06, -2.5592e-05,  ...,  1.7678e-02,
          9.3660e-03, -6.4699e-03],
        [ 2.0766e-02, -3.6505e-06, -4.9852e-05,  ...,  3.2575e-02,
          1.8310e-02, -1.2603e-02],
        ...,
        [-1.2958e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9627e-03,
         -6.9417e-05,  0.0000e+00],
        [-1.2958e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9627e-03,
         -6.9417e-05,  0.0000e+00],
        [-1.2958e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9627e-03,
         -6.9417e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3329.5439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2518, device='cuda:0')



h[100].sum tensor(-2.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-30.0989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0367, 0.0171, 0.0000],
        [0.0367, 0.0000, 0.0000,  ..., 0.0643, 0.0336, 0.0000],
        [0.0556, 0.0000, 0.0000,  ..., 0.0924, 0.0504, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71351.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0574, 0.0000,  ..., 0.2251, 0.0000, 0.0212],
        [0.0000, 0.1155, 0.0000,  ..., 0.3715, 0.0000, 0.0502],
        [0.0000, 0.1691, 0.0000,  ..., 0.5062, 0.0000, 0.0771],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0693, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0518, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0518, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620772.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.4183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.5613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.8684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0991],
        [ 0.1048],
        [ 0.1043],
        ...,
        [-0.7309],
        [-0.8831],
        [-0.9634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176676.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0126],
        [1.0105],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368575.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0127],
        [1.0106],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368586.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.1737e-03, -1.0118e-06, -1.4194e-05,  ...,  1.0949e-02,
          5.2947e-03, -3.6844e-03],
        [ 1.5337e-02, -2.6036e-06, -3.6524e-05,  ...,  2.5051e-02,
          1.3761e-02, -9.4806e-03],
        [ 1.9028e-02, -3.1816e-06, -4.4632e-05,  ...,  3.0171e-02,
          1.6835e-02, -1.1585e-02],
        ...,
        [-1.2868e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9852e-03,
         -8.6598e-05,  0.0000e+00],
        [-1.2868e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9852e-03,
         -8.6598e-05,  0.0000e+00],
        [-1.2868e-03,  0.0000e+00,  0.0000e+00,  ...,  1.9852e-03,
         -8.6598e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3442.5811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3394, device='cuda:0')



h[100].sum tensor(-2.6561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0140, device='cuda:0')



h[200].sum tensor(-32.2809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0000, 0.0000,  ..., 0.0552, 0.0281, 0.0000],
        [0.0578, 0.0000, 0.0000,  ..., 0.0955, 0.0521, 0.0000],
        [0.0868, 0.0000, 0.0000,  ..., 0.1357, 0.0763, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73031.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1270, 0.0000,  ..., 0.3985, 0.0000, 0.0557],
        [0.0000, 0.1635, 0.0000,  ..., 0.4894, 0.0000, 0.0738],
        [0.0000, 0.1900, 0.0000,  ..., 0.5546, 0.0000, 0.0874],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622381.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(505.4351, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.8817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.7407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0142],
        [-0.0053],
        ...,
        [-1.0098],
        [-1.0067],
        [-1.0058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207605.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0127],
        [1.0106],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368586.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0128],
        [1.0107],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368598.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00],
        [-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00],
        [-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00],
        ...,
        [-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00],
        [-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00],
        [-1.2865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0064e-03,
         -9.3886e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2836.8496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8760, device='cuda:0')



h[100].sum tensor(-1.7061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-20.8449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59458.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0495, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0498, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561626., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(447.7706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.9042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.1414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2410],
        [-1.2484],
        [-1.2482],
        ...,
        [-1.0125],
        [-1.0102],
        [-1.0093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206807.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0128],
        [1.0107],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368598.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0108],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368609.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00],
        [-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00],
        [-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00],
        ...,
        [-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00],
        [-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00],
        [-1.3088e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0227e-03,
         -8.1588e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2826.1206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8400, device='cuda:0')



h[100].sum tensor(-1.6482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-20.2444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58648.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555169.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.1992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-230.6683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.1889, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0699],
        [-1.1397],
        [-1.1926],
        ...,
        [-1.0131],
        [-1.0100],
        [-1.0091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208236.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0108],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368609.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0109],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368621.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3424e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0396e-03,
         -5.8122e-05,  0.0000e+00],
        [-1.3424e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0396e-03,
         -5.8122e-05,  0.0000e+00],
        [ 5.5122e-03, -9.0825e-07, -1.3840e-05,  ...,  1.1553e-02,
          5.6529e-03, -3.8912e-03],
        ...,
        [-1.3424e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0396e-03,
         -5.8122e-05,  0.0000e+00],
        [-1.3424e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0396e-03,
         -5.8122e-05,  0.0000e+00],
        [-1.3424e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0396e-03,
         -5.8122e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3053.5603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9975, device='cuda:0')



h[100].sum tensor(-1.9418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-23.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0224, 0.0084, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0179, 0.0057, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0161, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63361.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0000,  ..., 0.1531, 0.0000, 0.0050],
        [0.0000, 0.0147, 0.0000,  ..., 0.1244, 0.0000, 0.0013],
        [0.0000, 0.0107, 0.0000,  ..., 0.1133, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578574.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.4427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.5832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.2379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0225],
        [-0.1141],
        [-0.2819],
        ...,
        [-1.0050],
        [-1.0020],
        [-1.0011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183058.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0109],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368621.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0110],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368632.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8704e-03, -6.5613e-07, -1.0286e-05,  ...,  9.3305e-03,
          4.3331e-03, -2.9705e-03],
        [ 3.8704e-03, -6.5613e-07, -1.0286e-05,  ...,  9.3305e-03,
          4.3331e-03, -2.9705e-03],
        [-1.3699e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0567e-03,
         -3.3963e-05,  0.0000e+00],
        ...,
        [-1.3699e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0567e-03,
         -3.3963e-05,  0.0000e+00],
        [-1.3699e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0567e-03,
         -3.3963e-05,  0.0000e+00],
        [-1.3699e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0567e-03,
         -3.3963e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3476.4839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3244, device='cuda:0')



h[100].sum tensor(-2.5246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-31.3395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0150, 0.0000, 0.0000,  ..., 0.0349, 0.0158, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0284, 0.0119, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0217, 0.0080, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75374.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0426, 0.0000,  ..., 0.2032, 0.0000, 0.0130],
        [0.0000, 0.0356, 0.0000,  ..., 0.1862, 0.0000, 0.0096],
        [0.0000, 0.0235, 0.0000,  ..., 0.1496, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652104.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(496.2505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.4753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.5936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1063],
        [ 0.0734],
        [-0.0593],
        ...,
        [-0.9921],
        [-0.9894],
        [-0.9886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161330.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0110],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368632.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0131],
        [1.0111],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368644.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00],
        [-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00],
        [-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00],
        ...,
        [-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00],
        [-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00],
        [-1.3891e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0753e-03,
         -1.8073e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2867.0674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.9767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8160, device='cuda:0')



h[100].sum tensor(-1.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-19.6685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60752.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0577, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573554.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.6397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.9722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-292.1072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6459],
        [-0.8099],
        [-0.9361],
        ...,
        [-0.9797],
        [-0.9770],
        [-0.9760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145237.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0131],
        [1.0111],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368644.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0132],
        [1.0111],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368655.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00],
        [-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00],
        [-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00],
        ...,
        [-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00],
        [-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00],
        [-1.3954e-03,  0.0000e+00,  0.0000e+00,  ...,  2.0978e-03,
         -2.0769e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3042.1052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9535, device='cuda:0')



h[100].sum tensor(-1.8176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-22.8046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63746.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0570, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0624, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0766, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586353.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(441.8257, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.1986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-286.1984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6910],
        [-0.5053],
        [-0.2852],
        ...,
        [-0.9893],
        [-0.9867],
        [-0.9859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135633.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0132],
        [1.0111],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368655.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0133],
        [1.0112],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368665.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        ...,
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4274.9985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8778, device='cuda:0')



h[100].sum tensor(-3.6240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0196, device='cuda:0')



h[200].sum tensor(-45.7121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95239.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0700, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(757127.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(571.1906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.1795, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.8826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3569],
        [-0.5682],
        [-0.6622],
        ...,
        [-1.0035],
        [-1.0007],
        [-0.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138685., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0133],
        [1.0112],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368665.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0133],
        [1.0112],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368665.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3102e-03, -9.1635e-07, -1.5680e-05,  ...,  1.4214e-02,
          7.2150e-03, -4.9106e-03],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        ...,
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00],
        [-1.3922e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1308e-03,
         -3.9027e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3470.0247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2778, device='cuda:0')



h[100].sum tensor(-2.4455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-30.8463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0000, 0.0000,  ..., 0.0394, 0.0184, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0209, 0.0073, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74845.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0713, 0.0000,  ..., 0.2716, 0.0000, 0.0298],
        [0.0000, 0.0266, 0.0000,  ..., 0.1517, 0.0000, 0.0099],
        [0.0000, 0.0040, 0.0000,  ..., 0.0817, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644892.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.5618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.3893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0173],
        [-0.1468],
        [-0.3343],
        ...,
        [-1.0035],
        [-1.0007],
        [-0.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172063.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0133],
        [1.0112],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368665.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0134],
        [1.0113],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368676.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1604e-03,
         -5.7659e-05,  0.0000e+00],
        [-1.3865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1604e-03,
         -5.7659e-05,  0.0000e+00],
        [ 6.3765e-03, -7.7068e-07, -1.3589e-05,  ...,  1.2940e-02,
          6.4128e-03, -4.3734e-03],
        ...,
        [-1.3865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1604e-03,
         -5.7659e-05,  0.0000e+00],
        [-1.3865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1604e-03,
         -5.7659e-05,  0.0000e+00],
        [-1.3865e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1604e-03,
         -5.7659e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3806.3101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5646, device='cuda:0')



h[100].sum tensor(-2.9355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0163, device='cuda:0')



h[200].sum tensor(-37.2261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0197, 0.0065, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0177, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80824.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.1016, 0.0000, 0.0019],
        [0.0000, 0.0142, 0.0000,  ..., 0.1265, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672476.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(514.4661, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.7472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.9764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7438],
        [-0.4856],
        [-0.2196],
        ...,
        [-1.0154],
        [-1.0124],
        [-1.0116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156463.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0134],
        [1.0113],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368676.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368686.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5827e-03, -1.0258e-06, -1.8648e-05,  ...,  1.7413e-02,
          9.0703e-03, -6.1677e-03],
        [ 1.8025e-02, -1.8156e-06, -3.3006e-05,  ...,  2.9137e-02,
          1.6107e-02, -1.0916e-02],
        [ 3.3657e-02, -3.2779e-06, -5.9589e-05,  ...,  5.0844e-02,
          2.9136e-02, -1.9709e-02],
        ...,
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00],
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00],
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3155.3059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0678, device='cuda:0')



h[100].sum tensor(-1.9930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-25.4094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1480, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0332, 0.0000, 0.0000,  ..., 0.0588, 0.0299, 0.0000],
        [0.0793, 0.0000, 0.0000,  ..., 0.1268, 0.0705, 0.0000],
        [0.0989, 0.0000, 0.0000,  ..., 0.1539, 0.0868, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66515.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1048e-01, 0.0000e+00,  ..., 3.6886e-01, 0.0000e+00,
         4.9864e-02],
        [0.0000e+00, 1.9939e-01, 0.0000e+00,  ..., 5.9070e-01, 0.0000e+00,
         9.5194e-02],
        [0.0000e+00, 2.6561e-01, 0.0000e+00,  ..., 7.5352e-01, 0.0000e+00,
         1.2912e-01],
        ...,
        [0.0000e+00, 4.3240e-03, 0.0000e+00,  ..., 8.1986e-02, 0.0000e+00,
         2.7814e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4294e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4288e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596190.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.9203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.1029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.0608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1116],
        [ 0.1161],
        [ 0.1103],
        ...,
        [-0.5178],
        [-0.7736],
        [-0.9326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187881.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368686.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368686.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3165e-03, -9.0736e-07, -1.6495e-05,  ...,  1.5655e-02,
          8.0150e-03, -5.4556e-03],
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00],
        [ 8.3165e-03, -9.0736e-07, -1.6495e-05,  ...,  1.5655e-02,
          8.0150e-03, -5.4556e-03],
        ...,
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00],
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00],
        [-1.3828e-03,  0.0000e+00,  0.0000e+00,  ...,  2.1862e-03,
         -6.9278e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3358.8042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2323, device='cuda:0')



h[100].sum tensor(-2.2864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-29.1510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0000, 0.0000,  ..., 0.0199, 0.0066, 0.0000],
        [0.0301, 0.0000, 0.0000,  ..., 0.0584, 0.0295, 0.0000],
        [0.0066, 0.0000, 0.0000,  ..., 0.0200, 0.0066, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72421.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0000,  ..., 0.1274, 0.0000, 0.0034],
        [0.0000, 0.0409, 0.0000,  ..., 0.1956, 0.0000, 0.0138],
        [0.0000, 0.0320, 0.0000,  ..., 0.1736, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634957.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(479.9024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.9444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-276.9627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2477],
        [-0.0317],
        [ 0.0866],
        ...,
        [-1.0321],
        [-1.0290],
        [-1.0281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165673.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368686.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368696.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3886e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2099e-03,
         -6.5028e-05,  0.0000e+00],
        [ 4.5800e-03, -5.2593e-07, -9.8615e-06,  ...,  1.0500e-02,
          4.9111e-03, -3.3524e-03],
        [ 4.2798e-03, -4.9948e-07, -9.3655e-06,  ...,  1.0083e-02,
          4.6609e-03, -3.1838e-03],
        ...,
        [-1.3886e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2099e-03,
         -6.5028e-05,  0.0000e+00],
        [-1.3886e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2099e-03,
         -6.5028e-05,  0.0000e+00],
        [-1.3886e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2099e-03,
         -6.5028e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2975.4702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9299, device='cuda:0')



h[100].sum tensor(-1.7325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-22.2076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0000,  ..., 0.0173, 0.0050, 0.0000],
        [0.0169, 0.0000, 0.0000,  ..., 0.0364, 0.0163, 0.0000],
        [0.0355, 0.0000, 0.0000,  ..., 0.0660, 0.0340, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63025.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0206, 0.0000,  ..., 0.1365, 0.0000, 0.0064],
        [0.0000, 0.0466, 0.0000,  ..., 0.2118, 0.0000, 0.0171],
        [0.0000, 0.0709, 0.0000,  ..., 0.2761, 0.0000, 0.0291],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0541, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0541, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0540, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581728.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.8330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-233.2003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.5530, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1043],
        [ 0.0445],
        [ 0.1033],
        ...,
        [-1.0404],
        [-1.0373],
        [-1.0364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185859.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0135],
        [1.0114],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368696.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0136],
        [1.0115],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368707.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00],
        ...,
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00],
        [-1.3983e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2319e-03,
         -5.0470e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3402.8325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2675, device='cuda:0')



h[100].sum tensor(-2.3337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-30.0753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70143.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0528, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608620.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.7129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-284.8905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9559],
        [-1.0892],
        [-1.1811],
        ...,
        [-1.0464],
        [-1.0432],
        [-1.0423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181212.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0136],
        [1.0115],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368707.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0137],
        [1.0115],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368718.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00],
        [-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00],
        [-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00],
        ...,
        [-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00],
        [-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00],
        [-1.4122e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2507e-03,
         -2.8497e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3761.8792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5276, device='cuda:0')



h[100].sum tensor(-2.8245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0159, device='cuda:0')



h[200].sum tensor(-36.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81781.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.4331e-03, 0.0000e+00,  ..., 9.0312e-02, 0.0000e+00,
         1.0433e-04],
        [0.0000e+00, 1.6402e-04, 0.0000e+00,  ..., 8.3656e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0390e-02, 0.0000e+00,  ..., 1.5604e-01, 0.0000e+00,
         1.3405e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4688e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4682e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4676e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689488.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.2036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.4578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.6372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0112],
        [ 0.0037],
        [ 0.0402],
        ...,
        [-1.0501],
        [-1.0470],
        [-1.0461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163120.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0137],
        [1.0115],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368718.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0138],
        [1.0116],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368729.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2966e-02, -1.0560e-06, -2.1793e-05,  ...,  2.2272e-02,
          1.1999e-02, -8.0493e-03],
        [ 1.9663e-02, -1.5474e-06, -3.1935e-05,  ...,  3.1581e-02,
          1.7590e-02, -1.1795e-02],
        [ 1.2966e-02, -1.0560e-06, -2.1793e-05,  ...,  2.2272e-02,
          1.1999e-02, -8.0493e-03],
        ...,
        [-1.4235e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2690e-03,
         -1.2028e-05,  0.0000e+00],
        [-1.4235e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2690e-03,
         -1.2028e-05,  0.0000e+00],
        [-1.4235e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2690e-03,
         -1.2028e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3676.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4704, device='cuda:0')



h[100].sum tensor(-2.6829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-34.9525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0720, 0.0000, 0.0000,  ..., 0.1172, 0.0648, 0.0000],
        [0.0778, 0.0000, 0.0000,  ..., 0.1254, 0.0697, 0.0000],
        [0.1372, 0.0000, 0.0000,  ..., 0.2079, 0.1193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77901.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2104, 0.0000,  ..., 0.6330, 0.0000, 0.1062],
        [0.0000, 0.2512, 0.0000,  ..., 0.7357, 0.0000, 0.1279],
        [0.0000, 0.3326, 0.0000,  ..., 0.9387, 0.0000, 0.1708],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0638, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653577.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.3274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.1650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.0879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1951],
        [ 0.2036],
        [ 0.2102],
        ...,
        [-1.0253],
        [-0.9602],
        [-0.8478]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177277.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0138],
        [1.0116],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368729.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368739.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        ...,
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3363.9031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2140, device='cuda:0')



h[100].sum tensor(-2.2229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-29.1173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71709.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0566, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628572.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.9261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.1851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-285.9048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5064],
        [-0.7401],
        [-0.9187],
        ...,
        [-1.0524],
        [-1.0493],
        [-1.0484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168267.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368739.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368739.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0051e-02, -2.1698e-06, -4.6284e-05,  ...,  4.6034e-02,
          2.6267e-02, -1.7578e-02],
        [ 3.3177e-02, -2.3854e-06, -5.0882e-05,  ...,  5.0380e-02,
          2.8877e-02, -1.9324e-02],
        [ 2.0609e-02, -1.5188e-06, -3.2398e-05,  ...,  3.2907e-02,
          1.8384e-02, -1.2304e-02],
        ...,
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00],
        [-1.4212e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2802e-03,
         -7.4678e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2846.3394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.9183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8332, device='cuda:0')



h[100].sum tensor(-1.5043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-19.7047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1413, 0.0000, 0.0000,  ..., 0.2137, 0.1228, 0.0000],
        [0.1177, 0.0000, 0.0000,  ..., 0.1809, 0.1031, 0.0000],
        [0.0881, 0.0000, 0.0000,  ..., 0.1398, 0.0784, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59477.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3365, 0.0000,  ..., 0.9486, 0.0000, 0.1731],
        [0.0000, 0.3051, 0.0000,  ..., 0.8713, 0.0000, 0.1565],
        [0.0000, 0.2497, 0.0000,  ..., 0.7334, 0.0000, 0.1271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566935.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.1483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-229.0157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-313.8126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2002],
        [ 0.1931],
        [ 0.1850],
        ...,
        [-1.0566],
        [-1.0535],
        [-1.0526]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190158.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368739.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(905.0378, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0140],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368750.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2999e-03, -3.0548e-07, -6.7388e-06,  ...,  8.8484e-03,
          3.9382e-03, -2.6319e-03],
        [ 3.2999e-03, -3.0548e-07, -6.7388e-06,  ...,  8.8484e-03,
          3.9382e-03, -2.6319e-03],
        [-1.4195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2868e-03,
         -2.3097e-06,  0.0000e+00],
        ...,
        [-1.4195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2868e-03,
         -2.3097e-06,  0.0000e+00],
        [-1.4195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2868e-03,
         -2.3097e-06,  0.0000e+00],
        [-1.4195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2868e-03,
         -2.3097e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3037.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9810, device='cuda:0')



h[100].sum tensor(-1.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-23.0884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0000,  ..., 0.0212, 0.0072, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0214, 0.0072, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0214, 0.0073, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62763.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.1251, 0.0000, 0.0013],
        [0.0000, 0.0119, 0.0000,  ..., 0.1322, 0.0000, 0.0018],
        [0.0000, 0.0107, 0.0000,  ..., 0.1264, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583167.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(454.6228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.3479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-310.0104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1347],
        [-0.0687],
        [-0.0845],
        ...,
        [-1.0614],
        [-1.0583],
        [-1.0574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160376.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0140],
        [1.0117],
        ...,
        [1.0009],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368750.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0141],
        [1.0118],
        ...,
        [1.0009],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368761.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4139e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2918e-03,
         -2.1793e-06,  0.0000e+00],
        [ 9.7277e-03, -6.7662e-07, -1.5445e-05,  ...,  1.7784e-02,
          9.3014e-03, -6.2038e-03],
        [ 1.9892e-02, -1.2939e-06, -2.9535e-05,  ...,  3.1917e-02,
          1.7789e-02, -1.1863e-02],
        ...,
        [-1.4139e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2918e-03,
         -2.1793e-06,  0.0000e+00],
        [-1.4139e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2918e-03,
         -2.1793e-06,  0.0000e+00],
        [-1.4139e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2918e-03,
         -2.1793e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2797.1123, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7983, device='cuda:0')



h[100].sum tensor(-1.4144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-18.7296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0098, 0.0000, 0.0000,  ..., 0.0249, 0.0094, 0.0000],
        [0.0365, 0.0000, 0.0000,  ..., 0.0641, 0.0329, 0.0000],
        [0.0913, 0.0000, 0.0000,  ..., 0.1443, 0.0810, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58366.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0426, 0.0000,  ..., 0.1945, 0.0000, 0.0196],
        [0.0000, 0.1140, 0.0000,  ..., 0.3833, 0.0000, 0.0562],
        [0.0000, 0.2180, 0.0000,  ..., 0.6497, 0.0000, 0.1110],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564784.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(441.2126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-228.6447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.4769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2573e-01],
        [ 4.9791e-04],
        [ 1.2934e-01],
        ...,
        [-1.0702e+00],
        [-1.0671e+00],
        [-1.0662e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193402.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0141],
        [1.0118],
        ...,
        [1.0009],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368761.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0142],
        [1.0119],
        ...,
        [1.0008],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368772.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1663e-03, -4.3134e-07, -1.0195e-05,  ...,  1.2828e-02,
          6.3269e-03, -4.2119e-03],
        [-1.4098e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2930e-03,
          4.9842e-08,  0.0000e+00],
        [ 1.0965e-02, -7.0452e-07, -1.6651e-05,  ...,  1.9500e-02,
          1.0334e-02, -6.8795e-03],
        ...,
        [-1.4098e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2930e-03,
          4.9842e-08,  0.0000e+00],
        [-1.4098e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2930e-03,
          4.9842e-08,  0.0000e+00],
        [-1.4098e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2930e-03,
          4.9842e-08,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3224.7395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1246, device='cuda:0')



h[100].sum tensor(-1.9887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-26.4784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.5081e-02, 0.0000e+00, 0.0000e+00,  ..., 6.3962e-02, 3.2858e-02,
         0.0000e+00],
        [3.1028e-02, 0.0000e+00, 0.0000e+00,  ..., 6.0395e-02, 3.0686e-02,
         0.0000e+00],
        [2.2716e-02, 0.0000e+00, 0.0000e+00,  ..., 4.4868e-02, 2.1358e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.4582e-03, 2.0558e-07,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.4567e-03, 2.0555e-07,
         0.0000e+00],
        [1.4971e-02, 0.0000e+00, 0.0000e+00,  ..., 3.2295e-02, 1.3716e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66299.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1254, 0.0000,  ..., 0.4203, 0.0000, 0.0615],
        [0.0000, 0.0943, 0.0000,  ..., 0.3428, 0.0000, 0.0448],
        [0.0000, 0.0851, 0.0000,  ..., 0.3169, 0.0000, 0.0402],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0941, 0.0000, 0.0024],
        [0.0000, 0.0378, 0.0000,  ..., 0.1864, 0.0000, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602336.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(478.2340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.1793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.3077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2440],
        [ 0.2403],
        [ 0.2398],
        ...,
        [-0.9217],
        [-0.7126],
        [-0.4394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189292.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0142],
        [1.0119],
        ...,
        [1.0008],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368772.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0143],
        [1.0120],
        ...,
        [1.0008],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368783.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1975e-02, -7.1241e-07, -1.7445e-05,  ...,  2.0867e-02,
          1.1142e-02, -7.4141e-03],
        [ 1.0227e-02, -6.1922e-07, -1.5163e-05,  ...,  1.8437e-02,
          9.6826e-03, -6.4442e-03],
        [ 1.0003e-02, -6.0726e-07, -1.4870e-05,  ...,  1.8125e-02,
          9.4953e-03, -6.3197e-03],
        ...,
        [-1.3848e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2924e-03,
         -1.2434e-05,  0.0000e+00],
        [-1.3848e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2924e-03,
         -1.2434e-05,  0.0000e+00],
        [-1.3848e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2924e-03,
         -1.2434e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3212.8345, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0897, device='cuda:0')



h[100].sum tensor(-1.9450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-26.0390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0430, 0.0000, 0.0000,  ..., 0.0768, 0.0405, 0.0000],
        [0.0587, 0.0000, 0.0000,  ..., 0.0988, 0.0537, 0.0000],
        [0.0558, 0.0000, 0.0000,  ..., 0.0947, 0.0512, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67183.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1358, 0.0000,  ..., 0.4459, 0.0000, 0.0656],
        [0.0000, 0.1749, 0.0000,  ..., 0.5464, 0.0000, 0.0863],
        [0.0000, 0.1801, 0.0000,  ..., 0.5595, 0.0000, 0.0890],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0561, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0561, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0561, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609099.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(484.7484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.7445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.8216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1593],
        [ 0.1966],
        [ 0.2132],
        ...,
        [-1.0304],
        [-0.9818],
        [-0.9497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184698.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0143],
        [1.0120],
        ...,
        [1.0008],
        [0.9999],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368783.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0143],
        [1.0120],
        ...,
        [1.0008],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4344e-03, -2.8899e-07, -7.3365e-06,  ...,  1.0342e-02,
          4.8063e-03, -3.2078e-03],
        [ 4.5510e-03, -2.9481e-07, -7.4841e-06,  ...,  1.0504e-02,
          4.9035e-03, -3.2724e-03],
        [-1.3562e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2922e-03,
         -2.7104e-05,  0.0000e+00],
        ...,
        [-1.3562e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2922e-03,
         -2.7104e-05,  0.0000e+00],
        [-1.3562e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2922e-03,
         -2.7104e-05,  0.0000e+00],
        [-1.3562e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2922e-03,
         -2.7104e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2988.8699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9192, device='cuda:0')



h[100].sum tensor(-1.6101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-21.6737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0352, 0.0000, 0.0000,  ..., 0.0658, 0.0339, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0368, 0.0165, 0.0000],
        [0.0046, 0.0000, 0.0000,  ..., 0.0176, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60986.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0715, 0.0000,  ..., 0.2785, 0.0000, 0.0303],
        [0.0000, 0.0485, 0.0000,  ..., 0.2182, 0.0000, 0.0189],
        [0.0000, 0.0228, 0.0000,  ..., 0.1498, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572259.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.8111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.1112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-327.0319, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1524],
        [ 0.1479],
        [ 0.1242],
        ...,
        [-1.0876],
        [-1.0842],
        [-1.0830]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208811.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0143],
        [1.0120],
        ...,
        [1.0008],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0144],
        [1.0121],
        ...,
        [1.0008],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368806.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00],
        [-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00],
        [-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00],
        ...,
        [-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00],
        [-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00],
        [-1.3400e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2915e-03,
         -3.5752e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3440.3931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2409, device='cuda:0')



h[100].sum tensor(-2.1741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-29.4275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0203, 0.0066, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0149, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71242.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 6.6960e-03, 0.0000e+00,  ..., 1.0934e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2648e-03, 0.0000e+00,  ..., 9.3930e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3566e-06, 0.0000e+00,  ..., 7.5411e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7137e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7125e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7118e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628745.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.7914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.6950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-305.2456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6692],
        [-0.7863],
        [-0.9194],
        ...,
        [-1.0894],
        [-1.0862],
        [-1.0852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186173.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0144],
        [1.0121],
        ...,
        [1.0008],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368806.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0145],
        [1.0122],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368818.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00],
        [-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00],
        [-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00],
        ...,
        [-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00],
        [-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00],
        [-1.3300e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2888e-03,
         -4.1116e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3053.0415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9297, device='cuda:0')



h[100].sum tensor(-1.6257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-22.1254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64584.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0562, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0566, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601121.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.0669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.0210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.3349, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2055],
        [-1.2512],
        [-1.2840],
        ...,
        [-1.0846],
        [-1.0828],
        [-1.0825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180363.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0145],
        [1.0122],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368818.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        ...,
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2981.4792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8679, device='cuda:0')



h[100].sum tensor(-1.4943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-20.4495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0177, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61181.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0264, 0.0000,  ..., 0.1509, 0.0000, 0.0099],
        [0.0000, 0.0043, 0.0000,  ..., 0.0924, 0.0000, 0.0009],
        [0.0000, 0.0016, 0.0000,  ..., 0.0823, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579441.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.0299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.0760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.4644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0781],
        [-0.2364],
        [-0.3343],
        ...,
        [-1.0900],
        [-1.0869],
        [-1.0860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194626.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        ...,
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3293.0193, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1126, device='cuda:0')



h[100].sum tensor(-1.9011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-26.0177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1542, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66033.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596482.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(479.1229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.8731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.2754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2398],
        [-1.2623],
        [-1.2733],
        ...,
        [-1.0875],
        [-1.0840],
        [-1.0829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167963.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [ 1.7377e-02, -7.6142e-07, -2.1632e-05,  ...,  2.8277e-02,
          1.5556e-02, -1.0306e-02],
        ...,
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00],
        [-1.3241e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2904e-03,
         -4.4519e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3166.5474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0061, device='cuda:0')



h[100].sum tensor(-1.7360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-23.7573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0318, 0.0000, 0.0000,  ..., 0.0572, 0.0287, 0.0000],
        [0.0649, 0.0000, 0.0000,  ..., 0.1051, 0.0574, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64796.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0362, 0.0000,  ..., 0.1706, 0.0000, 0.0137],
        [0.0000, 0.1006, 0.0000,  ..., 0.3426, 0.0000, 0.0454],
        [0.0000, 0.1708, 0.0000,  ..., 0.5199, 0.0000, 0.0810],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594946.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(474.5506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.2899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-320.5038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0275],
        [ 0.0879],
        [ 0.1188],
        ...,
        [-1.0900],
        [-1.0869],
        [-1.0860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181384.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9998],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368829.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(405.6877, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368840.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9686e-03, -2.0102e-07, -5.9384e-06,  ...,  9.6475e-03,
          4.3689e-03, -2.9119e-03],
        [ 5.1351e-03, -2.4532e-07, -7.2473e-06,  ...,  1.1269e-02,
          5.3420e-03, -3.5537e-03],
        [-1.3239e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2933e-03,
         -4.5842e-05,  0.0000e+00],
        ...,
        [-1.3239e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2933e-03,
         -4.5842e-05,  0.0000e+00],
        [-1.3239e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2933e-03,
         -4.5842e-05,  0.0000e+00],
        [-1.3239e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2933e-03,
         -4.5842e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3594.6160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3236, device='cuda:0')



h[100].sum tensor(-2.2580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-31.0730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0374, 0.0000, 0.0000,  ..., 0.0687, 0.0355, 0.0000],
        [0.0171, 0.0000, 0.0000,  ..., 0.0368, 0.0164, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0273, 0.0107, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77083.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0782, 0.0000,  ..., 0.2941, 0.0000, 0.0323],
        [0.0000, 0.0551, 0.0000,  ..., 0.2350, 0.0000, 0.0209],
        [0.0000, 0.0386, 0.0000,  ..., 0.1917, 0.0000, 0.0127],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678102.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.2137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.8177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.2976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1681],
        [ 0.1593],
        [ 0.1376],
        ...,
        [-1.0893],
        [-1.0864],
        [-1.0855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176001.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0123],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368840.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0124],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368851.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00],
        [-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00],
        [-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00],
        ...,
        [-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00],
        [-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00],
        [-1.3266e-03,  0.0000e+00,  0.0000e+00,  ...,  2.2957e-03,
         -4.5258e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3722.8499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3874, device='cuda:0')



h[100].sum tensor(-2.3902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-33.0753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77984.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0888, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668616.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.4438, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.9634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-285.1989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8737],
        [-0.8053],
        [-0.7006],
        ...,
        [-1.0457],
        [-0.9509],
        [-0.7749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176880.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0124],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368851.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0148],
        [1.0124],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368863., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3257e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3034e-03,
         -4.5065e-05,  0.0000e+00],
        [-1.3257e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3034e-03,
         -4.5065e-05,  0.0000e+00],
        [ 5.4717e-03, -2.2402e-07, -7.1736e-06,  ...,  1.1748e-02,
          5.6241e-03, -3.7275e-03],
        ...,
        [-1.3257e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3034e-03,
         -4.5065e-05,  0.0000e+00],
        [-1.3257e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3034e-03,
         -4.5065e-05,  0.0000e+00],
        [-1.3257e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3034e-03,
         -4.5065e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3149.5767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9659, device='cuda:0')



h[100].sum tensor(-1.6317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-22.7048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0189, 0.0057, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0172, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66257.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0746, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.1009, 0.0000, 0.0015],
        [0.0000, 0.0085, 0.0000,  ..., 0.1126, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615271.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.1942, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.7720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-313.8542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5807],
        [-0.6831],
        [-0.7400],
        ...,
        [-1.0949],
        [-1.0920],
        [-1.0911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157202.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0148],
        [1.0124],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368863., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0149],
        [1.0125],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368874., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00],
        [-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00],
        [-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00],
        ...,
        [-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00],
        [-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00],
        [-1.3157e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3121e-03,
         -4.7252e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3785.1997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4434, device='cuda:0')



h[100].sum tensor(-2.4233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-33.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78038.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0119, 0.0000,  ..., 0.1179, 0.0000, 0.0014],
        [0.0000, 0.0109, 0.0000,  ..., 0.1151, 0.0000, 0.0008],
        [0.0000, 0.0072, 0.0000,  ..., 0.1015, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675372.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.6894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.1631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1733],
        [-1.2136],
        [-1.2291],
        ...,
        [-0.2390],
        [-0.2460],
        [-0.2786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156888.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0149],
        [1.0125],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368874., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0150],
        [1.0126],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368885.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3021e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3210e-03,
         -4.9237e-05,  0.0000e+00],
        [-1.3021e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3210e-03,
         -4.9237e-05,  0.0000e+00],
        [-1.3021e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3210e-03,
         -4.9237e-05,  0.0000e+00],
        ...,
        [-1.3021e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3210e-03,
         -4.9237e-05,  0.0000e+00],
        [-1.3021e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3210e-03,
         -4.9237e-05,  0.0000e+00],
        [ 7.2316e-03, -2.4301e-07, -8.4664e-06,  ...,  1.4177e-02,
          7.0667e-03, -4.6638e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3296.1743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0765, device='cuda:0')



h[100].sum tensor(-1.7965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-25.2778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0218, 0.0073, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0196, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67913.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0795, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0795, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0772, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.1140, 0.0000, 0.0028],
        [0.0000, 0.0143, 0.0000,  ..., 0.1294, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618142.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(486.5097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.8516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.3710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2295],
        [-0.3139],
        [-0.3319],
        ...,
        [-0.9497],
        [-0.8009],
        [-0.6779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188219.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0150],
        [1.0126],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368885.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0151],
        [1.0126],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368895.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00],
        [-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00],
        [-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00],
        ...,
        [-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00],
        [-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00],
        [-1.2933e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3293e-03,
         -4.8363e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3151.7065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9649, device='cuda:0')



h[100].sum tensor(-1.6059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-22.7225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64707.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.9433e-03, 0.0000e+00,  ..., 8.8408e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1556e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.6223e-05, 0.0000e+00,  ..., 7.2632e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.0011e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.9996e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.9987e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606324.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.1069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.7647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.2827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1686],
        [-0.2759],
        [-0.3385],
        ...,
        [-1.1371],
        [-1.1337],
        [-1.1326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183747.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0151],
        [1.0126],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368895.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0127],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368906.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00],
        [-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00],
        [-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00],
        ...,
        [-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00],
        [-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00],
        [-1.2946e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3397e-03,
         -4.8410e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2932.0127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7999, device='cuda:0')



h[100].sum tensor(-1.3248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-18.8514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60161.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0578, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0582, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0583, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588785.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.4638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.0130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-338.3318, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2133],
        [-1.1267],
        [-0.9895],
        ...,
        [-1.1486],
        [-1.1451],
        [-1.1439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204303.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0127],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368906.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0128],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368917.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3184e-02, -7.8229e-07, -3.1173e-05,  ...,  5.0276e-02,
          2.8721e-02, -1.8759e-02],
        [ 2.5840e-02, -6.1571e-07, -2.4535e-05,  ...,  4.0070e-02,
          2.2596e-02, -1.4765e-02],
        [ 2.1790e-02, -5.2385e-07, -2.0875e-05,  ...,  3.4443e-02,
          1.9218e-02, -1.2562e-02],
        ...,
        [-1.3051e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3501e-03,
         -4.3262e-05,  0.0000e+00],
        [-1.3051e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3501e-03,
         -4.3262e-05,  0.0000e+00],
        [-1.3051e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3501e-03,
         -4.3262e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3509.3799, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2394, device='cuda:0')



h[100].sum tensor(-2.0147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-28.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0973, 0.0000, 0.0000,  ..., 0.1520, 0.0854, 0.0000],
        [0.1238, 0.0000, 0.0000,  ..., 0.1889, 0.1075, 0.0000],
        [0.1308, 0.0000, 0.0000,  ..., 0.1987, 0.1133, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69091.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3085, 0.0000,  ..., 0.8631, 0.0000, 0.1509],
        [0.0000, 0.3730, 0.0000,  ..., 1.0215, 0.0000, 0.1840],
        [0.0000, 0.3990, 0.0000,  ..., 1.0849, 0.0000, 0.1972],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618422.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(496.0807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.4619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.7136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1389],
        [ 0.1286],
        [ 0.1190],
        ...,
        [-1.1559],
        [-1.1523],
        [-1.1512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225827.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0128],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368917.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0153],
        [1.0128],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368929., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6579e-02, -5.8485e-07, -2.4430e-05,  ...,  4.1118e-02,
          2.3220e-02, -1.5144e-02],
        [ 1.7446e-02, -3.9331e-07, -1.6429e-05,  ...,  2.8426e-02,
          1.5603e-02, -1.0185e-02],
        [ 3.6658e-02, -7.9624e-07, -3.3260e-05,  ...,  5.5125e-02,
          3.1627e-02, -2.0618e-02],
        ...,
        [-1.3076e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3632e-03,
         -4.0105e-05,  0.0000e+00],
        [-1.3076e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3632e-03,
         -4.0105e-05,  0.0000e+00],
        [-1.3076e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3632e-03,
         -4.0105e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3777.5593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4309, device='cuda:0')



h[100].sum tensor(-2.3117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-33.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0756, 0.0000, 0.0000,  ..., 0.1220, 0.0673, 0.0000],
        [0.1151, 0.0000, 0.0000,  ..., 0.1770, 0.1003, 0.0000],
        [0.0606, 0.0000, 0.0000,  ..., 0.1012, 0.0548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76828.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2079, 0.0000,  ..., 0.6184, 0.0000, 0.1001],
        [0.0000, 0.2551, 0.0000,  ..., 0.7354, 0.0000, 0.1240],
        [0.0000, 0.1983, 0.0000,  ..., 0.5954, 0.0000, 0.0951],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665999.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.7052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.9361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.3420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1847],
        [ 0.1860],
        [ 0.1659],
        ...,
        [-1.1571],
        [-1.1534],
        [-1.1521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180566.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0153],
        [1.0128],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368929., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0154],
        [1.0129],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368940.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7751e-02, -3.6926e-07, -1.6190e-05,  ...,  2.8872e-02,
          1.5878e-02, -1.0340e-02],
        [ 2.8592e-02, -5.7918e-07, -2.5394e-05,  ...,  4.3939e-02,
          2.4922e-02, -1.6218e-02],
        [ 3.2676e-02, -6.5828e-07, -2.8862e-05,  ...,  4.9616e-02,
          2.8329e-02, -1.8433e-02],
        ...,
        [-1.3169e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3692e-03,
         -3.0011e-05,  0.0000e+00],
        [-1.3169e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3692e-03,
         -3.0011e-05,  0.0000e+00],
        [-1.3169e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3692e-03,
         -3.0011e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3270.5376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0358, device='cuda:0')



h[100].sum tensor(-1.6609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-24.0358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0865, 0.0000, 0.0000,  ..., 0.1371, 0.0765, 0.0000],
        [0.0901, 0.0000, 0.0000,  ..., 0.1423, 0.0795, 0.0000],
        [0.1025, 0.0000, 0.0000,  ..., 0.1595, 0.0899, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64644.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2697, 0.0000,  ..., 0.7725, 0.0000, 0.1327],
        [0.0000, 0.2634, 0.0000,  ..., 0.7577, 0.0000, 0.1295],
        [0.0000, 0.2498, 0.0000,  ..., 0.7227, 0.0000, 0.1225],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604402.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(473.3975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.7744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-327.5367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1635],
        [ 0.1592],
        [ 0.1463],
        ...,
        [-1.1617],
        [-1.1583],
        [-1.1574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203642.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0154],
        [1.0129],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368940.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(535.5458, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0155],
        [1.0130],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368952.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00],
        [-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00],
        [-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00],
        ...,
        [-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00],
        [-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00],
        [-1.3209e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3740e-03,
         -2.3762e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3203.7471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.7765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9578, device='cuda:0')



h[100].sum tensor(-1.5412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-22.4298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64314.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0901, 0.0000, 0.0003],
        [0.0000, 0.0173, 0.0000,  ..., 0.1308, 0.0000, 0.0053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604246.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(469.4608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.2783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-327.7511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6353],
        [-0.3662],
        [-0.1151],
        ...,
        [-1.1627],
        [-1.1591],
        [-1.1564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207767.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0155],
        [1.0130],
        ...,
        [1.0007],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368952.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0157],
        [1.0131],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368963.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00],
        [-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00],
        [-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00],
        ...,
        [-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00],
        [-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00],
        [-1.3193e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3785e-03,
         -2.3787e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3043.2280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8199, device='cuda:0')



h[100].sum tensor(-1.3160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-19.2611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0181, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0188, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60998.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0982, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0826, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.1013, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593604.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.9329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-336.3143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8730],
        [-0.9730],
        [-0.9880],
        ...,
        [-1.1683],
        [-1.1648],
        [-1.1637]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207382.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0157],
        [1.0131],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368963.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0158],
        [1.0132],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368975.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00],
        [-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00],
        [-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00],
        ...,
        [-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00],
        [-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00],
        [-1.3227e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3859e-03,
         -1.7673e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3585.1416, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2317, device='cuda:0')



h[100].sum tensor(-1.9310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-28.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71025.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0606, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635950., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(493.5301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.6879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-314.8257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9264],
        [-0.7862],
        [-0.6120],
        ...,
        [-1.1721],
        [-1.1686],
        [-1.1675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182085.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0158],
        [1.0132],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368975.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0159],
        [1.0133],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368986.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4482e-03, -9.3774e-08, -5.0656e-06,  ...,  1.1802e-02,
          5.6380e-03, -3.6488e-03],
        [-1.3223e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3873e-03,
         -1.3824e-05,  0.0000e+00],
        [ 5.4482e-03, -9.3774e-08, -5.0656e-06,  ...,  1.1802e-02,
          5.6380e-03, -3.6488e-03],
        ...,
        [-1.3223e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3873e-03,
         -1.3824e-05,  0.0000e+00],
        [-1.3223e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3873e-03,
         -1.3824e-05,  0.0000e+00],
        [-1.3223e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3873e-03,
         -1.3824e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4107.6470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6128, device='cuda:0')



h[100].sum tensor(-2.5175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0168, device='cuda:0')



h[200].sum tensor(-37.2681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0174, 0.0046, 0.0000],
        [0.0196, 0.0000, 0.0000,  ..., 0.0444, 0.0208, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0175, 0.0047, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83499.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.1350, 0.0000, 0.0029],
        [0.0000, 0.0237, 0.0000,  ..., 0.1635, 0.0000, 0.0075],
        [0.0000, 0.0137, 0.0000,  ..., 0.1375, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703945.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(546.0801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.0197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-287.5015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0139],
        [-0.0147],
        [ 0.0118],
        ...,
        [-1.1809],
        [-1.1774],
        [-1.1763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174644.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0159],
        [1.0133],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368986.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0160],
        [1.0134],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368997.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3128e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3852e-03,
         -1.9090e-05,  0.0000e+00],
        [ 5.8126e-03, -9.0331e-08, -5.1632e-06,  ...,  1.2293e-02,
          5.9289e-03, -3.8338e-03],
        [-1.3128e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3852e-03,
         -1.9090e-05,  0.0000e+00],
        ...,
        [-1.3128e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3852e-03,
         -1.9090e-05,  0.0000e+00],
        [-1.3128e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3852e-03,
         -1.9090e-05,  0.0000e+00],
        [-1.3128e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3852e-03,
         -1.9090e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3253.7527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9602, device='cuda:0')



h[100].sum tensor(-1.5013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-22.3509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0174, 0.0000, 0.0000,  ..., 0.0394, 0.0178, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0360, 0.0157, 0.0000],
        [0.0249, 0.0000, 0.0000,  ..., 0.0517, 0.0251, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64840.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0409, 0.0000,  ..., 0.2078, 0.0000, 0.0156],
        [0.0000, 0.0439, 0.0000,  ..., 0.2157, 0.0000, 0.0172],
        [0.0000, 0.0448, 0.0000,  ..., 0.2189, 0.0000, 0.0176],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610469.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(470.4337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.9542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-336.0863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0126],
        [ 0.0576],
        [ 0.0222],
        ...,
        [-1.1903],
        [-1.1867],
        [-1.1856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197695.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0160],
        [1.0134],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368997.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0161],
        [1.0135],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369009.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0666e-02, -2.5441e-07, -1.5418e-05,  ...,  3.2937e-02,
          1.8320e-02, -1.1802e-02],
        [ 1.3183e-02, -1.6777e-07, -1.0167e-05,  ...,  2.2532e-02,
          1.2074e-02, -7.7831e-03],
        [ 1.5640e-02, -1.9622e-07, -1.1892e-05,  ...,  2.5949e-02,
          1.4125e-02, -9.1030e-03],
        ...,
        [-1.3058e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3849e-03,
         -2.1430e-05,  0.0000e+00],
        [-1.3058e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3849e-03,
         -2.1430e-05,  0.0000e+00],
        [-1.3058e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3849e-03,
         -2.1430e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3312.9797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0061, device='cuda:0')



h[100].sum tensor(-1.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-23.2940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0565, 0.0000, 0.0000,  ..., 0.0955, 0.0515, 0.0000],
        [0.0659, 0.0000, 0.0000,  ..., 0.1087, 0.0594, 0.0000],
        [0.0593, 0.0000, 0.0000,  ..., 0.0995, 0.0538, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68192.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1651, 0.0000,  ..., 0.5183, 0.0000, 0.0801],
        [0.0000, 0.1654, 0.0000,  ..., 0.5194, 0.0000, 0.0803],
        [0.0000, 0.1502, 0.0000,  ..., 0.4815, 0.0000, 0.0724],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636176.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(486.6024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.3874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.4539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1992],
        [ 0.2032],
        [ 0.2074],
        ...,
        [-1.2005],
        [-1.1969],
        [-1.1957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205996.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0161],
        [1.0135],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369009.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0136],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369020.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00],
        [-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00],
        [-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00],
        ...,
        [-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00],
        [-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00],
        [-1.3054e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3848e-03,
         -1.9894e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3171.4404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8865, device='cuda:0')



h[100].sum tensor(-1.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-20.6598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0165, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62875.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0197, 0.0000,  ..., 0.1462, 0.0000, 0.0060],
        [0.0000, 0.0068, 0.0000,  ..., 0.1040, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0738, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602847.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.2382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.8608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.9977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0307],
        [-0.2509],
        [-0.5020],
        ...,
        [-1.2068],
        [-1.2032],
        [-1.2020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217946.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0136],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369020.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0137],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369032., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00],
        [-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00],
        [-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00],
        ...,
        [-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00],
        [-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00],
        [-1.3071e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3836e-03,
         -2.0407e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4414.2100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8308, device='cuda:0')



h[100].sum tensor(-2.7539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0191, device='cuda:0')



h[200].sum tensor(-41.7096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0000, 0.0000,  ..., 0.0529, 0.0259, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87225.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1086, 0.0000,  ..., 0.3697, 0.0000, 0.0520],
        [0.0000, 0.0294, 0.0000,  ..., 0.1539, 0.0000, 0.0133],
        [0.0000, 0.0024, 0.0000,  ..., 0.0834, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712156.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(561.5386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.1091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.0300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0940],
        [-0.0178],
        [-0.1255],
        ...,
        [-1.2093],
        [-1.2057],
        [-1.2045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198015.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0137],
        ...,
        [1.0007],
        [0.9997],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369032., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0163],
        [1.0137],
        ...,
        [1.0006],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369043.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6260e-03, -6.0286e-08, -4.4164e-06,  ...,  1.2021e-02,
          5.7659e-03, -3.7068e-03],
        [ 1.4691e-02, -1.3910e-07, -1.0190e-05,  ...,  2.4627e-02,
          1.3333e-02, -8.5525e-03],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3786e-03,
         -2.2908e-05,  0.0000e+00],
        ...,
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3786e-03,
         -2.2908e-05,  0.0000e+00],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3786e-03,
         -2.2908e-05,  0.0000e+00],
        [-1.3086e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3786e-03,
         -2.2908e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3474.4800, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0991, device='cuda:0')



h[100].sum tensor(-1.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-25.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0709, 0.0000, 0.0000,  ..., 0.1156, 0.0635, 0.0000],
        [0.0356, 0.0000, 0.0000,  ..., 0.0647, 0.0330, 0.0000],
        [0.0373, 0.0000, 0.0000,  ..., 0.0671, 0.0344, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67203.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1801, 0.0000,  ..., 0.5559, 0.0000, 0.0882],
        [0.0000, 0.1260, 0.0000,  ..., 0.4212, 0.0000, 0.0603],
        [0.0000, 0.0953, 0.0000,  ..., 0.3434, 0.0000, 0.0445],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617046.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(474.7147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.6667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.1037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2233],
        [ 0.1944],
        [ 0.1014],
        ...,
        [-1.2100],
        [-1.2064],
        [-1.2052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193364.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0163],
        [1.0137],
        ...,
        [1.0006],
        [0.9997],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369043.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0164],
        [1.0138],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369055.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00],
        [-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00],
        [-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00],
        ...,
        [-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00],
        [-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00],
        [-1.3149e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3781e-03,
         -1.8908e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3158.3857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8397, device='cuda:0')



h[100].sum tensor(-1.2671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-19.4139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0157, 0.0000, 0.0000,  ..., 0.0352, 0.0153, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62910.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.0000],
        [0.0000, 0.0209, 0.0000,  ..., 0.1337, 0.0000, 0.0090],
        [0.0000, 0.0654, 0.0000,  ..., 0.2654, 0.0000, 0.0302],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606707.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(454.2521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.7729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.0434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1742],
        [-0.0536],
        [ 0.0742],
        ...,
        [-1.2108],
        [-1.2072],
        [-1.2060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191703.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0164],
        [1.0138],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369055.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(515.7523, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0164],
        [1.0139],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369067.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00],
        [-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00],
        [-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00],
        ...,
        [-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00],
        [-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00],
        [-1.3168e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3751e-03,
         -1.7460e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3375.3772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9813, device='cuda:0')



h[100].sum tensor(-1.4794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-22.7976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0349, 0.0151, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66949.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0876, 0.0000, 0.0007],
        [0.0000, 0.0269, 0.0000,  ..., 0.1582, 0.0000, 0.0114],
        [0.0000, 0.0663, 0.0000,  ..., 0.2703, 0.0000, 0.0301],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622910.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-244.9935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.5378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1580],
        [ 0.0035],
        [ 0.1171],
        ...,
        [-1.1837],
        [-1.1909],
        [-1.1964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197940.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0164],
        [1.0139],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369067.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0165],
        [1.0141],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369078.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00],
        [-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00],
        [-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00],
        ...,
        [-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00],
        [-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00],
        [-1.3195e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3749e-03,
         -1.6918e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3490.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0539, device='cuda:0')



h[100].sum tensor(-1.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-24.4697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68564.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0156, 0.0000,  ..., 0.1326, 0.0000, 0.0052],
        [0.0000, 0.0168, 0.0000,  ..., 0.1437, 0.0000, 0.0056],
        [0.0000, 0.0182, 0.0000,  ..., 0.1434, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628463.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(473.2258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.2555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.2389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0189],
        [ 0.0906],
        [ 0.1309],
        ...,
        [-1.2126],
        [-1.2091],
        [-1.2079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192326.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0165],
        [1.0141],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369078.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0166],
        [1.0142],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369090.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4790e-03, -3.8543e-08, -3.7953e-06,  ...,  1.1820e-02,
          5.6510e-03, -3.6074e-03],
        [-1.3172e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3744e-03,
         -1.8036e-05,  0.0000e+00],
        [ 5.4790e-03, -3.8543e-08, -3.7953e-06,  ...,  1.1820e-02,
          5.6510e-03, -3.6074e-03],
        ...,
        [-1.3172e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3744e-03,
         -1.8036e-05,  0.0000e+00],
        [-1.3172e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3744e-03,
         -1.8036e-05,  0.0000e+00],
        [-1.3172e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3744e-03,
         -1.8036e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3457.4099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0258, device='cuda:0')



h[100].sum tensor(-1.5224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-23.7330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0000, 0.0000,  ..., 0.0242, 0.0088, 0.0000],
        [0.0297, 0.0000, 0.0000,  ..., 0.0583, 0.0291, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0244, 0.0088, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68002.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0263, 0.0000,  ..., 0.1707, 0.0000, 0.0099],
        [0.0000, 0.0493, 0.0000,  ..., 0.2312, 0.0000, 0.0210],
        [0.0000, 0.0281, 0.0000,  ..., 0.1765, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629070.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(469.4823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.3457, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0937],
        [ 0.0371],
        [ 0.0213],
        ...,
        [-1.2154],
        [-1.2117],
        [-1.2089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170160.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0166],
        [1.0142],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369090.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0167],
        [1.0143],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369101.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3831e-02, -7.6352e-08, -8.1793e-06,  ...,  2.3406e-02,
          1.2605e-02, -8.0222e-03],
        [ 5.6474e-03, -3.5086e-08, -3.7586e-06,  ...,  1.2035e-02,
          5.7806e-03, -3.6864e-03],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3663e-03,
         -2.1561e-05,  0.0000e+00],
        ...,
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3663e-03,
         -2.1561e-05,  0.0000e+00],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3663e-03,
         -2.1561e-05,  0.0000e+00],
        [-1.3103e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3663e-03,
         -2.1561e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3229.3091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8609, device='cuda:0')



h[100].sum tensor(-1.2708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-19.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0000, 0.0000,  ..., 0.0367, 0.0162, 0.0000],
        [0.0185, 0.0000, 0.0000,  ..., 0.0390, 0.0176, 0.0000],
        [0.0295, 0.0000, 0.0000,  ..., 0.0580, 0.0289, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62657.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0679, 0.0000,  ..., 0.2731, 0.0000, 0.0308],
        [0.0000, 0.0618, 0.0000,  ..., 0.2601, 0.0000, 0.0273],
        [0.0000, 0.0714, 0.0000,  ..., 0.2865, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604739.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.7007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.8381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-350.2158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2085],
        [ 0.2198],
        [ 0.2282],
        ...,
        [-1.2336],
        [-1.2300],
        [-1.2288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193800.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0167],
        [1.0143],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369101.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0168],
        [1.0144],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369112.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5029e-03, -4.3755e-08, -5.1265e-06,  ...,  1.5991e-02,
          8.1587e-03, -5.1888e-03],
        [ 1.1945e-02, -5.9107e-08, -6.9251e-06,  ...,  2.0774e-02,
          1.1029e-02, -7.0093e-03],
        [ 1.1716e-02, -5.8085e-08, -6.8053e-06,  ...,  2.0455e-02,
          1.0838e-02, -6.8882e-03],
        ...,
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -2.1036e-05,  0.0000e+00],
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -2.1036e-05,  0.0000e+00],
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -2.1036e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4188.2646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5752, device='cuda:0')



h[100].sum tensor(-2.3011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-36.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0000, 0.0000,  ..., 0.0394, 0.0179, 0.0000],
        [0.0302, 0.0000, 0.0000,  ..., 0.0571, 0.0285, 0.0000],
        [0.0694, 0.0000, 0.0000,  ..., 0.1134, 0.0622, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83681.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0786, 0.0000,  ..., 0.3005, 0.0000, 0.0355],
        [0.0000, 0.1181, 0.0000,  ..., 0.3977, 0.0000, 0.0556],
        [0.0000, 0.1801, 0.0000,  ..., 0.5475, 0.0000, 0.0871],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0630, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0630, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708380.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(542.0610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.1169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-304.5428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1708],
        [ 0.1748],
        [ 0.1633],
        ...,
        [-1.2473],
        [-1.2436],
        [-1.2424]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211623.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0168],
        [1.0144],
        ...,
        [1.0005],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369112.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0169],
        [1.0145],
        ...,
        [1.0006],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369123.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3074e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -1.7328e-05,  0.0000e+00],
        [-1.3074e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -1.7328e-05,  0.0000e+00],
        [ 1.0268e-02, -4.5377e-08, -5.8503e-06,  ...,  1.8442e-02,
          9.6328e-03, -6.1117e-03],
        ...,
        [-1.3074e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -1.7328e-05,  0.0000e+00],
        [-1.3074e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -1.7328e-05,  0.0000e+00],
        [-1.3074e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3602e-03,
         -1.7328e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3249.1396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8931, device='cuda:0')



h[100].sum tensor(-1.2871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-20.4190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0259, 0.0098, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0230, 0.0080, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63072.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0192e-03, 0.0000e+00,  ..., 8.8244e-02, 0.0000e+00,
         2.2126e-05],
        [0.0000e+00, 2.0582e-02, 0.0000e+00,  ..., 1.4601e-01, 0.0000e+00,
         8.5236e-03],
        [0.0000e+00, 4.6454e-02, 0.0000e+00,  ..., 2.1605e-01, 0.0000e+00,
         2.0145e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3033e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3017e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3007e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610786.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.1583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.3164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-358.3428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6333],
        [-0.5369],
        [-0.3276],
        ...,
        [-1.2571],
        [-1.2536],
        [-1.2502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226871.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0169],
        [1.0145],
        ...,
        [1.0006],
        [0.9996],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369123.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0170],
        [1.0146],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369134.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6146e-02, -5.9702e-08, -8.5312e-06,  ...,  2.6614e-02,
          1.4538e-02, -9.1997e-03],
        [ 3.9064e-02, -1.3809e-07, -1.9733e-05,  ...,  5.8451e-02,
          3.3641e-02, -2.1279e-02],
        [ 1.5182e-02, -5.6406e-08, -8.0603e-06,  ...,  2.5276e-02,
          1.3735e-02, -8.6919e-03],
        ...,
        [-1.3084e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3668e-03,
         -1.1530e-05,  0.0000e+00],
        [-1.3084e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3668e-03,
         -1.1530e-05,  0.0000e+00],
        [-1.3084e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3668e-03,
         -1.1530e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3547.7854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0994, device='cuda:0')



h[100].sum tensor(-1.5942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-25.4386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0806, 0.0000, 0.0000,  ..., 0.1289, 0.0716, 0.0000],
        [0.0627, 0.0000, 0.0000,  ..., 0.1041, 0.0566, 0.0000],
        [0.0653, 0.0000, 0.0000,  ..., 0.1059, 0.0577, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69178.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1855, 0.0000,  ..., 0.5587, 0.0000, 0.0899],
        [0.0000, 0.1816, 0.0000,  ..., 0.5503, 0.0000, 0.0883],
        [0.0000, 0.1611, 0.0000,  ..., 0.4981, 0.0000, 0.0780],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638095., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.4590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.7283, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.3819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1437],
        [ 0.1209],
        [ 0.0418],
        ...,
        [-1.2677],
        [-1.2639],
        [-1.2627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211877., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0170],
        [1.0146],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369134.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0171],
        [1.0147],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369145.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1418e-02, -3.7651e-08, -6.0145e-06,  ...,  2.0053e-02,
          1.0597e-02, -6.6956e-03],
        [ 6.0361e-03, -2.1729e-08, -3.4710e-06,  ...,  1.2578e-02,
          6.1121e-03, -3.8641e-03],
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3764e-03,
         -8.9210e-06,  0.0000e+00],
        ...,
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3764e-03,
         -8.9210e-06,  0.0000e+00],
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3764e-03,
         -8.9210e-06,  0.0000e+00],
        [-1.3077e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3764e-03,
         -8.9210e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3339.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9534, device='cuda:0')



h[100].sum tensor(-1.3616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-21.8539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0376, 0.0000, 0.0000,  ..., 0.0692, 0.0357, 0.0000],
        [0.0208, 0.0000, 0.0000,  ..., 0.0423, 0.0196, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0200, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65094.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0798, 0.0000,  ..., 0.3044, 0.0000, 0.0361],
        [0.0000, 0.0524, 0.0000,  ..., 0.2347, 0.0000, 0.0227],
        [0.0000, 0.0219, 0.0000,  ..., 0.1502, 0.0000, 0.0086],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0634, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622068.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(473.2424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.7759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-359.2824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1525],
        [ 0.0675],
        [-0.0857],
        ...,
        [-1.2764],
        [-1.2725],
        [-1.2713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223187.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0171],
        [1.0147],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369145.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0172],
        [1.0147],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369157.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00],
        [-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00],
        [-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00],
        ...,
        [-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00],
        [-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00],
        [-1.3148e-03,  0.0000e+00,  0.0000e+00,  ...,  2.3892e-03,
         -2.0039e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3245.8838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8714, device='cuda:0')



h[100].sum tensor(-1.2473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-20.1375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62559.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610154., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(462.2552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-365.3237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7311],
        [-0.9824],
        [-1.1662],
        ...,
        [-1.2818],
        [-1.2779],
        [-1.2767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221372.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0172],
        [1.0147],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369157.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0172],
        [1.0148],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369168.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00],
        [-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00],
        [-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00],
        ...,
        [-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00],
        [-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00],
        [-1.3258e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4052e-03,
          1.0593e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3506.1729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0682, device='cuda:0')



h[100].sum tensor(-1.4934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-24.2530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.7263e-03, 4.2837e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.7803e-03, 4.3075e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.7913e-03, 4.3124e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9661e-03, 4.3893e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9639e-03, 4.3884e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9631e-03, 4.3880e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69268.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0642, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648308.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.3169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.9220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.8262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2855],
        [-1.1588],
        [-0.9779],
        ...,
        [-1.1426],
        [-1.0301],
        [-0.9567]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191358.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0172],
        [1.0148],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369168.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(506.8000, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0173],
        [1.0149],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369179.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3344e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4214e-03,
          2.2284e-05,  0.0000e+00],
        [-1.3344e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4214e-03,
          2.2284e-05,  0.0000e+00],
        [ 7.7676e-03, -1.6201e-08, -3.8880e-06,  ...,  1.5066e-02,
          7.6093e-03, -4.7657e-03],
        ...,
        [-1.3344e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4214e-03,
          2.2284e-05,  0.0000e+00],
        [-1.3344e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4214e-03,
          2.2284e-05,  0.0000e+00],
        [-1.3344e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4214e-03,
          2.2284e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3063.5020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7206, device='cuda:0')



h[100].sum tensor(-1.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0075, device='cuda:0')



h[200].sum tensor(-16.5870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.7923e-03, 9.0119e-05,
         0.0000e+00],
        [7.9020e-03, 0.0000e+00, 0.0000e+00,  ..., 2.2710e-02, 7.8089e-03,
         0.0000e+00],
        [2.8139e-02, 0.0000e+00, 0.0000e+00,  ..., 5.2722e-02, 2.5811e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0034e-02, 9.2346e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0032e-02, 9.2326e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0031e-02, 9.2319e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60226.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.1193, 0.0000, 0.0023],
        [0.0000, 0.0329, 0.0000,  ..., 0.1840, 0.0000, 0.0150],
        [0.0000, 0.0947, 0.0000,  ..., 0.3406, 0.0000, 0.0456],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609691.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.1768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.6514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0238],
        [ 0.0363],
        [ 0.1118],
        ...,
        [-1.2870],
        [-1.2831],
        [-1.2819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218384.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0173],
        [1.0149],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369179.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0173],
        [1.0149],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369190.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6238e-02, -3.9946e-08, -1.1386e-05,  ...,  4.0744e-02,
          2.2997e-02, -1.4414e-02],
        [ 2.3882e-02, -3.6532e-08, -1.0412e-05,  ...,  3.7470e-02,
          2.1033e-02, -1.3182e-02],
        [ 2.2310e-02, -3.4255e-08, -9.7636e-06,  ...,  3.5287e-02,
          1.9723e-02, -1.2361e-02],
        ...,
        [-1.3357e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4367e-03,
          1.1249e-05,  0.0000e+00],
        [-1.3357e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4367e-03,
          1.1249e-05,  0.0000e+00],
        [-1.3357e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4367e-03,
          1.1249e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3649.7754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1446, device='cuda:0')



h[100].sum tensor(-1.5954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-26.2163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.4084e-02, 0.0000e+00, 0.0000e+00,  ..., 9.0619e-02, 4.8508e-02,
         0.0000e+00],
        [7.7269e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2480e-01, 6.8987e-02,
         0.0000e+00],
        [8.4154e-02, 0.0000e+00, 0.0000e+00,  ..., 1.3439e-01, 7.4731e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0099e-02, 4.6624e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0097e-02, 4.6613e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0096e-02, 4.6610e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71070.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2009, 0.0000,  ..., 0.6037, 0.0000, 0.0992],
        [0.0000, 0.2277, 0.0000,  ..., 0.6710, 0.0000, 0.1128],
        [0.0000, 0.2356, 0.0000,  ..., 0.6913, 0.0000, 0.1168],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0658, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0657, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0657, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659593.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.2032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.2031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.5941, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2019],
        [ 0.2043],
        [ 0.2078],
        ...,
        [-1.2941],
        [-1.2903],
        [-1.2890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203634.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0173],
        [1.0149],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369190.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0174],
        [1.0150],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369201.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2504e-02, -1.5844e-08, -5.5213e-06,  ...,  2.1670e-02,
          1.1521e-02, -7.2204e-03],
        [ 4.6999e-03, -6.9069e-09, -2.4070e-06,  ...,  1.0828e-02,
          5.0156e-03, -3.1477e-03],
        [ 4.5194e-03, -6.7003e-09, -2.3350e-06,  ...,  1.0578e-02,
          4.8652e-03, -3.0535e-03],
        ...,
        [-1.3315e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4488e-03,
         -1.2346e-05,  0.0000e+00],
        [-1.3315e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4488e-03,
         -1.2346e-05,  0.0000e+00],
        [-1.3315e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4488e-03,
         -1.2346e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.3218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9150, device='cuda:0')



h[100].sum tensor(-1.2679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-20.9593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0220, 0.0000, 0.0000,  ..., 0.0480, 0.0228, 0.0000],
        [0.0325, 0.0000, 0.0000,  ..., 0.0627, 0.0316, 0.0000],
        [0.0258, 0.0000, 0.0000,  ..., 0.0534, 0.0260, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65018.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0868, 0.0000,  ..., 0.3302, 0.0000, 0.0399],
        [0.0000, 0.0931, 0.0000,  ..., 0.3462, 0.0000, 0.0432],
        [0.0000, 0.0807, 0.0000,  ..., 0.3150, 0.0000, 0.0369],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630027.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(471.1874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.6862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-360.3500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2445],
        [ 0.2402],
        [ 0.2328],
        ...,
        [-1.3035],
        [-1.2995],
        [-1.2982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217474.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0174],
        [1.0150],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369201.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0150],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369211.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8241e-02, -1.6977e-08, -7.5457e-06,  ...,  2.9644e-02,
          1.6273e-02, -1.0194e-02],
        [ 7.5010e-03, -7.6585e-09, -3.4039e-06,  ...,  1.4724e-02,
          7.3203e-03, -4.5988e-03],
        [ 8.4117e-03, -8.4487e-09, -3.7552e-06,  ...,  1.5989e-02,
          8.0795e-03, -5.0733e-03],
        ...,
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4606e-03,
         -3.7717e-05,  0.0000e+00],
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4606e-03,
         -3.7717e-05,  0.0000e+00],
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4606e-03,
         -3.7717e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3421.0483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.7723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9675, device='cuda:0')



h[100].sum tensor(-1.3321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-22.1513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0432, 0.0000, 0.0000,  ..., 0.0774, 0.0403, 0.0000],
        [0.0479, 0.0000, 0.0000,  ..., 0.0840, 0.0443, 0.0000],
        [0.0274, 0.0000, 0.0000,  ..., 0.0537, 0.0261, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66277.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0948, 0.0000,  ..., 0.3438, 0.0000, 0.0449],
        [0.0000, 0.0960, 0.0000,  ..., 0.3468, 0.0000, 0.0457],
        [0.0000, 0.0761, 0.0000,  ..., 0.2974, 0.0000, 0.0356],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635464., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(477.0593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.5327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.4154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1733],
        [ 0.1197],
        [ 0.0253],
        ...,
        [-1.3124],
        [-1.3084],
        [-1.3071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209656.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0150],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369211.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0175],
        [1.0151],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369222.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.9730e-03, -6.3290e-09, -3.8376e-06,  ...,  1.6783e-02,
          8.5291e-03, -5.3568e-03],
        [ 5.3473e-03, -4.1008e-09, -2.4865e-06,  ...,  1.1746e-02,
          5.5067e-03, -3.4709e-03],
        [ 3.8516e-03, -3.1816e-09, -1.9292e-06,  ...,  9.6676e-03,
          4.2598e-03, -2.6929e-03],
        ...,
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4748e-03,
         -5.5944e-05,  0.0000e+00],
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4748e-03,
         -5.5944e-05,  0.0000e+00],
        [-1.3256e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4748e-03,
         -5.5944e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3904.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3250, device='cuda:0')



h[100].sum tensor(-1.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-29.9452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.0000, 0.0000,  ..., 0.0594, 0.0295, 0.0000],
        [0.0264, 0.0000, 0.0000,  ..., 0.0524, 0.0252, 0.0000],
        [0.0483, 0.0000, 0.0000,  ..., 0.0847, 0.0445, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75503.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1523, 0.0000,  ..., 0.4852, 0.0000, 0.0739],
        [0.0000, 0.1197, 0.0000,  ..., 0.4073, 0.0000, 0.0571],
        [0.0000, 0.1323, 0.0000,  ..., 0.4394, 0.0000, 0.0634],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674775.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(516.1697, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.2950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-339.0168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1444],
        [ 0.1591],
        [ 0.1672],
        ...,
        [-1.3178],
        [-1.3137],
        [-1.3124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229333.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0175],
        [1.0151],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369222.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0176],
        [1.0152],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369233.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00],
        ...,
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00],
        [-1.3291e-03,  0.0000e+00,  0.0000e+00,  ...,  2.4898e-03,
         -7.0838e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3203.3774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7855, device='cuda:0')



h[100].sum tensor(-1.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-18.0046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0000,  ..., 0.0612, 0.0305, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61896.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0856, 0.0000,  ..., 0.3185, 0.0000, 0.0406],
        [0.0000, 0.0244, 0.0000,  ..., 0.1559, 0.0000, 0.0108],
        [0.0000, 0.0042, 0.0000,  ..., 0.1015, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614811.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.6708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.5643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0705],
        [-0.1180],
        [-0.3783],
        ...,
        [-1.3185],
        [-1.3140],
        [-1.3120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229842.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0176],
        [1.0152],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369233.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0177],
        [1.0153],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369244.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00],
        ...,
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00],
        [-1.3332e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5071e-03,
         -8.1593e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3907.5945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2816, device='cuda:0')



h[100].sum tensor(-1.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-29.2293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0361, 0.0154, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76977.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0922, 0.0000, 0.0004],
        [0.0000, 0.0125, 0.0000,  ..., 0.1191, 0.0000, 0.0050],
        [0.0000, 0.0455, 0.0000,  ..., 0.2219, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690352.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(515.0775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.3951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.3747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6187],
        [-0.4759],
        [-0.2194],
        ...,
        [-1.3190],
        [-1.3150],
        [-1.3137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171344.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0177],
        [1.0153],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369244.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0154],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369255.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5516e-03,  8.0410e-11, -1.9779e-06,  ...,  1.0702e-02,
          4.8126e-03, -3.0475e-03],
        [ 4.8273e-03,  8.4176e-11, -2.0705e-06,  ...,  1.1085e-02,
          5.0425e-03, -3.1902e-03],
        [ 1.0715e-02,  1.6459e-10, -4.0484e-06,  ...,  1.9266e-02,
          9.9507e-03, -6.2377e-03],
        ...,
        [-1.3365e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5207e-03,
         -9.5608e-05,  0.0000e+00],
        [-1.3365e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5207e-03,
         -9.5608e-05,  0.0000e+00],
        [-1.3365e-03,  0.0000e+00,  0.0000e+00,  ...,  2.5207e-03,
         -9.5608e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4112.9204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4142, device='cuda:0')



h[100].sum tensor(-1.8953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-32.2761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7521e-02, 2.7618e-10, 0.0000e+00,  ..., 3.8297e-02, 1.6665e-02,
         0.0000e+00],
        [3.5856e-02, 5.6392e-10, 0.0000e+00,  ..., 6.7629e-02, 3.4033e-02,
         0.0000e+00],
        [2.7905e-02, 4.5544e-10, 0.0000e+00,  ..., 5.6605e-02, 2.7410e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0455e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0454e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0453e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81370.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0805, 0.0000,  ..., 0.3106, 0.0000, 0.0370],
        [0.0000, 0.0865, 0.0000,  ..., 0.3283, 0.0000, 0.0396],
        [0.0000, 0.0912, 0.0000,  ..., 0.3402, 0.0000, 0.0419],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716600., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(532.1544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.0067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-321.6389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2050],
        [ 0.2160],
        [ 0.2178],
        ...,
        [-1.3198],
        [-1.3154],
        [-1.3136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196099.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0154],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369255.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0179],
        [1.0154],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369266.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3406e-03, -1.5655e-04,  0.0000e+00,  ...,  2.5328e-03,
         -1.0818e-04,  0.0000e+00],
        [ 1.0229e-02, -1.7585e-04, -3.7535e-06,  ...,  1.8608e-02,
          9.5365e-03, -5.9783e-03],
        [ 2.3654e-02, -1.9825e-04, -8.1089e-06,  ...,  3.7263e-02,
          2.0728e-02, -1.2915e-02],
        ...,
        [-1.3406e-03, -1.5655e-04,  0.0000e+00,  ...,  2.5328e-03,
         -1.0818e-04,  0.0000e+00],
        [-1.3406e-03, -1.5655e-04,  0.0000e+00,  ...,  2.5328e-03,
         -1.0818e-04,  0.0000e+00],
        [-1.3406e-03, -1.5655e-04,  0.0000e+00,  ...,  2.5328e-03,
         -1.0818e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3645.6167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0710, device='cuda:0')



h[100].sum tensor(-1.4185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-24.3013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0268, 0.0000, 0.0000,  ..., 0.0512, 0.0244, 0.0000],
        [0.0476, 0.0000, 0.0000,  ..., 0.0821, 0.0427, 0.0000],
        [0.0368, 0.0000, 0.0000,  ..., 0.0672, 0.0338, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69906.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0993, 0.0000,  ..., 0.3550, 0.0000, 0.0470],
        [0.0000, 0.1325, 0.0000,  ..., 0.4373, 0.0000, 0.0637],
        [0.0000, 0.1223, 0.0000,  ..., 0.4130, 0.0000, 0.0583],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651063.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(481.8872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-349.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1722],
        [ 0.1718],
        [ 0.1642],
        ...,
        [-1.3232],
        [-1.3193],
        [-1.3180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186788.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0179],
        [1.0154],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369266.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0179],
        [1.0155],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369277.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3454e-03, -2.9773e-04,  0.0000e+00,  ...,  2.5443e-03,
         -1.1479e-04,  0.0000e+00],
        [ 1.0000e-02, -3.3373e-04, -3.5545e-06,  ...,  1.8310e-02,
          9.3439e-03, -5.8530e-03],
        [-1.3454e-03, -2.9773e-04,  0.0000e+00,  ...,  2.5443e-03,
         -1.1479e-04,  0.0000e+00],
        ...,
        [-1.3454e-03, -2.9773e-04,  0.0000e+00,  ...,  2.5443e-03,
         -1.1479e-04,  0.0000e+00],
        [-1.3454e-03, -2.9773e-04,  0.0000e+00,  ...,  2.5443e-03,
         -1.1479e-04,  0.0000e+00],
        [-1.3454e-03, -2.9773e-04,  0.0000e+00,  ...,  2.5443e-03,
         -1.1479e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4079.2534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3804, device='cuda:0')



h[100].sum tensor(-1.8137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-31.2592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0363, 0.0000, 0.0000,  ..., 0.0682, 0.0343, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0234, 0.0077, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0264, 0.0095, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77790.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0585, 0.0000,  ..., 0.2558, 0.0000, 0.0260],
        [0.0000, 0.0295, 0.0000,  ..., 0.1830, 0.0000, 0.0117],
        [0.0000, 0.0436, 0.0000,  ..., 0.2180, 0.0000, 0.0187],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685941.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(514.4333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.7512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.0958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1423],
        [ 0.1434],
        [ 0.1468],
        ...,
        [-1.3280],
        [-1.3240],
        [-1.3227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194356.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0179],
        [1.0155],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369277.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(515.9163, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0180],
        [1.0156],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369287.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000],
        [-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000],
        [-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000],
        ...,
        [-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000],
        [-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000],
        [-0.0013, -0.0004,  0.0000,  ...,  0.0026, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3759.3218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1391, device='cuda:0')



h[100].sum tensor(-1.4904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-25.8421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72761.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0657, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0663, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0796, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666638.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.7969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2366],
        [-1.3492],
        [-1.4356],
        ...,
        [-1.3189],
        [-1.2792],
        [-1.2001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190935.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0180],
        [1.0156],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369287.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0180],
        [1.0157],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369298.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9075e-02, -6.5742e-04, -5.9660e-06,  ...,  3.0957e-02,
          1.6919e-02, -1.0504e-02],
        [ 1.0870e-02, -6.1021e-04, -3.5697e-06,  ...,  1.9555e-02,
          1.0077e-02, -6.2850e-03],
        [ 4.6518e-03, -5.7442e-04, -1.7535e-06,  ...,  1.0912e-02,
          4.8915e-03, -3.0872e-03],
        ...,
        [-1.3517e-03, -5.3988e-04,  0.0000e+00,  ...,  2.5688e-03,
         -1.1485e-04,  0.0000e+00],
        [-1.3517e-03, -5.3988e-04,  0.0000e+00,  ...,  2.5688e-03,
         -1.1485e-04,  0.0000e+00],
        [-1.3517e-03, -5.3988e-04,  0.0000e+00,  ...,  2.5688e-03,
         -1.1485e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3359.2473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.2260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8449, device='cuda:0')



h[100].sum tensor(-1.1016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-19.2161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0541, 0.0000, 0.0000,  ..., 0.0931, 0.0492, 0.0000],
        [0.0526, 0.0000, 0.0000,  ..., 0.0913, 0.0480, 0.0000],
        [0.0458, 0.0000, 0.0000,  ..., 0.0818, 0.0423, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63885.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1943, 0.0000,  ..., 0.5936, 0.0000, 0.0953],
        [0.0000, 0.1754, 0.0000,  ..., 0.5486, 0.0000, 0.0855],
        [0.0000, 0.1594, 0.0000,  ..., 0.5103, 0.0000, 0.0772],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624926., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.5650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.4583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.4129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1742],
        [ 0.1842],
        [ 0.1897],
        ...,
        [-1.3406],
        [-1.3367],
        [-1.3354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201552.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0180],
        [1.0157],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369298.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0181],
        [1.0158],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369309.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5116e-02, -7.5642e-04, -4.6454e-06,  ...,  2.5479e-02,
          1.3633e-02, -8.4588e-03],
        [ 1.5957e-02, -7.6219e-04, -4.8827e-06,  ...,  2.6649e-02,
          1.4335e-02, -8.8909e-03],
        [ 2.2204e-02, -8.0504e-04, -6.6440e-06,  ...,  3.5332e-02,
          1.9545e-02, -1.2098e-02],
        ...,
        [-1.3589e-03, -6.4343e-04,  0.0000e+00,  ...,  2.5792e-03,
         -1.0843e-04,  0.0000e+00],
        [-1.3589e-03, -6.4343e-04,  0.0000e+00,  ...,  2.5792e-03,
         -1.0843e-04,  0.0000e+00],
        [-1.3589e-03, -6.4343e-04,  0.0000e+00,  ...,  2.5792e-03,
         -1.0843e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4015.6252, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3115, device='cuda:0')



h[100].sum tensor(-1.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-29.9103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0864, 0.0000, 0.0000,  ..., 0.1382, 0.0762, 0.0000],
        [0.0647, 0.0000, 0.0000,  ..., 0.1061, 0.0571, 0.0000],
        [0.0345, 0.0000, 0.0000,  ..., 0.0623, 0.0309, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0213, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77689.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.7463e-01, 0.0000e+00,  ..., 7.8641e-01, 0.0000e+00,
         1.3709e-01],
        [0.0000e+00, 2.0501e-01, 0.0000e+00,  ..., 6.1683e-01, 0.0000e+00,
         1.0167e-01],
        [0.0000e+00, 1.3009e-01, 0.0000e+00,  ..., 4.3316e-01, 0.0000e+00,
         6.3465e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1743e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7114e-04, 0.0000e+00,  ..., 8.9877e-02, 0.0000e+00,
         1.9011e-04],
        [0.0000e+00, 1.9309e-02, 0.0000e+00,  ..., 1.5095e-01, 0.0000e+00,
         8.0759e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693463.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.1511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.0896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-338.6755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1694],
        [ 0.1811],
        [ 0.1915],
        ...,
        [-1.1769],
        [-0.9187],
        [-0.5433]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170959.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0181],
        [1.0158],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369309.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0159],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369320.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6031e-02, -8.7343e-04, -4.7339e-06,  ...,  2.6767e-02,
          1.4404e-02, -8.9162e-03],
        [ 8.7370e-03, -8.1614e-04, -2.7487e-06,  ...,  1.6627e-02,
          8.3188e-03, -5.1771e-03],
        [ 4.6436e-03, -7.8398e-04, -1.6346e-06,  ...,  1.0937e-02,
          4.9039e-03, -3.0787e-03],
        ...,
        [-1.3620e-03, -7.3681e-04,  0.0000e+00,  ...,  2.5878e-03,
         -1.0632e-04,  0.0000e+00],
        [-1.3620e-03, -7.3681e-04,  0.0000e+00,  ...,  2.5878e-03,
         -1.0632e-04,  0.0000e+00],
        [-1.3620e-03, -7.3681e-04,  0.0000e+00,  ...,  2.5878e-03,
         -1.0632e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3514.7056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9609, device='cuda:0')



h[100].sum tensor(-1.2369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-21.8379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0460, 0.0000, 0.0000,  ..., 0.0821, 0.0425, 0.0000],
        [0.0366, 0.0000, 0.0000,  ..., 0.0691, 0.0347, 0.0000],
        [0.0189, 0.0000, 0.0000,  ..., 0.0445, 0.0199, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67296.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0971, 0.0000,  ..., 0.3572, 0.0000, 0.0465],
        [0.0000, 0.0858, 0.0000,  ..., 0.3311, 0.0000, 0.0404],
        [0.0000, 0.0670, 0.0000,  ..., 0.2863, 0.0000, 0.0302],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643748.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(479.8429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.6687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-367.6112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2396],
        [ 0.2531],
        [ 0.2436],
        ...,
        [-1.3627],
        [-1.3588],
        [-1.3576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211279.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0159],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369320.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0159],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369331., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3633e-03, -8.2102e-04,  0.0000e+00,  ...,  2.5964e-03,
         -1.0371e-04,  0.0000e+00],
        [-1.3633e-03, -8.2102e-04,  0.0000e+00,  ...,  2.5964e-03,
         -1.0371e-04,  0.0000e+00],
        [ 1.5085e-02, -9.6501e-04, -4.3208e-06,  ...,  2.5465e-02,
          1.3621e-02, -8.4188e-03],
        ...,
        [-1.3633e-03, -8.2102e-04,  0.0000e+00,  ...,  2.5964e-03,
         -1.0371e-04,  0.0000e+00],
        [-1.3633e-03, -8.2102e-04,  0.0000e+00,  ...,  2.5964e-03,
         -1.0371e-04,  0.0000e+00],
        [-1.3633e-03, -8.2102e-04,  0.0000e+00,  ...,  2.5964e-03,
         -1.0371e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3634.4294, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0607, device='cuda:0')



h[100].sum tensor(-1.3453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-23.8973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0205, 0.0000, 0.0000,  ..., 0.0430, 0.0192, 0.0000],
        [0.0413, 0.0000, 0.0000,  ..., 0.0738, 0.0376, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69123.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0137, 0.0000,  ..., 0.1235, 0.0000, 0.0058],
        [0.0000, 0.0602, 0.0000,  ..., 0.2609, 0.0000, 0.0282],
        [0.0000, 0.1232, 0.0000,  ..., 0.4186, 0.0000, 0.0603],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656247.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(491.3687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.2274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-369.8612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7258],
        [-0.4070],
        [-0.1143],
        ...,
        [-1.3772],
        [-1.3731],
        [-1.3717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208447.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0181],
        [1.0159],
        ...,
        [1.0006],
        [0.9996],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369331., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0181],
        [1.0160],
        ...,
        [1.0006],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369342.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2509e-02, -1.0297e-03, -3.5173e-06,  ...,  2.1902e-02,
          1.1485e-02, -7.0904e-03],
        [ 1.1035e-02, -1.0156e-03, -3.1436e-06,  ...,  1.9853e-02,
          1.0255e-02, -6.3372e-03],
        [ 1.1193e-02, -1.0171e-03, -3.1835e-06,  ...,  2.0072e-02,
          1.0386e-02, -6.4176e-03],
        ...,
        [-1.3657e-03, -8.9696e-04,  0.0000e+00,  ...,  2.6095e-03,
         -9.4340e-05,  0.0000e+00],
        [-1.3657e-03, -8.9696e-04,  0.0000e+00,  ...,  2.6095e-03,
         -9.4340e-05,  0.0000e+00],
        [-1.3657e-03, -8.9696e-04,  0.0000e+00,  ...,  2.6095e-03,
         -9.4340e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3894.1106, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2425, device='cuda:0')



h[100].sum tensor(-1.5692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-28.0449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1722, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0496, 0.0000, 0.0000,  ..., 0.0873, 0.0456, 0.0000],
        [0.0316, 0.0000, 0.0000,  ..., 0.0623, 0.0307, 0.0000],
        [0.0329, 0.0000, 0.0000,  ..., 0.0642, 0.0317, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76934.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1362, 0.0000,  ..., 0.4530, 0.0000, 0.0674],
        [0.0000, 0.0952, 0.0000,  ..., 0.3538, 0.0000, 0.0460],
        [0.0000, 0.0785, 0.0000,  ..., 0.3125, 0.0000, 0.0374],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0677, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702219.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.8115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.0060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-353.3665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0374],
        [-0.0158],
        [-0.0749],
        ...,
        [-1.3863],
        [-1.3822],
        [-1.3809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212903.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0181],
        [1.0160],
        ...,
        [1.0006],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369342.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369353.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8884e-03, -1.0401e-03, -1.7737e-06,  ...,  1.2707e-02,
          5.9657e-03, -3.6998e-03],
        [ 3.4314e-03, -1.0148e-03, -1.1728e-06,  ...,  9.2906e-03,
          3.9150e-03, -2.4463e-03],
        [ 1.0683e-02, -1.0895e-03, -2.9465e-06,  ...,  1.9375e-02,
          9.9678e-03, -6.1461e-03],
        ...,
        [-1.3634e-03, -9.6545e-04,  0.0000e+00,  ...,  2.6229e-03,
         -8.7006e-05,  0.0000e+00],
        [-1.3634e-03, -9.6545e-04,  0.0000e+00,  ...,  2.6229e-03,
         -8.7006e-05,  0.0000e+00],
        [-1.3634e-03, -9.6545e-04,  0.0000e+00,  ...,  2.6229e-03,
         -8.7006e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3354.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8454, device='cuda:0')



h[100].sum tensor(-1.0626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-19.1069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0174, 0.0000, 0.0000,  ..., 0.0387, 0.0167, 0.0000],
        [0.0459, 0.0000, 0.0000,  ..., 0.0823, 0.0426, 0.0000],
        [0.0261, 0.0000, 0.0000,  ..., 0.0528, 0.0250, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64172.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0584, 0.0000,  ..., 0.2608, 0.0000, 0.0272],
        [0.0000, 0.0977, 0.0000,  ..., 0.3617, 0.0000, 0.0472],
        [0.0000, 0.0825, 0.0000,  ..., 0.3236, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637708., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(476.9762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.9164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.8352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0352],
        [ 0.1739],
        [ 0.1926],
        ...,
        [-1.3920],
        [-1.3879],
        [-1.3865]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244525.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369353.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369364.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3005e-02, -1.1844e-03, -3.3870e-06,  ...,  2.2598e-02,
          1.1886e-02, -7.3115e-03],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        ...,
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4137.7490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3847, device='cuda:0')



h[100].sum tensor(-1.7413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-31.5018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0000, 0.0000,  ..., 0.0272, 0.0098, 0.0000],
        [0.0132, 0.0000, 0.0000,  ..., 0.0310, 0.0121, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80270.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0543, 0.0000,  ..., 0.2470, 0.0000, 0.0251],
        [0.0000, 0.0311, 0.0000,  ..., 0.1869, 0.0000, 0.0133],
        [0.0000, 0.0138, 0.0000,  ..., 0.1435, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717604., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.1219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-277.9657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-350.9910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1762],
        [ 0.1496],
        [ 0.1306],
        ...,
        [-1.3965],
        [-1.3924],
        [-1.3911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219992.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369364.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369364.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        ...,
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00],
        [-1.3499e-03, -1.0272e-03,  0.0000e+00,  ...,  2.6361e-03,
         -9.4825e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3690.9553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0649, device='cuda:0')



h[100].sum tensor(-1.3422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-24.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0045, 0.0000, 0.0000,  ..., 0.0190, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71376.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0841, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.1244, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669209., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(506.7045, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.6038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.6361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3400],
        [-1.0827],
        [-0.7302],
        ...,
        [-1.3965],
        [-1.3924],
        [-1.3911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214581.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0182],
        [1.0161],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369364.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0182],
        [1.0162],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369375.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3316e-03, -1.0829e-03,  0.0000e+00,  ...,  2.6500e-03,
         -1.0442e-04,  0.0000e+00],
        [ 1.0758e-02, -1.2225e-03, -2.7511e-06,  ...,  1.9460e-02,
          9.9845e-03, -6.1467e-03],
        [ 5.7378e-03, -1.1645e-03, -1.6087e-06,  ...,  1.2480e-02,
          5.7951e-03, -3.5943e-03],
        ...,
        [-1.3316e-03, -1.0829e-03,  0.0000e+00,  ...,  2.6500e-03,
         -1.0442e-04,  0.0000e+00],
        [-1.3316e-03, -1.0829e-03,  0.0000e+00,  ...,  2.6500e-03,
         -1.0442e-04,  0.0000e+00],
        [-1.3316e-03, -1.0829e-03,  0.0000e+00,  ...,  2.6500e-03,
         -1.0442e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3943.7236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2503, device='cuda:0')



h[100].sum tensor(-1.5372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-27.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0234, 0.0000, 0.0000,  ..., 0.0507, 0.0236, 0.0000],
        [0.0132, 0.0000, 0.0000,  ..., 0.0348, 0.0141, 0.0000],
        [0.0398, 0.0000, 0.0000,  ..., 0.0737, 0.0374, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77117.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0465, 0.0000,  ..., 0.2327, 0.0000, 0.0196],
        [0.0000, 0.0537, 0.0000,  ..., 0.2515, 0.0000, 0.0232],
        [0.0000, 0.1043, 0.0000,  ..., 0.3781, 0.0000, 0.0491],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699736.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(529.1616, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.3127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.2393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2453],
        [ 0.2458],
        [ 0.2523],
        ...,
        [-1.3982],
        [-1.3942],
        [-1.3928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202933.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0182],
        [1.0162],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369375.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(476.9869, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0183],
        [1.0163],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369386.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        ...,
        [-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0011,  0.0000,  ...,  0.0027, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3406.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8368, device='cuda:0')



h[100].sum tensor(-1.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-18.9032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64360.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0709, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634783.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(474.1144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.0779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.2432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8129],
        [-0.7978],
        [-0.7157],
        ...,
        [-1.3970],
        [-1.3930],
        [-1.3917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219678.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0183],
        [1.0163],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369386.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0183],
        [1.0164],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369397.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        ...,
        [-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0012,  0.0000,  ...,  0.0027, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4865.9912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8940, device='cuda:0')



h[100].sum tensor(-2.2787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0198, device='cuda:0')



h[200].sum tensor(-41.9872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92305.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0666, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(765542.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(588.0038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.1407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-325.6364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3985],
        [-1.4339],
        [-1.4230],
        ...,
        [-1.3971],
        [-1.3932],
        [-1.3919]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206412.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0183],
        [1.0164],
        ...,
        [1.0006],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369397.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0184],
        [1.0165],
        ...,
        [1.0005],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369408.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3138e-03, -1.2193e-03,  0.0000e+00,  ...,  2.6905e-03,
         -1.1253e-04,  0.0000e+00],
        [-1.3138e-03, -1.2193e-03,  0.0000e+00,  ...,  2.6905e-03,
         -1.1253e-04,  0.0000e+00],
        [ 4.8397e-03, -1.2993e-03, -1.2556e-06,  ...,  1.1247e-02,
          5.0225e-03, -3.1129e-03],
        ...,
        [-1.3138e-03, -1.2193e-03,  0.0000e+00,  ...,  2.6905e-03,
         -1.1253e-04,  0.0000e+00],
        [-1.3138e-03, -1.2193e-03,  0.0000e+00,  ...,  2.6905e-03,
         -1.1253e-04,  0.0000e+00],
        [-1.3138e-03, -1.2193e-03,  0.0000e+00,  ...,  2.6905e-03,
         -1.1253e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3277.0308, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7287, device='cuda:0')



h[100].sum tensor(-0.8769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-16.2578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0197, 0.0051, 0.0000],
        [0.0168, 0.0000, 0.0000,  ..., 0.0380, 0.0160, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61790.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.1006, 0.0000, 0.0012],
        [0.0000, 0.0253, 0.0000,  ..., 0.1694, 0.0000, 0.0102],
        [0.0000, 0.0575, 0.0000,  ..., 0.2611, 0.0000, 0.0252],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624528.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.9104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.3225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.1136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1699],
        [-0.0034],
        [ 0.1282],
        ...,
        [-1.4003],
        [-1.3964],
        [-1.3951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214735.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0184],
        [1.0165],
        ...,
        [1.0005],
        [0.9995],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369408.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0185],
        [1.0166],
        ...,
        [1.0005],
        [0.9994],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369419.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5053e-02, -1.6093e-03, -5.1875e-06,  ...,  3.9370e-02,
          2.1898e-02, -1.3318e-02],
        [ 2.1093e-02, -1.5562e-03, -4.4084e-06,  ...,  3.3862e-02,
          1.8593e-02, -1.1318e-02],
        [ 2.0141e-02, -1.5435e-03, -4.2212e-06,  ...,  3.2539e-02,
          1.7798e-02, -1.0837e-02],
        ...,
        [-1.3154e-03, -1.2561e-03,  0.0000e+00,  ...,  2.7011e-03,
         -1.0937e-04,  0.0000e+00],
        [-1.3154e-03, -1.2561e-03,  0.0000e+00,  ...,  2.7011e-03,
         -1.0937e-04,  0.0000e+00],
        [-1.3154e-03, -1.2561e-03,  0.0000e+00,  ...,  2.7011e-03,
         -1.0937e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3636.6929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9714, device='cuda:0')



h[100].sum tensor(-1.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-21.8704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0652, 0.0000, 0.0000,  ..., 0.1090, 0.0584, 0.0000],
        [0.0928, 0.0000, 0.0000,  ..., 0.1475, 0.0814, 0.0000],
        [0.0631, 0.0000, 0.0000,  ..., 0.1062, 0.0567, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68781.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1575, 0.0000,  ..., 0.5109, 0.0000, 0.0762],
        [0.0000, 0.1888, 0.0000,  ..., 0.5881, 0.0000, 0.0923],
        [0.0000, 0.1572, 0.0000,  ..., 0.5102, 0.0000, 0.0761],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0700, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655660.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.7376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.8565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.8700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2390],
        [ 0.2257],
        [ 0.2109],
        ...,
        [-1.4048],
        [-1.4010],
        [-1.3997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201518.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0185],
        [1.0166],
        ...,
        [1.0005],
        [0.9994],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369419.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0185],
        [1.0166],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369430.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        ...,
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4104.5928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2944, device='cuda:0')



h[100].sum tensor(-1.5617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-29.3136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0317, 0.0122, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79821.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9605e-04, 0.0000e+00,  ..., 9.5069e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3669e-02, 0.0000e+00,  ..., 1.3889e-01, 0.0000e+00,
         5.0707e-03],
        [0.0000e+00, 4.1773e-02, 0.0000e+00,  ..., 2.2150e-01, 0.0000e+00,
         1.7507e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0088e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0081e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0069e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715680.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(535.2285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.0235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3480],
        [-0.0989],
        [ 0.0809],
        ...,
        [-1.4135],
        [-1.4095],
        [-1.4080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163094.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0185],
        [1.0166],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369430.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0186],
        [1.0167],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369441.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8110e-02, -1.5924e-03, -3.5492e-06,  ...,  2.9724e-02,
          1.6101e-02, -9.7748e-03],
        [ 9.7766e-03, -1.4752e-03, -2.0260e-06,  ...,  1.8135e-02,
          9.1449e-03, -5.5799e-03],
        [ 1.4511e-02, -1.5418e-03, -2.8915e-06,  ...,  2.4720e-02,
          1.3097e-02, -7.9633e-03],
        ...,
        [-1.3077e-03, -1.3192e-03,  0.0000e+00,  ...,  2.7192e-03,
         -1.0743e-04,  0.0000e+00],
        [ 8.2999e-03, -1.4544e-03, -1.7561e-06,  ...,  1.6081e-02,
          7.9123e-03, -4.8365e-03],
        [ 8.2999e-03, -1.4544e-03, -1.7561e-06,  ...,  1.6081e-02,
          7.9123e-03, -4.8365e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3672.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9933, device='cuda:0')



h[100].sum tensor(-1.1876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-22.4304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0233, 0.0000, 0.0000,  ..., 0.0489, 0.0224, 0.0000],
        [0.0536, 0.0000, 0.0000,  ..., 0.0931, 0.0488, 0.0000],
        [0.0297, 0.0000, 0.0000,  ..., 0.0580, 0.0278, 0.0000],
        ...,
        [0.0154, 0.0000, 0.0000,  ..., 0.0365, 0.0149, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0365, 0.0149, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0365, 0.0149, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68546.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0788, 0.0000,  ..., 0.3158, 0.0000, 0.0361],
        [0.0000, 0.1005, 0.0000,  ..., 0.3705, 0.0000, 0.0473],
        [0.0000, 0.0786, 0.0000,  ..., 0.3158, 0.0000, 0.0361],
        ...,
        [0.0000, 0.0246, 0.0000,  ..., 0.1797, 0.0000, 0.0093],
        [0.0000, 0.0329, 0.0000,  ..., 0.2029, 0.0000, 0.0126],
        [0.0000, 0.0329, 0.0000,  ..., 0.2029, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654322.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(491.2992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.1103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1964],
        [ 0.1913],
        [ 0.1756],
        ...,
        [-0.6808],
        [-0.5055],
        [-0.5048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217685.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0186],
        [1.0167],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369441.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0187],
        [1.0168],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369451.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        ...,
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000],
        [-0.0013, -0.0013,  0.0000,  ...,  0.0027, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3451.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8411, device='cuda:0')



h[100].sum tensor(-0.9957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-18.9221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1166, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64674.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0699, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0699, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0699, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640553.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(476.9607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.0863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.4730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3493],
        [-1.1578],
        [-0.8847],
        ...,
        [-1.4366],
        [-1.4325],
        [-1.4312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234318.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0187],
        [1.0168],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369451.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0188],
        [1.0168],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369462.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3069e-03, -1.3706e-03,  0.0000e+00,  ...,  2.7294e-03,
         -9.3804e-05,  0.0000e+00],
        [-1.3069e-03, -1.3706e-03,  0.0000e+00,  ...,  2.7294e-03,
         -9.3804e-05,  0.0000e+00],
        [-1.3069e-03, -1.3706e-03,  0.0000e+00,  ...,  2.7294e-03,
         -9.3804e-05,  0.0000e+00],
        ...,
        [ 3.5573e-03, -1.4417e-03, -8.2562e-07,  ...,  9.4950e-03,
          3.9671e-03, -2.4406e-03],
        [ 6.7336e-03, -1.4881e-03, -1.3647e-06,  ...,  1.3913e-02,
          6.6188e-03, -4.0343e-03],
        [-1.3069e-03, -1.3706e-03,  0.0000e+00,  ...,  2.7294e-03,
         -9.3804e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3668.4709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9977, device='cuda:0')



h[100].sum tensor(-1.1689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-22.3520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0401, 0.0000, 0.0000,  ..., 0.0747, 0.0376, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0336, 0.0132, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0230, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71410.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.1112, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0765, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0875, 0.0000,  ..., 0.3429, 0.0000, 0.0402],
        [0.0000, 0.0496, 0.0000,  ..., 0.2459, 0.0000, 0.0212],
        [0.0000, 0.0206, 0.0000,  ..., 0.1595, 0.0000, 0.0085]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682726.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(506.3579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.5551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.0742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4262],
        [-0.8050],
        [-1.1275],
        ...,
        [ 0.0860],
        [-0.1507],
        [-0.5339]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218263.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0188],
        [1.0168],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369462.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0189],
        [1.0169],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369473.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00],
        [-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00],
        [-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00],
        ...,
        [-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00],
        [-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00],
        [-1.3123e-03, -1.3926e-03,  0.0000e+00,  ...,  2.7325e-03,
         -8.2448e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3329.0474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7559, device='cuda:0')



h[100].sum tensor(-0.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0079, device='cuda:0')



h[200].sum tensor(-16.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1048, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62980.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0980, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0796, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0704, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637319.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(471.7878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.5830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.3006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2969],
        [-1.1588],
        [-0.9362],
        ...,
        [-0.9708],
        [-1.1812],
        [-1.3373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226147.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0189],
        [1.0169],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369473.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0189],
        [1.0170],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369485.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3171e-03, -1.4124e-03,  0.0000e+00,  ...,  2.7387e-03,
         -7.5948e-05,  0.0000e+00],
        [-1.3171e-03, -1.4124e-03,  0.0000e+00,  ...,  2.7387e-03,
         -7.5948e-05,  0.0000e+00],
        [-1.3171e-03, -1.4124e-03,  0.0000e+00,  ...,  2.7387e-03,
         -7.5948e-05,  0.0000e+00],
        ...,
        [-1.3171e-03, -1.4124e-03,  0.0000e+00,  ...,  2.7387e-03,
         -7.5948e-05,  0.0000e+00],
        [-1.3171e-03, -1.4124e-03,  0.0000e+00,  ...,  2.7387e-03,
         -7.5948e-05,  0.0000e+00],
        [ 1.2919e-02, -1.6268e-03, -2.2428e-06,  ...,  2.2543e-02,
          1.1812e-02, -7.1198e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4746.7695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7561, device='cuda:0')



h[100].sum tensor(-2.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0183, device='cuda:0')



h[200].sum tensor(-39.2446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0320, 0.0123, 0.0000],
        [0.0242, 0.0000, 0.0000,  ..., 0.0488, 0.0223, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92658.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0868, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.1482, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.1053, 0.0000, 0.0016],
        [0.0000, 0.0326, 0.0000,  ..., 0.1956, 0.0000, 0.0147],
        [0.0000, 0.0735, 0.0000,  ..., 0.3053, 0.0000, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(787495.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(595.2166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.5810, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.8411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0759],
        [-0.7547],
        [-0.3431],
        ...,
        [-1.0238],
        [-0.6470],
        [-0.2743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196078.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0189],
        [1.0170],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369485.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(513.4364, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0190],
        [1.0171],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369496.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00],
        [-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00],
        [-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00],
        ...,
        [-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00],
        [-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00],
        [-1.3180e-03, -1.4302e-03,  0.0000e+00,  ...,  2.7376e-03,
         -7.3190e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3298.9038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7250, device='cuda:0')



h[100].sum tensor(-0.8337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-16.2425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0000,  ..., 0.0212, 0.0059, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0162, 0.0030, 0.0000],
        [0.0296, 0.0000, 0.0000,  ..., 0.0561, 0.0268, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61638.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.1389, 0.0000, 0.0023],
        [0.0000, 0.0229, 0.0000,  ..., 0.1741, 0.0000, 0.0087],
        [0.0000, 0.0745, 0.0000,  ..., 0.3080, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632786., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.1097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.5817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.0627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4636],
        [-0.1444],
        [ 0.0935],
        ...,
        [-1.4677],
        [-1.4635],
        [-1.4621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252595.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0190],
        [1.0171],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369496.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0191],
        [1.0172],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369507.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00],
        [-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00],
        [-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00],
        ...,
        [-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00],
        [-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00],
        [-1.3204e-03, -1.4463e-03,  0.0000e+00,  ...,  2.7319e-03,
         -7.3542e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4568.5835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6200, device='cuda:0')



h[100].sum tensor(-1.8426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-36.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0000, 0.0000,  ..., 0.0384, 0.0162, 0.0000],
        [0.0177, 0.0000, 0.0000,  ..., 0.0395, 0.0169, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0209, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86360.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0377, 0.0000,  ..., 0.2156, 0.0000, 0.0147],
        [0.0000, 0.0382, 0.0000,  ..., 0.2172, 0.0000, 0.0149],
        [0.0000, 0.0192, 0.0000,  ..., 0.1634, 0.0000, 0.0065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(757158.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(565.5347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-287.7253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.6175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2511],
        [-0.3075],
        [-0.5336],
        ...,
        [-1.4749],
        [-1.4707],
        [-1.4693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178385.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0191],
        [1.0172],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369507.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0192],
        [1.0172],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369518.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9152e-03, -1.5579e-03, -8.7673e-07,  ...,  1.1393e-02,
          5.1224e-03, -3.1005e-03],
        [-1.3180e-03, -1.4608e-03,  0.0000e+00,  ...,  2.7256e-03,
         -7.9177e-05,  0.0000e+00],
        [-1.3180e-03, -1.4608e-03,  0.0000e+00,  ...,  2.7256e-03,
         -7.9177e-05,  0.0000e+00],
        ...,
        [-1.3180e-03, -1.4608e-03,  0.0000e+00,  ...,  2.7256e-03,
         -7.9177e-05,  0.0000e+00],
        [-1.3180e-03, -1.4608e-03,  0.0000e+00,  ...,  2.7256e-03,
         -7.9177e-05,  0.0000e+00],
        [-1.3180e-03, -1.4608e-03,  0.0000e+00,  ...,  2.7256e-03,
         -7.9177e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4483.9004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5357, device='cuda:0')



h[100].sum tensor(-1.7519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0160, device='cuda:0')



h[200].sum tensor(-34.5638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0000, 0.0000,  ..., 0.0399, 0.0171, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0199, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89097.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0487, 0.0000,  ..., 0.2425, 0.0000, 0.0198],
        [0.0000, 0.0244, 0.0000,  ..., 0.1809, 0.0000, 0.0080],
        [0.0000, 0.0152, 0.0000,  ..., 0.1567, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(783401., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(576.0103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.8101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.2015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1445],
        [ 0.0668],
        [-0.0066],
        ...,
        [-1.4769],
        [-1.4727],
        [-1.4713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219008.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0192],
        [1.0172],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369518.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0193],
        [1.0173],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369529.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0156e-02, -1.6543e-03, -1.5541e-06,  ...,  1.8677e-02,
          9.4983e-03, -5.6989e-03],
        [-1.3214e-03, -1.4739e-03,  0.0000e+00,  ...,  2.7188e-03,
         -7.7535e-05,  0.0000e+00],
        [-1.3214e-03, -1.4739e-03,  0.0000e+00,  ...,  2.7188e-03,
         -7.7535e-05,  0.0000e+00],
        ...,
        [-1.3214e-03, -1.4739e-03,  0.0000e+00,  ...,  2.7188e-03,
         -7.7535e-05,  0.0000e+00],
        [-1.3214e-03, -1.4739e-03,  0.0000e+00,  ...,  2.7188e-03,
         -7.7535e-05,  0.0000e+00],
        [-1.3214e-03, -1.4739e-03,  0.0000e+00,  ...,  2.7188e-03,
         -7.7535e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3748.8523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0138, device='cuda:0')



h[100].sum tensor(-1.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-22.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0493, 0.0000, 0.0000,  ..., 0.0852, 0.0443, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0385, 0.0163, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72308.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1644, 0.0000,  ..., 0.5271, 0.0000, 0.0779],
        [0.0000, 0.1091, 0.0000,  ..., 0.3910, 0.0000, 0.0503],
        [0.0000, 0.0709, 0.0000,  ..., 0.2954, 0.0000, 0.0312],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690516.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.5532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-256.9395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.6463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2089],
        [ 0.2131],
        [ 0.2157],
        ...,
        [-1.4856],
        [-1.4815],
        [-1.4801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190701.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0193],
        [1.0173],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369529.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0194],
        [1.0174],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369540.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00],
        [-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00],
        [-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00],
        ...,
        [-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00],
        [-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00],
        [-1.3242e-03, -1.4857e-03,  0.0000e+00,  ...,  2.7142e-03,
         -7.9703e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4532.1670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5614, device='cuda:0')



h[100].sum tensor(-1.7494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0163, device='cuda:0')



h[200].sum tensor(-34.9503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90912.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0711, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0704, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0724, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(805393.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(580.3025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-353.1549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8347],
        [-1.0638],
        [-1.2694],
        ...,
        [-1.4944],
        [-1.4902],
        [-1.4888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222825.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0194],
        [1.0174],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369540.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0195],
        [1.0175],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369551.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7199e-03, -1.5769e-03, -6.3325e-07,  ...,  9.7259e-03,
          4.1315e-03, -2.4979e-03],
        [ 1.2687e-02, -1.7200e-03, -1.7580e-06,  ...,  2.2191e-02,
          1.1610e-02, -6.9344e-03],
        [-1.3290e-03, -1.4964e-03,  0.0000e+00,  ...,  2.7075e-03,
         -7.8909e-05,  0.0000e+00],
        ...,
        [-1.3290e-03, -1.4964e-03,  0.0000e+00,  ...,  2.7075e-03,
         -7.8909e-05,  0.0000e+00],
        [-1.3290e-03, -1.4964e-03,  0.0000e+00,  ...,  2.7075e-03,
         -7.8909e-05,  0.0000e+00],
        [-1.3290e-03, -1.4964e-03,  0.0000e+00,  ...,  2.7075e-03,
         -7.8909e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3770.9800, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0350, device='cuda:0')



h[100].sum tensor(-1.1386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-22.8917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0704, 0.0000, 0.0000,  ..., 0.1164, 0.0629, 0.0000],
        [0.0183, 0.0000, 0.0000,  ..., 0.0402, 0.0174, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0309, 0.0118, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70652.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1332, 0.0000,  ..., 0.4497, 0.0000, 0.0611],
        [0.0000, 0.0690, 0.0000,  ..., 0.2914, 0.0000, 0.0295],
        [0.0000, 0.0333, 0.0000,  ..., 0.1963, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680720.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.1158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.2506, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.4996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1895],
        [ 0.0483],
        [-0.2151],
        ...,
        [-1.4963],
        [-1.4967],
        [-1.4960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209381.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0195],
        [1.0175],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369551.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0196],
        [1.0175],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369562.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3322e-03, -1.5060e-03,  0.0000e+00,  ...,  2.7068e-03,
         -8.5357e-05,  0.0000e+00],
        [ 6.4326e-03, -1.6306e-03, -9.3711e-07,  ...,  1.3499e-02,
          6.3883e-03, -3.8346e-03],
        [-1.3322e-03, -1.5060e-03,  0.0000e+00,  ...,  2.7068e-03,
         -8.5357e-05,  0.0000e+00],
        ...,
        [-1.3322e-03, -1.5060e-03,  0.0000e+00,  ...,  2.7068e-03,
         -8.5357e-05,  0.0000e+00],
        [-1.3322e-03, -1.5060e-03,  0.0000e+00,  ...,  2.7068e-03,
         -8.5357e-05,  0.0000e+00],
        [-1.3322e-03, -1.5060e-03,  0.0000e+00,  ...,  2.7068e-03,
         -8.5357e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4176.3457, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3300, device='cuda:0')



h[100].sum tensor(-1.4419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-29.1733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0232, 0.0000, 0.0000,  ..., 0.0507, 0.0235, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0200, 0.0053, 0.0000],
        [0.0239, 0.0000, 0.0000,  ..., 0.0480, 0.0220, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76449.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0254, 0.0000,  ..., 0.1817, 0.0000, 0.0069],
        [0.0000, 0.0308, 0.0000,  ..., 0.1939, 0.0000, 0.0101],
        [0.0000, 0.0756, 0.0000,  ..., 0.3045, 0.0000, 0.0323],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699298.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.0767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.1481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.3496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6403],
        [-0.3671],
        [-0.0911],
        ...,
        [-1.5129],
        [-1.5086],
        [-1.5072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221839.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0196],
        [1.0175],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369562.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0196],
        [1.0176],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369573.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5510e-03, -1.6259e-03, -8.0063e-07,  ...,  1.2288e-02,
          5.6694e-03, -3.3989e-03],
        [-1.3438e-03, -1.5146e-03,  0.0000e+00,  ...,  2.7060e-03,
         -7.8097e-05,  0.0000e+00],
        [-1.3438e-03, -1.5146e-03,  0.0000e+00,  ...,  2.7060e-03,
         -7.8097e-05,  0.0000e+00],
        ...,
        [-1.3438e-03, -1.5146e-03,  0.0000e+00,  ...,  2.7060e-03,
         -7.8097e-05,  0.0000e+00],
        [-1.3438e-03, -1.5146e-03,  0.0000e+00,  ...,  2.7060e-03,
         -7.8097e-05,  0.0000e+00],
        [-1.3438e-03, -1.5146e-03,  0.0000e+00,  ...,  2.7060e-03,
         -7.8097e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3670.7629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9483, device='cuda:0')



h[100].sum tensor(-1.0344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-21.0627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0000, 0.0000,  ..., 0.0408, 0.0178, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0208, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70038.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0933, 0.0000,  ..., 0.3493, 0.0000, 0.0408],
        [0.0000, 0.0474, 0.0000,  ..., 0.2267, 0.0000, 0.0188],
        [0.0000, 0.0218, 0.0000,  ..., 0.1592, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0732, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0732, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681510.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.5807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.1028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.6610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1069],
        [ 0.0833],
        [ 0.0653],
        ...,
        [-1.5123],
        [-1.5079],
        [-1.5064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219063.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0196],
        [1.0176],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369573.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0197],
        [1.0177],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369584.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00],
        [-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00],
        [-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00],
        ...,
        [-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00],
        [-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00],
        [-1.3564e-03, -1.5224e-03,  0.0000e+00,  ...,  2.7061e-03,
         -6.9701e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3965.4438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1414, device='cuda:0')



h[100].sum tensor(-1.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-25.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0279, 0.0100, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0161, 0.0030, 0.0000],
        [0.0053, 0.0000, 0.0000,  ..., 0.0203, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76580.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0248, 0.0000,  ..., 0.1844, 0.0000, 0.0064],
        [0.0000, 0.0183, 0.0000,  ..., 0.1679, 0.0000, 0.0034],
        [0.0000, 0.0222, 0.0000,  ..., 0.1776, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717697.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.7235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.3728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.0717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0234],
        [ 0.0593],
        [ 0.1046],
        ...,
        [-1.5218],
        [-1.5175],
        [-1.5160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207625.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0197],
        [1.0177],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369584.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0198],
        [1.0178],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369594.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3668e-03, -1.5295e-03,  0.0000e+00,  ...,  2.7161e-03,
         -7.1699e-05,  0.0000e+00],
        [-1.3668e-03, -1.5295e-03,  0.0000e+00,  ...,  2.7161e-03,
         -7.1699e-05,  0.0000e+00],
        [ 3.9163e-03, -1.6156e-03, -5.6780e-07,  ...,  1.0059e-02,
          4.3325e-03, -2.5958e-03],
        ...,
        [-1.3668e-03, -1.5295e-03,  0.0000e+00,  ...,  2.7161e-03,
         -7.1699e-05,  0.0000e+00],
        [-1.3668e-03, -1.5295e-03,  0.0000e+00,  ...,  2.7161e-03,
         -7.1699e-05,  0.0000e+00],
        [-1.3668e-03, -1.5295e-03,  0.0000e+00,  ...,  2.7161e-03,
         -7.1699e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4118.2729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2406, device='cuda:0')



h[100].sum tensor(-1.3447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-27.7312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0313, 0.0120, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0380, 0.0159, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77726.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0302, 0.0000,  ..., 0.1954, 0.0000, 0.0097],
        [0.0000, 0.0371, 0.0000,  ..., 0.2159, 0.0000, 0.0126],
        [0.0000, 0.0519, 0.0000,  ..., 0.2546, 0.0000, 0.0195],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717302.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.7633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.8024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.3537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2078],
        [ 0.2134],
        [ 0.2212],
        ...,
        [-1.5315],
        [-1.5272],
        [-1.5257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223918.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0198],
        [1.0178],
        ...,
        [1.0005],
        [0.9994],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369594.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(519.5381, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0199],
        [1.0178],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369605.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5629e-02, -1.9780e-03, -2.7920e-06,  ...,  4.0268e-02,
          2.2451e-02, -1.3248e-02],
        [ 1.6806e-02, -1.8335e-03, -1.8798e-06,  ...,  2.8004e-02,
          1.5095e-02, -8.9201e-03],
        [ 8.2632e-03, -1.6937e-03, -9.9676e-07,  ...,  1.6130e-02,
          7.9730e-03, -4.7297e-03],
        ...,
        [-1.3788e-03, -1.5358e-03,  0.0000e+00,  ...,  2.7274e-03,
         -6.5481e-05,  0.0000e+00],
        [-1.3788e-03, -1.5358e-03,  0.0000e+00,  ...,  2.7274e-03,
         -6.5481e-05,  0.0000e+00],
        [-1.3788e-03, -1.5358e-03,  0.0000e+00,  ...,  2.7274e-03,
         -6.5481e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3924.0024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1023, device='cuda:0')



h[100].sum tensor(-1.1827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-24.5460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0774, 0.0000, 0.0000,  ..., 0.1265, 0.0690, 0.0000],
        [0.0764, 0.0000, 0.0000,  ..., 0.1252, 0.0681, 0.0000],
        [0.0376, 0.0000, 0.0000,  ..., 0.0673, 0.0336, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74385.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2033, 0.0000,  ..., 0.6226, 0.0000, 0.0965],
        [0.0000, 0.1901, 0.0000,  ..., 0.5909, 0.0000, 0.0898],
        [0.0000, 0.1300, 0.0000,  ..., 0.4429, 0.0000, 0.0599],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0738, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0738, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0738, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704037.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(502.1312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.6519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.0214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2070],
        [ 0.1989],
        [ 0.1771],
        ...,
        [-1.5372],
        [-1.5329],
        [-1.5314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222731.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0199],
        [1.0178],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369605.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0179],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369616.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00],
        [-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00],
        [-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00],
        ...,
        [-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00],
        [-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00],
        [-1.3897e-03, -1.5415e-03,  0.0000e+00,  ...,  2.7388e-03,
         -5.2484e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4108.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2367, device='cuda:0')



h[100].sum tensor(-1.3001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-27.1554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76979.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714706., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(511.8779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.9362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.3242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6920],
        [-1.6134],
        [-1.4804],
        ...,
        [-1.5409],
        [-1.5365],
        [-1.5350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218952.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0179],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369616.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0185],
        [1.0200],
        [1.0180],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369627., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00],
        [-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00],
        [-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00],
        ...,
        [-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00],
        [-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00],
        [-1.3992e-03, -1.5467e-03,  0.0000e+00,  ...,  2.7514e-03,
         -3.9993e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3708.9644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9268, device='cuda:0')



h[100].sum tensor(-0.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-20.7223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70478.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.1002, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0746, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0721, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686249.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(484.0086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.6222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7528],
        [-1.0416],
        [-1.2068],
        ...,
        [-1.5427],
        [-1.5381],
        [-1.5365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226335.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0185],
        [1.0200],
        [1.0180],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369627., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0201],
        [1.0181],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369637.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9211e-03, -1.7057e-03, -8.5753e-07,  ...,  1.5735e-02,
          7.7545e-03, -4.5539e-03],
        [ 9.4741e-03, -1.7314e-03, -1.0003e-06,  ...,  1.7894e-02,
          9.0494e-03, -5.3119e-03],
        [-1.4088e-03, -1.5514e-03,  0.0000e+00,  ...,  2.7651e-03,
         -2.5302e-05,  0.0000e+00],
        ...,
        [-1.4088e-03, -1.5514e-03,  0.0000e+00,  ...,  2.7651e-03,
         -2.5302e-05,  0.0000e+00],
        [-1.4088e-03, -1.5514e-03,  0.0000e+00,  ...,  2.7651e-03,
         -2.5302e-05,  0.0000e+00],
        [-1.4088e-03, -1.5514e-03,  0.0000e+00,  ...,  2.7651e-03,
         -2.5302e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4242.5664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2914, device='cuda:0')



h[100].sum tensor(-1.3578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-28.7248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0890, 0.0000, 0.0000,  ..., 0.1429, 0.0789, 0.0000],
        [0.0303, 0.0000, 0.0000,  ..., 0.0574, 0.0276, 0.0000],
        [0.0097, 0.0000, 0.0000,  ..., 0.0267, 0.0092, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79582.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2950, 0.0000,  ..., 0.8480, 0.0000, 0.1436],
        [0.0000, 0.1625, 0.0000,  ..., 0.5234, 0.0000, 0.0774],
        [0.0000, 0.0733, 0.0000,  ..., 0.3011, 0.0000, 0.0330],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727269.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.8899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.6300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.3801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1187],
        [ 0.1226],
        [ 0.1142],
        ...,
        [-1.5404],
        [-1.5361],
        [-1.5349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206800.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0201],
        [1.0181],
        ...,
        [1.0005],
        [0.9993],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369637.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0202],
        [1.0182],
        ...,
        [1.0005],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369648.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00],
        [-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00],
        [-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00],
        ...,
        [-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00],
        [-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00],
        [-1.4113e-03, -1.5556e-03,  0.0000e+00,  ...,  2.7784e-03,
         -1.3287e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3914.1675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0590, device='cuda:0')



h[100].sum tensor(-1.1049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-23.5267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0247, 0.0080, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75274.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0320, 0.0000,  ..., 0.1953, 0.0000, 0.0125],
        [0.0000, 0.0498, 0.0000,  ..., 0.2456, 0.0000, 0.0213],
        [0.0000, 0.0726, 0.0000,  ..., 0.3066, 0.0000, 0.0320],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0743, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711926.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(505.3075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-256.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.7477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1159],
        [ 0.1512],
        [ 0.1733],
        ...,
        [-1.5537],
        [-1.5491],
        [-1.5475]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200283.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0202],
        [1.0182],
        ...,
        [1.0005],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369648.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0203],
        [1.0182],
        ...,
        [1.0005],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369659.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4117e-03, -1.5593e-03,  0.0000e+00,  ...,  2.7858e-03,
         -2.4789e-06,  0.0000e+00],
        [-1.4117e-03, -1.5593e-03,  0.0000e+00,  ...,  2.7858e-03,
         -2.4789e-06,  0.0000e+00],
        [ 1.7228e-02, -1.8692e-03, -1.5828e-06,  ...,  2.8699e-02,
          1.5541e-02, -9.0670e-03],
        ...,
        [-1.4117e-03, -1.5593e-03,  0.0000e+00,  ...,  2.7858e-03,
         -2.4789e-06,  0.0000e+00],
        [-1.4117e-03, -1.5593e-03,  0.0000e+00,  ...,  2.7858e-03,
         -2.4789e-06,  0.0000e+00],
        [-1.4117e-03, -1.5593e-03,  0.0000e+00,  ...,  2.7858e-03,
         -2.4789e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3721.3291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9306, device='cuda:0')



h[100].sum tensor(-0.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-20.4673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0176, 0.0000, 0.0000,  ..., 0.0378, 0.0159, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0330, 0.0130, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68296.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.1064, 0.0000, 0.0011],
        [0.0000, 0.0307, 0.0000,  ..., 0.1922, 0.0000, 0.0127],
        [0.0000, 0.0486, 0.0000,  ..., 0.2417, 0.0000, 0.0210],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671406.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(478.6242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.8316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.5123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2784],
        [-0.9631],
        [-0.6516],
        ...,
        [-1.5620],
        [-1.5577],
        [-1.5563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221024.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0203],
        [1.0182],
        ...,
        [1.0005],
        [0.9993],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369659.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0203],
        [1.0183],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369670.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4081e-02, -1.8208e-03, -1.2641e-06,  ...,  2.4328e-02,
          1.2928e-02, -7.5225e-03],
        [ 1.0752e-02, -1.7654e-03, -9.9244e-07,  ...,  1.9700e-02,
          1.0152e-02, -5.9060e-03],
        [ 3.2220e-02, -2.1231e-03, -2.7442e-06,  ...,  4.9544e-02,
          2.8054e-02, -1.6331e-02],
        ...,
        [-1.4106e-03, -1.5627e-03,  0.0000e+00,  ...,  2.7917e-03,
          1.0483e-05,  0.0000e+00],
        [-1.4106e-03, -1.5627e-03,  0.0000e+00,  ...,  2.7917e-03,
          1.0483e-05,  0.0000e+00],
        [-1.4106e-03, -1.5627e-03,  0.0000e+00,  ...,  2.7917e-03,
          1.0483e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4095.7749, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1666, device='cuda:0')



h[100].sum tensor(-1.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-25.9814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.2347e-02, 0.0000e+00, 0.0000e+00,  ..., 6.0269e-02, 2.9400e-02,
         0.0000e+00],
        [8.9816e-02, 0.0000e+00, 0.0000e+00,  ..., 1.4426e-01, 7.9737e-02,
         0.0000e+00],
        [8.3961e-02, 0.0000e+00, 0.0000e+00,  ..., 1.3614e-01, 7.4860e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1635e-02, 4.3693e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1632e-02, 4.3680e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1631e-02, 4.3674e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75996.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1138, 0.0000,  ..., 0.4029, 0.0000, 0.0538],
        [0.0000, 0.2098, 0.0000,  ..., 0.6418, 0.0000, 0.1016],
        [0.0000, 0.2459, 0.0000,  ..., 0.7329, 0.0000, 0.1195],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710612., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.7006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.0810, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.1988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0283],
        [ 0.1764],
        [ 0.2199],
        ...,
        [-1.5684],
        [-1.5639],
        [-1.5623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231774.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0203],
        [1.0183],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369670.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0204],
        [1.0184],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369680.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7540e-03, -1.7188e-03, -7.1865e-07,  ...,  1.5542e-02,
          7.6679e-03, -4.4435e-03],
        [-1.4122e-03, -1.5658e-03,  0.0000e+00,  ...,  2.7989e-03,
          2.4356e-05,  0.0000e+00],
        [ 3.3882e-03, -1.6460e-03, -3.7636e-07,  ...,  9.4724e-03,
          4.0274e-03, -2.3271e-03],
        ...,
        [ 1.4525e-02, -1.8319e-03, -1.2495e-06,  ...,  2.4955e-02,
          1.3315e-02, -7.7261e-03],
        [-1.4122e-03, -1.5658e-03,  0.0000e+00,  ...,  2.7989e-03,
          2.4356e-05,  0.0000e+00],
        [-1.4122e-03, -1.5658e-03,  0.0000e+00,  ...,  2.7989e-03,
          2.4356e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4603.8418, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4992, device='cuda:0')



h[100].sum tensor(-1.5426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-33.4866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.0000, 0.0000,  ..., 0.0584, 0.0283, 0.0000],
        [0.0201, 0.0000, 0.0000,  ..., 0.0474, 0.0217, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0305, 0.0115, 0.0000],
        ...,
        [0.0121, 0.0000, 0.0000,  ..., 0.0305, 0.0114, 0.0000],
        [0.0151, 0.0000, 0.0000,  ..., 0.0348, 0.0140, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84477.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1125, 0.0000,  ..., 0.4076, 0.0000, 0.0524],
        [0.0000, 0.0795, 0.0000,  ..., 0.3287, 0.0000, 0.0354],
        [0.0000, 0.0508, 0.0000,  ..., 0.2577, 0.0000, 0.0209],
        ...,
        [0.0000, 0.0318, 0.0000,  ..., 0.2045, 0.0000, 0.0126],
        [0.0000, 0.0227, 0.0000,  ..., 0.1755, 0.0000, 0.0091],
        [0.0000, 0.0027, 0.0000,  ..., 0.1058, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(749752.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(548.0204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.7533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.9447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2790],
        [ 0.2851],
        [ 0.2850],
        ...,
        [-0.6406],
        [-0.8833],
        [-1.1790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219587.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0204],
        [1.0184],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369680.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0205],
        [1.0185],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369691.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00],
        [-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00],
        [-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00],
        ...,
        [-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00],
        [-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00],
        [-1.4157e-03, -1.5686e-03,  0.0000e+00,  ...,  2.8077e-03,
          2.9619e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4147.6689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2000, device='cuda:0')



h[100].sum tensor(-1.2083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-26.3997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0114, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79183.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737285.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.4384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.8921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.1865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7726],
        [-1.7745],
        [-1.7607],
        ...,
        [-1.5788],
        [-1.5743],
        [-1.5728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219524.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0205],
        [1.0185],
        ...,
        [1.0004],
        [0.9992],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369691.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0186],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369701.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        ...,
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4609.8008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5048, device='cuda:0')



h[100].sum tensor(-1.5180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(-33.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1439e-02, 8.7661e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1507e-02, 8.8183e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1521e-02, 8.8286e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1754e-02, 9.0070e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1750e-02, 9.0043e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1749e-02, 9.0032e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86328.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0711, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0721, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0740, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(765399.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.7910, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.6457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.7404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6485],
        [-1.5347],
        [-1.3654],
        ...,
        [-1.5884],
        [-1.5838],
        [-1.5822]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214536.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0186],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369701.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0186],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369701.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        ...,
        [ 6.8481e-03, -1.7097e-03, -5.9846e-07,  ...,  1.4319e-02,
          6.9210e-03, -3.9964e-03],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00],
        [-1.4218e-03, -1.5711e-03,  0.0000e+00,  ...,  2.8191e-03,
          2.1603e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4379.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3569, device='cuda:0')



h[100].sum tensor(-1.3589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-29.8828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1439e-02, 8.7661e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1507e-02, 8.8183e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1521e-02, 8.8286e-05,
         0.0000e+00],
        ...,
        [2.3221e-02, 0.0000e+00, 0.0000e+00,  ..., 4.8168e-02, 2.1936e-02,
         0.0000e+00],
        [7.1380e-03, 0.0000e+00, 0.0000e+00,  ..., 2.3737e-02, 7.2815e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1749e-02, 9.0032e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83193.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.1249, 0.0000, 0.0022],
        [0.0000, 0.0229, 0.0000,  ..., 0.1741, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0891, 0.0000,  ..., 0.3507, 0.0000, 0.0417],
        [0.0000, 0.0394, 0.0000,  ..., 0.2153, 0.0000, 0.0171],
        [0.0000, 0.0071, 0.0000,  ..., 0.1174, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762565.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(547.1786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.6240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.5758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4669],
        [-0.1743],
        [ 0.0390],
        ...,
        [ 0.0330],
        [-0.3529],
        [-0.8445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207532.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0186],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369701.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0187],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369711.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00],
        [-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00],
        [-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00],
        ...,
        [-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00],
        [-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00],
        [-1.4299e-03, -1.5733e-03,  0.0000e+00,  ...,  2.8286e-03,
          2.8262e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3750.4175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9287, device='cuda:0')



h[100].sum tensor(-0.9128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-20.2047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0000,  ..., 0.0206, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68240.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.1404, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0919, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675773.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-244.4471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-415.6311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4321],
        [-0.7432],
        [-0.9083],
        ...,
        [-1.5954],
        [-1.5907],
        [-1.5891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238656.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0187],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369711.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0209],
        [1.0189],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369722.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2478e-03, -1.7213e-03, -5.7978e-07,  ...,  1.4921e-02,
          7.2945e-03, -4.1838e-03],
        [-1.4375e-03, -1.5754e-03,  0.0000e+00,  ...,  2.8392e-03,
          4.5205e-05,  0.0000e+00],
        [ 2.3303e-02, -1.9911e-03, -1.6515e-06,  ...,  3.7253e-02,
          2.0695e-02, -1.1918e-02],
        ...,
        [-1.4375e-03, -1.5754e-03,  0.0000e+00,  ...,  2.8392e-03,
          4.5205e-05,  0.0000e+00],
        [-1.4375e-03, -1.5754e-03,  0.0000e+00,  ...,  2.8392e-03,
          4.5205e-05,  0.0000e+00],
        [-1.4375e-03, -1.5754e-03,  0.0000e+00,  ...,  2.8392e-03,
          4.5205e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4165.2446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1993, device='cuda:0')



h[100].sum tensor(-1.1804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-26.2978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0000, 0.0000,  ..., 0.0215, 0.0062, 0.0000],
        [0.0427, 0.0000, 0.0000,  ..., 0.0792, 0.0407, 0.0000],
        [0.0213, 0.0000, 0.0000,  ..., 0.0453, 0.0204, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78385.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0702, 0.0000,  ..., 0.3035, 0.0000, 0.0335],
        [0.0000, 0.1027, 0.0000,  ..., 0.3880, 0.0000, 0.0498],
        [0.0000, 0.1002, 0.0000,  ..., 0.3814, 0.0000, 0.0487],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(729621.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(527.7908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.4630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.6995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3019],
        [ 0.3085],
        [ 0.3148],
        ...,
        [-1.6009],
        [-1.5962],
        [-1.5946]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201640.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0209],
        [1.0189],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369722.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0210],
        [1.0190],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369732.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0255e-02, -1.7738e-03, -7.4896e-07,  ...,  1.9110e-02,
          9.8083e-03, -5.6197e-03],
        [ 5.9380e-03, -1.7012e-03, -4.7230e-07,  ...,  1.3105e-02,
          6.2054e-03, -3.5438e-03],
        [ 4.3215e-03, -1.6740e-03, -3.6871e-07,  ...,  1.0856e-02,
          4.8564e-03, -2.7665e-03],
        ...,
        [-1.4321e-03, -1.5772e-03,  0.0000e+00,  ...,  2.8537e-03,
          5.4926e-05,  0.0000e+00],
        [-1.4321e-03, -1.5772e-03,  0.0000e+00,  ...,  2.8537e-03,
          5.4926e-05,  0.0000e+00],
        [-1.4321e-03, -1.5772e-03,  0.0000e+00,  ...,  2.8537e-03,
          5.4926e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3725.2065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8773, device='cuda:0')



h[100].sum tensor(-0.8597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-19.2788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0000, 0.0000,  ..., 0.0628, 0.0310, 0.0000],
        [0.0277, 0.0000, 0.0000,  ..., 0.0583, 0.0282, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0288, 0.0105, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68145.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0637, 0.0000,  ..., 0.2927, 0.0000, 0.0297],
        [0.0000, 0.0493, 0.0000,  ..., 0.2560, 0.0000, 0.0225],
        [0.0000, 0.0214, 0.0000,  ..., 0.1786, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0748, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0747, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0747, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679474.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(485.6929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.4777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.0005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1891],
        [-0.0285],
        [-0.3926],
        ...,
        [-1.6016],
        [-1.5973],
        [-1.5962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232534.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0210],
        [1.0190],
        ...,
        [1.0003],
        [0.9991],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369732.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0211],
        [1.0190],
        ...,
        [1.0003],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369743.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00],
        [-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00],
        [-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00],
        ...,
        [-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00],
        [-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00],
        [-1.4279e-03, -1.5789e-03,  0.0000e+00,  ...,  2.8703e-03,
          7.2077e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4159.7363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1524, device='cuda:0')



h[100].sum tensor(-1.1256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-25.4066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.0003, 0.0000],
        [0.0227, 0.0000, 0.0000,  ..., 0.0474, 0.0217, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77326.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0907, 0.0000, 0.0005],
        [0.0000, 0.0157, 0.0000,  ..., 0.1424, 0.0000, 0.0070],
        [0.0000, 0.0689, 0.0000,  ..., 0.3006, 0.0000, 0.0329],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(722102.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.5510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.9802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.2714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7290],
        [-0.3124],
        [ 0.0146],
        ...,
        [-1.6043],
        [-1.5997],
        [-1.5981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210873.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0211],
        [1.0190],
        ...,
        [1.0003],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369743.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0197],
        [1.0212],
        [1.0191],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369753.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7168e-03, -1.6838e-03, -3.6219e-07,  ...,  1.1420e-02,
          5.2045e-03, -2.9395e-03],
        [-1.4190e-03, -1.5803e-03,  0.0000e+00,  ...,  2.8873e-03,
          8.6105e-05,  0.0000e+00],
        [-1.4190e-03, -1.5803e-03,  0.0000e+00,  ...,  2.8873e-03,
          8.6105e-05,  0.0000e+00],
        ...,
        [-1.4190e-03, -1.5803e-03,  0.0000e+00,  ...,  2.8873e-03,
          8.6105e-05,  0.0000e+00],
        [-1.4190e-03, -1.5803e-03,  0.0000e+00,  ...,  2.8873e-03,
          8.6105e-05,  0.0000e+00],
        [-1.4190e-03, -1.5803e-03,  0.0000e+00,  ...,  2.8873e-03,
          8.6105e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3803.1401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8991, device='cuda:0')



h[100].sum tensor(-0.8649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-19.6513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0000,  ..., 0.0400, 0.0173, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0339, 0.0136, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71333.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0392, 0.0000,  ..., 0.2309, 0.0000, 0.0168],
        [0.0000, 0.0269, 0.0000,  ..., 0.1967, 0.0000, 0.0108],
        [0.0000, 0.0062, 0.0000,  ..., 0.1247, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701756.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(497.5501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.8082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.2642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0849],
        [-0.0531],
        [-0.2851],
        ...,
        [-1.6071],
        [-1.6026],
        [-1.6009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232409.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0197],
        [1.0212],
        [1.0191],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369753.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0213],
        [1.0192],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369763.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00],
        [-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00],
        [-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00],
        ...,
        [-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00],
        [-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00],
        [-1.4103e-03, -1.5817e-03,  0.0000e+00,  ...,  2.9003e-03,
          9.5433e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4247.3721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1887, device='cuda:0')



h[100].sum tensor(-1.1358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-25.9752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0223, 0.0067, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0288, 0.0105, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0351, 0.0143, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78345.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0192, 0.0000,  ..., 0.1795, 0.0000, 0.0067],
        [0.0000, 0.0233, 0.0000,  ..., 0.1897, 0.0000, 0.0089],
        [0.0000, 0.0283, 0.0000,  ..., 0.2016, 0.0000, 0.0114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0761, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(729681.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.5566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.4644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.4334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1573],
        [ 0.1022],
        [ 0.0120],
        ...,
        [-1.6107],
        [-1.6063],
        [-1.6047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217139.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0213],
        [1.0192],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369763.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0213],
        [1.0193],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369774.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7641e-03, -1.7039e-03, -3.8946e-07,  ...,  1.2875e-02,
          6.0843e-03, -3.4211e-03],
        [-1.4037e-03, -1.5829e-03,  0.0000e+00,  ...,  2.9106e-03,
          1.0808e-04,  0.0000e+00],
        [ 2.0684e-02, -1.9556e-03, -1.2001e-06,  ...,  3.3615e-02,
          1.8524e-02, -1.0542e-02],
        ...,
        [-1.4037e-03, -1.5829e-03,  0.0000e+00,  ...,  2.9106e-03,
          1.0808e-04,  0.0000e+00],
        [-1.4037e-03, -1.5829e-03,  0.0000e+00,  ...,  2.9106e-03,
          1.0808e-04,  0.0000e+00],
        [-1.4037e-03, -1.5829e-03,  0.0000e+00,  ...,  2.9106e-03,
          1.0808e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3959.8972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9721, device='cuda:0')



h[100].sum tensor(-0.9276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-21.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0201, 0.0054, 0.0000],
        [0.0270, 0.0000, 0.0000,  ..., 0.0534, 0.0253, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0376, 0.0158, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71447.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0288, 0.0000,  ..., 0.2024, 0.0000, 0.0116],
        [0.0000, 0.0549, 0.0000,  ..., 0.2661, 0.0000, 0.0250],
        [0.0000, 0.0541, 0.0000,  ..., 0.2624, 0.0000, 0.0248],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0765, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0765, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0764, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691985., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.9994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.6183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.7281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1699],
        [ 0.1639],
        [ 0.0771],
        ...,
        [-1.6144],
        [-1.6099],
        [-1.6084]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215021.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0213],
        [1.0193],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369774.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0214],
        [1.0193],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369784.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4735e-03, -1.7000e-03, -3.5811e-07,  ...,  1.2473e-02,
          5.8497e-03, -3.2733e-03],
        [-1.3975e-03, -1.5840e-03,  0.0000e+00,  ...,  2.9222e-03,
          1.2222e-04,  0.0000e+00],
        [-1.3975e-03, -1.5840e-03,  0.0000e+00,  ...,  2.9222e-03,
          1.2222e-04,  0.0000e+00],
        ...,
        [-1.3975e-03, -1.5840e-03,  0.0000e+00,  ...,  2.9222e-03,
          1.2222e-04,  0.0000e+00],
        [-1.3975e-03, -1.5840e-03,  0.0000e+00,  ...,  2.9222e-03,
          1.2222e-04,  0.0000e+00],
        [-1.3975e-03, -1.5840e-03,  0.0000e+00,  ...,  2.9222e-03,
          1.2222e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3975.7998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9754, device='cuda:0')



h[100].sum tensor(-0.9214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-21.3519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.0324, 0.0128, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0217, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72843.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0524, 0.0000,  ..., 0.2590, 0.0000, 0.0231],
        [0.0000, 0.0172, 0.0000,  ..., 0.1588, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.1009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703306.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(499.8698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.5881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.5850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0307],
        [-0.2367],
        [-0.5693],
        ...,
        [-1.6202],
        [-1.6158],
        [-1.6141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205255.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0214],
        [1.0193],
        ...,
        [1.0002],
        [0.9990],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369784.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0215],
        [1.0194],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369794.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9440e-03, -1.6921e-03, -3.1687e-07,  ...,  1.1744e-02,
          5.4115e-03, -3.0144e-03],
        [-1.3950e-03, -1.5850e-03,  0.0000e+00,  ...,  2.9335e-03,
          1.2820e-04,  0.0000e+00],
        [-1.3950e-03, -1.5850e-03,  0.0000e+00,  ...,  2.9335e-03,
          1.2820e-04,  0.0000e+00],
        ...,
        [-1.3950e-03, -1.5850e-03,  0.0000e+00,  ...,  2.9335e-03,
          1.2820e-04,  0.0000e+00],
        [-1.3950e-03, -1.5850e-03,  0.0000e+00,  ...,  2.9335e-03,
          1.2820e-04,  0.0000e+00],
        [-1.3950e-03, -1.5850e-03,  0.0000e+00,  ...,  2.9335e-03,
          1.2820e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3849.9331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8849, device='cuda:0')



h[100].sum tensor(-0.8313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-19.3913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0000, 0.0000,  ..., 0.0336, 0.0135, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0210, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70825.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0357, 0.0000,  ..., 0.2188, 0.0000, 0.0146],
        [0.0000, 0.0144, 0.0000,  ..., 0.1618, 0.0000, 0.0056],
        [0.0000, 0.0019, 0.0000,  ..., 0.1274, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0769, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0769, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698526.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(492.0164, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.5372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.2756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1023],
        [ 0.0089],
        [-0.0520],
        ...,
        [-1.6303],
        [-1.6261],
        [-1.6247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203169.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0215],
        [1.0194],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369794.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(976.9863, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0216],
        [1.0195],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369804.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4161e-02, -2.0177e-03, -1.2248e-06,  ...,  3.8455e-02,
          2.1416e-02, -1.2129e-02],
        [-1.3903e-03, -1.5859e-03,  0.0000e+00,  ...,  2.9456e-03,
          1.2309e-04,  0.0000e+00],
        [ 1.4473e-02, -1.8540e-03, -7.6039e-07,  ...,  2.4991e-02,
          1.3342e-02, -7.5298e-03],
        ...,
        [-1.3903e-03, -1.5859e-03,  0.0000e+00,  ...,  2.9456e-03,
          1.2309e-04,  0.0000e+00],
        [-1.3903e-03, -1.5859e-03,  0.0000e+00,  ...,  2.9456e-03,
          1.2309e-04,  0.0000e+00],
        [-1.3903e-03, -1.5859e-03,  0.0000e+00,  ...,  2.9456e-03,
          1.2309e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4458.0879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3113, device='cuda:0')



h[100].sum tensor(-1.2145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-28.5165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0667, 0.0000, 0.0000,  ..., 0.1105, 0.0596, 0.0000],
        [0.0788, 0.0000, 0.0000,  ..., 0.1295, 0.0709, 0.0000],
        [0.0168, 0.0000, 0.0000,  ..., 0.0393, 0.0169, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81122.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2549, 0.0000,  ..., 0.7563, 0.0000, 0.1246],
        [0.0000, 0.1990, 0.0000,  ..., 0.6215, 0.0000, 0.0965],
        [0.0000, 0.1026, 0.0000,  ..., 0.3861, 0.0000, 0.0480],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744095.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(536.4931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.5971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.5610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1398],
        [ 0.1723],
        [ 0.2021],
        ...,
        [-1.6478],
        [-1.6432],
        [-1.6415]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228062.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0216],
        [1.0195],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369804.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0217],
        [1.0195],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369815.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3903e-03, -1.5867e-03,  0.0000e+00,  ...,  2.9536e-03,
          1.3508e-04,  0.0000e+00],
        [ 1.3260e-02, -1.8344e-03, -6.7331e-07,  ...,  2.3312e-02,
          1.2342e-02, -6.9414e-03],
        [ 1.3237e-02, -1.8340e-03, -6.7226e-07,  ...,  2.3280e-02,
          1.2323e-02, -6.9306e-03],
        ...,
        [-1.3903e-03, -1.5867e-03,  0.0000e+00,  ...,  2.9536e-03,
          1.3508e-04,  0.0000e+00],
        [-1.3903e-03, -1.5867e-03,  0.0000e+00,  ...,  2.9536e-03,
          1.3508e-04,  0.0000e+00],
        [-1.3903e-03, -1.5867e-03,  0.0000e+00,  ...,  2.9536e-03,
          1.3508e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4121.2705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.7485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0803, device='cuda:0')



h[100].sum tensor(-0.9898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-23.3964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1497, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0326, 0.0129, 0.0000],
        [0.0243, 0.0000, 0.0000,  ..., 0.0498, 0.0232, 0.0000],
        [0.0781, 0.0000, 0.0000,  ..., 0.1285, 0.0704, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73629.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0314, 0.0000,  ..., 0.2006, 0.0000, 0.0143],
        [0.0000, 0.0741, 0.0000,  ..., 0.3116, 0.0000, 0.0345],
        [0.0000, 0.1510, 0.0000,  ..., 0.5018, 0.0000, 0.0727],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0771, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711435.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(504.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.3939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.7899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3915],
        [-0.1512],
        [ 0.0671],
        ...,
        [-1.6562],
        [-1.6516],
        [-1.6499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214066.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0217],
        [1.0195],
        ...,
        [1.0002],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369815.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0217],
        [1.0195],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369825.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0291e-03, -1.7468e-03, -4.1511e-07,  ...,  1.6057e-02,
          7.9908e-03, -4.4562e-03],
        [ 2.9781e-02, -2.1147e-03, -1.3735e-06,  ...,  4.6282e-02,
          2.6114e-02, -1.4744e-02],
        [ 1.3154e-02, -1.8335e-03, -6.4093e-07,  ...,  2.3179e-02,
          1.2261e-02, -6.8803e-03],
        ...,
        [-1.3922e-03, -1.5874e-03,  0.0000e+00,  ...,  2.9649e-03,
          1.4081e-04,  0.0000e+00],
        [-1.3922e-03, -1.5874e-03,  0.0000e+00,  ...,  2.9649e-03,
          1.4081e-04,  0.0000e+00],
        [-1.3922e-03, -1.5874e-03,  0.0000e+00,  ...,  2.9649e-03,
          1.4081e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3783.9690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8458, device='cuda:0')



h[100].sum tensor(-0.7696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-18.3108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0886, 0.0000, 0.0000,  ..., 0.1430, 0.0791, 0.0000],
        [0.0649, 0.0000, 0.0000,  ..., 0.1101, 0.0594, 0.0000],
        [0.0662, 0.0000, 0.0000,  ..., 0.1121, 0.0605, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69113.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1678, 0.0000,  ..., 0.5449, 0.0000, 0.0809],
        [0.0000, 0.1755, 0.0000,  ..., 0.5663, 0.0000, 0.0847],
        [0.0000, 0.1736, 0.0000,  ..., 0.5619, 0.0000, 0.0836],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695570.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.0273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.4171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-425.8003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1913],
        [ 0.1980],
        [ 0.1904],
        ...,
        [-1.6659],
        [-1.6612],
        [-1.6594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237968.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0217],
        [1.0195],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369825.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0218],
        [1.0196],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369836.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4677.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4483, device='cuda:0')



h[100].sum tensor(-1.3095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-31.3650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0214, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0006, 0.0000],
        [0.0233, 0.0000, 0.0000,  ..., 0.0485, 0.0224, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85770.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0293, 0.0000,  ..., 0.2051, 0.0000, 0.0119],
        [0.0000, 0.0433, 0.0000,  ..., 0.2384, 0.0000, 0.0191],
        [0.0000, 0.1228, 0.0000,  ..., 0.4345, 0.0000, 0.0587],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0777, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0776, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0776, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(780532.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(554.3437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.0471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.1594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2082],
        [ 0.1808],
        [ 0.1525],
        ...,
        [-1.6709],
        [-1.6662],
        [-1.6644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228735.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0218],
        [1.0196],
        ...,
        [1.0001],
        [0.9989],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369836.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0219],
        [1.0196],
        ...,
        [1.0001],
        [0.9989],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369846.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6877e-03, -1.7764e-03, -4.4895e-07,  ...,  1.8391e-02,
          9.3972e-03, -5.2279e-03],
        [ 3.9837e-03, -1.6798e-03, -2.1805e-07,  ...,  1.0465e-02,
          4.6443e-03, -2.5392e-03],
        [ 1.6143e-02, -1.8857e-03, -7.1025e-07,  ...,  2.7362e-02,
          1.4776e-02, -8.2707e-03],
        ...,
        [-1.4031e-03, -1.5886e-03,  0.0000e+00,  ...,  2.9788e-03,
          1.5584e-04,  0.0000e+00],
        [-1.4031e-03, -1.5886e-03,  0.0000e+00,  ...,  2.9788e-03,
          1.5584e-04,  0.0000e+00],
        [-1.4031e-03, -1.5886e-03,  0.0000e+00,  ...,  2.9788e-03,
          1.5584e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4253.2168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1540, device='cuda:0')



h[100].sum tensor(-1.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-24.8885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0000, 0.0000,  ..., 0.0632, 0.0312, 0.0000],
        [0.0419, 0.0000, 0.0000,  ..., 0.0784, 0.0403, 0.0000],
        [0.0288, 0.0000, 0.0000,  ..., 0.0602, 0.0294, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76027.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1005, 0.0000,  ..., 0.3891, 0.0000, 0.0467],
        [0.0000, 0.1081, 0.0000,  ..., 0.4080, 0.0000, 0.0506],
        [0.0000, 0.1021, 0.0000,  ..., 0.3936, 0.0000, 0.0474],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0780, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0780, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720210.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.8344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.0805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.5708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2816],
        [ 0.2647],
        [ 0.2388],
        ...,
        [-1.6755],
        [-1.6710],
        [-1.6693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232594.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0219],
        [1.0196],
        ...,
        [1.0001],
        [0.9989],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369846.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0219],
        [1.0196],
        ...,
        [1.0001],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369856.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4227e-02, -1.8540e-03, -6.0662e-07,  ...,  2.4718e-02,
          1.3191e-02, -7.3591e-03],
        [ 2.0859e-02, -1.9663e-03, -8.6386e-07,  ...,  3.3933e-02,
          1.8717e-02, -1.0480e-02],
        [ 1.8128e-02, -1.9201e-03, -7.5794e-07,  ...,  3.0139e-02,
          1.6441e-02, -9.1948e-03],
        ...,
        [-1.4110e-03, -1.5891e-03,  0.0000e+00,  ...,  2.9843e-03,
          1.5912e-04,  0.0000e+00],
        [-1.4110e-03, -1.5891e-03,  0.0000e+00,  ...,  2.9843e-03,
          1.5912e-04,  0.0000e+00],
        [-1.4110e-03, -1.5891e-03,  0.0000e+00,  ...,  2.9843e-03,
          1.5912e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4400.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2485, device='cuda:0')



h[100].sum tensor(-1.1084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-26.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0571, 0.0000, 0.0000,  ..., 0.0995, 0.0530, 0.0000],
        [0.0703, 0.0000, 0.0000,  ..., 0.1180, 0.0641, 0.0000],
        [0.0738, 0.0000, 0.0000,  ..., 0.1228, 0.0669, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77281.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1388, 0.0000,  ..., 0.4810, 0.0000, 0.0668],
        [0.0000, 0.1720, 0.0000,  ..., 0.5641, 0.0000, 0.0835],
        [0.0000, 0.1821, 0.0000,  ..., 0.5887, 0.0000, 0.0886],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0854, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0889, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0854, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724512.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.6698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.7039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.7640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2354],
        [ 0.2539],
        [ 0.2562],
        ...,
        [-1.4705],
        [-1.4236],
        [-1.4233]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236547.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0219],
        [1.0196],
        ...,
        [1.0001],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369856.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0220],
        [1.0197],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369866.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0618e-02, -2.1325e-03, -1.1907e-06,  ...,  4.7515e-02,
          2.6866e-02, -1.5051e-02],
        [ 1.0983e-02, -1.7998e-03, -4.6098e-07,  ...,  2.0225e-02,
          1.0502e-02, -5.8269e-03],
        [ 1.5724e-02, -1.8801e-03, -6.3719e-07,  ...,  2.6815e-02,
          1.4454e-02, -8.0543e-03],
        ...,
        [-1.4202e-03, -1.5896e-03,  0.0000e+00,  ...,  2.9871e-03,
          1.6612e-04,  0.0000e+00],
        [-1.4202e-03, -1.5896e-03,  0.0000e+00,  ...,  2.9871e-03,
          1.6612e-04,  0.0000e+00],
        [-1.4202e-03, -1.5896e-03,  0.0000e+00,  ...,  2.9871e-03,
          1.6612e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4147.6938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0610, device='cuda:0')



h[100].sum tensor(-0.9427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-23.0360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1471, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0567, 0.0000, 0.0000,  ..., 0.0989, 0.0527, 0.0000],
        [0.0713, 0.0000, 0.0000,  ..., 0.1193, 0.0649, 0.0000],
        [0.0642, 0.0000, 0.0000,  ..., 0.1095, 0.0590, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74324.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1727, 0.0000,  ..., 0.5614, 0.0000, 0.0845],
        [0.0000, 0.1700, 0.0000,  ..., 0.5565, 0.0000, 0.0830],
        [0.0000, 0.1515, 0.0000,  ..., 0.5139, 0.0000, 0.0736],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717185.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(504.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.3482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.1595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2348],
        [ 0.2529],
        [ 0.2690],
        ...,
        [-1.6856],
        [-1.6809],
        [-1.6793]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230744.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0220],
        [1.0197],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369866.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0220],
        [1.0197],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369876.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6083e-02, -1.8868e-03, -6.2342e-07,  ...,  2.7327e-02,
          1.4753e-02, -8.2122e-03],
        [ 1.3013e-02, -1.8348e-03, -5.1415e-07,  ...,  2.3061e-02,
          1.2195e-02, -6.7728e-03],
        [ 1.5859e-02, -1.8830e-03, -6.1547e-07,  ...,  2.7016e-02,
          1.4567e-02, -8.1075e-03],
        ...,
        [-1.4274e-03, -1.5901e-03,  0.0000e+00,  ...,  2.9898e-03,
          1.5968e-04,  0.0000e+00],
        [-1.4274e-03, -1.5901e-03,  0.0000e+00,  ...,  2.9898e-03,
          1.5968e-04,  0.0000e+00],
        [-1.4274e-03, -1.5901e-03,  0.0000e+00,  ...,  2.9898e-03,
          1.5968e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3934.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9237, device='cuda:0')



h[100].sum tensor(-0.8094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-19.9133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0470, 0.0000, 0.0000,  ..., 0.0856, 0.0447, 0.0000],
        [0.0632, 0.0000, 0.0000,  ..., 0.1081, 0.0582, 0.0000],
        [0.0717, 0.0000, 0.0000,  ..., 0.1199, 0.0652, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70062.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1911, 0.0000,  ..., 0.6123, 0.0000, 0.0938],
        [0.0000, 0.2326, 0.0000,  ..., 0.7150, 0.0000, 0.1148],
        [0.0000, 0.2410, 0.0000,  ..., 0.7345, 0.0000, 0.1189],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0789, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698891.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.3644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.1477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-426.2192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1815],
        [ 0.1779],
        [ 0.1774],
        ...,
        [-1.6991],
        [-1.6943],
        [-1.6925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226559.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0220],
        [1.0197],
        ...,
        [1.0000],
        [0.9988],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369876.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0221],
        [1.0198],
        ...,
        [1.0000],
        [0.9988],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369886.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9529e-03, -1.6988e-03, -2.1790e-07,  ...,  1.1877e-02,
          5.4603e-03, -2.9918e-03],
        [-1.4367e-03, -1.5904e-03,  0.0000e+00,  ...,  2.9955e-03,
          1.3454e-04,  0.0000e+00],
        [-1.4367e-03, -1.5904e-03,  0.0000e+00,  ...,  2.9955e-03,
          1.3454e-04,  0.0000e+00],
        ...,
        [-1.4367e-03, -1.5904e-03,  0.0000e+00,  ...,  2.9955e-03,
          1.3454e-04,  0.0000e+00],
        [-1.4367e-03, -1.5904e-03,  0.0000e+00,  ...,  2.9955e-03,
          1.3454e-04,  0.0000e+00],
        [-1.4367e-03, -1.5904e-03,  0.0000e+00,  ...,  2.9955e-03,
          1.3454e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5005.7705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6768, device='cuda:0')



h[100].sum tensor(-1.4463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0175, device='cuda:0')



h[200].sum tensor(-35.8214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0114, 0.0000, 0.0000,  ..., 0.0321, 0.0125, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0213, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95756.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0296, 0.0000,  ..., 0.2080, 0.0000, 0.0123],
        [0.0000, 0.0106, 0.0000,  ..., 0.1474, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.1039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0785, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0866, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.1135, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(845807.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(594.8798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.7001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.9164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0562],
        [-0.3930],
        [-0.7687],
        ...,
        [-1.6616],
        [-1.5257],
        [-1.2774]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226533.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0221],
        [1.0198],
        ...,
        [1.0000],
        [0.9988],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369886.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        ...,
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3928.6321, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9369, device='cuda:0')



h[100].sum tensor(-0.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-20.2015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70244.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0754, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703652.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(491.7835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.6676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.1433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9443],
        [-1.9607],
        [-1.9624],
        ...,
        [-1.7352],
        [-1.7304],
        [-1.7286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260017.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [ 3.5758e-03, -1.6759e-03, -1.6395e-07,  ...,  9.9808e-03,
          4.2826e-03, -2.3467e-03],
        [ 3.5758e-03, -1.6759e-03, -1.6395e-07,  ...,  9.9808e-03,
          4.2826e-03, -2.3467e-03],
        ...,
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3909.6772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9276, device='cuda:0')



h[100].sum tensor(-0.7991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-19.9241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0000, 0.0000,  ..., 0.0251, 0.0081, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0252, 0.0082, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0253, 0.0082, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70732.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.1583, 0.0000, 0.0032],
        [0.0000, 0.0050, 0.0000,  ..., 0.1469, 0.0000, 0.0011],
        [0.0000, 0.0051, 0.0000,  ..., 0.1433, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710771.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.4572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.2873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.4249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5548],
        [-0.8577],
        [-1.1360],
        ...,
        [-1.7359],
        [-1.7309],
        [-1.7290]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277476.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        ...,
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4316.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1924, device='cuda:0')



h[100].sum tensor(-1.0378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-25.8755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79604.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0173, 0.0000,  ..., 0.1545, 0.0000, 0.0065],
        [0.0000, 0.0153, 0.0000,  ..., 0.1474, 0.0000, 0.0055],
        [0.0000, 0.0120, 0.0000,  ..., 0.1343, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(756144.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(531.4361, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.8999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.2462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0154],
        [-0.0170],
        [-0.0834],
        ...,
        [-1.5572],
        [-1.5407],
        [-1.4697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265505.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3327e-03, -1.6718e-03, -1.5601e-07,  ...,  9.6429e-03,
          4.0800e-03, -2.2331e-03],
        [ 5.6372e-03, -1.7109e-03, -2.3127e-07,  ...,  1.2847e-02,
          6.0009e-03, -3.3104e-03],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        ...,
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00],
        [-1.4445e-03, -1.5908e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.7821e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4645.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4021, device='cuda:0')



h[100].sum tensor(-1.2309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-30.6926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0339, 0.0000, 0.0000,  ..., 0.0675, 0.0336, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0328, 0.0127, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0223, 0.0064, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86145.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0726, 0.0000,  ..., 0.3196, 0.0000, 0.0344],
        [0.0000, 0.0400, 0.0000,  ..., 0.2337, 0.0000, 0.0179],
        [0.0000, 0.0152, 0.0000,  ..., 0.1542, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0852, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0851, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(785786.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(557.8075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.2346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.3848, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1745],
        [-0.0440],
        [-0.4076],
        ...,
        [-1.5449],
        [-1.5084],
        [-1.5383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253862.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0221],
        [1.0198],
        ...,
        [0.9999],
        [0.9987],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369895.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0221],
        [1.0199],
        ...,
        [0.9999],
        [0.9987],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369905.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4530e-03, -1.5911e-03,  0.0000e+00,  ...,  3.0045e-03,
          7.9003e-05,  0.0000e+00],
        [ 7.3107e-03, -1.7398e-03, -2.7404e-07,  ...,  1.5188e-02,
          7.3847e-03, -4.0898e-03],
        [ 1.4003e-02, -1.8533e-03, -4.8329e-07,  ...,  2.4492e-02,
          1.2963e-02, -7.2128e-03],
        ...,
        [-1.4530e-03, -1.5911e-03,  0.0000e+00,  ...,  3.0045e-03,
          7.9003e-05,  0.0000e+00],
        [-1.4530e-03, -1.5911e-03,  0.0000e+00,  ...,  3.0045e-03,
          7.9003e-05,  0.0000e+00],
        [-1.4530e-03, -1.5911e-03,  0.0000e+00,  ...,  3.0045e-03,
          7.9003e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4100.9551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0508, device='cuda:0')



h[100].sum tensor(-0.9028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-22.6621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1456, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0000, 0.0000,  ..., 0.0347, 0.0138, 0.0000],
        [0.0241, 0.0000, 0.0000,  ..., 0.0520, 0.0242, 0.0000],
        [0.0435, 0.0000, 0.0000,  ..., 0.0811, 0.0416, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73074.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0402, 0.0000,  ..., 0.2296, 0.0000, 0.0183],
        [0.0000, 0.0844, 0.0000,  ..., 0.3489, 0.0000, 0.0408],
        [0.0000, 0.1331, 0.0000,  ..., 0.4730, 0.0000, 0.0654],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714229.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.1779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.9222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.8353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3035],
        [ 0.0394],
        [ 0.2185],
        ...,
        [-1.7448],
        [-1.7398],
        [-1.7379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296314.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0221],
        [1.0199],
        ...,
        [0.9999],
        [0.9987],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369905.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0222],
        [1.0199],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369914.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1083e-02, -1.8043e-03, -3.7560e-07,  ...,  2.0449e-02,
          1.0543e-02, -5.8459e-03],
        [-1.4642e-03, -1.5914e-03,  0.0000e+00,  ...,  3.0032e-03,
          8.2500e-05,  0.0000e+00],
        [ 8.3222e-03, -1.7575e-03, -2.9296e-07,  ...,  1.6610e-02,
          8.2416e-03, -4.5596e-03],
        ...,
        [-1.4642e-03, -1.5914e-03,  0.0000e+00,  ...,  3.0032e-03,
          8.2500e-05,  0.0000e+00],
        [-1.4642e-03, -1.5914e-03,  0.0000e+00,  ...,  3.0032e-03,
          8.2500e-05,  0.0000e+00],
        [-1.4642e-03, -1.5914e-03,  0.0000e+00,  ...,  3.0032e-03,
          8.2500e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3706.4478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7667, device='cuda:0')



h[100].sum tensor(-0.6597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0080, device='cuda:0')



h[200].sum tensor(-16.6717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0000, 0.0000,  ..., 0.0401, 0.0171, 0.0000],
        [0.0378, 0.0000, 0.0000,  ..., 0.0732, 0.0368, 0.0000],
        [0.0571, 0.0000, 0.0000,  ..., 0.0979, 0.0517, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66898.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0923, 0.0000,  ..., 0.3733, 0.0000, 0.0449],
        [0.0000, 0.1573, 0.0000,  ..., 0.5352, 0.0000, 0.0782],
        [0.0000, 0.2327, 0.0000,  ..., 0.7192, 0.0000, 0.1168],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690358.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(474.5609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.0594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.8210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2876],
        [ 0.2781],
        [ 0.2555],
        ...,
        [-1.7459],
        [-1.7410],
        [-1.7391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283497.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0222],
        [1.0199],
        ...,
        [0.9998],
        [0.9986],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369914.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0222],
        [1.0200],
        ...,
        [0.9998],
        [0.9986],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369923.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4726e-03, -1.5916e-03,  0.0000e+00,  ...,  3.0049e-03,
          8.0567e-05,  0.0000e+00],
        [ 7.5533e-03, -1.7448e-03, -2.5861e-07,  ...,  1.5555e-02,
          7.6061e-03, -4.1982e-03],
        [ 4.6286e-03, -1.6952e-03, -1.7481e-07,  ...,  1.1488e-02,
          5.1676e-03, -2.8379e-03],
        ...,
        [-1.4726e-03, -1.5916e-03,  0.0000e+00,  ...,  3.0049e-03,
          8.0567e-05,  0.0000e+00],
        [-1.4726e-03, -1.5916e-03,  0.0000e+00,  ...,  3.0049e-03,
          8.0567e-05,  0.0000e+00],
        [-1.4726e-03, -1.5916e-03,  0.0000e+00,  ...,  3.0049e-03,
          8.0567e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3900.7861, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.3810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8943, device='cuda:0')



h[100].sum tensor(-0.7557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-19.2284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0250, 0.0080, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0314, 0.0118, 0.0000],
        [0.0481, 0.0000, 0.0000,  ..., 0.0875, 0.0454, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68165.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0221, 0.0000,  ..., 0.1802, 0.0000, 0.0100],
        [0.0000, 0.0515, 0.0000,  ..., 0.2649, 0.0000, 0.0241],
        [0.0000, 0.1077, 0.0000,  ..., 0.4115, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0794, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0794, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690195.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.3719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.3569, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.2277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0974],
        [ 0.1936],
        [ 0.2664],
        ...,
        [-1.7443],
        [-1.7394],
        [-1.7363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268087.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0222],
        [1.0200],
        ...,
        [0.9998],
        [0.9986],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369923.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0222],
        [1.0200],
        ...,
        [0.9998],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369933.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00],
        [-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00],
        [-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00],
        ...,
        [-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00],
        [-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00],
        [-1.4790e-03, -1.5919e-03,  0.0000e+00,  ...,  3.0018e-03,
          9.9531e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3901.0527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.9764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8635, device='cuda:0')



h[100].sum tensor(-0.7348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-18.8223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72384.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0771, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.1094, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0802, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723209.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.0541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.7713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.4820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0959],
        [-0.8903],
        [-0.5482],
        ...,
        [-1.7371],
        [-1.7326],
        [-1.7310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225331.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0222],
        [1.0200],
        ...,
        [0.9998],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369933.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0222],
        [1.0201],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369943.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4163.5151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0468, device='cuda:0')



h[100].sum tensor(-0.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-22.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75385.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0777, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0781, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0783, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(729895.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(498.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.8636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-415.3346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8041],
        [-1.7694],
        [-1.7008],
        ...,
        [-1.7326],
        [-1.7278],
        [-1.7260]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221216.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0222],
        [1.0201],
        ...,
        [0.9997],
        [0.9985],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369943.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0223],
        [1.0201],
        ...,
        [0.9997],
        [0.9985],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369953.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3983.3599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.9182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8969, device='cuda:0')



h[100].sum tensor(-0.7461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-19.3750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71852.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0779, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0783, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0785, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712302.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(481.8668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.5522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-426.4473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6486],
        [-1.7444],
        [-1.8233],
        ...,
        [-1.7300],
        [-1.7251],
        [-1.7231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200655.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0223],
        [1.0201],
        ...,
        [0.9997],
        [0.9985],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369953.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0223],
        [1.0202],
        ...,
        [0.9997],
        [0.9985],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369962.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00],
        [-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00],
        [-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00],
        ...,
        [-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00],
        [-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00],
        [-1.4543e-03, -1.5924e-03,  0.0000e+00,  ...,  3.0006e-03,
          8.9864e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4083.3635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9606, device='cuda:0')



h[100].sum tensor(-0.7905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-20.6665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74443.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0778, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(729564.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(492.0936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.5520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.4131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6732],
        [-1.7884],
        [-1.8560],
        ...,
        [-1.7368],
        [-1.7321],
        [-1.7303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212417.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0223],
        [1.0202],
        ...,
        [0.9997],
        [0.9985],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369962.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(825.3004, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0224],
        [1.0203],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369972.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4495e-03, -1.5926e-03,  0.0000e+00,  ...,  3.0013e-03,
          8.6009e-05,  0.0000e+00],
        [ 5.2665e-03, -1.7066e-03, -1.5404e-07,  ...,  1.2336e-02,
          5.6818e-03, -3.0955e-03],
        [ 3.5526e-03, -1.6775e-03, -1.1473e-07,  ...,  9.9538e-03,
          4.2537e-03, -2.3055e-03],
        ...,
        [-1.4495e-03, -1.5926e-03,  0.0000e+00,  ...,  3.0013e-03,
          8.6009e-05,  0.0000e+00],
        [-1.4495e-03, -1.5926e-03,  0.0000e+00,  ...,  3.0013e-03,
          8.6009e-05,  0.0000e+00],
        [-1.4495e-03, -1.5926e-03,  0.0000e+00,  ...,  3.0013e-03,
          8.6009e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3834.9336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.6088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7839, device='cuda:0')



h[100].sum tensor(-0.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-17.0686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0217, 0.0060, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0330, 0.0128, 0.0000],
        [0.0453, 0.0000, 0.0000,  ..., 0.0835, 0.0430, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70269.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0179, 0.0000,  ..., 0.1655, 0.0000, 0.0075],
        [0.0000, 0.0499, 0.0000,  ..., 0.2581, 0.0000, 0.0220],
        [0.0000, 0.1115, 0.0000,  ..., 0.4138, 0.0000, 0.0528],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709212.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(477.8714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.2650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.2234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0038],
        [ 0.1206],
        [ 0.2059],
        ...,
        [-1.7448],
        [-1.7400],
        [-1.7382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245856.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0224],
        [1.0203],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369972.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0225],
        [1.0204],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369982.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.0698e-03, -1.7034e-03, -1.4287e-07,  ...,  1.2062e-02,
          5.5162e-03, -2.9978e-03],
        [-1.4459e-03, -1.5927e-03,  0.0000e+00,  ...,  3.0063e-03,
          8.7502e-05,  0.0000e+00],
        [-1.4459e-03, -1.5927e-03,  0.0000e+00,  ...,  3.0063e-03,
          8.7502e-05,  0.0000e+00],
        ...,
        [-1.4459e-03, -1.5927e-03,  0.0000e+00,  ...,  3.0063e-03,
          8.7502e-05,  0.0000e+00],
        [-1.4459e-03, -1.5927e-03,  0.0000e+00,  ...,  3.0063e-03,
          8.7502e-05,  0.0000e+00],
        [-1.4459e-03, -1.5927e-03,  0.0000e+00,  ...,  3.0063e-03,
          8.7502e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4528.5293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2695, device='cuda:0')



h[100].sum tensor(-1.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-27.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0000, 0.0000,  ..., 0.0399, 0.0169, 0.0000],
        [0.0314, 0.0000, 0.0000,  ..., 0.0621, 0.0302, 0.0000],
        [0.0267, 0.0000, 0.0000,  ..., 0.0536, 0.0251, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80914.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0772, 0.0000,  ..., 0.3276, 0.0000, 0.0354],
        [0.0000, 0.1051, 0.0000,  ..., 0.3958, 0.0000, 0.0496],
        [0.0000, 0.1103, 0.0000,  ..., 0.4076, 0.0000, 0.0524],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0808, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(759435.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(522.4869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.7042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.5442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2232],
        [ 0.2092],
        [ 0.1951],
        ...,
        [-1.7540],
        [-1.7492],
        [-1.7474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217956.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0225],
        [1.0204],
        ...,
        [0.9996],
        [0.9984],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369982.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0226],
        [1.0204],
        ...,
        [0.9996],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369991.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7494e-02, -1.9145e-03, -3.9702e-07,  ...,  2.9343e-02,
          1.5895e-02, -8.7005e-03],
        [ 2.5566e-02, -2.0516e-03, -5.6620e-07,  ...,  4.0562e-02,
          2.2620e-02, -1.2408e-02],
        [ 1.7980e-02, -1.9228e-03, -4.0721e-07,  ...,  3.0019e-02,
          1.6300e-02, -8.9238e-03],
        ...,
        [-1.4495e-03, -1.5929e-03,  0.0000e+00,  ...,  3.0139e-03,
          1.1101e-04,  0.0000e+00],
        [-1.4495e-03, -1.5929e-03,  0.0000e+00,  ...,  3.0139e-03,
          1.1101e-04,  0.0000e+00],
        [-1.4495e-03, -1.5929e-03,  0.0000e+00,  ...,  3.0139e-03,
          1.1101e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4572.3262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2821, device='cuda:0')



h[100].sum tensor(-1.0362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-27.6526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0961, 0.0000, 0.0000,  ..., 0.1540, 0.0854, 0.0000],
        [0.0834, 0.0000, 0.0000,  ..., 0.1364, 0.0749, 0.0000],
        [0.0682, 0.0000, 0.0000,  ..., 0.1154, 0.0622, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83449.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2803, 0.0000,  ..., 0.8250, 0.0000, 0.1379],
        [0.0000, 0.2610, 0.0000,  ..., 0.7794, 0.0000, 0.1282],
        [0.0000, 0.2308, 0.0000,  ..., 0.7070, 0.0000, 0.1129],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0812, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0812, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0812, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(769344.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(535.1197, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.3608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.5768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0919],
        [ 0.0915],
        [ 0.0950],
        ...,
        [-1.6083],
        [-1.5542],
        [-1.5525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213609.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0226],
        [1.0204],
        ...,
        [0.9996],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369991.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0227],
        [1.0205],
        ...,
        [0.9995],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370001.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4480.6348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2179, device='cuda:0')



h[100].sum tensor(-0.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-26.3135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80787.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0195, 0.0000,  ..., 0.1775, 0.0000, 0.0071],
        [0.0000, 0.0213, 0.0000,  ..., 0.1842, 0.0000, 0.0080],
        [0.0000, 0.0248, 0.0000,  ..., 0.1962, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0817, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0817, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0817, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(756362.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.8984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.3147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.4446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1000],
        [ 0.1067],
        [ 0.1188],
        ...,
        [-1.7708],
        [-1.7660],
        [-1.7643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184209.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0227],
        [1.0205],
        ...,
        [0.9995],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370001.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0228],
        [1.0205],
        ...,
        [0.9995],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370010.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0030,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3762.1704, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.8595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7522, device='cuda:0')



h[100].sum tensor(-0.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0078, device='cuda:0')



h[200].sum tensor(-16.1369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0007, 0.0000],
        [0.0063, 0.0000, 0.0000,  ..., 0.0233, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66212.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.1842e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0213e-01, 0.0000e+00,
         1.0958e-04],
        [0.0000e+00, 1.5680e-02, 0.0000e+00,  ..., 1.6049e-01, 0.0000e+00,
         6.6273e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.1764e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.1740e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.1716e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688802.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(469.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.4718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.9919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3682],
        [-0.9928],
        [-0.5378],
        ...,
        [-1.7852],
        [-1.7804],
        [-1.7786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258949.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0228],
        [1.0205],
        ...,
        [0.9995],
        [0.9984],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370010.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0229],
        [1.0206],
        ...,
        [0.9995],
        [0.9983],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370020.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4655e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0463e-03,
          1.8514e-04,  0.0000e+00],
        [-1.4655e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0463e-03,
          1.8514e-04,  0.0000e+00],
        [ 1.6773e-02, -1.9030e-03, -3.3347e-07,  ...,  2.8400e-02,
          1.5387e-02, -8.3341e-03],
        ...,
        [-1.4655e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0463e-03,
          1.8514e-04,  0.0000e+00],
        [-1.4655e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0463e-03,
          1.8514e-04,  0.0000e+00],
        [-1.4655e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0463e-03,
          1.8514e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4127.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0099, device='cuda:0')



h[100].sum tensor(-0.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-21.4630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0008, 0.0000],
        [0.0171, 0.0000, 0.0000,  ..., 0.0384, 0.0163, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0337, 0.0135, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73564.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.1131, 0.0000, 0.0010],
        [0.0000, 0.0245, 0.0000,  ..., 0.1907, 0.0000, 0.0110],
        [0.0000, 0.0345, 0.0000,  ..., 0.2242, 0.0000, 0.0157],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0819, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0819, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0819, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(728092., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.0229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.3399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.5620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2676],
        [-0.8053],
        [-0.3720],
        ...,
        [-1.7962],
        [-1.7914],
        [-1.7900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237818.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0229],
        [1.0206],
        ...,
        [0.9995],
        [0.9983],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370020.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0229],
        [1.0206],
        ...,
        [0.9994],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370029.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4606e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0674e-03,
          1.7302e-04,  0.0000e+00],
        [ 4.7851e-03, -1.6993e-03, -1.0906e-07,  ...,  1.1749e-02,
          5.3786e-03, -2.8489e-03],
        [ 4.1611e-03, -1.6887e-03, -9.8168e-08,  ...,  1.0882e-02,
          4.8585e-03, -2.5642e-03],
        ...,
        [-1.4606e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0674e-03,
          1.7302e-04,  0.0000e+00],
        [-1.4606e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0674e-03,
          1.7302e-04,  0.0000e+00],
        [-1.4606e-03, -1.5932e-03,  0.0000e+00,  ...,  3.0674e-03,
          1.7302e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3945.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.0595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8919, device='cuda:0')



h[100].sum tensor(-0.6906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-18.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0213, 0.0060, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0278, 0.0098, 0.0000],
        [0.0220, 0.0000, 0.0000,  ..., 0.0514, 0.0240, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71480.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0228, 0.0000,  ..., 0.1867, 0.0000, 0.0091],
        [0.0000, 0.0383, 0.0000,  ..., 0.2397, 0.0000, 0.0168],
        [0.0000, 0.0584, 0.0000,  ..., 0.2935, 0.0000, 0.0266],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723532.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(496.7640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.0984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.4584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0580],
        [ 0.2034],
        [ 0.2805],
        ...,
        [-1.8158],
        [-1.8108],
        [-1.8089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258135.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0229],
        [1.0206],
        ...,
        [0.9994],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370029.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0231],
        [1.0206],
        ...,
        [0.9994],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370038.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3948e-02, -2.0248e-03, -4.2352e-07,  ...,  3.8397e-02,
          2.1326e-02, -1.1565e-02],
        [ 3.2836e-02, -2.1757e-03, -5.7171e-07,  ...,  5.0751e-02,
          2.8733e-02, -1.5611e-02],
        [ 2.4952e-02, -2.0418e-03, -4.4025e-07,  ...,  3.9792e-02,
          2.2163e-02, -1.2022e-02],
        ...,
        [-1.4524e-03, -1.5933e-03,  0.0000e+00,  ...,  3.0897e-03,
          1.5823e-04,  0.0000e+00],
        [-1.4524e-03, -1.5933e-03,  0.0000e+00,  ...,  3.0897e-03,
          1.5823e-04,  0.0000e+00],
        [-1.4524e-03, -1.5933e-03,  0.0000e+00,  ...,  3.0897e-03,
          1.5823e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4366.2275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1659, device='cuda:0')



h[100].sum tensor(-0.9022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-24.9207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1111, 0.0000, 0.0000,  ..., 0.1753, 0.0982, 0.0000],
        [0.1263, 0.0000, 0.0000,  ..., 0.1964, 0.1108, 0.0000],
        [0.1240, 0.0000, 0.0000,  ..., 0.1932, 0.1089, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79779.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3061, 0.0000,  ..., 0.9001, 0.0000, 0.1530],
        [0.0000, 0.3447, 0.0000,  ..., 0.9949, 0.0000, 0.1725],
        [0.0000, 0.3382, 0.0000,  ..., 0.9793, 0.0000, 0.1691],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0812, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(767238.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(531.8409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.0931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.0392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1914],
        [ 0.1812],
        [ 0.1710],
        ...,
        [-1.8305],
        [-1.8254],
        [-1.8235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249440.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0231],
        [1.0206],
        ...,
        [0.9994],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370038.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0232],
        [1.0207],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370048.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4025.1042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9425, device='cuda:0')



h[100].sum tensor(-0.7159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-19.9127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0250, 0.0081, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0189, 0.0044, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0218, 0.0060, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72386.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.1548, 0.0000, 0.0008],
        [0.0000, 0.0031, 0.0000,  ..., 0.1439, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.1565, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0814, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(728727.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(500.9323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.7843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4718],
        [-0.3610],
        [-0.2345],
        ...,
        [-1.8385],
        [-1.8334],
        [-1.8317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261776.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0232],
        [1.0207],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370048.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0233],
        [1.0208],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370057.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5650e-03, -1.7804e-03, -1.6719e-07,  ...,  1.8420e-02,
          9.3456e-03, -4.9923e-03],
        [ 6.2786e-03, -1.7245e-03, -1.1726e-07,  ...,  1.3852e-02,
          6.6071e-03, -3.5014e-03],
        [-1.4393e-03, -1.5934e-03,  0.0000e+00,  ...,  3.1248e-03,
          1.7589e-04,  0.0000e+00],
        ...,
        [-1.4393e-03, -1.5934e-03,  0.0000e+00,  ...,  3.1248e-03,
          1.7589e-04,  0.0000e+00],
        [-1.4393e-03, -1.5934e-03,  0.0000e+00,  ...,  3.1248e-03,
          1.7589e-04,  0.0000e+00],
        [-1.4393e-03, -1.5934e-03,  0.0000e+00,  ...,  3.1248e-03,
          1.7589e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4190.2017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0270, device='cuda:0')



h[100].sum tensor(-0.7825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-21.9172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0383, 0.0000, 0.0000,  ..., 0.0740, 0.0375, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0374, 0.0155, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0238, 0.0073, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74234.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0802, 0.0000,  ..., 0.3442, 0.0000, 0.0382],
        [0.0000, 0.0472, 0.0000,  ..., 0.2618, 0.0000, 0.0215],
        [0.0000, 0.0236, 0.0000,  ..., 0.1967, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0822, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0822, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0822, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734112.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(507.0019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.1113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.5767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2535],
        [ 0.1793],
        [ 0.0301],
        ...,
        [-1.8407],
        [-1.8355],
        [-1.8337]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245300.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0233],
        [1.0208],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370057.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(496.4734, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0234],
        [1.0208],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370067.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0031,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4024.3564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8962, device='cuda:0')



h[100].sum tensor(-0.6825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-19.2481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71335.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0797, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0838, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0827, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0826, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0826, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(722181.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(493.0101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.1661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.6647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8923],
        [-1.7510],
        [-1.5226],
        ...,
        [-1.6541],
        [-1.7868],
        [-1.8221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258503.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0234],
        [1.0208],
        ...,
        [0.9993],
        [0.9982],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370067.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0235],
        [1.0209],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370077.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4629.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2957, device='cuda:0')



h[100].sum tensor(-0.9647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-27.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1796, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82893.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.1417, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0834, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(779000.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.7664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.4838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1192],
        [-0.3825],
        [-0.6024],
        ...,
        [-1.8410],
        [-1.8320],
        [-1.8051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212855.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0235],
        [1.0209],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370077.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0236],
        [1.0210],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370086.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1260e-02, -1.8092e-03, -1.6748e-07,  ...,  2.0824e-02,
          1.0751e-02, -5.7272e-03],
        [ 6.1656e-03, -1.7226e-03, -1.0025e-07,  ...,  1.3742e-02,
          6.5063e-03, -3.4281e-03],
        [ 1.5480e-02, -1.8809e-03, -2.2318e-07,  ...,  2.6691e-02,
          1.4268e-02, -7.6318e-03],
        ...,
        [-1.4307e-03, -1.5936e-03,  0.0000e+00,  ...,  3.1831e-03,
          1.7658e-04,  0.0000e+00],
        [-1.4307e-03, -1.5936e-03,  0.0000e+00,  ...,  3.1831e-03,
          1.7658e-04,  0.0000e+00],
        [-1.4307e-03, -1.5936e-03,  0.0000e+00,  ...,  3.1831e-03,
          1.7658e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4833.1592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3965, device='cuda:0')



h[100].sum tensor(-1.0508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-30.0513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0254, 0.0000, 0.0000,  ..., 0.0563, 0.0267, 0.0000],
        [0.0376, 0.0000, 0.0000,  ..., 0.0715, 0.0358, 0.0000],
        [0.0190, 0.0000, 0.0000,  ..., 0.0436, 0.0190, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85239.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0719, 0.0000,  ..., 0.3297, 0.0000, 0.0336],
        [0.0000, 0.0835, 0.0000,  ..., 0.3570, 0.0000, 0.0398],
        [0.0000, 0.0666, 0.0000,  ..., 0.3133, 0.0000, 0.0313],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(785487.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(546.3735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.8463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.6581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3034],
        [ 0.3272],
        [ 0.3223],
        ...,
        [-1.8321],
        [-1.8206],
        [-1.8158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233375.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0236],
        [1.0210],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370086.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0237],
        [1.0211],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370094.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5731e-03, -1.6957e-03, -7.5589e-08,  ...,  1.1549e-02,
          5.1744e-03, -2.7055e-03],
        [-1.4320e-03, -1.5936e-03,  0.0000e+00,  ...,  3.2008e-03,
          1.6984e-04,  0.0000e+00],
        [-1.4320e-03, -1.5936e-03,  0.0000e+00,  ...,  3.2008e-03,
          1.6984e-04,  0.0000e+00],
        ...,
        [-1.4320e-03, -1.5936e-03,  0.0000e+00,  ...,  3.2008e-03,
          1.6984e-04,  0.0000e+00],
        [-1.4320e-03, -1.5936e-03,  0.0000e+00,  ...,  3.2008e-03,
          1.6984e-04,  0.0000e+00],
        [-1.4320e-03, -1.5936e-03,  0.0000e+00,  ...,  3.2008e-03,
          1.6984e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3990.8840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8470, device='cuda:0')



h[100].sum tensor(-0.6258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-18.0222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0000, 0.0000,  ..., 0.0417, 0.0179, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0216, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71262.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0613, 0.0000,  ..., 0.2997, 0.0000, 0.0292],
        [0.0000, 0.0266, 0.0000,  ..., 0.1985, 0.0000, 0.0116],
        [0.0000, 0.0029, 0.0000,  ..., 0.1274, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724863.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.0419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.6183, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1971],
        [ 0.0296],
        [-0.2202],
        ...,
        [-1.8541],
        [-1.8489],
        [-1.8470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215863., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0237],
        [1.0211],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370094.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0238],
        [1.0212],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370103.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4361e-03, -1.5937e-03,  0.0000e+00,  ...,  3.2153e-03,
          1.6250e-04,  0.0000e+00],
        [-1.4361e-03, -1.5937e-03,  0.0000e+00,  ...,  3.2153e-03,
          1.6250e-04,  0.0000e+00],
        [ 1.0259e-02, -1.7924e-03, -1.4038e-07,  ...,  1.9476e-02,
          9.9108e-03, -5.2604e-03],
        ...,
        [-1.4361e-03, -1.5937e-03,  0.0000e+00,  ...,  3.2153e-03,
          1.6250e-04,  0.0000e+00],
        [-1.4361e-03, -1.5937e-03,  0.0000e+00,  ...,  3.2153e-03,
          1.6250e-04,  0.0000e+00],
        [-1.4361e-03, -1.5937e-03,  0.0000e+00,  ...,  3.2153e-03,
          1.6250e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3893.8982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.4241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7819, device='cuda:0')



h[100].sum tensor(-0.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-16.5858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0007, 0.0000],
        [0.0139, 0.0000, 0.0000,  ..., 0.0366, 0.0147, 0.0000],
        [0.0114, 0.0000, 0.0000,  ..., 0.0352, 0.0138, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68890.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.1519, 0.0000, 0.0015],
        [0.0000, 0.0209, 0.0000,  ..., 0.1992, 0.0000, 0.0101],
        [0.0000, 0.0346, 0.0000,  ..., 0.2365, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714729.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(478.5844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-231.8629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.8233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0064],
        [ 0.0511],
        [ 0.1529],
        ...,
        [-1.8529],
        [-1.8543],
        [-1.8533]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267132.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0238],
        [1.0212],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370103.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0239],
        [1.0213],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370112.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4192.6738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9843, device='cuda:0')



h[100].sum tensor(-0.7082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-20.6840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73191.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0950, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0892, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733352.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(496.6077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.2620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.2885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0146],
        [-0.9828],
        [-0.8067],
        ...,
        [-1.7251],
        [-1.8250],
        [-1.8587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272036.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0239],
        [1.0213],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370112.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0240],
        [1.0214],
        ...,
        [0.9992],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370120.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4369.5098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0900, device='cuda:0')



h[100].sum tensor(-0.7852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-23.0951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77188.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0838, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0838, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0838, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750762.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.8321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.7323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.1773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1185],
        [-1.4227],
        [-1.6296],
        ...,
        [-1.8815],
        [-1.8763],
        [-1.8744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253501.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0240],
        [1.0214],
        ...,
        [0.9992],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370120.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0242],
        [1.0215],
        ...,
        [0.9992],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370129.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        ...,
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0015, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4120.7803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9261, device='cuda:0')



h[100].sum tensor(-0.6588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-19.5140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72174.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0807, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0813, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0841, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0841, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0840, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731476., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.1771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.7564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.9102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0364],
        [-1.9522],
        [-1.8109],
        ...,
        [-1.8883],
        [-1.8831],
        [-1.8812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250681.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0242],
        [1.0215],
        ...,
        [0.9992],
        [0.9981],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370129.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0243],
        [1.0217],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370138.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4613e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2518e-03,
          1.4588e-04,  0.0000e+00],
        [ 1.0836e-02, -1.8029e-03, -1.2184e-07,  ...,  2.0364e-02,
          1.0408e-02, -5.4961e-03],
        [ 1.1603e-02, -1.8160e-03, -1.2943e-07,  ...,  2.1431e-02,
          1.1047e-02, -5.8387e-03],
        ...,
        [-1.4613e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2518e-03,
          1.4588e-04,  0.0000e+00],
        [-1.4613e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2518e-03,
          1.4588e-04,  0.0000e+00],
        [-1.4613e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2518e-03,
          1.4588e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4401.9790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1119, device='cuda:0')



h[100].sum tensor(-0.7842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-23.3925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0000, 0.0000,  ..., 0.0307, 0.0110, 0.0000],
        [0.0207, 0.0000, 0.0000,  ..., 0.0462, 0.0203, 0.0000],
        [0.0644, 0.0000, 0.0000,  ..., 0.1114, 0.0594, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77677.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0220, 0.0000,  ..., 0.1864, 0.0000, 0.0115],
        [0.0000, 0.0528, 0.0000,  ..., 0.2830, 0.0000, 0.0277],
        [0.0000, 0.1172, 0.0000,  ..., 0.4490, 0.0000, 0.0613],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0840, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0840, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0840, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762858.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.9248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.0415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.6451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6273],
        [-0.3331],
        [-0.0126],
        ...,
        [-1.8983],
        [-1.8930],
        [-1.8911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245109.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0243],
        [1.0217],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370138.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0245],
        [1.0218],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370147.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4808.7427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3798, device='cuda:0')



h[100].sum tensor(-0.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-29.0291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87322.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0840, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0837, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0837, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(809478.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.8618, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.2678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.1365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1913],
        [-1.0686],
        [-0.7832],
        ...,
        [-1.9050],
        [-1.9022],
        [-1.9011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228500.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0245],
        [1.0218],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370147.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(504.5575, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0246],
        [1.0219],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370156.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00],
        [-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00],
        [-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00],
        ...,
        [-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00],
        [-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00],
        [-1.4185e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2475e-03,
          7.1043e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4317.6021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0538, device='cuda:0')



h[100].sum tensor(-0.7319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-22.1432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78878.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0798, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0802, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0805, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(778442.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.4597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.9891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.7608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6332],
        [-1.7453],
        [-1.8115],
        ...,
        [-1.9220],
        [-1.9166],
        [-1.9147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296799.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0246],
        [1.0219],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370156.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0248],
        [1.0219],
        ...,
        [0.9991],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370165.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00],
        [-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00],
        [-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00],
        ...,
        [-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00],
        [-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00],
        [-1.3977e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2446e-03,
          5.4296e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4375.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0722, device='cuda:0')



h[100].sum tensor(-0.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-22.7589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77129.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0822, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.1213, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754920.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.4462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.3468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.6582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4111],
        [-1.1035],
        [-0.7337],
        ...,
        [-1.9261],
        [-1.9208],
        [-1.9189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244657.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0248],
        [1.0219],
        ...,
        [0.9991],
        [0.9980],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370165.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0249],
        [1.0220],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370174.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1782e-02, -1.8178e-03, -1.0721e-07,  ...,  2.1561e-02,
          1.1066e-02, -5.8429e-03],
        [ 2.2809e-02, -2.0052e-03, -1.9691e-07,  ...,  3.6890e-02,
          2.0252e-02, -1.0732e-02],
        [ 2.7881e-02, -2.0913e-03, -2.3816e-07,  ...,  4.3941e-02,
          2.4477e-02, -1.2980e-02],
        ...,
        [-1.3964e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2401e-03,
          8.6343e-05,  0.0000e+00],
        [-1.3964e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2401e-03,
          8.6343e-05,  0.0000e+00],
        [-1.3964e-03, -1.5938e-03,  0.0000e+00,  ...,  3.2401e-03,
          8.6343e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4202.2437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9491, device='cuda:0')



h[100].sum tensor(-0.6515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-19.9925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0806, 0.0000, 0.0000,  ..., 0.1332, 0.0723, 0.0000],
        [0.1092, 0.0000, 0.0000,  ..., 0.1730, 0.0961, 0.0000],
        [0.1088, 0.0000, 0.0000,  ..., 0.1725, 0.0958, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74939.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2536, 0.0000,  ..., 0.7700, 0.0000, 0.1259],
        [0.0000, 0.2968, 0.0000,  ..., 0.8759, 0.0000, 0.1479],
        [0.0000, 0.3109, 0.0000,  ..., 0.9104, 0.0000, 0.1549],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750839., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(507.1053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.8648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.9839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2121],
        [ 0.2088],
        [ 0.2111],
        ...,
        [-1.9186],
        [-1.9132],
        [-1.9111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238135.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0249],
        [1.0220],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370174.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0251],
        [1.0220],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370183.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0029e-02, -1.7881e-03, -8.8496e-08,  ...,  1.9127e-02,
          9.6628e-03, -5.0597e-03],
        [ 7.8760e-03, -1.7516e-03, -7.1834e-08,  ...,  1.6134e-02,
          7.8696e-03, -4.1071e-03],
        [ 1.9688e-02, -1.9522e-03, -1.6326e-07,  ...,  3.2552e-02,
          1.7708e-02, -9.3340e-03],
        ...,
        [-1.4049e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2337e-03,
          1.3904e-04,  0.0000e+00],
        [-1.4049e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2337e-03,
          1.3904e-04,  0.0000e+00],
        [-1.4049e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2337e-03,
          1.3904e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3731.1042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.7619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6238, device='cuda:0')



h[100].sum tensor(-0.4217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0065, device='cuda:0')



h[200].sum tensor(-13.0323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0000, 0.0000,  ..., 0.0395, 0.0163, 0.0000],
        [0.0545, 0.0000, 0.0000,  ..., 0.0970, 0.0507, 0.0000],
        [0.0568, 0.0000, 0.0000,  ..., 0.1002, 0.0527, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65011.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0430, 0.0000,  ..., 0.2501, 0.0000, 0.0200],
        [0.0000, 0.0994, 0.0000,  ..., 0.3914, 0.0000, 0.0484],
        [0.0000, 0.1208, 0.0000,  ..., 0.4451, 0.0000, 0.0591],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700583.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.9861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.2050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-465.4124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4827],
        [-0.0638],
        [ 0.1708],
        ...,
        [-1.9069],
        [-1.9019],
        [-1.9002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256481., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0251],
        [1.0220],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370183.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0252],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370192.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5556e-03, -1.6953e-03, -4.3970e-08,  ...,  1.1528e-02,
          5.1493e-03, -2.6379e-03],
        [-1.4162e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2275e-03,
          1.7544e-04,  0.0000e+00],
        [ 4.5556e-03, -1.6953e-03, -4.3970e-08,  ...,  1.1528e-02,
          5.1493e-03, -2.6379e-03],
        ...,
        [-1.4162e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2275e-03,
          1.7544e-04,  0.0000e+00],
        [-1.4162e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2275e-03,
          1.7544e-04,  0.0000e+00],
        [-1.4162e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2275e-03,
          1.7544e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4147.2832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8856, device='cuda:0')



h[100].sum tensor(-0.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-18.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0000, 0.0000,  ..., 0.0201, 0.0048, 0.0000],
        [0.0164, 0.0000, 0.0000,  ..., 0.0441, 0.0192, 0.0000],
        [0.0036, 0.0000, 0.0000,  ..., 0.0202, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72057.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0020, 0.0000,  ..., 0.1307, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.1656, 0.0000, 0.0020],
        [0.0000, 0.0020, 0.0000,  ..., 0.1317, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731005.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.7724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.0939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.0030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1381],
        [-1.1833],
        [-1.3466],
        ...,
        [-1.9021],
        [-1.8971],
        [-1.8954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225936.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0252],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370192.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0254],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370201.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4190e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2282e-03,
          1.7739e-04,  0.0000e+00],
        [ 6.7955e-03, -1.7334e-03, -5.7521e-08,  ...,  1.4645e-02,
          7.0178e-03, -3.6218e-03],
        [ 2.9183e-02, -2.1138e-03, -2.1428e-07,  ...,  4.5758e-02,
          2.5660e-02, -1.3492e-02],
        ...,
        [-1.4190e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2282e-03,
          1.7739e-04,  0.0000e+00],
        [-1.4190e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2282e-03,
          1.7739e-04,  0.0000e+00],
        [-1.4190e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2282e-03,
          1.7739e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6111.7026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1951, device='cuda:0')



h[100].sum tensor(-1.4522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0229, device='cuda:0')



h[200].sum tensor(-45.5278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0000, 0.0000,  ..., 0.0439, 0.0191, 0.0000],
        [0.0610, 0.0000, 0.0000,  ..., 0.1040, 0.0551, 0.0000],
        [0.0816, 0.0000, 0.0000,  ..., 0.1348, 0.0736, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(111237.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1106, 0.0000,  ..., 0.4191, 0.0000, 0.0544],
        [0.0000, 0.2249, 0.0000,  ..., 0.7008, 0.0000, 0.1119],
        [0.0000, 0.3174, 0.0000,  ..., 0.9282, 0.0000, 0.1582],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(934395.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(649.6335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.2611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-352.5316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1260],
        [ 0.1363],
        [ 0.1196],
        ...,
        [-1.9089],
        [-1.9039],
        [-1.9022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205111.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0254],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370201.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0250],
        [1.0255],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370209.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3863.9592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7083, device='cuda:0')



h[100].sum tensor(-0.4684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0074, device='cuda:0')



h[200].sum tensor(-14.7915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0982, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67633.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0837, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0868, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0868, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715637.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(470.3981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.2732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-455.7845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8855],
        [-1.7152],
        [-1.4334],
        ...,
        [-1.9257],
        [-1.9206],
        [-1.9189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268901.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0250],
        [1.0255],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370209.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0256],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370217.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4268.8652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9909, device='cuda:0')



h[100].sum tensor(-0.6467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-20.5665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74719.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0829, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0864, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0864, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0864, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(748602.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(499.5897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.3412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.8380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9382],
        [-1.9722],
        [-1.9584],
        ...,
        [-1.9462],
        [-1.9411],
        [-1.9393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236391.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0256],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370217.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0257],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370225.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0526e-02, -2.1368e-03, -1.9220e-07,  ...,  4.7632e-02,
          2.6708e-02, -1.4012e-02],
        [ 1.9679e-02, -1.9525e-03, -1.2695e-07,  ...,  3.2556e-02,
          1.7675e-02, -9.2551e-03],
        [ 9.5637e-03, -1.7806e-03, -6.6097e-08,  ...,  1.8496e-02,
          9.2517e-03, -4.8188e-03],
        ...,
        [-1.4235e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2247e-03,
          1.0209e-04,  0.0000e+00],
        [-1.4235e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2247e-03,
          1.0209e-04,  0.0000e+00],
        [-1.4235e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2247e-03,
          1.0209e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5026.0239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4722, device='cuda:0')



h[100].sum tensor(-0.9718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-31.1294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0907, 0.0000, 0.0000,  ..., 0.1473, 0.0808, 0.0000],
        [0.0650, 0.0000, 0.0000,  ..., 0.1117, 0.0594, 0.0000],
        [0.0352, 0.0000, 0.0000,  ..., 0.0683, 0.0334, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90311.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2372, 0.0000,  ..., 0.7340, 0.0000, 0.1193],
        [0.0000, 0.1993, 0.0000,  ..., 0.6422, 0.0000, 0.1000],
        [0.0000, 0.1495, 0.0000,  ..., 0.5201, 0.0000, 0.0746],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0860, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(831910.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(565.2025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.2199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.1762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2956],
        [ 0.3070],
        [ 0.3186],
        ...,
        [-1.9667],
        [-1.9614],
        [-1.9596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220672.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0257],
        [1.0221],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370225.5625, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:24.927348
evaluation loss: 524.2843017578125
epoch: 0 mean loss: 503.0529479980469
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0258],
        [1.0222],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370233.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2341e-02, -1.8278e-03, -7.8672e-08,  ...,  2.2352e-02,
          1.1536e-02, -6.0259e-03],
        [-1.4230e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2208e-03,
          7.4410e-05,  0.0000e+00],
        [-1.4230e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2208e-03,
          7.4410e-05,  0.0000e+00],
        ...,
        [-1.4230e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2208e-03,
          7.4410e-05,  0.0000e+00],
        [-1.4230e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2208e-03,
          7.4410e-05,  0.0000e+00],
        [-1.4230e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2208e-03,
          7.4410e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4740.4653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3153, device='cuda:0')



h[100].sum tensor(-0.8496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-27.4101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0487, 0.0000, 0.0000,  ..., 0.0869, 0.0445, 0.0000],
        [0.0395, 0.0000, 0.0000,  ..., 0.0742, 0.0369, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0275, 0.0089, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83813.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1716, 0.0000,  ..., 0.5721, 0.0000, 0.0866],
        [0.0000, 0.1441, 0.0000,  ..., 0.5042, 0.0000, 0.0726],
        [0.0000, 0.1110, 0.0000,  ..., 0.4209, 0.0000, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0858, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0858, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(792546.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.4761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.4218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-427.1463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2076],
        [ 0.1878],
        [ 0.1626],
        ...,
        [-1.9860],
        [-1.9806],
        [-1.9788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251725.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0258],
        [1.0222],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370233.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 0.0 event: 0 loss: tensor(54.9128, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0259],
        [1.0222],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370242.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00],
        [-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00],
        [-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00],
        ...,
        [-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00],
        [-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00],
        [-1.4241e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2180e-03,
          6.6155e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4578.4482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2100, device='cuda:0')



h[100].sum tensor(-0.7773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-25.2596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0131, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82115.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0823, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0828, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0858, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0858, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0858, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(793361.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.6765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.2883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.0655, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1072],
        [-1.9943],
        [-1.8223],
        ...,
        [-1.9995],
        [-1.9941],
        [-1.9921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265862.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0259],
        [1.0222],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370242.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0260],
        [1.0223],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370251.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00],
        [-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00],
        [-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00],
        ...,
        [-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00],
        [-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00],
        [-1.4248e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2184e-03,
          7.0578e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4045.6619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8598, device='cuda:0')



h[100].sum tensor(-0.5481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-17.9409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0429, 0.0000, 0.0000,  ..., 0.0788, 0.0396, 0.0000],
        [0.0286, 0.0000, 0.0000,  ..., 0.0570, 0.0265, 0.0000],
        [0.0232, 0.0000, 0.0000,  ..., 0.0496, 0.0221, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73613.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1386, 0.0000,  ..., 0.4933, 0.0000, 0.0706],
        [0.0000, 0.1067, 0.0000,  ..., 0.4139, 0.0000, 0.0543],
        [0.0000, 0.0751, 0.0000,  ..., 0.3351, 0.0000, 0.0379],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0860, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(760753.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(499.6836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.5735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-452.1196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3074],
        [ 0.2689],
        [ 0.1345],
        ...,
        [-2.0087],
        [-2.0033],
        [-2.0014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283387.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0260],
        [1.0223],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370251.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0261],
        [1.0223],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370260.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6907e-03, -1.6808e-03, -2.5009e-08,  ...,  1.0324e-02,
          4.3301e-03, -2.2249e-03],
        [ 1.0968e-02, -1.8044e-03, -6.0627e-08,  ...,  2.0438e-02,
          1.0390e-02, -5.3937e-03],
        [ 7.6447e-03, -1.7479e-03, -4.4361e-08,  ...,  1.5819e-02,
          7.6224e-03, -3.9466e-03],
        ...,
        [-1.4191e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2217e-03,
          7.5438e-05,  0.0000e+00],
        [-1.4191e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2217e-03,
          7.5438e-05,  0.0000e+00],
        [-1.4191e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2217e-03,
          7.5438e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4189.6572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9453, device='cuda:0')



h[100].sum tensor(-0.5969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-19.6808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0393, 0.0000, 0.0000,  ..., 0.0758, 0.0379, 0.0000],
        [0.0234, 0.0000, 0.0000,  ..., 0.0539, 0.0247, 0.0000],
        [0.0202, 0.0000, 0.0000,  ..., 0.0474, 0.0208, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74428.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0852, 0.0000,  ..., 0.3624, 0.0000, 0.0427],
        [0.0000, 0.0738, 0.0000,  ..., 0.3353, 0.0000, 0.0367],
        [0.0000, 0.0601, 0.0000,  ..., 0.3006, 0.0000, 0.0298],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0864, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0863, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0863, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(757530.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(501.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.1454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-450.7968, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3083],
        [ 0.3130],
        [ 0.2750],
        ...,
        [-2.0118],
        [-2.0063],
        [-2.0044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271987.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0261],
        [1.0223],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370260.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0261],
        [1.0224],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370269.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00],
        [-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00],
        [-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00],
        ...,
        [-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00],
        [-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00],
        [-1.4144e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2226e-03,
          8.5355e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4106.0024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8802, device='cuda:0')



h[100].sum tensor(-0.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-18.2433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72199.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0999, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0868, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0868, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0867, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742751.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(490.0198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.9321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.4169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4683],
        [-1.3278],
        [-0.9854],
        ...,
        [-1.9524],
        [-1.9933],
        [-1.9995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277751., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0261],
        [1.0224],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370269.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0262],
        [1.0224],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370279.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2873e-03, -1.7417e-03, -3.8333e-08,  ...,  1.5313e-02,
          7.3285e-03, -3.7735e-03],
        [-1.4110e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2246e-03,
          8.7452e-05,  0.0000e+00],
        [-1.4110e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2246e-03,
          8.7452e-05,  0.0000e+00],
        ...,
        [-1.4110e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2246e-03,
          8.7452e-05,  0.0000e+00],
        [-1.4110e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2246e-03,
          8.7452e-05,  0.0000e+00],
        [-1.4110e-03, -1.5939e-03,  0.0000e+00,  ...,  3.2246e-03,
          8.7452e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4738.5508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2613, device='cuda:0')



h[100].sum tensor(-0.7919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-26.4918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0303, 0.0000, 0.0000,  ..., 0.0593, 0.0280, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0256, 0.0078, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85660.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0766, 0.0000,  ..., 0.3356, 0.0000, 0.0386],
        [0.0000, 0.0279, 0.0000,  ..., 0.1980, 0.0000, 0.0140],
        [0.0000, 0.0024, 0.0000,  ..., 0.1158, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0871, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0871, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0871, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(815964.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(542.7891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.4196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.3743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0502],
        [-0.5170],
        [-1.0939],
        ...,
        [-2.0093],
        [-2.0039],
        [-2.0021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235951.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0262],
        [1.0224],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370279.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0263],
        [1.0225],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370288.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00],
        [-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00],
        [-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00],
        ...,
        [-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00],
        [-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00],
        [-1.4114e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2266e-03,
          8.9114e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4543.6001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1131, device='cuda:0')



h[100].sum tensor(-0.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-23.5515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81417.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.1274, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0846, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0875, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0875, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0874, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(790551.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.7867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.9488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6945],
        [-1.1936],
        [-1.5469],
        ...,
        [-2.0085],
        [-2.0032],
        [-2.0014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254960.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0263],
        [1.0225],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370288.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0264],
        [1.0225],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370297.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4585e-02, -1.8658e-03, -6.3422e-08,  ...,  2.5463e-02,
          1.3419e-02, -6.9158e-03],
        [-1.4137e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2283e-03,
          1.0061e-04,  0.0000e+00],
        [-1.4137e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2283e-03,
          1.0061e-04,  0.0000e+00],
        ...,
        [-1.4137e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2283e-03,
          1.0061e-04,  0.0000e+00],
        [-1.4137e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2283e-03,
          1.0061e-04,  0.0000e+00],
        [-1.4137e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2283e-03,
          1.0061e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4187.3706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8837, device='cuda:0')



h[100].sum tensor(-0.5425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-18.4122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.0000, 0.0000,  ..., 0.0540, 0.0249, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0361, 0.0141, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74216.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1185, 0.0000,  ..., 0.4366, 0.0000, 0.0599],
        [0.0000, 0.0495, 0.0000,  ..., 0.2618, 0.0000, 0.0251],
        [0.0000, 0.0030, 0.0000,  ..., 0.1437, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0880, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0880, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0879, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(753415.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.7940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-236.5940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.1331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0857],
        [-0.0461],
        [-0.1432],
        ...,
        [-2.0026],
        [-1.9971],
        [-1.9951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223019., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0264],
        [1.0225],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370297.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0265],
        [1.0225],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370305.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3951.9023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7293, device='cuda:0')



h[100].sum tensor(-0.4388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-15.0042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0000, 0.0000,  ..., 0.0454, 0.0197, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0295, 0.0101, 0.0000],
        [0.0188, 0.0000, 0.0000,  ..., 0.0415, 0.0173, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69718.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0399, 0.0000,  ..., 0.2454, 0.0000, 0.0202],
        [0.0000, 0.0339, 0.0000,  ..., 0.2306, 0.0000, 0.0173],
        [0.0000, 0.0471, 0.0000,  ..., 0.2628, 0.0000, 0.0240],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0882, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0882, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0882, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734024.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(469.4494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.9663, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.0371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1072],
        [ 0.0495],
        [ 0.1578],
        ...,
        [-2.0072],
        [-2.0020],
        [-2.0002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248012.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0265],
        [1.0225],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370305.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0265],
        [1.0266],
        [1.0226],
        ...,
        [0.9993],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370314.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4152e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2342e-03,
          1.1029e-04,  0.0000e+00],
        [ 4.9576e-03, -1.7022e-03, -2.2698e-08,  ...,  1.2092e-02,
          5.4159e-03, -2.7450e-03],
        [-1.4152e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2342e-03,
          1.1029e-04,  0.0000e+00],
        ...,
        [-1.4152e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2342e-03,
          1.1029e-04,  0.0000e+00],
        [-1.4152e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2342e-03,
          1.1029e-04,  0.0000e+00],
        [-1.4152e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2342e-03,
          1.1029e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3861.7512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6547, device='cuda:0')



h[100].sum tensor(-0.3946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0068, device='cuda:0')



h[200].sum tensor(-13.5895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0000,  ..., 0.0222, 0.0059, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0262, 0.0082, 0.0000],
        [0.0258, 0.0000, 0.0000,  ..., 0.0573, 0.0268, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68437.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.1527, 0.0000, 0.0038],
        [0.0000, 0.0167, 0.0000,  ..., 0.1864, 0.0000, 0.0077],
        [0.0000, 0.0363, 0.0000,  ..., 0.2423, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0885, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0885, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730334.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(462.9411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-222.8855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.0254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4435],
        [-0.1894],
        [-0.0051],
        ...,
        [-2.0108],
        [-2.0056],
        [-2.0028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230052.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0265],
        [1.0266],
        [1.0226],
        ...,
        [0.9993],
        [0.9983],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370314.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0267],
        [1.0227],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370323.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4207e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2355e-03,
          1.1255e-04,  0.0000e+00],
        [ 1.9638e-02, -1.9518e-03, -7.1070e-08,  ...,  3.2507e-02,
          1.7647e-02, -9.0554e-03],
        [ 1.9913e-02, -1.9565e-03, -7.1998e-08,  ...,  3.2889e-02,
          1.7876e-02, -9.1735e-03],
        ...,
        [-1.4207e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2355e-03,
          1.1255e-04,  0.0000e+00],
        [-1.4207e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2355e-03,
          1.1255e-04,  0.0000e+00],
        [-1.4207e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2355e-03,
          1.1255e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4317.8779, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9434, device='cuda:0')



h[100].sum tensor(-0.5654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-19.6178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1308, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0253, 0.0000, 0.0000,  ..., 0.0524, 0.0240, 0.0000],
        [0.0352, 0.0000, 0.0000,  ..., 0.0682, 0.0334, 0.0000],
        [0.0771, 0.0000, 0.0000,  ..., 0.1286, 0.0695, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75788.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0721, 0.0000,  ..., 0.3276, 0.0000, 0.0369],
        [0.0000, 0.1277, 0.0000,  ..., 0.4661, 0.0000, 0.0652],
        [0.0000, 0.2073, 0.0000,  ..., 0.6627, 0.0000, 0.1056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0886, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(761842.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.1516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.9794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.3102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2649],
        [ 0.2600],
        [ 0.2495],
        ...,
        [-2.0201],
        [-2.0149],
        [-2.0132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227177.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0267],
        [1.0227],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370323.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 10.0 event: 50 loss: tensor(472.9675, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0269],
        [1.0228],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370331.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0193e-03, -1.7204e-03, -2.3790e-08,  ...,  1.3585e-02,
          6.3015e-03, -3.1946e-03],
        [ 7.5879e-03, -1.7471e-03, -2.8805e-08,  ...,  1.5765e-02,
          7.6077e-03, -3.8680e-03],
        [ 1.3404e-02, -1.8459e-03, -4.7398e-08,  ...,  2.3851e-02,
          1.2451e-02, -6.3647e-03],
        ...,
        [-1.4230e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2390e-03,
          1.0399e-04,  0.0000e+00],
        [-1.4230e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2390e-03,
          1.0399e-04,  0.0000e+00],
        [-1.4230e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2390e-03,
          1.0399e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4922.4106, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3259, device='cuda:0')



h[100].sum tensor(-0.7919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-27.6769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0338, 0.0000, 0.0000,  ..., 0.0683, 0.0334, 0.0000],
        [0.0370, 0.0000, 0.0000,  ..., 0.0729, 0.0361, 0.0000],
        [0.0239, 0.0000, 0.0000,  ..., 0.0547, 0.0252, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85122.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0580, 0.0000,  ..., 0.2948, 0.0000, 0.0296],
        [0.0000, 0.0817, 0.0000,  ..., 0.3557, 0.0000, 0.0417],
        [0.0000, 0.1061, 0.0000,  ..., 0.4175, 0.0000, 0.0539],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0886, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0886, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0886, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(797346.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.9672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-425.4851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2689],
        [ 0.2738],
        [ 0.2708],
        ...,
        [-2.0340],
        [-2.0286],
        [-2.0269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227897.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0269],
        [1.0228],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370331.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0270],
        [1.0229],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370339.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5704.1030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8469, device='cuda:0')



h[100].sum tensor(-1.0818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0193, device='cuda:0')



h[200].sum tensor(-38.0873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0004, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0281, 0.0093, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98253.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1083, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.1605, 0.0000, 0.0054],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(863280.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.1973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.6434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.1610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5891],
        [-1.2130],
        [-0.7368],
        ...,
        [-2.0476],
        [-2.0421],
        [-2.0403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231610.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0270],
        [1.0229],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370339.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0271],
        [1.0230],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370347.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9716e-02, -1.9536e-03, -6.0622e-08,  ...,  3.2658e-02,
          1.7731e-02, -9.0506e-03],
        [ 2.9318e-02, -2.1168e-03, -8.8137e-08,  ...,  4.6009e-02,
          2.5730e-02, -1.3158e-02],
        [ 2.0548e-02, -1.9677e-03, -6.3006e-08,  ...,  3.3815e-02,
          1.8424e-02, -9.4065e-03],
        ...,
        [-1.4395e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2420e-03,
          1.0673e-04,  0.0000e+00],
        [-1.4395e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2420e-03,
          1.0673e-04,  0.0000e+00],
        [-1.4395e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2420e-03,
          1.0673e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4938.2427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3509, device='cuda:0')



h[100].sum tensor(-0.7838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-27.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0909, 0.0000, 0.0000,  ..., 0.1477, 0.0810, 0.0000],
        [0.1078, 0.0000, 0.0000,  ..., 0.1714, 0.0952, 0.0000],
        [0.0884, 0.0000, 0.0000,  ..., 0.1446, 0.0791, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86544.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2205, 0.0000,  ..., 0.7005, 0.0000, 0.1147],
        [0.0000, 0.2757, 0.0000,  ..., 0.8388, 0.0000, 0.1432],
        [0.0000, 0.2651, 0.0000,  ..., 0.8140, 0.0000, 0.1378],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(813703.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.6111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.9565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.6398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2245],
        [ 0.2612],
        [ 0.2695],
        ...,
        [-2.0586],
        [-2.0533],
        [-2.0518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244851.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0271],
        [1.0230],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370347.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0273],
        [1.0230],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370355.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0032,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4867.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2812, device='cuda:0')



h[100].sum tensor(-0.7495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-26.7809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86293.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0853, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0860, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0889, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0889, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0889, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(822781.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(546.4633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.1181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.6727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0736],
        [-2.0120],
        [-1.8706],
        ...,
        [-1.8981],
        [-2.0203],
        [-2.0538]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248048.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0273],
        [1.0230],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370355.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0274],
        [1.0231],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370362.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8529e-03, -1.7520e-03, -2.3838e-08,  ...,  1.6177e-02,
          7.8580e-03, -3.9620e-03],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [ 1.4371e-02, -1.8628e-03, -4.0557e-08,  ...,  2.5241e-02,
          1.3289e-02, -6.7408e-03],
        ...,
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4662.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1625, device='cuda:0')



h[100].sum tensor(-0.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-23.9967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0000, 0.0000,  ..., 0.0240, 0.0069, 0.0000],
        [0.0354, 0.0000, 0.0000,  ..., 0.0709, 0.0349, 0.0000],
        [0.0187, 0.0000, 0.0000,  ..., 0.0456, 0.0198, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83290.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0000,  ..., 0.1815, 0.0000, 0.0087],
        [0.0000, 0.0509, 0.0000,  ..., 0.2836, 0.0000, 0.0273],
        [0.0000, 0.0521, 0.0000,  ..., 0.2891, 0.0000, 0.0278],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(806270., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(536.1205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.7469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.2722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5915],
        [-0.1214],
        [ 0.1503],
        ...,
        [-2.0829],
        [-2.0773],
        [-2.0754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218311.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0274],
        [1.0231],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370362.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0274],
        [1.0231],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370362.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7885e-03, -1.7339e-03, -2.1108e-08,  ...,  1.4697e-02,
          6.9711e-03, -3.5083e-03],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [ 6.7885e-03, -1.7339e-03, -2.1108e-08,  ...,  1.4697e-02,
          6.9711e-03, -3.5083e-03],
        ...,
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00],
        [-1.4405e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2524e-03,
          1.1391e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4246.7207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8963, device='cuda:0')



h[100].sum tensor(-0.5133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-18.4757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0000, 0.0000,  ..., 0.0228, 0.0062, 0.0000],
        [0.0248, 0.0000, 0.0000,  ..., 0.0561, 0.0261, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0230, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74592.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.1502, 0.0000, 0.0029],
        [0.0000, 0.0181, 0.0000,  ..., 0.1981, 0.0000, 0.0102],
        [0.0000, 0.0048, 0.0000,  ..., 0.1514, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(764871.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(501.0309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.6620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.9347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6144],
        [-1.4927],
        [-1.5285],
        ...,
        [-2.0829],
        [-2.0773],
        [-2.0754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277297.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0274],
        [1.0231],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370362.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0275],
        [1.0232],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370371., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6411e-02, -1.8973e-03, -4.3277e-08,  ...,  2.8079e-02,
          1.4983e-02, -7.5931e-03],
        [ 1.5065e-02, -1.8744e-03, -4.0011e-08,  ...,  2.6206e-02,
          1.3861e-02, -7.0200e-03],
        [ 2.3981e-02, -2.0260e-03, -6.1636e-08,  ...,  3.8606e-02,
          2.1291e-02, -1.0814e-02],
        ...,
        [-1.4320e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2645e-03,
          1.1526e-04,  0.0000e+00],
        [-1.4320e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2645e-03,
          1.1526e-04,  0.0000e+00],
        [-1.4320e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2645e-03,
          1.1526e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4532.9453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0771, device='cuda:0')



h[100].sum tensor(-0.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-22.0983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0667, 0.0000, 0.0000,  ..., 0.1143, 0.0610, 0.0000],
        [0.0888, 0.0000, 0.0000,  ..., 0.1451, 0.0794, 0.0000],
        [0.1085, 0.0000, 0.0000,  ..., 0.1725, 0.0958, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78333.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2047, 0.0000,  ..., 0.6659, 0.0000, 0.1071],
        [0.0000, 0.2525, 0.0000,  ..., 0.7849, 0.0000, 0.1319],
        [0.0000, 0.3064, 0.0000,  ..., 0.9182, 0.0000, 0.1595],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(776646.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.2762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.9366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.1386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3155],
        [ 0.3062],
        [ 0.2974],
        ...,
        [-2.0905],
        [-2.0847],
        [-2.0828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259890.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0275],
        [1.0232],
        ...,
        [0.9993],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370371., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0276],
        [1.0233],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370379.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8572e-03, -1.7008e-03, -1.4405e-08,  ...,  1.2013e-02,
          5.3615e-03, -2.6691e-03],
        [ 4.4452e-03, -1.6938e-03, -1.3461e-08,  ...,  1.1440e-02,
          5.0182e-03, -2.4940e-03],
        [-1.4269e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2742e-03,
          1.2603e-04,  0.0000e+00],
        ...,
        [-1.4269e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2742e-03,
          1.2603e-04,  0.0000e+00],
        [-1.4269e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2742e-03,
          1.2603e-04,  0.0000e+00],
        [-1.4269e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2742e-03,
          1.2603e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5516.5537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6820, device='cuda:0')



h[100].sum tensor(-0.9532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0175, device='cuda:0')



h[200].sum tensor(-34.8231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0439, 0.0000, 0.0000,  ..., 0.0825, 0.0419, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0293, 0.0100, 0.0000],
        [0.0203, 0.0000, 0.0000,  ..., 0.0478, 0.0211, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98759.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1031, 0.0000,  ..., 0.4134, 0.0000, 0.0542],
        [0.0000, 0.0500, 0.0000,  ..., 0.2809, 0.0000, 0.0267],
        [0.0000, 0.0575, 0.0000,  ..., 0.3009, 0.0000, 0.0305],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0891, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0891, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0890, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(885827.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.9005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.3294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.5966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3490],
        [ 0.3595],
        [ 0.3709],
        ...,
        [-2.0197],
        [-2.0138],
        [-2.0261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222216.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0276],
        [1.0233],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370379.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0277],
        [1.0233],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370387.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4886.0112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2663, device='cuda:0')



h[100].sum tensor(-0.7117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-26.1927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85301.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0862, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0865, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0894, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0894, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0894, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(815281.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.0773, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.6638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2117],
        [-2.2218],
        [-2.1960],
        ...,
        [-2.0928],
        [-2.0751],
        [-2.0404]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204101.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0277],
        [1.0233],
        ...,
        [0.9992],
        [0.9982],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370387.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0278],
        [1.0234],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370395.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4404e-03, -1.7275e-03, -1.6066e-08,  ...,  1.4216e-02,
          6.6830e-03, -3.3230e-03],
        [ 6.8182e-03, -1.7339e-03, -1.6838e-08,  ...,  1.4741e-02,
          6.9976e-03, -3.4828e-03],
        [ 6.4404e-03, -1.7275e-03, -1.6066e-08,  ...,  1.4216e-02,
          6.6830e-03, -3.3230e-03],
        ...,
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5240.1074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4944, device='cuda:0')



h[100].sum tensor(-0.8267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-30.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0313, 0.0000, 0.0000,  ..., 0.0650, 0.0315, 0.0000],
        [0.0304, 0.0000, 0.0000,  ..., 0.0639, 0.0308, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0345, 0.0131, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93055.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0465, 0.0000,  ..., 0.2719, 0.0000, 0.0243],
        [0.0000, 0.0474, 0.0000,  ..., 0.2757, 0.0000, 0.0248],
        [0.0000, 0.0309, 0.0000,  ..., 0.2337, 0.0000, 0.0162],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(858809.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.5668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.9167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.5969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0165],
        [ 0.1922],
        [ 0.2271],
        ...,
        [-2.0874],
        [-2.0794],
        [-2.0801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219715.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0278],
        [1.0234],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370395.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0278],
        [1.0234],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370395.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5285e-03, -1.6950e-03, -1.2155e-08,  ...,  1.1558e-02,
          5.0906e-03, -2.5140e-03],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        ...,
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00],
        [-1.4133e-03, -1.5940e-03,  0.0000e+00,  ...,  3.2971e-03,
          1.4184e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4519.5190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0389, device='cuda:0')



h[100].sum tensor(-0.5707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-21.1627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0000, 0.0000,  ..., 0.0460, 0.0201, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0220, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78239.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0623, 0.0000,  ..., 0.3083, 0.0000, 0.0325],
        [0.0000, 0.0248, 0.0000,  ..., 0.1990, 0.0000, 0.0130],
        [0.0000, 0.0020, 0.0000,  ..., 0.1226, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(780450.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(514.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.2565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.3612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0350],
        [-0.3660],
        [-0.9127],
        ...,
        [-2.1045],
        [-2.0988],
        [-2.0970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244049.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0278],
        [1.0234],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370395.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0279],
        [1.0235],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370404.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4317.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8869, device='cuda:0')



h[100].sum tensor(-0.4867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-18.1804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0007, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0341, 0.0130, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73623.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0974, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.1219, 0.0000, 0.0017],
        [0.0000, 0.0230, 0.0000,  ..., 0.1964, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0902, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(756019., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.2035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.5806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-457.1012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0078],
        [-0.9886],
        [-0.8100],
        ...,
        [-2.1039],
        [-2.0982],
        [-2.0964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270966.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0279],
        [1.0235],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370404.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0280],
        [1.0236],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370413.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4980.4722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2890, device='cuda:0')



h[100].sum tensor(-0.7062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-26.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87439.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.1412, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0913, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0912, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0912, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0911, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(830237., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(548.1751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.9999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.9075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9806],
        [-1.2781],
        [-1.3095],
        ...,
        [-2.1020],
        [-2.0965],
        [-2.0947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200479.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0280],
        [1.0236],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370413.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0282],
        [1.0237],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370421.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6999e-03, -1.6809e-03, -8.7946e-09,  ...,  1.0429e-02,
          4.4647e-03, -2.1517e-03],
        [ 5.0954e-03, -1.7046e-03, -1.1195e-08,  ...,  1.2369e-02,
          5.6268e-03, -2.7388e-03],
        [-1.4140e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3203e-03,
          2.0652e-04,  0.0000e+00],
        ...,
        [ 6.9814e-03, -1.7366e-03, -1.4438e-08,  ...,  1.4991e-02,
          7.1972e-03, -3.5323e-03],
        [-1.4140e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3203e-03,
          2.0652e-04,  0.0000e+00],
        [-1.4140e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3203e-03,
          2.0652e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4769.2441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1576, device='cuda:0')



h[100].sum tensor(-0.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-23.6767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0437, 0.0000, 0.0000,  ..., 0.0824, 0.0421, 0.0000],
        [0.0171, 0.0000, 0.0000,  ..., 0.0414, 0.0175, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0230, 0.0064, 0.0000],
        ...,
        [0.0594, 0.0000, 0.0000,  ..., 0.1028, 0.0541, 0.0000],
        [0.0521, 0.0000, 0.0000,  ..., 0.0926, 0.0480, 0.0000],
        [0.0376, 0.0000, 0.0000,  ..., 0.0704, 0.0347, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81412.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0898, 0.0000,  ..., 0.3825, 0.0000, 0.0462],
        [0.0000, 0.0512, 0.0000,  ..., 0.2853, 0.0000, 0.0266],
        [0.0000, 0.0202, 0.0000,  ..., 0.1884, 0.0000, 0.0105],
        ...,
        [0.0000, 0.2672, 0.0000,  ..., 0.8226, 0.0000, 0.1363],
        [0.0000, 0.2085, 0.0000,  ..., 0.6776, 0.0000, 0.1064],
        [0.0000, 0.1363, 0.0000,  ..., 0.4983, 0.0000, 0.0697]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(797724.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.3541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.3746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-437.3905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2283],
        [ 0.0015],
        [-0.4234],
        ...,
        [ 0.2705],
        [ 0.2789],
        [ 0.2146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237665.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0282],
        [1.0237],
        ...,
        [0.9992],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370421.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0283],
        [1.0238],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370429.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4123e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3206e-03,
          2.0034e-04,  0.0000e+00],
        [ 8.5075e-03, -1.7625e-03, -1.6088e-08,  ...,  1.7109e-02,
          8.4594e-03, -4.1659e-03],
        [ 8.5075e-03, -1.7625e-03, -1.6088e-08,  ...,  1.7109e-02,
          8.4594e-03, -4.1659e-03],
        ...,
        [-1.4123e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3206e-03,
          2.0034e-04,  0.0000e+00],
        [-1.4123e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3206e-03,
          2.0034e-04,  0.0000e+00],
        [-1.4123e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3206e-03,
          2.0034e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4526.8267, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9969, device='cuda:0')



h[100].sum tensor(-0.5345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-20.4204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0000,  ..., 0.0391, 0.0161, 0.0000],
        [0.0224, 0.0000, 0.0000,  ..., 0.0509, 0.0231, 0.0000],
        [0.0347, 0.0000, 0.0000,  ..., 0.0680, 0.0333, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77761.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0653, 0.0000,  ..., 0.3179, 0.0000, 0.0334],
        [0.0000, 0.1055, 0.0000,  ..., 0.4194, 0.0000, 0.0539],
        [0.0000, 0.1554, 0.0000,  ..., 0.5433, 0.0000, 0.0793],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0915, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0915, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0914, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(784051.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(508.5993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.2741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.9755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2381],
        [ 0.2197],
        [ 0.1995],
        ...,
        [-2.1248],
        [-2.1192],
        [-2.1173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275918.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0283],
        [1.0238],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370429.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0279],
        [1.0284],
        [1.0239],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370437.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [ 4.7859e-03, -1.6992e-03, -9.4714e-09,  ...,  1.1936e-02,
          5.3540e-03, -2.5968e-03],
        ...,
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4591.9268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0469, device='cuda:0')



h[100].sum tensor(-0.5502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-21.1775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0008, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0225, 0.0061, 0.0000],
        [0.0117, 0.0000, 0.0000,  ..., 0.0340, 0.0130, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80667.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1156, 0.0000, 0.0000],
        [0.0000, 0.0162, 0.0000,  ..., 0.1783, 0.0000, 0.0082],
        [0.0000, 0.0432, 0.0000,  ..., 0.2626, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(802486.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.4399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.6806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5274],
        [-0.1432],
        [ 0.1102],
        ...,
        [-2.1313],
        [-2.1257],
        [-2.1240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227665.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0279],
        [1.0284],
        [1.0239],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370437.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0279],
        [1.0284],
        [1.0239],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370437.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5899e-02, -1.8881e-03, -2.6462e-08,  ...,  2.7383e-02,
          1.4606e-02, -7.2553e-03],
        [ 1.6512e-02, -1.8985e-03, -2.7399e-08,  ...,  2.8235e-02,
          1.5116e-02, -7.5121e-03],
        [ 1.3721e-02, -1.8511e-03, -2.3132e-08,  ...,  2.4356e-02,
          1.2793e-02, -6.3423e-03],
        ...,
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00],
        [-1.4093e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3250e-03,
          1.9655e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4367.6074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8918, device='cuda:0')



h[100].sum tensor(-0.4741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-18.2496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0609, 0.0000, 0.0000,  ..., 0.1062, 0.0563, 0.0000],
        [0.0664, 0.0000, 0.0000,  ..., 0.1140, 0.0609, 0.0000],
        [0.0833, 0.0000, 0.0000,  ..., 0.1376, 0.0750, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74720.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1591, 0.0000,  ..., 0.5539, 0.0000, 0.0814],
        [0.0000, 0.1769, 0.0000,  ..., 0.5992, 0.0000, 0.0904],
        [0.0000, 0.1956, 0.0000,  ..., 0.6456, 0.0000, 0.0997],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(770095.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.9991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.2260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.8560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3359],
        [ 0.3362],
        [ 0.3306],
        ...,
        [-2.1366],
        [-2.1309],
        [-2.1291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262231.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0279],
        [1.0284],
        [1.0239],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370437.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0285],
        [1.0240],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370446.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4009e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3373e-03,
          1.8149e-04,  0.0000e+00],
        [ 5.2585e-03, -1.7071e-03, -9.5933e-09,  ...,  1.2593e-02,
          5.7245e-03, -2.7861e-03],
        [-1.4009e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3373e-03,
          1.8149e-04,  0.0000e+00],
        ...,
        [-1.4009e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3373e-03,
          1.8149e-04,  0.0000e+00],
        [-1.4009e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3373e-03,
          1.8149e-04,  0.0000e+00],
        [-1.4009e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3373e-03,
          1.8149e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4597.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0290, device='cuda:0')



h[100].sum tensor(-0.5427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-21.0456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0238, 0.0000, 0.0000,  ..., 0.0547, 0.0253, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0215, 0.0054, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0233, 0.0064, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79184.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0473, 0.0000,  ..., 0.2759, 0.0000, 0.0235],
        [0.0000, 0.0223, 0.0000,  ..., 0.2025, 0.0000, 0.0104],
        [0.0000, 0.0082, 0.0000,  ..., 0.1560, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0916, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(791818.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.9363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.4019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.1818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1381],
        [-0.0238],
        [-0.2923],
        ...,
        [-2.1445],
        [-2.1393],
        [-2.1375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247799.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0285],
        [1.0240],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370446.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0287],
        [1.0241],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370454.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0033,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4911.5439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2201, device='cuda:0')



h[100].sum tensor(-0.6379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-24.9259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0007, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0305, 0.0107, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0263, 0.0081, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84638.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.1447, 0.0000, 0.0040],
        [0.0000, 0.0556, 0.0000,  ..., 0.2848, 0.0000, 0.0270],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0958, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.1347, 0.0000, 0.0018],
        [0.0000, 0.0288, 0.0000,  ..., 0.2178, 0.0000, 0.0134]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(817636.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.2921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.5542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.6234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7211],
        [-0.3393],
        [-0.0801],
        ...,
        [-1.7205],
        [-1.1946],
        [-0.5617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220591.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0287],
        [1.0241],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370454.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0288],
        [1.0242],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370462.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4318.5464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8304, device='cuda:0')



h[100].sum tensor(-0.4318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-17.0000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73140.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0913, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0888, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0891, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0920, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0920, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762807.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.2970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.4066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-461.6372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5259],
        [-1.8532],
        [-2.0685],
        ...,
        [-2.1605],
        [-2.1547],
        [-2.1528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274283.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0288],
        [1.0242],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370462.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 30.0 event: 150 loss: tensor(815.9726, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0288],
        [1.0243],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370470.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4662.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0444, device='cuda:0')



h[100].sum tensor(-0.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-21.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79773.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0915, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0899, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0892, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0922, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0922, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0922, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(794708.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(515.8925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.3558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.2244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1891],
        [-1.4310],
        [-1.6327],
        ...,
        [-2.1635],
        [-2.1573],
        [-2.1551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238679.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0288],
        [1.0243],
        ...,
        [0.9991],
        [0.9981],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370470.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0289],
        [1.0244],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370478.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3810e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3842e-03,
          1.7371e-04,  0.0000e+00],
        [-1.3810e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3842e-03,
          1.7371e-04,  0.0000e+00],
        [ 5.8973e-03, -1.7176e-03, -8.2306e-09,  ...,  1.3500e-02,
          6.2305e-03, -3.0226e-03],
        ...,
        [-1.3810e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3842e-03,
          1.7371e-04,  0.0000e+00],
        [-1.3810e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3842e-03,
          1.7371e-04,  0.0000e+00],
        [-1.3810e-03, -1.5940e-03,  0.0000e+00,  ...,  3.3842e-03,
          1.7371e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4902.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1613, device='cuda:0')



h[100].sum tensor(-0.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-24.2100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0138, 0.0007, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0323, 0.0117, 0.0000],
        [0.0290, 0.0000, 0.0000,  ..., 0.0601, 0.0284, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85062.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.1542, 0.0000, 0.0046],
        [0.0000, 0.0445, 0.0000,  ..., 0.2620, 0.0000, 0.0214],
        [0.0000, 0.1026, 0.0000,  ..., 0.4145, 0.0000, 0.0513],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0924, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0924, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0923, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(828251.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.1918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.1805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.4746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4992],
        [-0.0893],
        [ 0.1705],
        ...,
        [-2.1153],
        [-2.1585],
        [-2.1653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247848.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0289],
        [1.0244],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370478.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0291],
        [1.0245],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370487.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4358.7217, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8260, device='cuda:0')



h[100].sum tensor(-0.4212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-16.9644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1145, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73875.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0890, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0899, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(769687.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(490.7149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.7251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-461.6249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2885],
        [-2.3450],
        [-2.3886],
        ...,
        [-2.1815],
        [-2.1756],
        [-2.1735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252749.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0291],
        [1.0245],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370487.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0292],
        [1.0245],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370495.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5490e-02, -2.0508e-03, -2.6863e-08,  ...,  4.0767e-02,
          2.2551e-02, -1.1124e-02],
        [ 2.7297e-02, -2.0815e-03, -2.8668e-08,  ...,  4.3279e-02,
          2.4055e-02, -1.1872e-02],
        [ 2.1522e-02, -1.9833e-03, -2.2897e-08,  ...,  3.5252e-02,
          1.9248e-02, -9.4824e-03],
        ...,
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5413.1885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4676, device='cuda:0')



h[100].sum tensor(-0.7485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-30.3733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1088, 0.0000, 0.0000,  ..., 0.1731, 0.0960, 0.0000],
        [0.1130, 0.0000, 0.0000,  ..., 0.1791, 0.0996, 0.0000],
        [0.1246, 0.0000, 0.0000,  ..., 0.1951, 0.1092, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95530.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3203, 0.0000,  ..., 0.9539, 0.0000, 0.1631],
        [0.0000, 0.3539, 0.0000,  ..., 1.0373, 0.0000, 0.1802],
        [0.0000, 0.3746, 0.0000,  ..., 1.0878, 0.0000, 0.1904],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(892190.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(581.4184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.5854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.0916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2291],
        [ 0.2122],
        [ 0.2045],
        ...,
        [-2.1918],
        [-2.1859],
        [-2.1840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256112.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0292],
        [1.0245],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370495.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0292],
        [1.0245],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370495.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7093e-03, -1.7146e-03, -7.0947e-09,  ...,  1.3270e-02,
          6.0855e-03, -2.9381e-03],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        ...,
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00],
        [-1.3901e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4005e-03,
          1.7609e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4398.6436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8510, device='cuda:0')



h[100].sum tensor(-0.4263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-17.2995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0222, 0.0057, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0241, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74666.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.1455, 0.0000, 0.0008],
        [0.0000, 0.0025, 0.0000,  ..., 0.1338, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(774802.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.5681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-241.5132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.7517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7237],
        [-1.7759],
        [-1.7919],
        ...,
        [-2.1918],
        [-2.1859],
        [-2.1840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248465.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0292],
        [1.0245],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370495.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0293],
        [1.0246],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370503.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1592e-02, -1.8147e-03, -1.2190e-08,  ...,  2.1463e-02,
          1.0983e-02, -5.3646e-03],
        [ 1.3452e-02, -1.8463e-03, -1.3936e-08,  ...,  2.4049e-02,
          1.2532e-02, -6.1332e-03],
        [ 4.0180e-02, -2.3005e-03, -3.9027e-08,  ...,  6.1209e-02,
          3.4783e-02, -1.7175e-02],
        ...,
        [-1.3929e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4100e-03,
          1.7301e-04,  0.0000e+00],
        [-1.3929e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4100e-03,
          1.7301e-04,  0.0000e+00],
        [-1.3929e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4100e-03,
          1.7301e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5106.5366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2690, device='cuda:0')



h[100].sum tensor(-0.6423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-26.2627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0979, 0.0000, 0.0000,  ..., 0.1580, 0.0870, 0.0000],
        [0.1125, 0.0000, 0.0000,  ..., 0.1784, 0.0991, 0.0000],
        [0.0663, 0.0000, 0.0000,  ..., 0.1143, 0.0607, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89892.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2892, 0.0000,  ..., 0.8813, 0.0000, 0.1482],
        [0.0000, 0.3142, 0.0000,  ..., 0.9438, 0.0000, 0.1609],
        [0.0000, 0.2556, 0.0000,  ..., 0.7994, 0.0000, 0.1310],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0931, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0931, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0931, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(859040.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(558.4719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.3164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.8022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2632],
        [ 0.2559],
        [ 0.2598],
        ...,
        [-2.2017],
        [-2.1958],
        [-2.1939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243186.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0293],
        [1.0246],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370503.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0294],
        [1.0247],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370511.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5009.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2353, device='cuda:0')



h[100].sum tensor(-0.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-24.9479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0394, 0.0158, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86768.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.1282, 0.0000, 0.0009],
        [0.0000, 0.0226, 0.0000,  ..., 0.2105, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(839607., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(548.0323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.0546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.0015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1739],
        [-0.9116],
        [-0.4847],
        ...,
        [-2.2161],
        [-2.2101],
        [-2.2082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239900.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0294],
        [1.0247],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370511.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0295],
        [1.0247],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370519.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9132e-03, -1.7181e-03, -6.0402e-09,  ...,  1.3580e-02,
          6.2253e-03, -3.0068e-03],
        [ 3.3089e-03, -1.6739e-03, -3.8863e-09,  ...,  9.9583e-03,
          4.0566e-03, -1.9346e-03],
        [ 1.0612e-02, -1.7980e-03, -9.9265e-09,  ...,  2.0114e-02,
          1.0138e-02, -4.9414e-03],
        ...,
        [-1.3902e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4239e-03,
          1.4367e-04,  0.0000e+00],
        [-1.3902e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4239e-03,
          1.4367e-04,  0.0000e+00],
        [-1.3902e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4239e-03,
          1.4367e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4409.2178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8468, device='cuda:0')



h[100].sum tensor(-0.4145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-17.2099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0000, 0.0000,  ..., 0.0346, 0.0129, 0.0000],
        [0.0353, 0.0000, 0.0000,  ..., 0.0711, 0.0347, 0.0000],
        [0.0205, 0.0000, 0.0000,  ..., 0.0486, 0.0212, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75197.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0359, 0.0000,  ..., 0.2455, 0.0000, 0.0178],
        [0.0000, 0.0640, 0.0000,  ..., 0.3241, 0.0000, 0.0331],
        [0.0000, 0.0551, 0.0000,  ..., 0.3019, 0.0000, 0.0285],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0926, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0926, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0926, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(781911.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.2378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.1161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.0484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0297],
        [ 0.1828],
        [ 0.1365],
        ...,
        [-2.1857],
        [-2.1465],
        [-2.1157]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298524.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0295],
        [1.0247],
        ...,
        [0.9992],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370519.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0295],
        [1.0248],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370528.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5739.6689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6542, device='cuda:0')



h[100].sum tensor(-0.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0173, device='cuda:0')



h[200].sum tensor(-33.9441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0140, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0200, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97368.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0892, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.1346, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(884141.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.4615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.0468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2322],
        [-2.0387],
        [-1.7248],
        ...,
        [-2.0653],
        [-1.8968],
        [-1.7340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253876.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0295],
        [1.0248],
        ...,
        [0.9991],
        [0.9981],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370528.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0296],
        [1.0248],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370537.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0034,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4880.9639, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1348, device='cuda:0')



h[100].sum tensor(-0.5377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-22.6673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0000, 0.0000,  ..., 0.0343, 0.0128, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82815.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0276, 0.0000,  ..., 0.2257, 0.0000, 0.0146],
        [0.0000, 0.0096, 0.0000,  ..., 0.1631, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.1319, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0933, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0932, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0932, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(816603.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(533.1756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.1640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0959],
        [-0.3195],
        [-0.5036],
        ...,
        [-2.2325],
        [-2.2264],
        [-2.2244]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261577.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0296],
        [1.0248],
        ...,
        [0.9991],
        [0.9980],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370537.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 40.0 event: 200 loss: tensor(442.1225, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0291],
        [1.0297],
        [1.0249],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370545.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5673e-03, -1.6779e-03, -3.3601e-09,  ...,  1.0304e-02,
          4.2677e-03, -2.0205e-03],
        [ 3.5673e-03, -1.6779e-03, -3.3601e-09,  ...,  1.0304e-02,
          4.2677e-03, -2.0205e-03],
        [-1.3670e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4415e-03,
          1.5789e-04,  0.0000e+00],
        ...,
        [-1.3670e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4415e-03,
          1.5789e-04,  0.0000e+00],
        [-1.3670e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4415e-03,
          1.5789e-04,  0.0000e+00],
        [-1.3670e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4415e-03,
          1.5789e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4738.5103, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0128, device='cuda:0')



h[100].sum tensor(-0.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-20.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0268, 0.0083, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0083, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0083, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79921.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.1867, 0.0000, 0.0055],
        [0.0000, 0.0050, 0.0000,  ..., 0.1647, 0.0000, 0.0023],
        [0.0000, 0.0023, 0.0000,  ..., 0.1458, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0934, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0933, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0933, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(797275.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.1204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.2733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.1028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3848],
        [-0.8563],
        [-1.3359],
        ...,
        [-2.2298],
        [-2.2238],
        [-2.2218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241511.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0291],
        [1.0297],
        [1.0249],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370545.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0298],
        [1.0250],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370554.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3510e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4522e-03,
          1.5625e-04,  0.0000e+00],
        [ 2.0980e-02, -1.9736e-03, -1.4234e-08,  ...,  3.4510e-02,
          1.8755e-02, -9.1272e-03],
        [-1.3510e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4522e-03,
          1.5625e-04,  0.0000e+00],
        ...,
        [-1.3510e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4522e-03,
          1.5625e-04,  0.0000e+00],
        [-1.3510e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4522e-03,
          1.5625e-04,  0.0000e+00],
        [-1.3510e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4522e-03,
          1.5625e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5393.4956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3824, device='cuda:0')



h[100].sum tensor(-0.6640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-28.4253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0331, 0.0000, 0.0000,  ..., 0.0639, 0.0305, 0.0000],
        [0.0422, 0.0000, 0.0000,  ..., 0.0769, 0.0381, 0.0000],
        [0.0912, 0.0000, 0.0000,  ..., 0.1489, 0.0813, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92537.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0748, 0.0000,  ..., 0.3426, 0.0000, 0.0383],
        [0.0000, 0.1091, 0.0000,  ..., 0.4299, 0.0000, 0.0559],
        [0.0000, 0.1405, 0.0000,  ..., 0.5089, 0.0000, 0.0721],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0935, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(863587., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(571.2255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.6219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.6591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2104],
        [ 0.0382],
        [ 0.0358],
        ...,
        [-2.2285],
        [-2.2225],
        [-2.2206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230877.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0298],
        [1.0250],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370554.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0299],
        [1.0251],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370563.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5098e-02, -2.2135e-03, -2.1731e-08,  ...,  5.4143e-02,
          3.0512e-02, -1.4867e-02],
        [ 3.3110e-02, -2.1797e-03, -2.0546e-08,  ...,  5.1379e-02,
          2.8857e-02, -1.4056e-02],
        [ 3.3514e-02, -2.1866e-03, -2.0787e-08,  ...,  5.1941e-02,
          2.9193e-02, -1.4221e-02],
        ...,
        [-1.3419e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4605e-03,
          1.6122e-04,  0.0000e+00],
        [-1.3419e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4605e-03,
          1.6122e-04,  0.0000e+00],
        [-1.3419e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4605e-03,
          1.6122e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5067.6670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1853, device='cuda:0')



h[100].sum tensor(-0.5560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-23.9883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1353, 0.0000, 0.0000,  ..., 0.2100, 0.1179, 0.0000],
        [0.1488, 0.0000, 0.0000,  ..., 0.2289, 0.1292, 0.0000],
        [0.1460, 0.0000, 0.0000,  ..., 0.2251, 0.1269, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85841.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4126, 0.0000,  ..., 1.1854, 0.0000, 0.2117],
        [0.0000, 0.4299, 0.0000,  ..., 1.2290, 0.0000, 0.2205],
        [0.0000, 0.4057, 0.0000,  ..., 1.1697, 0.0000, 0.2080],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(828694.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.1806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.1828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.0007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2139],
        [ 0.2139],
        [ 0.2237],
        ...,
        [-2.2304],
        [-2.2244],
        [-2.2225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220135.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0299],
        [1.0251],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370563.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0253],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370571.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5348.9355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3651, device='cuda:0')



h[100].sum tensor(-0.6272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-27.2678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90155.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.1272, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0989, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0949, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(851457.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.7198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.9487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.9820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8018],
        [-1.2400],
        [-1.5846],
        ...,
        [-2.2297],
        [-2.2237],
        [-2.2218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237956.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0253],
        ...,
        [0.9991],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370571.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0254],
        ...,
        [0.9990],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370580.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [ 8.4377e-03, -1.7602e-03, -5.0913e-09,  ...,  1.7068e-02,
          8.3408e-03, -3.9731e-03],
        ...,
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4679.0220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9233, device='cuda:0')



h[100].sum tensor(-0.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-18.5996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0000, 0.0000,  ..., 0.0385, 0.0154, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0283, 0.0092, 0.0000],
        [0.0269, 0.0000, 0.0000,  ..., 0.0555, 0.0255, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78400.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0540, 0.0000,  ..., 0.2937, 0.0000, 0.0277],
        [0.0000, 0.0492, 0.0000,  ..., 0.2828, 0.0000, 0.0252],
        [0.0000, 0.0797, 0.0000,  ..., 0.3596, 0.0000, 0.0408],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(796925.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(511.0265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.6290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.5372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3416],
        [ 0.3397],
        [ 0.3340],
        ...,
        [-2.2325],
        [-2.2266],
        [-2.2248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235394.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0254],
        ...,
        [0.9990],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370580.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0254],
        ...,
        [0.9990],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370580.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0756e-02, -1.7996e-03, -6.2989e-09,  ...,  2.0292e-02,
          1.0272e-02, -4.9154e-03],
        [ 4.5235e-03, -1.6936e-03, -3.0524e-09,  ...,  1.1624e-02,
          5.0804e-03, -2.3820e-03],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        ...,
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00],
        [-1.3363e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4735e-03,
          1.9951e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5543.7646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4524, device='cuda:0')



h[100].sum tensor(-0.6743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-29.5421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0475, 0.0000, 0.0000,  ..., 0.0879, 0.0449, 0.0000],
        [0.0292, 0.0000, 0.0000,  ..., 0.0587, 0.0274, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0227, 0.0059, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93400.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1289, 0.0000,  ..., 0.4849, 0.0000, 0.0662],
        [0.0000, 0.0889, 0.0000,  ..., 0.3843, 0.0000, 0.0457],
        [0.0000, 0.0454, 0.0000,  ..., 0.2747, 0.0000, 0.0231],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(864047.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(572.7913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.6949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.3758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3078],
        [ 0.3222],
        [ 0.3293],
        ...,
        [-2.2325],
        [-2.2266],
        [-2.2248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206969.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0300],
        [1.0254],
        ...,
        [0.9990],
        [0.9980],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370580.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0301],
        [1.0255],
        ...,
        [0.9990],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370588.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4709.0283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9370, device='cuda:0')



h[100].sum tensor(-0.4286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-18.9233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77756.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0379, 0.0000,  ..., 0.2308, 0.0000, 0.0195],
        [0.0000, 0.0033, 0.0000,  ..., 0.1296, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.1002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(787209.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(509.6116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.0777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-452.4900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0930],
        [-0.3800],
        [-0.5892],
        ...,
        [-2.2386],
        [-2.2333],
        [-2.2319]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228628.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0301],
        [1.0255],
        ...,
        [0.9990],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370588.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0301],
        [1.0256],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370596.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3303e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4882e-03,
          1.9187e-04,  0.0000e+00],
        [-1.3303e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4882e-03,
          1.9187e-04,  0.0000e+00],
        [ 5.5197e-03, -1.7104e-03, -3.1078e-09,  ...,  1.3015e-02,
          5.8971e-03, -2.7741e-03],
        ...,
        [-1.3303e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4882e-03,
          1.9187e-04,  0.0000e+00],
        [-1.3303e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4882e-03,
          1.9187e-04,  0.0000e+00],
        [-1.3303e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4882e-03,
          1.9187e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4968.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1135, device='cuda:0')



h[100].sum tensor(-0.5003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-22.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0000, 0.0000,  ..., 0.0332, 0.0121, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0242, 0.0067, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0361, 0.0138, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82030.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0166, 0.0000,  ..., 0.2003, 0.0000, 0.0084],
        [0.0000, 0.0182, 0.0000,  ..., 0.2056, 0.0000, 0.0089],
        [0.0000, 0.0380, 0.0000,  ..., 0.2583, 0.0000, 0.0191],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0945, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0945, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0944, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(808180.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(531.2452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.8700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.5800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0152],
        [ 0.1323],
        [ 0.2416],
        ...,
        [-2.2592],
        [-2.2532],
        [-2.2514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243057.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0301],
        [1.0256],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370596.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0302],
        [1.0258],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4771.3818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9912, device='cuda:0')



h[100].sum tensor(-0.4434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-19.8845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0293, 0.0096, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78747.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1181, 0.0000, 0.0006],
        [0.0000, 0.0150, 0.0000,  ..., 0.1769, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(796308., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(519.6179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.7045, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2775],
        [-1.3367],
        [-1.1855],
        ...,
        [-2.2794],
        [-2.2732],
        [-2.2712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241052.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0302],
        [1.0258],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0303],
        [1.0259],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370612.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3190e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4974e-03,
          1.6836e-04,  0.0000e+00],
        [ 1.2464e-02, -1.8283e-03, -5.4289e-09,  ...,  2.2663e-02,
          1.1643e-02, -5.5600e-03],
        [-1.3190e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4974e-03,
          1.6836e-04,  0.0000e+00],
        ...,
        [-1.3190e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4974e-03,
          1.6836e-04,  0.0000e+00],
        [-1.3190e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4974e-03,
          1.6836e-04,  0.0000e+00],
        [-1.3190e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4974e-03,
          1.6836e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4718.3530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9703, device='cuda:0')



h[100].sum tensor(-0.4285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-19.3669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0174, 0.0000, 0.0000,  ..., 0.0423, 0.0174, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0306, 0.0104, 0.0000],
        [0.0612, 0.0000, 0.0000,  ..., 0.1071, 0.0562, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79630.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0363, 0.0000,  ..., 0.2507, 0.0000, 0.0181],
        [0.0000, 0.0379, 0.0000,  ..., 0.2543, 0.0000, 0.0190],
        [0.0000, 0.0895, 0.0000,  ..., 0.3842, 0.0000, 0.0454],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0939, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0939, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0938, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(810825., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.8156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-256.7888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-464.3367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1975],
        [ 0.1282],
        [ 0.0798],
        ...,
        [-2.2983],
        [-2.2920],
        [-2.2900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244598.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0303],
        [1.0259],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370612.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 50.0 event: 250 loss: tensor(461.2318, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0303],
        [1.0261],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370620.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3213e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5031e-03,
          1.7343e-04,  0.0000e+00],
        [-1.3213e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5031e-03,
          1.7343e-04,  0.0000e+00],
        [ 7.1450e-03, -1.7379e-03, -3.1034e-09,  ...,  1.5274e-02,
          7.2210e-03, -3.4086e-03],
        ...,
        [-1.3213e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5031e-03,
          1.7343e-04,  0.0000e+00],
        [-1.3213e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5031e-03,
          1.7343e-04,  0.0000e+00],
        [-1.3213e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5031e-03,
          1.7343e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6915.4287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.2103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.3101, device='cuda:0')



h[100].sum tensor(-1.0324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0241, device='cuda:0')



h[200].sum tensor(-47.0211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0007, 0.0000],
        [0.0107, 0.0000, 0.0000,  ..., 0.0332, 0.0119, 0.0000],
        [0.0273, 0.0000, 0.0000,  ..., 0.0582, 0.0269, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(120537.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.1477, 0.0000, 0.0029],
        [0.0000, 0.0349, 0.0000,  ..., 0.2406, 0.0000, 0.0167],
        [0.0000, 0.0803, 0.0000,  ..., 0.3649, 0.0000, 0.0402],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0940, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0939, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0939, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1024138.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(695.1675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-338.8271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.5418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5434],
        [-0.2096],
        [ 0.0894],
        ...,
        [-2.3144],
        [-2.3080],
        [-2.3059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214987.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0303],
        [1.0261],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370620.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0262],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370629.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5116.9917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2097, device='cuda:0')



h[100].sum tensor(-0.5328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-24.4561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89323.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1160, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1097, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0941, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0941, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(877416.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(568.8251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.6091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.0944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0754],
        [-1.2477],
        [-1.4407],
        ...,
        [-2.3260],
        [-2.3195],
        [-2.3173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292082.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0262],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370629.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0263],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370636.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5734e-02, -1.8839e-03, -5.4020e-09,  ...,  2.7237e-02,
          1.4387e-02, -6.8425e-03],
        [ 7.2329e-03, -1.7394e-03, -2.7101e-09,  ...,  1.5418e-02,
          7.3114e-03, -3.4327e-03],
        [ 7.2214e-03, -1.7392e-03, -2.7065e-09,  ...,  1.5402e-02,
          7.3019e-03, -3.4281e-03],
        ...,
        [-1.3253e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5202e-03,
          1.8807e-04,  0.0000e+00],
        [-1.3253e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5202e-03,
          1.8807e-04,  0.0000e+00],
        [-1.3253e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5202e-03,
          1.8807e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4968.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1276, device='cuda:0')



h[100].sum tensor(-0.4875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-22.5530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0407, 0.0000, 0.0000,  ..., 0.0785, 0.0391, 0.0000],
        [0.0425, 0.0000, 0.0000,  ..., 0.0812, 0.0407, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0368, 0.0141, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84582.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0780, 0.0000,  ..., 0.3591, 0.0000, 0.0394],
        [0.0000, 0.0678, 0.0000,  ..., 0.3339, 0.0000, 0.0341],
        [0.0000, 0.0315, 0.0000,  ..., 0.2404, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0944, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0944, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(840703.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(549.6470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.2556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.7876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3450],
        [ 0.1833],
        [-0.1393],
        ...,
        [-2.3301],
        [-2.3248],
        [-2.3240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249787.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0263],
        ...,
        [0.9990],
        [0.9979],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370636.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0264],
        ...,
        [0.9990],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370644.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6184e-02, -1.8917e-03, -5.1482e-09,  ...,  2.7872e-02,
          1.4783e-02, -7.0126e-03],
        [ 1.0808e-02, -1.8003e-03, -3.5677e-09,  ...,  2.0396e-02,
          1.0307e-02, -4.8598e-03],
        [ 1.5826e-02, -1.8856e-03, -5.0428e-09,  ...,  2.7373e-02,
          1.4484e-02, -6.8690e-03],
        ...,
        [-1.3296e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5206e-03,
          2.0309e-04,  0.0000e+00],
        [-1.3296e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5206e-03,
          2.0309e-04,  0.0000e+00],
        [-1.3296e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5206e-03,
          2.0309e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4918.6841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0990, device='cuda:0')



h[100].sum tensor(-0.4676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-21.8034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0000, 0.0000,  ..., 0.0763, 0.0379, 0.0000],
        [0.0447, 0.0000, 0.0000,  ..., 0.0843, 0.0426, 0.0000],
        [0.0272, 0.0000, 0.0000,  ..., 0.0582, 0.0270, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82657.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1151, 0.0000,  ..., 0.4570, 0.0000, 0.0587],
        [0.0000, 0.1124, 0.0000,  ..., 0.4504, 0.0000, 0.0573],
        [0.0000, 0.0883, 0.0000,  ..., 0.3899, 0.0000, 0.0450],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0949, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0949, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(828236.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(541.4982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.1288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-464.6245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4111],
        [ 0.4161],
        [ 0.4189],
        ...,
        [-2.3407],
        [-2.3341],
        [-2.3319]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249290.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0264],
        ...,
        [0.9990],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370644.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0265],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370653.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4703.8115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9432, device='cuda:0')



h[100].sum tensor(-0.4046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-19.0114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78574.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0909, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0914, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0918, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0949, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(810583.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.4374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.5522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2778],
        [-2.2224],
        [-2.0932],
        ...,
        [-2.3487],
        [-2.3421],
        [-2.3400]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280027.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0265],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370653.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0266],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370661.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5038.2183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1593, device='cuda:0')



h[100].sum tensor(-0.4864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-23.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1607, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82250.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0909, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0914, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.1130, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0949, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(819206., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.0615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.2338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-468.4663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2061],
        [-1.4247],
        [-1.3716],
        ...,
        [-2.3556],
        [-2.3492],
        [-2.3471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286048.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0266],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370661.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0306],
        [1.0267],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370670.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5729.7485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5681, device='cuda:0')



h[100].sum tensor(-0.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-31.4116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97057.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.1406, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1133, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0951, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0951, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0951, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(898840.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.5717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.4244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.1125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3914],
        [-0.7211],
        [-1.0484],
        ...,
        [-2.3560],
        [-2.3496],
        [-2.3476]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236006.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0306],
        [1.0267],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370670.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0306],
        [1.0269],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370678.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2353e-02, -1.9959e-03, -5.1044e-09,  ...,  3.6388e-02,
          1.9863e-02, -9.3973e-03],
        [ 5.3068e-03, -1.7062e-03, -1.4256e-09,  ...,  1.2694e-02,
          5.6805e-03, -2.6245e-03],
        [ 1.9403e-02, -1.9458e-03, -4.4677e-09,  ...,  3.2287e-02,
          1.7409e-02, -8.2251e-03],
        ...,
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5128e-03,
          1.8468e-04,  0.0000e+00],
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5128e-03,
          1.8468e-04,  0.0000e+00],
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5128e-03,
          1.8468e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6441.6211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.9899, device='cuda:0')



h[100].sum tensor(-0.8319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0208, device='cuda:0')



h[200].sum tensor(-40.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0321, 0.0000, 0.0000,  ..., 0.0645, 0.0307, 0.0000],
        [0.0840, 0.0000, 0.0000,  ..., 0.1387, 0.0751, 0.0000],
        [0.0516, 0.0000, 0.0000,  ..., 0.0937, 0.0482, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(113641.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1251, 0.0000,  ..., 0.4712, 0.0000, 0.0616],
        [0.0000, 0.2054, 0.0000,  ..., 0.6715, 0.0000, 0.1023],
        [0.0000, 0.2123, 0.0000,  ..., 0.6896, 0.0000, 0.1058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0954, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.1565, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1009390.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(665.5734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-323.0329, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.7588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2718],
        [ 0.2893],
        [ 0.2863],
        ...,
        [-1.9625],
        [-1.5008],
        [-0.8812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231738.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0306],
        [1.0269],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370678.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0307],
        [1.0270],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370687.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5574.6655, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4515, device='cuda:0')



h[100].sum tensor(-0.6002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-29.1068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93528.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0931, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0925, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0956, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0956, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0956, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(886822.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(581.7770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.4418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.7106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7771],
        [-2.0685],
        [-2.2786],
        ...,
        [-2.3594],
        [-2.3531],
        [-2.3511]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227164.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0307],
        [1.0270],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370687.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0307],
        [1.0271],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370696.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4905.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0367, device='cuda:0')



h[100].sum tensor(-0.4209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-20.5729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82892.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0922, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0932, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0964, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(834892.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(533.4799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.8537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-469.4669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4900],
        [-2.4345],
        [-2.3360],
        ...,
        [-2.3543],
        [-2.3482],
        [-2.3463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230322.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0307],
        [1.0271],
        ...,
        [0.9991],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370696.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 60.0 event: 300 loss: tensor(510.2315, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0272],
        ...,
        [0.9992],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370705., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5254.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2464, device='cuda:0')



h[100].sum tensor(-0.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-24.7963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0008, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0240, 0.0065, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86982.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0958, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.1712, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(846537.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(550.1547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.3456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.9048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5398],
        [-1.1329],
        [-0.6594],
        ...,
        [-2.3624],
        [-2.3562],
        [-2.3542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247398.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0272],
        ...,
        [0.9992],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370705., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0272],
        ...,
        [0.9992],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370705., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4531e-02, -1.8628e-03, -2.6738e-09,  ...,  2.5481e-02,
          1.3349e-02, -6.2503e-03],
        [ 1.9619e-02, -1.9492e-03, -3.5335e-09,  ...,  3.2550e-02,
          1.7579e-02, -8.2598e-03],
        [ 9.0401e-03, -1.7695e-03, -1.7462e-09,  ...,  1.7854e-02,
          8.7844e-03, -4.0819e-03],
        ...,
        [-1.2953e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4945e-03,
          1.9213e-04,  0.0000e+00],
        [-1.2953e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4945e-03,
          1.9213e-04,  0.0000e+00],
        [-1.2953e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4945e-03,
          1.9213e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4527.9111, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7943, device='cuda:0')



h[100].sum tensor(-0.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-15.8406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0916, 0.0000, 0.0000,  ..., 0.1489, 0.0813, 0.0000],
        [0.0761, 0.0000, 0.0000,  ..., 0.1276, 0.0685, 0.0000],
        [0.0640, 0.0000, 0.0000,  ..., 0.1089, 0.0573, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76176.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3209, 0.0000,  ..., 0.9440, 0.0000, 0.1574],
        [0.0000, 0.2894, 0.0000,  ..., 0.8680, 0.0000, 0.1418],
        [0.0000, 0.2222, 0.0000,  ..., 0.7035, 0.0000, 0.1085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(804175., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(505.1463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.3415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.4679, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1139],
        [ 0.1135],
        [ 0.1031],
        ...,
        [-2.3619],
        [-2.3559],
        [-2.3541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231984.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0272],
        ...,
        [0.9992],
        [0.9980],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370705., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0273],
        ...,
        [0.9992],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370712.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7658e-03, -1.6799e-03, -7.8577e-10,  ...,  1.0524e-02,
          4.3588e-03, -1.9947e-03],
        [ 1.1121e-02, -1.8049e-03, -1.9278e-09,  ...,  2.0741e-02,
          1.0472e-02, -4.8938e-03],
        [-1.2945e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4936e-03,
          1.5264e-04,  0.0000e+00],
        ...,
        [-1.2945e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4936e-03,
          1.5264e-04,  0.0000e+00],
        [-1.2945e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4936e-03,
          1.5264e-04,  0.0000e+00],
        [-1.2945e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4936e-03,
          1.5264e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5951.3154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7037, device='cuda:0')



h[100].sum tensor(-0.6735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0178, device='cuda:0')



h[200].sum tensor(-33.4444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0402, 0.0000, 0.0000,  ..., 0.0776, 0.0385, 0.0000],
        [0.0161, 0.0000, 0.0000,  ..., 0.0423, 0.0173, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0381, 0.0148, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96542.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0700, 0.0000,  ..., 0.3319, 0.0000, 0.0324],
        [0.0000, 0.0471, 0.0000,  ..., 0.2765, 0.0000, 0.0211],
        [0.0000, 0.0317, 0.0000,  ..., 0.2300, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0958, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(886600.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.8582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.5493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1041],
        [ 0.0338],
        [-0.2599],
        ...,
        [-2.3736],
        [-2.3467],
        [-2.3007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285413.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0308],
        [1.0273],
        ...,
        [0.9992],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370712.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0309],
        [1.0274],
        ...,
        [0.9992],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370720.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4709.9404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9185, device='cuda:0')



h[100].sum tensor(-0.3650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-18.2681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77866.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0993, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0933, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0927, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(807330.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(514.5945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.8023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.5219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6901],
        [-1.8160],
        [-1.8988],
        ...,
        [-2.3923],
        [-2.3860],
        [-2.3840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262917.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0309],
        [1.0274],
        ...,
        [0.9992],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370720.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0309],
        [1.0275],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370727.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3318e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4902e-03,
          1.2595e-04,  0.0000e+00],
        [ 2.9179e-02, -2.1127e-03, -3.9867e-09,  ...,  4.5917e-02,
          2.5528e-02, -1.1994e-02],
        [ 1.3042e-02, -1.8384e-03, -1.8782e-09,  ...,  2.3478e-02,
          1.2093e-02, -5.6506e-03],
        ...,
        [ 9.6492e-03, -1.7807e-03, -1.4348e-09,  ...,  1.8760e-02,
          9.2683e-03, -4.3166e-03],
        [ 6.3760e-03, -1.7250e-03, -1.0071e-09,  ...,  1.4208e-02,
          6.5432e-03, -3.0300e-03],
        [ 9.6492e-03, -1.7807e-03, -1.4348e-09,  ...,  1.8760e-02,
          9.2683e-03, -4.3166e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5038.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1395, device='cuda:0')



h[100].sum tensor(-0.4438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-22.3944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0994, 0.0000, 0.0000,  ..., 0.1600, 0.0878, 0.0000],
        [0.0364, 0.0000, 0.0000,  ..., 0.0708, 0.0343, 0.0000],
        [0.1010, 0.0000, 0.0000,  ..., 0.1626, 0.0892, 0.0000],
        ...,
        [0.0148, 0.0000, 0.0000,  ..., 0.0392, 0.0152, 0.0000],
        [0.0431, 0.0000, 0.0000,  ..., 0.0825, 0.0411, 0.0000],
        [0.0334, 0.0000, 0.0000,  ..., 0.0690, 0.0330, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81661.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2390, 0.0000,  ..., 0.7621, 0.0000, 0.1228],
        [0.0000, 0.1759, 0.0000,  ..., 0.6055, 0.0000, 0.0904],
        [0.0000, 0.2481, 0.0000,  ..., 0.7869, 0.0000, 0.1276],
        ...,
        [0.0000, 0.0296, 0.0000,  ..., 0.2384, 0.0000, 0.0134],
        [0.0000, 0.0637, 0.0000,  ..., 0.3282, 0.0000, 0.0315],
        [0.0000, 0.0726, 0.0000,  ..., 0.3512, 0.0000, 0.0361]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(819870.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(532.2170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.0206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-478.4743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3026],
        [ 0.3080],
        [ 0.2984],
        ...,
        [-0.9457],
        [-0.4462],
        [-0.0890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286456.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0309],
        [1.0275],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370727.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0310],
        [1.0276],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370735.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4706.2080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9274, device='cuda:0')



h[100].sum tensor(-0.3618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-18.4014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78508.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0107, 0.0000,  ..., 0.1650, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0961, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0960, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0960, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(815587.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.3409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.3563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.4412, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4613],
        [-0.8450],
        [-0.9932],
        ...,
        [-2.4151],
        [-2.4086],
        [-2.4067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288832.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0310],
        [1.0276],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370735.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0312],
        [1.0277],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370743.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0035,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4942.6646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0729, device='cuda:0')



h[100].sum tensor(-0.4147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-21.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83308.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.1328, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(838638.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.1796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.5493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.9417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2854],
        [-1.3748],
        [-1.1321],
        ...,
        [-2.4198],
        [-2.4142],
        [-2.4128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248830.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0312],
        [1.0277],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370743.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0313],
        [1.0279],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370751.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00],
        [-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00],
        [-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00],
        ...,
        [-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00],
        [-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00],
        [-1.3457e-03, -1.5940e-03,  0.0000e+00,  ...,  3.4954e-03,
          9.3402e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4399.7695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.1663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7443, device='cuda:0')



h[100].sum tensor(-0.2843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0078, device='cuda:0')



h[200].sum tensor(-14.6935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0004, 0.0000],
        [0.0031, 0.0000, 0.0000,  ..., 0.0207, 0.0041, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0270, 0.0079, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72143.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.1365, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.1688, 0.0000, 0.0029],
        [0.0000, 0.0155, 0.0000,  ..., 0.1971, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0958, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(783354.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.0065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.1717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.7805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5713],
        [-0.2564],
        [-0.0568],
        ...,
        [-2.4366],
        [-2.4300],
        [-2.4280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321829.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0313],
        [1.0279],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370751.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0306],
        [1.0314],
        [1.0280],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370759., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0630e-02, -1.7977e-03, -1.0805e-09,  ...,  2.0158e-02,
          1.0055e-02, -4.6743e-03],
        [ 3.6030e-03, -1.6781e-03, -4.4625e-10,  ...,  1.0380e-02,
          4.1968e-03, -1.9304e-03],
        [ 5.6862e-03, -1.7136e-03, -6.3427e-10,  ...,  1.3279e-02,
          5.9333e-03, -2.7438e-03],
        ...,
        [-1.3410e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5005e-03,
          7.5562e-05,  0.0000e+00],
        [-1.3410e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5005e-03,
          7.5562e-05,  0.0000e+00],
        [-1.3410e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5005e-03,
          7.5562e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4953.9126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0844, device='cuda:0')



h[100].sum tensor(-0.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-21.4295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0000, 0.0000,  ..., 0.0426, 0.0172, 0.0000],
        [0.0406, 0.0000, 0.0000,  ..., 0.0786, 0.0388, 0.0000],
        [0.0177, 0.0000, 0.0000,  ..., 0.0430, 0.0174, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81506.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0432, 0.0000,  ..., 0.2782, 0.0000, 0.0232],
        [0.0000, 0.0659, 0.0000,  ..., 0.3384, 0.0000, 0.0355],
        [0.0000, 0.0484, 0.0000,  ..., 0.2920, 0.0000, 0.0260],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0956, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0956, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(827560.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(532.9389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.0242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-485.5734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2111],
        [ 0.3379],
        [ 0.3738],
        ...,
        [-2.4459],
        [-2.4400],
        [-2.4385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329839.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0306],
        [1.0314],
        [1.0280],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370759., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0316],
        [1.0282],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370767.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00],
        [-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00],
        [-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00],
        ...,
        [-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00],
        [-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00],
        [-1.3260e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5083e-03,
          6.9616e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4878.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0295, device='cuda:0')



h[100].sum tensor(-0.3859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-20.2671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82693.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.1012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0925, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(840377.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.3475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.3525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-486.3046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7148],
        [-1.2234],
        [-1.6474],
        ...,
        [-2.4478],
        [-2.4412],
        [-2.4392]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258068.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0316],
        [1.0282],
        ...,
        [0.9993],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370767.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 70.0 event: 350 loss: tensor(445.6911, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0317],
        [1.0283],
        ...,
        [0.9994],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370775.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6396e-02, -1.8952e-03, -1.3081e-09,  ...,  2.8152e-02,
          1.4823e-02, -6.8861e-03],
        [ 2.2315e-02, -1.9959e-03, -1.7455e-09,  ...,  3.6387e-02,
          1.9756e-02, -9.1883e-03],
        [ 2.9137e-02, -2.1120e-03, -2.2495e-09,  ...,  4.5878e-02,
          2.5441e-02, -1.1842e-02],
        ...,
        [-1.3099e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5198e-03,
          6.9308e-05,  0.0000e+00],
        [-1.3099e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5198e-03,
          6.9308e-05,  0.0000e+00],
        [-1.3099e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5198e-03,
          6.9308e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5561.7920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4292, device='cuda:0')



h[100].sum tensor(-0.5347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-28.3028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0761, 0.0000, 0.0000,  ..., 0.1278, 0.0682, 0.0000],
        [0.0843, 0.0000, 0.0000,  ..., 0.1393, 0.0750, 0.0000],
        [0.1014, 0.0000, 0.0000,  ..., 0.1632, 0.0893, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92360.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1775, 0.0000,  ..., 0.6188, 0.0000, 0.0949],
        [0.0000, 0.2193, 0.0000,  ..., 0.7271, 0.0000, 0.1174],
        [0.0000, 0.2553, 0.0000,  ..., 0.8196, 0.0000, 0.1365],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0958, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0957, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(877805.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(572.3054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.2816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.9753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2982],
        [ 0.3193],
        [ 0.3323],
        ...,
        [-2.4476],
        [-2.4411],
        [-2.4391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253945.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0317],
        [1.0283],
        ...,
        [0.9994],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370775.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0318],
        [1.0284],
        ...,
        [0.9994],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370783.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00],
        [-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00],
        [-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00],
        ...,
        [-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00],
        [-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00],
        [-1.3020e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5324e-03,
          8.8679e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5066.5791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1199, device='cuda:0')



h[100].sum tensor(-0.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-22.0111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0363, 0.0135, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85140.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0251, 0.0000,  ..., 0.2229, 0.0000, 0.0116],
        [0.0000, 0.0054, 0.0000,  ..., 0.1520, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.1177, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(849828.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(540.1280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.3188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-480.7716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2788],
        [-0.7251],
        [-1.2016],
        ...,
        [-2.4452],
        [-2.4386],
        [-2.4357]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242749.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0318],
        [1.0284],
        ...,
        [0.9994],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370783.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0319],
        [1.0285],
        ...,
        [0.9994],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370791.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5465e-03,
          1.1629e-04,  0.0000e+00],
        [ 8.5578e-03, -1.7616e-03, -5.8855e-10,  ...,  1.7254e-02,
          8.3255e-03, -3.8178e-03],
        [ 5.8290e-03, -1.7152e-03, -4.2560e-10,  ...,  1.3459e-02,
          6.0527e-03, -2.7608e-03],
        ...,
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5465e-03,
          1.1629e-04,  0.0000e+00],
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5465e-03,
          1.1629e-04,  0.0000e+00],
        [-1.2986e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5465e-03,
          1.1629e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5090.7456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1229, device='cuda:0')



h[100].sum tensor(-0.4096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-22.0346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1556, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0213, 0.0000, 0.0000,  ..., 0.0478, 0.0204, 0.0000],
        [0.0190, 0.0000, 0.0000,  ..., 0.0448, 0.0185, 0.0000],
        [0.0450, 0.0000, 0.0000,  ..., 0.0848, 0.0425, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86903.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0422, 0.0000,  ..., 0.2717, 0.0000, 0.0216],
        [0.0000, 0.0474, 0.0000,  ..., 0.2869, 0.0000, 0.0243],
        [0.0000, 0.0802, 0.0000,  ..., 0.3726, 0.0000, 0.0416],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(863571., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.0304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.2505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-474.8770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2952],
        [ 0.3007],
        [ 0.3287],
        ...,
        [-2.4419],
        [-2.4355],
        [-2.4335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237614.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0319],
        [1.0285],
        ...,
        [0.9994],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370791.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0320],
        [1.0287],
        ...,
        [0.9994],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370799.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2981e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5604e-03,
          1.4328e-04,  0.0000e+00],
        [ 3.3641e-03, -1.6733e-03, -2.4886e-10,  ...,  1.0043e-02,
          4.0252e-03, -1.8022e-03],
        [ 3.3641e-03, -1.6733e-03, -2.4886e-10,  ...,  1.0043e-02,
          4.0252e-03, -1.8022e-03],
        ...,
        [-1.2981e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5604e-03,
          1.4328e-04,  0.0000e+00],
        [-1.2981e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5604e-03,
          1.4328e-04,  0.0000e+00],
        [-1.2981e-03, -1.5940e-03,  0.0000e+00,  ...,  3.5604e-03,
          1.4328e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5193.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1863, device='cuda:0')



h[100].sum tensor(-0.4255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-23.0728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0267, 0.0078, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0269, 0.0079, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0269, 0.0079, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86145.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.1455, 0.0000, 0.0006],
        [0.0000, 0.0075, 0.0000,  ..., 0.1768, 0.0000, 0.0039],
        [0.0000, 0.0297, 0.0000,  ..., 0.2448, 0.0000, 0.0154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(854148.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.7994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.5159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.6874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.7422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2129],
        [-0.5936],
        [-0.1079],
        ...,
        [-2.4418],
        [-2.4354],
        [-2.4335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222207.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0320],
        [1.0287],
        ...,
        [0.9994],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370799.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0321],
        [1.0288],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370807.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4760.9072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9095, device='cuda:0')



h[100].sum tensor(-0.3274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-17.9007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77942.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0932, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0938, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0974, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0974, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0973, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(813334.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(508.0419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7.0814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.2113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.0732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6323],
        [-2.6012],
        [-2.5390],
        ...,
        [-2.4552],
        [-2.4487],
        [-2.4468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299743.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0321],
        [1.0288],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370807.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0322],
        [1.0289],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370815.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5028.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0853, device='cuda:0')



h[100].sum tensor(-0.3847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-21.2004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0000, 0.0000,  ..., 0.0419, 0.0169, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87342.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0329, 0.0000,  ..., 0.2462, 0.0000, 0.0158],
        [0.0000, 0.0059, 0.0000,  ..., 0.1529, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.1149, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0973, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(885389.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(547.3137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.4245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-476.5095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5064],
        [-1.0583],
        [-1.5658],
        ...,
        [-2.4694],
        [-2.4628],
        [-2.4608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268714.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0322],
        [1.0289],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370815.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0313],
        [1.0324],
        [1.0290],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370822.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4829.8892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9672, device='cuda:0')



h[100].sum tensor(-0.3399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-18.8872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0279, 0.0084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81902.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.1774, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0973, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(848061.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.0012, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21.0248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.0192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-493.1982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9810],
        [-1.6064],
        [-2.0476],
        ...,
        [-2.4839],
        [-2.4773],
        [-2.4753]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265344.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0313],
        [1.0324],
        [1.0290],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370822.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0325],
        [1.0291],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370830.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0036,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4640.6040, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8571, device='cuda:0')



h[100].sum tensor(-0.2980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-16.6923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1188, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0005, 0.0000],
        [0.0188, 0.0000, 0.0000,  ..., 0.0449, 0.0185, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77824.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.1546, 0.0000, 0.0042],
        [0.0000, 0.0436, 0.0000,  ..., 0.2732, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(824304.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(510.1202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27.2369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.2421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-506.2404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6593],
        [-1.0067],
        [-0.3641],
        ...,
        [-2.4991],
        [-2.4925],
        [-2.4905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283530.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0325],
        [1.0291],
        ...,
        [0.9993],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370830.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0326],
        [1.0292],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370838.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7292e-03, -1.6966e-03, -1.6978e-10,  ...,  1.2038e-02,
          5.1586e-03, -2.3113e-03],
        [-1.3095e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6451e-03,
          1.3478e-04,  0.0000e+00],
        [-1.3095e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6451e-03,
          1.3478e-04,  0.0000e+00],
        ...,
        [-1.3095e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6451e-03,
          1.3478e-04,  0.0000e+00],
        [-1.3095e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6451e-03,
          1.3478e-04,  0.0000e+00],
        [-1.3095e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6451e-03,
          1.3478e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4640.3589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8518, device='cuda:0')



h[100].sum tensor(-0.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-16.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0000, 0.0000,  ..., 0.0433, 0.0175, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0237, 0.0057, 0.0000],
        [0.0313, 0.0000, 0.0000,  ..., 0.0624, 0.0289, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77740.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0621, 0.0000,  ..., 0.3285, 0.0000, 0.0318],
        [0.0000, 0.0468, 0.0000,  ..., 0.2891, 0.0000, 0.0239],
        [0.0000, 0.0840, 0.0000,  ..., 0.3837, 0.0000, 0.0436],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0971, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(822884.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(509.9516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42.4618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.4101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-508.8209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3395],
        [ 0.3256],
        [ 0.2999],
        ...,
        [-2.5052],
        [-2.4988],
        [-2.4974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282626.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0326],
        [1.0292],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370838.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0326],
        [1.0293],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370846.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4704.6660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8985, device='cuda:0')



h[100].sum tensor(-0.3073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-17.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79519.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0927, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0964, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(834910.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.2829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70.6623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.6363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.4749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3139],
        [-1.9722],
        [-1.4666],
        ...,
        [-2.5242],
        [-2.5175],
        [-2.5155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319468.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0326],
        [1.0293],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370846.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0326],
        [1.0293],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370846.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3161e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6639e-03,
          1.2853e-04,  0.0000e+00],
        [ 1.1359e-02, -1.8094e-03, -3.0632e-10,  ...,  2.1280e-02,
          1.0673e-02, -4.8421e-03],
        [ 5.0912e-03, -1.7029e-03, -1.5485e-10,  ...,  1.2569e-02,
          5.4589e-03, -2.4478e-03],
        ...,
        [-1.3161e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6639e-03,
          1.2853e-04,  0.0000e+00],
        [-1.3161e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6639e-03,
          1.2853e-04,  0.0000e+00],
        [-1.3161e-03, -1.5940e-03,  0.0000e+00,  ...,  3.6639e-03,
          1.2853e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5040.5796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0939, device='cuda:0')



h[100].sum tensor(-0.3778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-21.5077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0000, 0.0000,  ..., 0.0405, 0.0158, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0444, 0.0181, 0.0000],
        [0.0706, 0.0000, 0.0000,  ..., 0.1209, 0.0638, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82079.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0353, 0.0000,  ..., 0.2514, 0.0000, 0.0175],
        [0.0000, 0.0767, 0.0000,  ..., 0.3689, 0.0000, 0.0394],
        [0.0000, 0.1673, 0.0000,  ..., 0.6004, 0.0000, 0.0871],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0970, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0969, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(836587.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.0565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63.8367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.4524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.2252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3329],
        [ 0.0706],
        [ 0.2717],
        ...,
        [-2.5242],
        [-2.5175],
        [-2.5155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284580.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0326],
        [1.0293],
        ...,
        [0.9993],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370846.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0327],
        [1.0294],
        ...,
        [0.9993],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370854.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5360.3799, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2927, device='cuda:0')



h[100].sum tensor(-0.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-25.2121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92322.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0940, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0971, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(900431.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(570.2155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83.0242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.7222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.1639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1737],
        [-2.2944],
        [-2.3931],
        ...,
        [-2.5306],
        [-2.5238],
        [-2.5218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264762.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0327],
        [1.0294],
        ...,
        [0.9993],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370854.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0328],
        [1.0295],
        ...,
        [0.9992],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370862.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5747.4326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5108, device='cuda:0')



h[100].sum tensor(-0.5113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-29.5918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0216, 0.0045, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0284, 0.0085, 0.0000],
        [0.0033, 0.0000, 0.0000,  ..., 0.0218, 0.0046, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95779.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.1637, 0.0000, 0.0003],
        [0.0000, 0.0046, 0.0000,  ..., 0.1752, 0.0000, 0.0006],
        [0.0000, 0.0023, 0.0000,  ..., 0.1726, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0976, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(911763.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(582.9418, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(116.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.9218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.4962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1951],
        [-0.2525],
        [-0.2007],
        ...,
        [-2.5304],
        [-2.5237],
        [-2.5217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280886.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0328],
        [1.0295],
        ...,
        [0.9992],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370862.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0329],
        [1.0296],
        ...,
        [0.9992],
        [0.9980],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370870.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        ...,
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0013, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4928.2305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0167, device='cuda:0')



h[100].sum tensor(-0.3376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-19.6959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82820.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0217, 0.0000,  ..., 0.2100, 0.0000, 0.0107],
        [0.0000, 0.0047, 0.0000,  ..., 0.1645, 0.0000, 0.0019],
        [0.0000, 0.0021, 0.0000,  ..., 0.1446, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0978, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(850437.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.0988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(148.0118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.5034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.6978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1216],
        [ 0.0612],
        [ 0.0370],
        ...,
        [-2.5240],
        [-2.5171],
        [-2.5122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274337., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0329],
        [1.0296],
        ...,
        [0.9992],
        [0.9980],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370870.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0330],
        [1.0298],
        ...,
        [0.9992],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370878.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4455e-02, -2.2026e-03, -4.0577e-10,  ...,  5.3471e-02,
          2.9948e-02, -1.3576e-02],
        [ 2.4360e-02, -2.0310e-03, -2.9136e-10,  ...,  3.9439e-02,
          2.1549e-02, -9.7479e-03],
        [ 1.2216e-02, -1.8246e-03, -1.5372e-10,  ...,  2.2560e-02,
          1.1446e-02, -5.1431e-03],
        ...,
        [-1.3472e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7075e-03,
          1.6123e-04,  0.0000e+00],
        [-1.3472e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7075e-03,
          1.6123e-04,  0.0000e+00],
        [-1.3472e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7075e-03,
          1.6123e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4908.2412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9882, device='cuda:0')



h[100].sum tensor(-0.3288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-19.3417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1070, 0.0000, 0.0000,  ..., 0.1716, 0.0943, 0.0000],
        [0.0985, 0.0000, 0.0000,  ..., 0.1600, 0.0873, 0.0000],
        [0.0806, 0.0000, 0.0000,  ..., 0.1352, 0.0724, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80641.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2558, 0.0000,  ..., 0.8284, 0.0000, 0.1357],
        [0.0000, 0.2211, 0.0000,  ..., 0.7412, 0.0000, 0.1173],
        [0.0000, 0.1731, 0.0000,  ..., 0.6203, 0.0000, 0.0919],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0978, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(834333.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.1296, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(179.8822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.9921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-502.6829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2938],
        [ 0.3183],
        [ 0.3435],
        ...,
        [-2.5385],
        [-2.5319],
        [-2.5299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288865.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0330],
        [1.0298],
        ...,
        [0.9992],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370878.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0331],
        [1.0299],
        ...,
        [0.9992],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370886.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8861e-02, -2.2777e-03, -3.5278e-10,  ...,  5.9619e-02,
          3.3615e-02, -1.5221e-02],
        [ 1.0897e-02, -1.8023e-03, -1.0747e-10,  ...,  2.0748e-02,
          1.0348e-02, -4.6370e-03],
        [-1.3539e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7184e-03,
          1.5490e-04,  0.0000e+00],
        ...,
        [-1.3539e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7184e-03,
          1.5490e-04,  0.0000e+00],
        [-1.3539e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7184e-03,
          1.5490e-04,  0.0000e+00],
        [-1.3539e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7184e-03,
          1.5490e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4900.8169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9900, device='cuda:0')



h[100].sum tensor(-0.3229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-19.1535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0637, 0.0000, 0.0000,  ..., 0.1115, 0.0583, 0.0000],
        [0.0785, 0.0000, 0.0000,  ..., 0.1304, 0.0695, 0.0000],
        [0.0303, 0.0000, 0.0000,  ..., 0.0614, 0.0282, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81791.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1768, 0.0000,  ..., 0.6294, 0.0000, 0.0943],
        [0.0000, 0.1783, 0.0000,  ..., 0.6337, 0.0000, 0.0950],
        [0.0000, 0.1224, 0.0000,  ..., 0.4908, 0.0000, 0.0653],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(844035.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.0414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(194.0588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.7787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.3631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2660],
        [ 0.3021],
        [ 0.3233],
        ...,
        [-2.5452],
        [-2.5386],
        [-2.5366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275677.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0331],
        [1.0299],
        ...,
        [0.9992],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370886.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0332],
        [1.0300],
        ...,
        [0.9992],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370895., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0037,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5782.5586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5344, device='cuda:0')



h[100].sum tensor(-0.4909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0160, device='cuda:0')



h[200].sum tensor(-29.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96518.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(921074.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(579.5366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(189.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.3248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-465.6075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5890],
        [-2.6421],
        [-2.6605],
        ...,
        [-2.5408],
        [-2.5341],
        [-2.5320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246209.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0332],
        [1.0300],
        ...,
        [0.9992],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370895., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0333],
        [1.0301],
        ...,
        [0.9991],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370903.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3191e-02, -1.8414e-03, -6.2723e-11,  ...,  2.3969e-02,
          1.2273e-02, -5.4872e-03],
        [ 2.8963e-02, -2.1095e-03, -1.3070e-10,  ...,  4.5892e-02,
          2.5394e-02, -1.1434e-02],
        [ 2.2868e-02, -2.0059e-03, -1.0443e-10,  ...,  3.7419e-02,
          2.0324e-02, -9.1360e-03],
        ...,
        [-1.3615e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7402e-03,
          1.6570e-04,  0.0000e+00],
        [-1.3615e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7402e-03,
          1.6570e-04,  0.0000e+00],
        [-1.3615e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7402e-03,
          1.6570e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5554.4917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3545, device='cuda:0')



h[100].sum tensor(-0.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-26.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0708, 0.0000, 0.0000,  ..., 0.1216, 0.0643, 0.0000],
        [0.0761, 0.0000, 0.0000,  ..., 0.1291, 0.0687, 0.0000],
        [0.0956, 0.0000, 0.0000,  ..., 0.1562, 0.0849, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93362.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1395, 0.0000,  ..., 0.5336, 0.0000, 0.0745],
        [0.0000, 0.1784, 0.0000,  ..., 0.6357, 0.0000, 0.0953],
        [0.0000, 0.2063, 0.0000,  ..., 0.7078, 0.0000, 0.1100],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(899321.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(565.8816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(209.3746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.6617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-473.4987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1600],
        [ 0.2250],
        [ 0.2102],
        ...,
        [-2.5479],
        [-2.5413],
        [-2.5393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229805.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0333],
        [1.0301],
        ...,
        [0.9991],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370903.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0334],
        [1.0302],
        ...,
        [0.9991],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370909.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2913e-03, -1.6906e-03, -1.3527e-11,  ...,  1.1647e-02,
          4.9085e-03, -2.1382e-03],
        [ 1.7432e-02, -1.9141e-03, -4.4823e-11,  ...,  2.9917e-02,
          1.5846e-02, -7.0850e-03],
        [-1.3886e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7494e-03,
          1.8103e-04,  0.0000e+00],
        ...,
        [-1.3886e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7494e-03,
          1.8103e-04,  0.0000e+00],
        [-1.3886e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7494e-03,
          1.8103e-04,  0.0000e+00],
        [-1.3886e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7494e-03,
          1.8103e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6187.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7413, device='cuda:0')



h[100].sum tensor(-0.5569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0182, device='cuda:0')



h[200].sum tensor(-33.8616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0563, 0.0000, 0.0000,  ..., 0.1015, 0.0523, 0.0000],
        [0.0213, 0.0000, 0.0000,  ..., 0.0511, 0.0221, 0.0000],
        [0.0363, 0.0000, 0.0000,  ..., 0.0739, 0.0357, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102374.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1311, 0.0000,  ..., 0.5208, 0.0000, 0.0714],
        [0.0000, 0.0944, 0.0000,  ..., 0.4262, 0.0000, 0.0515],
        [0.0000, 0.0790, 0.0000,  ..., 0.3859, 0.0000, 0.0431],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0985, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0985, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0985, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940446.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(604.3446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(245.9849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.5318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-450.1923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3370],
        [ 0.3255],
        [ 0.2789],
        ...,
        [-2.5532],
        [-2.5467],
        [-2.5447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226802.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0334],
        [1.0302],
        ...,
        [0.9991],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370909.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0303],
        ...,
        [0.9991],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370916.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4040e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7660e-03,
          1.7677e-04,  0.0000e+00],
        [-1.4040e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7660e-03,
          1.7677e-04,  0.0000e+00],
        [ 4.6419e-03, -1.6968e-03, -3.8407e-12,  ...,  1.2174e-02,
          5.2099e-03, -2.2720e-03],
        ...,
        [-1.4040e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7660e-03,
          1.7677e-04,  0.0000e+00],
        [-1.4040e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7660e-03,
          1.7677e-04,  0.0000e+00],
        [-1.4040e-03, -1.5940e-03,  0.0000e+00,  ...,  3.7660e-03,
          1.7677e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6019.1587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6304, device='cuda:0')



h[100].sum tensor(-0.5200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0170, device='cuda:0')



h[200].sum tensor(-31.8807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0007, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0242, 0.0059, 0.0000],
        [0.0182, 0.0000, 0.0000,  ..., 0.0450, 0.0183, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101513.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1277, 0.0000, 0.0006],
        [0.0000, 0.0130, 0.0000,  ..., 0.1888, 0.0000, 0.0076],
        [0.0000, 0.0350, 0.0000,  ..., 0.2710, 0.0000, 0.0195],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0985, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0984, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0984, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(945117.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(602.4408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(281.9436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.0027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-452.8882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2410],
        [-0.6062],
        [-0.0876],
        ...,
        [-2.5656],
        [-2.5591],
        [-2.5571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238362.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0303],
        ...,
        [0.9991],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370916.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 90.0 event: 450 loss: tensor(956.7008, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0304],
        ...,
        [0.9991],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370922.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016,  0.0000,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5117.7285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0907, device='cuda:0')



h[100].sum tensor(-0.3454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-21.3497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88590.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0983, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0982, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0982, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(897007.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(549.4521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(293.7271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.3931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-485.1465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9907],
        [-1.0993],
        [-0.8998],
        ...,
        [-2.5790],
        [-2.5724],
        [-2.5704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247925.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0304],
        ...,
        [0.9991],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370922.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0304],
        ...,
        [0.9990],
        [0.9977],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370929.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0002,  ...,  0.0038,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4871.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9694, device='cuda:0')



h[100].sum tensor(-0.2981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-18.5823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80469.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.1338, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.1184, 0.0000, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.1518, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(835948.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(516.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(375.0776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.6590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-504.4878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9155],
        [-0.8557],
        [-0.4806],
        ...,
        [-2.5885],
        [-2.5804],
        [-2.5667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292924.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0334],
        [1.0304],
        ...,
        [0.9990],
        [0.9977],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370929.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0305],
        ...,
        [0.9990],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370936.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052, -0.0017, -0.0004,  ...,  0.0130,  0.0056, -0.0025],
        [ 0.0038, -0.0017, -0.0004,  ...,  0.0111,  0.0045, -0.0020],
        [ 0.0104, -0.0018, -0.0004,  ...,  0.0203,  0.0100, -0.0044],
        ...,
        [-0.0014, -0.0016, -0.0004,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0004,  ...,  0.0038,  0.0001,  0.0000],
        [-0.0014, -0.0016, -0.0004,  ...,  0.0038,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4629.7651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8135, device='cuda:0')



h[100].sum tensor(-0.2493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-15.6664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1128, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0000, 0.0000,  ..., 0.0437, 0.0174, 0.0000],
        [0.0385, 0.0000, 0.0000,  ..., 0.0775, 0.0376, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0441, 0.0176, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76925.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0325, 0.0000,  ..., 0.2629, 0.0000, 0.0188],
        [0.0000, 0.0576, 0.0000,  ..., 0.3352, 0.0000, 0.0332],
        [0.0000, 0.0370, 0.0000,  ..., 0.2804, 0.0000, 0.0217],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0983, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0982, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0982, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(822733.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(499.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(399.6269, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.1607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-512.1292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0434],
        [ 0.1274],
        [-0.0395],
        ...,
        [-2.5812],
        [-2.5596],
        [-2.4916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281538.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0305],
        ...,
        [0.9990],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370936.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0306],
        ...,
        [0.9990],
        [0.9976],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370944.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0005,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0005,  ...,  0.0038,  0.0002,  0.0000],
        [ 0.0058, -0.0017, -0.0006,  ...,  0.0139,  0.0062, -0.0027],
        ...,
        [-0.0014, -0.0016, -0.0005,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0005,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0005,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5418.6431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3015, device='cuda:0')



h[100].sum tensor(-0.3911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-24.7842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0000, 0.0000,  ..., 0.0460, 0.0188, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0262, 0.0069, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0339, 0.0114, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89843.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0411, 0.0000,  ..., 0.2816, 0.0000, 0.0237],
        [0.0000, 0.0148, 0.0000,  ..., 0.2143, 0.0000, 0.0099],
        [0.0000, 0.0241, 0.0000,  ..., 0.2411, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0986, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0986, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0986, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(884164.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(551.4174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(466.8049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.2059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-478.7710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0081],
        [-0.0941],
        [-0.0052],
        ...,
        [-2.5948],
        [-2.5882],
        [-2.5863]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283786.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0306],
        ...,
        [0.9990],
        [0.9976],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370944.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0306],
        ...,
        [0.9989],
        [0.9976],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370951.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0007,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0007,  ...,  0.0038,  0.0002,  0.0000],
        [ 0.0061, -0.0017, -0.0007,  ...,  0.0143,  0.0065, -0.0028],
        ...,
        [-0.0014, -0.0016, -0.0007,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0007,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0007,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5601.9141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3851, device='cuda:0')



h[100].sum tensor(-0.4179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-26.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0063, 0.0000, 0.0000,  ..., 0.0266, 0.0072, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0247, 0.0060, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95042.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1134, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.1478, 0.0000, 0.0020],
        [0.0000, 0.0054, 0.0000,  ..., 0.1752, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0991, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0991, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0991, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(917351.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(569.5082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(536.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.2782, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-464.0875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6583],
        [-1.1380],
        [-0.5921],
        ...,
        [-2.5901],
        [-2.5836],
        [-2.5817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257619.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0306],
        ...,
        [0.9989],
        [0.9976],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370951.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0307],
        ...,
        [0.9989],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370959.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0008,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6097.2441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6466, device='cuda:0')



h[100].sum tensor(-0.4986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0172, device='cuda:0')



h[200].sum tensor(-32.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101109.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0953, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0959, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0963, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0997, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(935687.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(589.1500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(623.5641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.6332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.5903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7111],
        [-2.6783],
        [-2.6034],
        ...,
        [-2.5727],
        [-2.5686],
        [-2.5675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224866.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0307],
        ...,
        [0.9989],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370959.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0307],
        ...,
        [0.9989],
        [0.9975],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370966.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048, -0.0017, -0.0010,  ...,  0.0125,  0.0054, -0.0023],
        [-0.0014, -0.0016, -0.0009,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0009,  ...,  0.0038,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0009,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0009,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0009,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5128.4023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0766, device='cuda:0')



h[100].sum tensor(-0.3177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-20.6458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0000, 0.0000,  ..., 0.0454, 0.0185, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0247, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84863.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0369, 0.0000,  ..., 0.2747, 0.0000, 0.0207],
        [0.0000, 0.0139, 0.0000,  ..., 0.1906, 0.0000, 0.0080],
        [0.0000, 0.0000, 0.0000,  ..., 0.1336, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(854613.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(519.1110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(796.9790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.8626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-485.7296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0964],
        [-0.6127],
        [-1.1851],
        ...,
        [-2.5789],
        [-2.5726],
        [-2.5708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239499.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0335],
        [1.0307],
        ...,
        [0.9989],
        [0.9975],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370966.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0336],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370973.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0347, -0.0022, -0.0014,  ...,  0.0541,  0.0303, -0.0134],
        [-0.0014, -0.0016, -0.0010,  ...,  0.0038,  0.0002,  0.0000],
        [ 0.0200, -0.0020, -0.0012,  ...,  0.0336,  0.0180, -0.0079],
        ...,
        [-0.0014, -0.0016, -0.0010,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0010,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0010,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5388.4980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2110, device='cuda:0')



h[100].sum tensor(-0.3589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-23.5171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0609, 0.0000, 0.0000,  ..., 0.1065, 0.0552, 0.0000],
        [0.0898, 0.0000, 0.0000,  ..., 0.1489, 0.0805, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0411, 0.0159, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89652.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1922, 0.0000,  ..., 0.6752, 0.0000, 0.1047],
        [0.0000, 0.1504, 0.0000,  ..., 0.5675, 0.0000, 0.0821],
        [0.0000, 0.0540, 0.0000,  ..., 0.3160, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(886376.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.1559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(880.8128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.9274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-474.2675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2424],
        [ 0.1054],
        [-0.2937],
        ...,
        [-2.5821],
        [-2.5758],
        [-2.5739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256761.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0336],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370973.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0336],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370981.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0011,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0011,  ...,  0.0038,  0.0002,  0.0000],
        [ 0.0087, -0.0018, -0.0012,  ...,  0.0179,  0.0087, -0.0037],
        ...,
        [-0.0014, -0.0016, -0.0011,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0011,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0011,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5749.5908, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4074, device='cuda:0')



h[100].sum tensor(-0.4170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-27.5497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0009, 0.0000],
        [0.0090, 0.0000, 0.0000,  ..., 0.0304, 0.0096, 0.0000],
        [0.0336, 0.0000, 0.0000,  ..., 0.0667, 0.0313, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98015.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.1639, 0.0000, 0.0041],
        [0.0000, 0.0526, 0.0000,  ..., 0.2980, 0.0000, 0.0278],
        [0.0000, 0.1342, 0.0000,  ..., 0.5260, 0.0000, 0.0726],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(929000.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(572.9202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(874.0114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.4799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.4536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2462],
        [-0.0157],
        [ 0.1104],
        ...,
        [-2.5848],
        [-2.5786],
        [-2.5768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230980.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0336],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370981.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0335],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370988.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0012,  ...,  0.0038,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4685.2554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.3072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7931, device='cuda:0')



h[100].sum tensor(-0.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-15.1386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0010, 0.0000],
        [0.0212, 0.0000, 0.0000,  ..., 0.0475, 0.0199, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0162, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77428.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.1399, 0.0000, 0.0013],
        [0.0000, 0.0303, 0.0000,  ..., 0.2440, 0.0000, 0.0160],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(824146.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.0190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1020.6976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.4138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-503.6558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9823],
        [-1.4066],
        [-0.7067],
        ...,
        [-2.4294],
        [-2.5450],
        [-2.5731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254167.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0335],
        [1.0308],
        ...,
        [0.9988],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370988.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 100.0 event: 500 loss: tensor(475.4550, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0335],
        [1.0308],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370994.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0013,  ...,  0.0038,  0.0003,  0.0000],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0038,  0.0003,  0.0000],
        [ 0.0047, -0.0017, -0.0013,  ...,  0.0124,  0.0054, -0.0023],
        ...,
        [-0.0014, -0.0016, -0.0013,  ...,  0.0038,  0.0003,  0.0000],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0038,  0.0003,  0.0000],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0038,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4781.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.3143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8435, device='cuda:0')



h[100].sum tensor(-0.2414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-16.2215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0010, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0247, 0.0063, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0232, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78888.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1091, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1366, 0.0000, 0.0008],
        [0.0000, 0.0007, 0.0000,  ..., 0.1507, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(832091.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.5524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1011.3665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.7689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-500.6979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9207],
        [-1.5730],
        [-1.1355],
        ...,
        [-2.5795],
        [-2.5361],
        [-2.4780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248429.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0335],
        [1.0308],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370994.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0335],
        [1.0309],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371001.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052, -0.0017, -0.0014,  ...,  0.0131,  0.0058, -0.0025],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0039,  0.0002,  0.0000],
        [ 0.0052, -0.0017, -0.0014,  ...,  0.0131,  0.0058, -0.0025],
        ...,
        [-0.0014, -0.0016, -0.0013,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0013,  ...,  0.0039,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5990.3877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5908, device='cuda:0')



h[100].sum tensor(-0.4461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-30.2261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0451, 0.0186, 0.0000],
        [0.0292, 0.0000, 0.0000,  ..., 0.0648, 0.0303, 0.0000],
        [0.0092, 0.0000, 0.0000,  ..., 0.0309, 0.0099, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101854.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0425, 0.0000,  ..., 0.2930, 0.0000, 0.0230],
        [0.0000, 0.0422, 0.0000,  ..., 0.2944, 0.0000, 0.0228],
        [0.0000, 0.0166, 0.0000,  ..., 0.2170, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(949599.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(768.8970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.5638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.8361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3316],
        [ 0.1408],
        [-0.2636],
        ...,
        [-2.6160],
        [-2.6097],
        [-2.6078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197456.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0335],
        [1.0309],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371001.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0335],
        [1.0309],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371007.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0014, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5492.2832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2886, device='cuda:0')



h[100].sum tensor(-0.3614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-24.6994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0009, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0242, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91108.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1260, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.1601, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(893164.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(550.3950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(899.2604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.2464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-478.3884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3437],
        [-1.2522],
        [-0.9999],
        ...,
        [-2.5404],
        [-2.6066],
        [-2.6299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271165.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0335],
        [1.0309],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371007.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0336],
        [1.0310],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371014.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [ 0.0079, -0.0018, -0.0016,  ...,  0.0169,  0.0080, -0.0034],
        [ 0.0066, -0.0017, -0.0016,  ...,  0.0151,  0.0069, -0.0029],
        ...,
        [-0.0015, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0015, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0015, -0.0016, -0.0014,  ...,  0.0039,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4573.0317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.6999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7430, device='cuda:0')



h[100].sum tensor(-0.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0077, device='cuda:0')



h[200].sum tensor(-14.2534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0294, 0.0089, 0.0000],
        [0.0200, 0.0000, 0.0000,  ..., 0.0482, 0.0200, 0.0000],
        [0.0510, 0.0000, 0.0000,  ..., 0.0954, 0.0483, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75954.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0269, 0.0000,  ..., 0.2461, 0.0000, 0.0141],
        [0.0000, 0.0534, 0.0000,  ..., 0.3182, 0.0000, 0.0285],
        [0.0000, 0.1004, 0.0000,  ..., 0.4435, 0.0000, 0.0538],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(822358.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(489.9602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(977.2128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.7004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-516.4865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1960],
        [ 0.2646],
        [ 0.2981],
        ...,
        [-2.6639],
        [-2.6565],
        [-2.6483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307323.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0336],
        [1.0310],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371014.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0336],
        [1.0311],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371021.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105, -0.0018, -0.0017,  ...,  0.0206,  0.0102, -0.0044],
        [ 0.0189, -0.0019, -0.0018,  ...,  0.0322,  0.0172, -0.0074],
        [ 0.0197, -0.0020, -0.0018,  ...,  0.0334,  0.0179, -0.0077],
        ...,
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0002,  0.0000],
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4946.3652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9718, device='cuda:0')



h[100].sum tensor(-0.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-18.4364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0590, 0.0000, 0.0000,  ..., 0.1065, 0.0550, 0.0000],
        [0.0812, 0.0000, 0.0000,  ..., 0.1375, 0.0735, 0.0000],
        [0.0712, 0.0000, 0.0000,  ..., 0.1236, 0.0652, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83357.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1700, 0.0000,  ..., 0.6276, 0.0000, 0.0921],
        [0.0000, 0.1845, 0.0000,  ..., 0.6650, 0.0000, 0.0999],
        [0.0000, 0.1511, 0.0000,  ..., 0.5772, 0.0000, 0.0817],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(866006.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(519.1870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1030.3595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.2262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.1985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3452],
        [ 0.3233],
        [ 0.2621],
        ...,
        [-2.6669],
        [-2.6603],
        [-2.6584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292861.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0336],
        [1.0311],
        ...,
        [0.9988],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371021.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0336],
        [1.0311],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371029.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0140, -0.0019, -0.0018,  ...,  0.0255,  0.0132, -0.0056],
        [ 0.0076, -0.0017, -0.0017,  ...,  0.0166,  0.0078, -0.0033],
        [ 0.0307, -0.0021, -0.0021,  ...,  0.0487,  0.0271, -0.0117],
        ...,
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0015,  ...,  0.0039,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5051., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0209, device='cuda:0')



h[100].sum tensor(-0.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-19.4535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0409, 0.0000, 0.0000,  ..., 0.0814, 0.0401, 0.0000],
        [0.0770, 0.0000, 0.0000,  ..., 0.1319, 0.0703, 0.0000],
        [0.0321, 0.0000, 0.0000,  ..., 0.0673, 0.0316, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83898.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0738, 0.0000,  ..., 0.3744, 0.0000, 0.0401],
        [0.0000, 0.1154, 0.0000,  ..., 0.4845, 0.0000, 0.0628],
        [0.0000, 0.0868, 0.0000,  ..., 0.4099, 0.0000, 0.0472],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(860944.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(518.1310, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1103.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.7180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-493.4835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3286],
        [ 0.0304],
        [ 0.2160],
        ...,
        [-2.6634],
        [-2.6569],
        [-2.6550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261332.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0336],
        [1.0311],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371029.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0337],
        [1.0312],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371036.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0039,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4923.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.1310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9314, device='cuda:0')



h[100].sum tensor(-0.2518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-17.7976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0000,  ..., 0.0265, 0.0074, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81414.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.1697, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(847371.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(504.8939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1283.8335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.1189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-496.5611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6443],
        [-1.1270],
        [-1.4536],
        ...,
        [-2.6582],
        [-2.6517],
        [-2.6498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243989.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0337],
        [1.0312],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371036.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0337],
        [1.0312],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371044.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0017, -0.0017,  ...,  0.0110,  0.0045, -0.0018],
        [ 0.0035, -0.0017, -0.0017,  ...,  0.0110,  0.0045, -0.0018],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6435.3335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8368, device='cuda:0')



h[100].sum tensor(-0.4906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0192, device='cuda:0')



h[200].sum tensor(-34.9755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0268, 0.0000, 0.0000,  ..., 0.0621, 0.0287, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0437, 0.0176, 0.0000],
        [0.0331, 0.0000, 0.0000,  ..., 0.0711, 0.0340, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(110020.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0627, 0.0000,  ..., 0.3562, 0.0000, 0.0349],
        [0.0000, 0.0537, 0.0000,  ..., 0.3327, 0.0000, 0.0299],
        [0.0000, 0.0689, 0.0000,  ..., 0.3730, 0.0000, 0.0381],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1001917.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(622.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1350.0253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-319.4757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-426.8756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4429],
        [ 0.4611],
        [ 0.4174],
        ...,
        [-2.6647],
        [-2.6582],
        [-2.6562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250037.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0337],
        [1.0312],
        ...,
        [0.9987],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371044.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0338],
        [1.0313],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371051.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0016,  ...,  0.0040,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5445.8467, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2452, device='cuda:0')



h[100].sum tensor(-0.3298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-23.7123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0013, 0.0000],
        [0.0216, 0.0000, 0.0000,  ..., 0.0487, 0.0205, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90583.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1314, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.1711, 0.0000, 0.0041],
        [0.0000, 0.0435, 0.0000,  ..., 0.2817, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(892886.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.2181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1395.5438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.5641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-479.4497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4439],
        [-0.2716],
        [-0.0075],
        ...,
        [-2.5946],
        [-2.6193],
        [-2.6434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247363.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0338],
        [1.0313],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371051.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0338],
        [1.0313],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371058.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        ...,
        [ 0.0122, -0.0018, -0.0019,  ...,  0.0230,  0.0116, -0.0049],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5883.5801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4910, device='cuda:0')



h[100].sum tensor(-0.3958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-28.7051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0012, 0.0000],
        ...,
        [0.0186, 0.0000, 0.0000,  ..., 0.0494, 0.0207, 0.0000],
        [0.0172, 0.0000, 0.0000,  ..., 0.0452, 0.0181, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100849.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1087, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0550, 0.0000,  ..., 0.3298, 0.0000, 0.0258],
        [0.0000, 0.0358, 0.0000,  ..., 0.2678, 0.0000, 0.0158],
        [0.0000, 0.0051, 0.0000,  ..., 0.1570, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(956156.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(589.4575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1430.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.6204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.6469, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5596],
        [-1.9423],
        [-2.1492],
        ...,
        [-0.0769],
        [-0.6568],
        [-1.4530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254608.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0338],
        [1.0313],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371058.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 110.0 event: 550 loss: tensor(475.3348, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0339],
        [1.0314],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371065.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0017, -0.0018,  ...,  0.0111,  0.0045, -0.0018],
        [ 0.0050, -0.0017, -0.0018,  ...,  0.0130,  0.0057, -0.0023],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5440.4160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2521, device='cuda:0')



h[100].sum tensor(-0.3227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-23.6049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0453, 0.0000, 0.0000,  ..., 0.0881, 0.0440, 0.0000],
        [0.0182, 0.0000, 0.0000,  ..., 0.0463, 0.0189, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0260, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90049.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0846, 0.0000,  ..., 0.3962, 0.0000, 0.0399],
        [0.0000, 0.0470, 0.0000,  ..., 0.2985, 0.0000, 0.0207],
        [0.0000, 0.0182, 0.0000,  ..., 0.2003, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1013, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(892377.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.6396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1608.8997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.4252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-488.7919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1635],
        [-0.1133],
        [-0.6323],
        ...,
        [-2.7062],
        [-2.6995],
        [-2.6975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265777.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0339],
        [1.0314],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371065.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0340],
        [1.0315],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371073.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [ 0.0036, -0.0017, -0.0018,  ...,  0.0111,  0.0045, -0.0018],
        ...,
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6022.4102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5728, device='cuda:0')



h[100].sum tensor(-0.4068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-30.0107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0224, 0.0048, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0473, 0.0196, 0.0000],
        [0.0334, 0.0000, 0.0000,  ..., 0.0697, 0.0330, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102662.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0143, 0.0000,  ..., 0.1916, 0.0000, 0.0055],
        [0.0000, 0.0430, 0.0000,  ..., 0.2911, 0.0000, 0.0180],
        [0.0000, 0.0731, 0.0000,  ..., 0.3697, 0.0000, 0.0328],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(961898.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(596.0368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1697.9799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.0124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.0099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6862],
        [-0.1296],
        [ 0.1983],
        ...,
        [-2.7076],
        [-2.7009],
        [-2.6989]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236632.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0340],
        [1.0315],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371073.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0341],
        [1.0315],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371080.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0017,  ...,  0.0040,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5048.1304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9914, device='cuda:0')



h[100].sum tensor(-0.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-18.7976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84377.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0977, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0984, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0987, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(868978.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.3088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2150.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.0922, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.2278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7102],
        [-2.5564],
        [-2.3095],
        ...,
        [-2.7057],
        [-2.6990],
        [-2.6969]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268053.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0341],
        [1.0315],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371080.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0341],
        [1.0316],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371087.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [ 0.0046, -0.0017, -0.0019,  ...,  0.0126,  0.0055, -0.0022],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5440.3213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2222, device='cuda:0')



h[100].sum tensor(-0.3065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-23.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0016, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0255, 0.0069, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0299, 0.0095, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91490.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1187, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.1683, 0.0000, 0.0027],
        [0.0000, 0.0160, 0.0000,  ..., 0.2141, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(905814.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(546.8101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2207.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.8323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.7581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8641],
        [-1.2654],
        [-0.7025],
        ...,
        [-2.7034],
        [-2.6967],
        [-2.6947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211308.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0341],
        [1.0316],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371087.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0342],
        [1.0317],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371093.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0040,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5284.1304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1282, device='cuda:0')



h[100].sum tensor(-0.2807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-21.2426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0168, 0.0016, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0334, 0.0115, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86648.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1346, 0.0000, 0.0000],
        [0.0000, 0.0140, 0.0000,  ..., 0.2022, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1030, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(877446., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.1688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2366.0349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.1056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.0890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0307],
        [-1.4973],
        [-0.9087],
        ...,
        [-2.6966],
        [-2.6853],
        [-2.6812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239375.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0342],
        [1.0317],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371093.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0342],
        [1.0318],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371100.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0314, -0.0022, -0.0024,  ...,  0.0498,  0.0277, -0.0118],
        [ 0.0067, -0.0017, -0.0020,  ...,  0.0155,  0.0072, -0.0029],
        [ 0.0252, -0.0020, -0.0023,  ...,  0.0413,  0.0226, -0.0096],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6423.2510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8022, device='cuda:0')



h[100].sum tensor(-0.4465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0188, device='cuda:0')



h[200].sum tensor(-34.0880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0543, 0.0000, 0.0000,  ..., 0.1010, 0.0519, 0.0000],
        [0.1308, 0.0000, 0.0000,  ..., 0.2076, 0.1155, 0.0000],
        [0.0808, 0.0000, 0.0000,  ..., 0.1381, 0.0740, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108759.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1948, 0.0000,  ..., 0.6708, 0.0000, 0.0881],
        [0.0000, 0.2779, 0.0000,  ..., 0.8768, 0.0000, 0.1273],
        [0.0000, 0.2513, 0.0000,  ..., 0.8115, 0.0000, 0.1147],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(990617.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.5727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2120.5532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-320.9111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.0757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2606],
        [ 0.2377],
        [ 0.2096],
        ...,
        [-2.7224],
        [-2.7073],
        [-2.6962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222874.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0342],
        [1.0318],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371100.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0342],
        [1.0318],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371106.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [ 0.0175, -0.0019, -0.0022,  ...,  0.0305,  0.0162, -0.0068],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5099.5356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.9277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0308, device='cuda:0')



h[100].sum tensor(-0.2503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-19.2717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0000, 0.0000,  ..., 0.0440, 0.0177, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0394, 0.0148, 0.0000],
        [0.0652, 0.0000, 0.0000,  ..., 0.1165, 0.0609, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84762.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0207, 0.0000,  ..., 0.2146, 0.0000, 0.0080],
        [0.0000, 0.0288, 0.0000,  ..., 0.2489, 0.0000, 0.0098],
        [0.0000, 0.0707, 0.0000,  ..., 0.3568, 0.0000, 0.0290],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(871070.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(522.2496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2456.6147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.6898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-504.4137, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4989],
        [-1.1131],
        [-0.8104],
        ...,
        [-2.7524],
        [-2.7455],
        [-2.7434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275864.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0342],
        [1.0318],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371106.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0319],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371113.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5155.5088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0486, device='cuda:0')



h[100].sum tensor(-0.2561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-19.8885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84664.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0975, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0994, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1027, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(868987.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(522.6125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2451.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.6973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-507.7702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3663],
        [-2.4427],
        [-2.4332],
        ...,
        [-2.7320],
        [-2.7546],
        [-2.7568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279001.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0319],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371113.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0320],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371120.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [ 0.0035, -0.0017, -0.0019,  ...,  0.0112,  0.0046, -0.0018],
        ...,
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0018,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5008.3271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.1207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9683, device='cuda:0')



h[100].sum tensor(-0.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-18.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0015, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0304, 0.0094, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0360, 0.0128, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81330.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1280, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.1863, 0.0000, 0.0023],
        [0.0000, 0.0180, 0.0000,  ..., 0.2250, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(849588.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(507.8893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2632.7251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-264.4586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-512.7165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5185],
        [-0.9438],
        [-0.4870],
        ...,
        [-2.7640],
        [-2.7571],
        [-2.7550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308998.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0320],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371120.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0320],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371127.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [ 0.0046, -0.0017, -0.0020,  ...,  0.0126,  0.0055, -0.0022],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [ 0.0148, -0.0019, -0.0022,  ...,  0.0269,  0.0140, -0.0058],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5638.1680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3181, device='cuda:0')



h[100].sum tensor(-0.3122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-24.6742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0016, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0258, 0.0069, 0.0000],
        [0.0179, 0.0000, 0.0000,  ..., 0.0464, 0.0192, 0.0000],
        ...,
        [0.0157, 0.0000, 0.0000,  ..., 0.0416, 0.0161, 0.0000],
        [0.0125, 0.0000, 0.0000,  ..., 0.0371, 0.0134, 0.0000],
        [0.0565, 0.0000, 0.0000,  ..., 0.1050, 0.0540, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94394.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.1592, 0.0000, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.2032, 0.0000, 0.0045],
        [0.0000, 0.0369, 0.0000,  ..., 0.2770, 0.0000, 0.0131],
        ...,
        [0.0000, 0.0169, 0.0000,  ..., 0.2102, 0.0000, 0.0065],
        [0.0000, 0.0234, 0.0000,  ..., 0.2406, 0.0000, 0.0074],
        [0.0000, 0.0604, 0.0000,  ..., 0.3387, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(922607.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(556.7173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2460.9092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.1529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-482.1673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0766],
        [ 0.1477],
        [ 0.2249],
        ...,
        [-1.6602],
        [-1.2155],
        [-0.9806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213490., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0320],
        ...,
        [0.9987],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371127.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 120.0 event: 600 loss: tensor(510.5203, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0321],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371134.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5272.0776, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0853, device='cuda:0')



h[100].sum tensor(-0.2552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-20.3418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89047.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(897990.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(532.8869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2600.2620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-277.7589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-491.2306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1094],
        [-2.5023],
        [-2.7273],
        ...,
        [-2.7470],
        [-2.7402],
        [-2.7381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252791., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0342],
        [1.0321],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371134.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0322],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371141.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5031.6670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9228, device='cuda:0')



h[100].sum tensor(-0.2165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-17.4058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0256, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0016, 0.0000],
        [0.0111, 0.0000, 0.0000,  ..., 0.0346, 0.0122, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84118., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1436, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1361, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.1804, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(871906.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(508.9453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2743.1890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.6846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.6217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0951],
        [-2.0897],
        [-1.8943],
        ...,
        [-2.7432],
        [-2.7363],
        [-2.7340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238417.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0322],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371141.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0323],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371148.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5617.1846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2589, device='cuda:0')



h[100].sum tensor(-0.2927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-23.7412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0169, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91759.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(901116.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(538.6516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2521.8269, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-484.5243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1564],
        [-1.8816],
        [-1.4295],
        ...,
        [-2.7466],
        [-2.7401],
        [-2.7382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196767.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0323],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371148.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0324],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371154.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0041,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5200.3740, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0209, device='cuda:0')



h[100].sum tensor(-0.2336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-19.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0500, 0.0000, 0.0000,  ..., 0.0931, 0.0469, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0295, 0.0089, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85045.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1174, 0.0000,  ..., 0.4760, 0.0000, 0.0496],
        [0.0000, 0.0600, 0.0000,  ..., 0.3317, 0.0000, 0.0231],
        [0.0000, 0.0390, 0.0000,  ..., 0.2665, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(869018.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2606.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.3112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-498.9806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3032],
        [ 0.2600],
        [ 0.1906],
        ...,
        [-2.7645],
        [-2.7575],
        [-2.7553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241949.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0324],
        ...,
        [0.9987],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371154.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0340],
        [1.0324],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371160.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5209.4102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0336, device='cuda:0')



h[100].sum tensor(-0.2338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-19.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86685.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1170, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(882242.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.0314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2292.0107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.7574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.0939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0280],
        [-2.4696],
        [-2.7514],
        ...,
        [-2.7842],
        [-2.7772],
        [-2.7750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251376.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0340],
        [1.0324],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371160.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0325],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371167.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5266.7466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0672, device='cuda:0')



h[100].sum tensor(-0.2402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-19.9998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86041.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.1538, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1394, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(873357., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.1757, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2184.4651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.1649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.1098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6219],
        [-0.7136],
        [-0.8431],
        ...,
        [-2.7991],
        [-2.7926],
        [-2.7911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252683.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0325],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371167.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0325],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371173.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5231.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0400, device='cuda:0')



h[100].sum tensor(-0.2323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-19.5071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84848.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0996, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1006, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(869403.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(519.8193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2249.0444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.9821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.7503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5445],
        [-2.5884],
        [-2.6023],
        ...,
        [-2.8076],
        [-2.8005],
        [-2.7982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292384.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0325],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371173.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371180.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5242.9473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0381, device='cuda:0')



h[100].sum tensor(-0.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-19.4752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86144.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1087, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.1716, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(882250.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.7421, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2006.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.1645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-500.2969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8321],
        [-0.4833],
        [-0.1536],
        ...,
        [-2.8076],
        [-2.8005],
        [-2.7984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242602.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371180.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371187.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5247.0576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0487, device='cuda:0')



h[100].sum tensor(-0.2270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-19.4002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0176, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85457.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1443, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1574, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(874835.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(526.2224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1964.7144, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.8406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-503.0764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2019],
        [-2.0576],
        [-1.7895],
        ...,
        [-2.3067],
        [-1.9299],
        [-1.6754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256316.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371187.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371194.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [ 0.0138, -0.0019, -0.0022,  ...,  0.0255,  0.0132, -0.0053],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6580.4131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8250, device='cuda:0')



h[100].sum tensor(-0.3938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0190, device='cuda:0')



h[200].sum tensor(-33.9585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0017, 0.0000],
        [0.0257, 0.0000, 0.0000,  ..., 0.0574, 0.0257, 0.0000],
        [0.0258, 0.0000, 0.0000,  ..., 0.0575, 0.0258, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(106667.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0096, 0.0000,  ..., 0.2051, 0.0000, 0.0014],
        [0.0000, 0.0480, 0.0000,  ..., 0.3036, 0.0000, 0.0187],
        [0.0000, 0.0690, 0.0000,  ..., 0.3574, 0.0000, 0.0281],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982023.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.0919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1674.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-311.7625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-457.0806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2484],
        [ 0.2865],
        [ 0.3151],
        ...,
        [-2.8167],
        [-2.8097],
        [-2.8075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226573.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0326],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371194.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 130.0 event: 650 loss: tensor(518.5384, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0327],
        ...,
        [0.9988],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371201.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0017, -0.0020,  ...,  0.0113,  0.0047, -0.0018],
        [ 0.0210, -0.0020, -0.0024,  ...,  0.0355,  0.0191, -0.0078],
        [ 0.0334, -0.0022, -0.0026,  ...,  0.0527,  0.0294, -0.0121],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5854.8423, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4021, device='cuda:0')



h[100].sum tensor(-0.2990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-26.0111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0696, 0.0000, 0.0000,  ..., 0.1225, 0.0647, 0.0000],
        [0.0792, 0.0000, 0.0000,  ..., 0.1360, 0.0727, 0.0000],
        [0.1072, 0.0000, 0.0000,  ..., 0.1751, 0.0960, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97801.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2240, 0.0000,  ..., 0.7441, 0.0000, 0.0988],
        [0.0000, 0.2470, 0.0000,  ..., 0.8026, 0.0000, 0.1092],
        [0.0000, 0.2825, 0.0000,  ..., 0.8911, 0.0000, 0.1255],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(947286.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(584.7230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1621.1300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.8647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-482.9241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3648],
        [ 0.3613],
        [ 0.3435],
        ...,
        [-2.8333],
        [-2.8263],
        [-2.8242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244689.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0327],
        ...,
        [0.9988],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371201.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0327],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371208.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0114, -0.0018, -0.0022,  ...,  0.0221,  0.0111, -0.0045],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [ 0.0339, -0.0022, -0.0026,  ...,  0.0533,  0.0298, -0.0122],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6465.3965, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7493, device='cuda:0')



h[100].sum tensor(-0.3727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0182, device='cuda:0')



h[200].sum tensor(-32.7061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0093, 0.0000, 0.0000,  ..., 0.0323, 0.0107, 0.0000],
        [0.0741, 0.0000, 0.0000,  ..., 0.1290, 0.0684, 0.0000],
        [0.0704, 0.0000, 0.0000,  ..., 0.1216, 0.0640, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102755.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0469, 0.0000,  ..., 0.2885, 0.0000, 0.0183],
        [0.0000, 0.1412, 0.0000,  ..., 0.5334, 0.0000, 0.0600],
        [0.0000, 0.1870, 0.0000,  ..., 0.6482, 0.0000, 0.0808],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1393, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(949731.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(609.8516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1572.1501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.4524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.8057, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2547],
        [ 0.1714],
        [ 0.3287],
        ...,
        [-2.7107],
        [-2.4696],
        [-2.0735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266668.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0327],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371208.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371214.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [ 0.0111, -0.0018, -0.0022,  ...,  0.0216,  0.0108, -0.0043],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4864.9448, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8195, device='cuda:0')



h[100].sum tensor(-0.1722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-15.2495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0017, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0420, 0.0164, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0403, 0.0154, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79588.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1369, 0.0000, 0.0000],
        [0.0000, 0.0187, 0.0000,  ..., 0.2159, 0.0000, 0.0062],
        [0.0000, 0.0266, 0.0000,  ..., 0.2480, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(854594.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(519.6888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1591.5719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.4432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-535.2196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4742],
        [-0.8439],
        [-0.3440],
        ...,
        [-2.8618],
        [-2.8548],
        [-2.8527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295895.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371214.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371221.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0212, -0.0020, -0.0024,  ...,  0.0356,  0.0192, -0.0078],
        [ 0.0213, -0.0020, -0.0024,  ...,  0.0359,  0.0194, -0.0079],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0042,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5759.7803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3453, device='cuda:0')



h[100].sum tensor(-0.2788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0140, device='cuda:0')



h[200].sum tensor(-24.9057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0894, 0.0000, 0.0000,  ..., 0.1499, 0.0810, 0.0000],
        [0.0646, 0.0000, 0.0000,  ..., 0.1135, 0.0592, 0.0000],
        [0.0576, 0.0000, 0.0000,  ..., 0.1059, 0.0546, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94940.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1907, 0.0000,  ..., 0.6528, 0.0000, 0.0814],
        [0.0000, 0.1925, 0.0000,  ..., 0.6595, 0.0000, 0.0823],
        [0.0000, 0.1809, 0.0000,  ..., 0.6328, 0.0000, 0.0770],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(932703.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(582.9965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1506.1421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.5911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-499.8551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3139],
        [ 0.3416],
        [ 0.3505],
        ...,
        [-2.8655],
        [-2.8585],
        [-2.8564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262015.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371221.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371228.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0017, -0.0020,  ...,  0.0110,  0.0045, -0.0017],
        [ 0.0052, -0.0017, -0.0021,  ...,  0.0135,  0.0060, -0.0023],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5071.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9226, device='cuda:0')



h[100].sum tensor(-0.1919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-17.2934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0000, 0.0000,  ..., 0.0798, 0.0392, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0380, 0.0142, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0270, 0.0076, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83035.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0896, 0.0000,  ..., 0.4043, 0.0000, 0.0354],
        [0.0000, 0.0383, 0.0000,  ..., 0.2684, 0.0000, 0.0135],
        [0.0000, 0.0107, 0.0000,  ..., 0.1785, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(874225.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.2589, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1620.6376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.0493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-527.9656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2073],
        [-0.1322],
        [-0.6647],
        ...,
        [-2.8678],
        [-2.8608],
        [-2.8589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268455.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0340],
        [1.0328],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371228.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371235.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5213.4204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0170, device='cuda:0')



h[100].sum tensor(-0.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-18.6590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0019, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0294, 0.0091, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85881.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.1411, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.1695, 0.0000, 0.0000],
        [0.0000, 0.0220, 0.0000,  ..., 0.2227, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(893907.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1558.4839, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.1478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-520.7617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4078],
        [-0.2218],
        [-0.0169],
        ...,
        [-2.8698],
        [-2.8628],
        [-2.8609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264544.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371235.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371242.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5204.3306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9877, device='cuda:0')



h[100].sum tensor(-0.2012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-18.4565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84817.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(880745.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(542.7625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1575.5962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.2303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-525.4902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2198],
        [-1.7281],
        [-2.1541],
        ...,
        [-2.8632],
        [-2.8620],
        [-2.8629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267470.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371242.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371249.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0098, -0.0018, -0.0022,  ...,  0.0197,  0.0097, -0.0038],
        [ 0.0112, -0.0018, -0.0022,  ...,  0.0218,  0.0109, -0.0043],
        [ 0.0115, -0.0018, -0.0022,  ...,  0.0222,  0.0112, -0.0044],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5268.3779, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0312, device='cuda:0')



h[100].sum tensor(-0.2061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-19.0761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0362, 0.0000, 0.0000,  ..., 0.0757, 0.0368, 0.0000],
        [0.0433, 0.0000, 0.0000,  ..., 0.0858, 0.0427, 0.0000],
        [0.0564, 0.0000, 0.0000,  ..., 0.1040, 0.0536, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86203.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1640, 0.0000,  ..., 0.5861, 0.0000, 0.0681],
        [0.0000, 0.1471, 0.0000,  ..., 0.5462, 0.0000, 0.0605],
        [0.0000, 0.1270, 0.0000,  ..., 0.4966, 0.0000, 0.0514],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(891288.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(550.1905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1494.4723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-524.9003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3086],
        [ 0.3239],
        [ 0.2944],
        ...,
        [-2.8849],
        [-2.8778],
        [-2.8757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262353.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0329],
        ...,
        [0.9988],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371249.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371256.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104, -0.0018, -0.0022,  ...,  0.0207,  0.0103, -0.0040],
        [ 0.0036, -0.0017, -0.0020,  ...,  0.0112,  0.0046, -0.0017],
        [ 0.0054, -0.0017, -0.0021,  ...,  0.0137,  0.0061, -0.0023],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5110.6636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9283, device='cuda:0')



h[100].sum tensor(-0.1847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-17.2480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0000, 0.0000,  ..., 0.0454, 0.0186, 0.0000],
        [0.0395, 0.0000, 0.0000,  ..., 0.0805, 0.0396, 0.0000],
        [0.0174, 0.0000, 0.0000,  ..., 0.0458, 0.0188, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82236.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0422, 0.0000,  ..., 0.2836, 0.0000, 0.0133],
        [0.0000, 0.0651, 0.0000,  ..., 0.3418, 0.0000, 0.0234],
        [0.0000, 0.0370, 0.0000,  ..., 0.2697, 0.0000, 0.0116],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(868102.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(535.6135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1706.3875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.2910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-534.7939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0783],
        [-0.0162],
        [-0.3600],
        ...,
        [-2.8911],
        [-2.8841],
        [-2.8823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305712.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371256.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0328],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371263.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040, -0.0017, -0.0020,  ...,  0.0118,  0.0050, -0.0019],
        [ 0.0109, -0.0018, -0.0022,  ...,  0.0214,  0.0107, -0.0042],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6174.2275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5508, device='cuda:0')



h[100].sum tensor(-0.3030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-28.5422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0382, 0.0000, 0.0000,  ..., 0.0786, 0.0385, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0456, 0.0187, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0417, 0.0164, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(105845.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0646, 0.0000,  ..., 0.3401, 0.0000, 0.0233],
        [0.0000, 0.0414, 0.0000,  ..., 0.2830, 0.0000, 0.0131],
        [0.0000, 0.0250, 0.0000,  ..., 0.2303, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1057, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1056, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1056, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1004402., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(633.4277, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1461.8213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.6607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-480.7959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1484],
        [-0.0627],
        [-0.5627],
        ...,
        [-2.8908],
        [-2.8836],
        [-2.8814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243898.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0328],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371263.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 140.0 event: 700 loss: tensor(469.5571, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0337],
        [1.0328],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371269.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0237, -0.0020, -0.0025,  ...,  0.0392,  0.0214, -0.0086],
        [ 0.0489, -0.0024, -0.0030,  ...,  0.0742,  0.0423, -0.0171],
        [ 0.0239, -0.0020, -0.0025,  ...,  0.0394,  0.0215, -0.0086],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0042,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5088.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9050, device='cuda:0')



h[100].sum tensor(-0.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-16.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1204, 0.0000, 0.0000,  ..., 0.1930, 0.1069, 0.0000],
        [0.1124, 0.0000, 0.0000,  ..., 0.1820, 0.1003, 0.0000],
        [0.1229, 0.0000, 0.0000,  ..., 0.1966, 0.1090, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81972.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2549, 0.0000,  ..., 0.8092, 0.0000, 0.1091],
        [0.0000, 0.2745, 0.0000,  ..., 0.8589, 0.0000, 0.1179],
        [0.0000, 0.2740, 0.0000,  ..., 0.8582, 0.0000, 0.1175],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(868544., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.8376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1791.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.3577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-536.5321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3608],
        [ 0.3506],
        [ 0.3542],
        ...,
        [-2.8896],
        [-2.8828],
        [-2.8809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275196.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0337],
        [1.0328],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371269.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0337],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371275.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [ 0.0048, -0.0017, -0.0021,  ...,  0.0129,  0.0056, -0.0021],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5227.6113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9808, device='cuda:0')



h[100].sum tensor(-0.1887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-18.0956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0273, 0.0000, 0.0000,  ..., 0.0636, 0.0296, 0.0000],
        [0.0038, 0.0000, 0.0000,  ..., 0.0249, 0.0064, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0266, 0.0074, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83988.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0332, 0.0000,  ..., 0.2641, 0.0000, 0.0097],
        [0.0000, 0.0080, 0.0000,  ..., 0.1824, 0.0000, 0.0010],
        [0.0000, 0.0027, 0.0000,  ..., 0.1731, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(875281.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.6175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1750.5483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.3052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-532.6865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0851],
        [-1.4467],
        [-1.6070],
        ...,
        [-2.8947],
        [-2.8878],
        [-2.8860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278602.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0337],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371275.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371282.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [ 0.0145, -0.0019, -0.0023,  ...,  0.0264,  0.0137, -0.0054],
        [ 0.0224, -0.0020, -0.0024,  ...,  0.0374,  0.0203, -0.0081],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5548.6338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1649, device='cuda:0')



h[100].sum tensor(-0.2219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-21.4702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0000, 0.0000,  ..., 0.0607, 0.0278, 0.0000],
        [0.0401, 0.0000, 0.0000,  ..., 0.0797, 0.0391, 0.0000],
        [0.0623, 0.0000, 0.0000,  ..., 0.1126, 0.0588, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89546.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1336, 0.0000,  ..., 0.5152, 0.0000, 0.0554],
        [0.0000, 0.1653, 0.0000,  ..., 0.5956, 0.0000, 0.0699],
        [0.0000, 0.2032, 0.0000,  ..., 0.6909, 0.0000, 0.0871],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(903629.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(570.1577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1640.7468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.1902, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-522.6120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2190],
        [ 0.2278],
        [ 0.2337],
        ...,
        [-2.8987],
        [-2.8916],
        [-2.8893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249999.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371282.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371288.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [ 0.0052, -0.0017, -0.0021,  ...,  0.0135,  0.0060, -0.0022],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5253.5981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9956, device='cuda:0')



h[100].sum tensor(-0.1870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-18.2603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0021, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0273, 0.0078, 0.0000],
        [0.0345, 0.0000, 0.0000,  ..., 0.0700, 0.0332, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85063.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.1632, 0.0000, 0.0018],
        [0.0000, 0.0322, 0.0000,  ..., 0.2473, 0.0000, 0.0113],
        [0.0000, 0.0890, 0.0000,  ..., 0.4040, 0.0000, 0.0350],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(885993., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(554.6430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1598.9438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.3969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-535.7672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5818],
        [-0.1113],
        [ 0.1976],
        ...,
        [-2.9086],
        [-2.9017],
        [-2.8999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271082.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0329],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371288.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371295.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0043,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4764.8198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.7876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7028, device='cuda:0')



h[100].sum tensor(-0.1320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0073, device='cuda:0')



h[200].sum tensor(-13.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0281, 0.0000, 0.0000,  ..., 0.0609, 0.0278, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77981.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0426, 0.0000,  ..., 0.2828, 0.0000, 0.0151],
        [0.0000, 0.0075, 0.0000,  ..., 0.1626, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.1246, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(855220.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.6250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1545.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.8381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.5629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5124],
        [-1.3198],
        [-2.0443],
        ...,
        [-2.9175],
        [-2.9105],
        [-2.9088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277042.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371295.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371301.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0116, -0.0018, -0.0022,  ...,  0.0224,  0.0113, -0.0044],
        [ 0.0248, -0.0020, -0.0025,  ...,  0.0408,  0.0223, -0.0088],
        [ 0.0366, -0.0022, -0.0027,  ...,  0.0572,  0.0321, -0.0128],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4899.9619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.2782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7867, device='cuda:0')



h[100].sum tensor(-0.1449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-14.4009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0815, 0.0000, 0.0000,  ..., 0.1395, 0.0746, 0.0000],
        [0.1191, 0.0000, 0.0000,  ..., 0.1919, 0.1059, 0.0000],
        [0.1789, 0.0000, 0.0000,  ..., 0.2752, 0.1556, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78577.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2755, 0.0000,  ..., 0.8681, 0.0000, 0.1198],
        [0.0000, 0.4072, 0.0000,  ..., 1.1940, 0.0000, 0.1792],
        [0.0000, 0.5437, 0.0000,  ..., 1.5303, 0.0000, 0.2405],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(851149.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.7510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1511.7473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.7857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.6172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2542],
        [ 0.1874],
        [ 0.1215],
        ...,
        [-2.9252],
        [-2.9182],
        [-2.9164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303875., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371301.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371307.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5297.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0201, device='cuda:0')



h[100].sum tensor(-0.1845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-18.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0285, 0.0083, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0020, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0312, 0.0098, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86522.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.1514, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.1374, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.1799, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(895983.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(569.4656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1374.4884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.5970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-540.1257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8629],
        [-1.6503],
        [-1.1899],
        ...,
        [-2.9280],
        [-2.9210],
        [-2.9192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285270., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0338],
        [1.0330],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371307.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0331],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371314.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0017, -0.0020,  ...,  0.0111,  0.0045, -0.0016],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5926.0459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3511, device='cuda:0')



h[100].sum tensor(-0.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-25.0829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0000, 0.0000,  ..., 0.0423, 0.0165, 0.0000],
        [0.0276, 0.0000, 0.0000,  ..., 0.0627, 0.0286, 0.0000],
        [0.0489, 0.0000, 0.0000,  ..., 0.0925, 0.0464, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96447.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0877, 0.0000,  ..., 0.4029, 0.0000, 0.0351],
        [0.0000, 0.1282, 0.0000,  ..., 0.5058, 0.0000, 0.0537],
        [0.0000, 0.1733, 0.0000,  ..., 0.6182, 0.0000, 0.0741],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940778.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(611.3914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1312.2905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.4034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-517.2753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3257],
        [ 0.3203],
        [ 0.3124],
        ...,
        [-2.9266],
        [-2.9196],
        [-2.9179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268223.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0331],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371314.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0331],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371314.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4993.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8265, device='cuda:0')



h[100].sum tensor(-0.1500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-15.1783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0291, 0.0086, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0237, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81881.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1555, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(874799.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(551.2946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1303.0643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.8140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.6208, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2483],
        [-2.3490],
        [-2.4911],
        ...,
        [-2.9311],
        [-2.9241],
        [-2.9223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263805.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0331],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371314.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0332],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371320.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047, -0.0017, -0.0021,  ...,  0.0129,  0.0056, -0.0020],
        [ 0.0045, -0.0017, -0.0021,  ...,  0.0127,  0.0054, -0.0020],
        [ 0.0106, -0.0018, -0.0022,  ...,  0.0212,  0.0105, -0.0040],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5469.4409, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0927, device='cuda:0')



h[100].sum tensor(-0.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-20.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0175, 0.0000, 0.0000,  ..., 0.0465, 0.0190, 0.0000],
        [0.0367, 0.0000, 0.0000,  ..., 0.0776, 0.0376, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0469, 0.0192, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88000.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0442, 0.0000,  ..., 0.2916, 0.0000, 0.0157],
        [0.0000, 0.0758, 0.0000,  ..., 0.3772, 0.0000, 0.0299],
        [0.0000, 0.0644, 0.0000,  ..., 0.3487, 0.0000, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(896482.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(576.2682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1268.0442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.4716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-537.2321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2834],
        [ 0.3900],
        [ 0.4408],
        ...,
        [-2.9289],
        [-2.9221],
        [-2.9205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249305.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0338],
        [1.0332],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371320.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 150.0 event: 750 loss: tensor(410.8122, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0332],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371327.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5353.5459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0350, device='cuda:0')



h[100].sum tensor(-0.1812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-18.6735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86534.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(894347.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(570.5192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1433.1183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.6730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-539.0951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4022],
        [-2.4226],
        [-2.4077],
        ...,
        [-2.9286],
        [-2.9229],
        [-2.9223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284135.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0332],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371327.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0333],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371333.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5006.0767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8168, device='cuda:0')



h[100].sum tensor(-0.1433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-14.9009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0236, 0.0054, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0293, 0.0088, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0238, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82325.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.1561, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.1718, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.1538, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(882026.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(554.1541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1332.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.1116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-550.6093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3766],
        [-1.1533],
        [-1.0721],
        ...,
        [-2.9368],
        [-2.9298],
        [-2.9281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257864.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0333],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371333.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0333],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371339.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0113, -0.0018, -0.0022,  ...,  0.0222,  0.0111, -0.0043],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5852.4443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2990, device='cuda:0')



h[100].sum tensor(-0.2269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-23.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0000, 0.0000,  ..., 0.0281, 0.0080, 0.0000],
        [0.0345, 0.0000, 0.0000,  ..., 0.0726, 0.0345, 0.0000],
        [0.0545, 0.0000, 0.0000,  ..., 0.1004, 0.0511, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93920., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0404, 0.0000,  ..., 0.2738, 0.0000, 0.0157],
        [0.0000, 0.1169, 0.0000,  ..., 0.4829, 0.0000, 0.0492],
        [0.0000, 0.1872, 0.0000,  ..., 0.6613, 0.0000, 0.0815],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(925043.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(604.6172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1269.9426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.9259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-524.2017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0180],
        [ 0.3065],
        [ 0.4074],
        ...,
        [-2.9451],
        [-2.9383],
        [-2.9368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254911.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0333],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371339.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0334],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371346.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070, -0.0017, -0.0021,  ...,  0.0163,  0.0076, -0.0028],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5028.5947, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.9735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8301, device='cuda:0')



h[100].sum tensor(-0.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-15.0656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0249, 0.0000, 0.0000,  ..., 0.0570, 0.0252, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0306, 0.0094, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80935.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0758, 0.0000,  ..., 0.3760, 0.0000, 0.0307],
        [0.0000, 0.0270, 0.0000,  ..., 0.2328, 0.0000, 0.0100],
        [0.0000, 0.0000, 0.0000,  ..., 0.1459, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(866046.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(553.6037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1284.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.3544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.7817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2178],
        [-0.1448],
        [-0.6738],
        ...,
        [-2.9547],
        [-2.9477],
        [-2.9459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275968.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0339],
        [1.0334],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371346.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0340],
        [1.0335],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371352.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0213, -0.0020, -0.0024,  ...,  0.0361,  0.0194, -0.0075],
        [ 0.0252, -0.0020, -0.0025,  ...,  0.0415,  0.0226, -0.0088],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5099.8564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.9097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8596, device='cuda:0')



h[100].sum tensor(-0.1470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-15.7097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0219, 0.0000, 0.0000,  ..., 0.0507, 0.0214, 0.0000],
        [0.0489, 0.0000, 0.0000,  ..., 0.0905, 0.0452, 0.0000],
        [0.1170, 0.0000, 0.0000,  ..., 0.1896, 0.1043, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82736.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0605, 0.0000,  ..., 0.3239, 0.0000, 0.0232],
        [0.0000, 0.1309, 0.0000,  ..., 0.5144, 0.0000, 0.0548],
        [0.0000, 0.2206, 0.0000,  ..., 0.7429, 0.0000, 0.0960],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1077, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(877459.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(562.2678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1358.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.2993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-551.5430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0116],
        [ 0.3106],
        [ 0.4423],
        ...,
        [-2.9613],
        [-2.9542],
        [-2.9525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301313., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0340],
        [1.0335],
        ...,
        [0.9989],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371352.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0340],
        [1.0335],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371360., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5879.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3069, device='cuda:0')



h[100].sum tensor(-0.2197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-23.6933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92311.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0679e-03, 0.0000e+00,  ..., 1.5425e-01, 0.0000e+00,
         1.2264e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1986e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0626e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0802e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0797e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0799e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(910886.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.2985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1215.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.2764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-529.8393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7538],
        [-1.3842],
        [-1.9051],
        ...,
        [-2.9505],
        [-2.9443],
        [-2.9436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252412.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0340],
        [1.0335],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371360., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0341],
        [1.0336],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371366.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5257.6323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.0665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9376, device='cuda:0')



h[100].sum tensor(-0.1562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-16.9961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84282.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1120, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1084, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1084, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(879841.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(566.7040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1198.3025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.9879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.2366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6996],
        [-2.5118],
        [-2.1107],
        ...,
        [-2.9581],
        [-2.9512],
        [-2.9495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251446.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0341],
        [1.0336],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371366.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0341],
        [1.0337],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371373.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056, -0.0017, -0.0021,  ...,  0.0142,  0.0064, -0.0023],
        [ 0.0152, -0.0019, -0.0023,  ...,  0.0275,  0.0143, -0.0055],
        [ 0.0195, -0.0020, -0.0024,  ...,  0.0336,  0.0179, -0.0069],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7058.7988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.9124, device='cuda:0')



h[100].sum tensor(-0.3250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0199, device='cuda:0')



h[200].sum tensor(-35.7016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0293, 0.0000, 0.0000,  ..., 0.0630, 0.0289, 0.0000],
        [0.0668, 0.0000, 0.0000,  ..., 0.1194, 0.0625, 0.0000],
        [0.0819, 0.0000, 0.0000,  ..., 0.1405, 0.0752, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(114547.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0982, 0.0000,  ..., 0.4294, 0.0000, 0.0385],
        [0.0000, 0.1803, 0.0000,  ..., 0.6384, 0.0000, 0.0759],
        [0.0000, 0.2383, 0.0000,  ..., 0.7849, 0.0000, 0.1023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1088, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1088, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1031742.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(689.1516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1296.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-331.4603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-473.4262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4017],
        [ 0.3833],
        [ 0.3600],
        ...,
        [-2.9423],
        [-2.9438],
        [-2.9450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255493.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0341],
        [1.0337],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371373.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0342],
        [1.0337],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371380.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0184, -0.0019, -0.0024,  ...,  0.0319,  0.0170, -0.0065],
        [ 0.0333, -0.0022, -0.0027,  ...,  0.0527,  0.0294, -0.0114],
        [ 0.0174, -0.0019, -0.0023,  ...,  0.0305,  0.0162, -0.0062],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5234.3350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9098, device='cuda:0')



h[100].sum tensor(-0.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-16.4268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0939, 0.0000, 0.0000,  ..., 0.1569, 0.0852, 0.0000],
        [0.0979, 0.0000, 0.0000,  ..., 0.1627, 0.0886, 0.0000],
        [0.1039, 0.0000, 0.0000,  ..., 0.1710, 0.0935, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83384.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1892, 0.0000,  ..., 0.6581, 0.0000, 0.0793],
        [0.0000, 0.2270, 0.0000,  ..., 0.7556, 0.0000, 0.0964],
        [0.0000, 0.2331, 0.0000,  ..., 0.7717, 0.0000, 0.0990],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(874526.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.5080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1388.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.2953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-547.3745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2679],
        [ 0.4040],
        [ 0.4530],
        ...,
        [-2.9554],
        [-2.9487],
        [-2.9471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245485.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0342],
        [1.0337],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371380.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0342],
        [1.0338],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371387.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0238, -0.0020, -0.0025,  ...,  0.0394,  0.0215, -0.0083],
        [ 0.0475, -0.0024, -0.0030,  ...,  0.0725,  0.0412, -0.0161],
        [ 0.0465, -0.0024, -0.0029,  ...,  0.0710,  0.0404, -0.0157],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5297.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9343, device='cuda:0')



h[100].sum tensor(-0.1515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-16.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1215, 0.0000, 0.0000,  ..., 0.1952, 0.1081, 0.0000],
        [0.1638, 0.0000, 0.0000,  ..., 0.2543, 0.1433, 0.0000],
        [0.1719, 0.0000, 0.0000,  ..., 0.2655, 0.1500, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85623.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2907, 0.0000,  ..., 0.9101, 0.0000, 0.1241],
        [0.0000, 0.3882, 0.0000,  ..., 1.1561, 0.0000, 0.1679],
        [0.0000, 0.4032, 0.0000,  ..., 1.1945, 0.0000, 0.1745],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(891883.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(566.5046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1448.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.1007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-542.1782, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2572],
        [ 0.3500],
        [ 0.3894],
        ...,
        [-2.9567],
        [-2.9497],
        [-2.9469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233198.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0342],
        [1.0338],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371387.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 160.0 event: 800 loss: tensor(484.6666, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0343],
        [1.0338],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371393.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090, -0.0018, -0.0022,  ...,  0.0188,  0.0091, -0.0034],
        [ 0.0236, -0.0020, -0.0025,  ...,  0.0392,  0.0213, -0.0082],
        [ 0.0200, -0.0020, -0.0024,  ...,  0.0342,  0.0183, -0.0070],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6010.7363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3482, device='cuda:0')



h[100].sum tensor(-0.2160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-24.3821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0659, 0.0000, 0.0000,  ..., 0.1178, 0.0617, 0.0000],
        [0.0658, 0.0000, 0.0000,  ..., 0.1179, 0.0617, 0.0000],
        [0.0689, 0.0000, 0.0000,  ..., 0.1222, 0.0643, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95785.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1249, 0.0000,  ..., 0.4948, 0.0000, 0.0492],
        [0.0000, 0.1443, 0.0000,  ..., 0.5450, 0.0000, 0.0578],
        [0.0000, 0.1430, 0.0000,  ..., 0.5421, 0.0000, 0.0571],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940635.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.2145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1380.0037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.8464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-518.7965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3786],
        [ 0.4217],
        [ 0.4109],
        ...,
        [-2.9700],
        [-2.9651],
        [-2.9643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238125.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0343],
        [1.0338],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371393.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0344],
        [1.0339],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371399.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0126, -0.0018, -0.0022,  ...,  0.0238,  0.0121, -0.0046],
        [ 0.0112, -0.0018, -0.0022,  ...,  0.0219,  0.0110, -0.0041],
        [ 0.0123, -0.0018, -0.0022,  ...,  0.0235,  0.0119, -0.0045],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0143, -0.0019, -0.0023,  ...,  0.0262,  0.0136, -0.0051],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5769.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2236, device='cuda:0')



h[100].sum tensor(-0.1925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-21.9357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1696, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0438, 0.0000, 0.0000,  ..., 0.0872, 0.0433, 0.0000],
        [0.0454, 0.0000, 0.0000,  ..., 0.0895, 0.0447, 0.0000],
        [0.0438, 0.0000, 0.0000,  ..., 0.0874, 0.0434, 0.0000],
        ...,
        [0.0152, 0.0000, 0.0000,  ..., 0.0419, 0.0160, 0.0000],
        [0.0121, 0.0000, 0.0000,  ..., 0.0377, 0.0135, 0.0000],
        [0.0546, 0.0000, 0.0000,  ..., 0.1031, 0.0525, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91688.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1003, 0.0000,  ..., 0.4349, 0.0000, 0.0378],
        [0.0000, 0.1004, 0.0000,  ..., 0.4362, 0.0000, 0.0379],
        [0.0000, 0.0983, 0.0000,  ..., 0.4316, 0.0000, 0.0369],
        ...,
        [0.0000, 0.0169, 0.0000,  ..., 0.2131, 0.0000, 0.0054],
        [0.0000, 0.0236, 0.0000,  ..., 0.2424, 0.0000, 0.0053],
        [0.0000, 0.0598, 0.0000,  ..., 0.3373, 0.0000, 0.0186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(917458.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(595.0751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1293.9419, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.4706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-531.4080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4927],
        [ 0.5027],
        [ 0.5012],
        ...,
        [-1.5845],
        [-1.3170],
        [-1.1825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226042.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0344],
        [1.0339],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371399.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0344],
        [1.0339],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371405.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6093.8369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3934, device='cuda:0')



h[100].sum tensor(-0.2201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-25.3141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1931, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96910.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1383, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1103, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1103, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(941916.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.3796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1361.1246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.7009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-518.0374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9301],
        [-1.4458],
        [-1.8782],
        ...,
        [-3.0069],
        [-3.0000],
        [-2.9983]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227513.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0344],
        [1.0339],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371405.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0345],
        [1.0340],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371411.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0110, -0.0018, -0.0022,  ...,  0.0217,  0.0108, -0.0040],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5947.1904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3164, device='cuda:0')



h[100].sum tensor(-0.2049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-23.7851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0204, 0.0000, 0.0000,  ..., 0.0508, 0.0215, 0.0000],
        [0.0279, 0.0000, 0.0000,  ..., 0.0633, 0.0290, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0429, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92970.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.1715, 0.0000, 0.0011],
        [0.0000, 0.0458, 0.0000,  ..., 0.2936, 0.0000, 0.0152],
        [0.0000, 0.0828, 0.0000,  ..., 0.3956, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1303, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.1703, 0.0000, 0.0000],
        [0.0000, 0.0319, 0.0000,  ..., 0.2646, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(920817.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.4402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1442.7654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.7860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-526.7574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7380],
        [-0.2240],
        [ 0.1848],
        ...,
        [-2.2119],
        [-1.4520],
        [-0.6413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238279.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0345],
        [1.0340],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371411.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0345],
        [1.0340],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371417.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099, -0.0018, -0.0022,  ...,  0.0202,  0.0099, -0.0037],
        [ 0.0287, -0.0021, -0.0026,  ...,  0.0464,  0.0256, -0.0098],
        [ 0.0037, -0.0017, -0.0021,  ...,  0.0115,  0.0047, -0.0017],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5250.1992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9212, device='cuda:0')



h[100].sum tensor(-0.1414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-16.5594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1041, 0.0000, 0.0000,  ..., 0.1713, 0.0935, 0.0000],
        [0.0500, 0.0000, 0.0000,  ..., 0.0961, 0.0486, 0.0000],
        [0.0563, 0.0000, 0.0000,  ..., 0.1049, 0.0538, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82161.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2087, 0.0000,  ..., 0.7135, 0.0000, 0.0863],
        [0.0000, 0.1560, 0.0000,  ..., 0.5825, 0.0000, 0.0626],
        [0.0000, 0.1249, 0.0000,  ..., 0.5037, 0.0000, 0.0485],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(870161.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(556.7932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1710.6785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.0110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.0222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4415],
        [ 0.4234],
        [ 0.3504],
        ...,
        [-3.0376],
        [-3.0307],
        [-3.0290]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289704.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0345],
        [1.0340],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371417.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0341],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371424.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0018, -0.0022,  ...,  0.0183,  0.0088, -0.0032],
        [ 0.0185, -0.0019, -0.0024,  ...,  0.0322,  0.0171, -0.0065],
        [ 0.0086, -0.0018, -0.0022,  ...,  0.0184,  0.0088, -0.0032],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6131.3330, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4106, device='cuda:0')



h[100].sum tensor(-0.2164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-25.5807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0633, 0.0000, 0.0000,  ..., 0.1124, 0.0583, 0.0000],
        [0.0622, 0.0000, 0.0000,  ..., 0.1131, 0.0587, 0.0000],
        [0.0631, 0.0000, 0.0000,  ..., 0.1123, 0.0582, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99327.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2403, 0.0000,  ..., 0.7926, 0.0000, 0.0999],
        [0.0000, 0.2675, 0.0000,  ..., 0.8629, 0.0000, 0.1119],
        [0.0000, 0.2549, 0.0000,  ..., 0.8314, 0.0000, 0.1063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(960786.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(626.1080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1639.9080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.8694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-511.5081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2825],
        [ 0.2864],
        [ 0.2799],
        ...,
        [-3.0445],
        [-3.0375],
        [-3.0359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256865.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0341],
        ...,
        [0.9989],
        [0.9974],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371424.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0342],
        ...,
        [0.9989],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371431.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5364.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9758, device='cuda:0')



h[100].sum tensor(-0.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-17.5459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86170.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(899975.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(570.4731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1706.7983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.9264, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-543.7397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0200],
        [-2.4486],
        [-2.7516],
        ...,
        [-3.0474],
        [-3.0405],
        [-3.0389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252387.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0342],
        ...,
        [0.9989],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371431.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0342],
        ...,
        [0.9989],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371431.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5215.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8817, device='cuda:0')



h[100].sum tensor(-0.1342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-16.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82909.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(881145.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(557.1874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1776.1750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.3106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-551.1453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0263],
        [-3.1071],
        [-3.1663],
        ...,
        [-3.0474],
        [-3.0405],
        [-3.0389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265636.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0346],
        [1.0342],
        ...,
        [0.9989],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371431.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0347],
        [1.0343],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371438.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127, -0.0018, -0.0022,  ...,  0.0241,  0.0122, -0.0046],
        [ 0.0223, -0.0020, -0.0024,  ...,  0.0374,  0.0202, -0.0077],
        [ 0.0148, -0.0019, -0.0023,  ...,  0.0269,  0.0139, -0.0052],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7494.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1826, device='cuda:0')



h[100].sum tensor(-0.3268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0228, device='cuda:0')



h[200].sum tensor(-39.3516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3025, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0357, 0.0000, 0.0000,  ..., 0.0740, 0.0354, 0.0000],
        [0.0533, 0.0000, 0.0000,  ..., 0.1006, 0.0513, 0.0000],
        [0.1076, 0.0000, 0.0000,  ..., 0.1762, 0.0964, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(127717.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1263, 0.0000,  ..., 0.5065, 0.0000, 0.0481],
        [0.0000, 0.1918, 0.0000,  ..., 0.6727, 0.0000, 0.0770],
        [0.0000, 0.2845, 0.0000,  ..., 0.9052, 0.0000, 0.1179],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1152768.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(739.1957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1577.2704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.2824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.9456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4171],
        [ 0.3704],
        [ 0.3215],
        ...,
        [-3.0498],
        [-3.0429],
        [-3.0413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225947.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0347],
        [1.0343],
        ...,
        [0.9988],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371438.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0345],
        ...,
        [0.9988],
        [0.9973],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371444.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0110, -0.0018, -0.0022,  ...,  0.0216,  0.0108, -0.0040],
        [ 0.0042, -0.0017, -0.0021,  ...,  0.0122,  0.0052, -0.0018],
        [ 0.0201, -0.0020, -0.0024,  ...,  0.0344,  0.0184, -0.0069],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5984.1157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3052, device='cuda:0')



h[100].sum tensor(-0.1949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-23.6892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1809, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0226, 0.0000, 0.0000,  ..., 0.0578, 0.0258, 0.0000],
        [0.0454, 0.0000, 0.0000,  ..., 0.0897, 0.0448, 0.0000],
        [0.0244, 0.0000, 0.0000,  ..., 0.0585, 0.0262, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98936.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0930, 0.0000,  ..., 0.4267, 0.0000, 0.0336],
        [0.0000, 0.0925, 0.0000,  ..., 0.4262, 0.0000, 0.0334],
        [0.0000, 0.0712, 0.0000,  ..., 0.3722, 0.0000, 0.0238],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(978444.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(621.0282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1610.6741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.9249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-513.9764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5146],
        [ 0.5201],
        [ 0.5085],
        ...,
        [-3.0500],
        [-3.0431],
        [-3.0416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217429.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0345],
        ...,
        [0.9988],
        [0.9973],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371444.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 170.0 event: 850 loss: tensor(466.0362, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0349],
        [1.0346],
        ...,
        [0.9987],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371451.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0078, -0.0018, -0.0021,  ...,  0.0172,  0.0081, -0.0030],
        [ 0.0080, -0.0018, -0.0021,  ...,  0.0175,  0.0083, -0.0030],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5923.7676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2857, device='cuda:0')



h[100].sum tensor(-0.1879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-23.0487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0463, 0.0000, 0.0000,  ..., 0.0909, 0.0455, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0427, 0.0166, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0319, 0.0102, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96758.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0764, 0.0000,  ..., 0.3808, 0.0000, 0.0259],
        [0.0000, 0.0318, 0.0000,  ..., 0.2609, 0.0000, 0.0095],
        [0.0000, 0.0110, 0.0000,  ..., 0.1879, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(965244.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(614.0485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1676.7235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.2899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-521.7151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2929],
        [-1.0026],
        [-1.7723],
        ...,
        [-3.0606],
        [-3.0537],
        [-3.0523]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243839.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0349],
        [1.0346],
        ...,
        [0.9987],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371451.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0350],
        [1.0347],
        ...,
        [0.9986],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371457.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0267, -0.0021, -0.0025,  ...,  0.0436,  0.0239, -0.0090],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5866.2515, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2499, device='cuda:0')



h[100].sum tensor(-0.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-22.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0598, 0.0000, 0.0000,  ..., 0.1097, 0.0566, 0.0000],
        [0.0209, 0.0000, 0.0000,  ..., 0.0517, 0.0219, 0.0000],
        [0.0630, 0.0000, 0.0000,  ..., 0.1144, 0.0593, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94984.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0781, 0.0000,  ..., 0.3850, 0.0000, 0.0267],
        [0.0000, 0.0565, 0.0000,  ..., 0.3311, 0.0000, 0.0170],
        [0.0000, 0.0818, 0.0000,  ..., 0.3964, 0.0000, 0.0282],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(943081.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.0559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1716.2666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.8372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-528.5656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5311],
        [-0.5452],
        [-0.6512],
        ...,
        [-3.0620],
        [-3.0558],
        [-3.0562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269336.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0350],
        [1.0347],
        ...,
        [0.9986],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371457.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0351],
        [1.0348],
        ...,
        [0.9986],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371464.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0017, -0.0021,  ...,  0.0146,  0.0065, -0.0023],
        [ 0.0202, -0.0020, -0.0024,  ...,  0.0346,  0.0185, -0.0069],
        [ 0.0252, -0.0020, -0.0025,  ...,  0.0416,  0.0227, -0.0085],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5535.5415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0626, device='cuda:0')



h[100].sum tensor(-0.1528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-19.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0294, 0.0000, 0.0000,  ..., 0.0634, 0.0289, 0.0000],
        [0.0708, 0.0000, 0.0000,  ..., 0.1253, 0.0658, 0.0000],
        [0.1212, 0.0000, 0.0000,  ..., 0.1955, 0.1077, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89006.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1016, 0.0000,  ..., 0.4456, 0.0000, 0.0369],
        [0.0000, 0.1938, 0.0000,  ..., 0.6827, 0.0000, 0.0781],
        [0.0000, 0.2946, 0.0000,  ..., 0.9398, 0.0000, 0.1232],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(915205.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(585.3511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1653.2650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.4602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.3863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4502],
        [ 0.4320],
        [ 0.4107],
        ...,
        [-3.0833],
        [-3.0764],
        [-3.0751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249109.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0351],
        [1.0348],
        ...,
        [0.9986],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371464.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0351],
        [1.0349],
        ...,
        [0.9985],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371471.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096, -0.0018, -0.0022,  ...,  0.0198,  0.0097, -0.0035],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5473.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0344, device='cuda:0')



h[100].sum tensor(-0.1454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-18.3409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0200, 0.0000, 0.0000,  ..., 0.0503, 0.0212, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0344, 0.0116, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86764.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0463, 0.0000,  ..., 0.3027, 0.0000, 0.0146],
        [0.0000, 0.0182, 0.0000,  ..., 0.2172, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.1500, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(899336.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.2410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1816.6770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.3142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-549.0412, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3270],
        [-0.8582],
        [-1.3672],
        ...,
        [-3.0838],
        [-3.0770],
        [-3.0757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269989.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0351],
        [1.0349],
        ...,
        [0.9985],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371471.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0352],
        [1.0350],
        ...,
        [0.9984],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371477.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106, -0.0018, -0.0022,  ...,  0.0212,  0.0105, -0.0038],
        [ 0.0038, -0.0017, -0.0021,  ...,  0.0117,  0.0049, -0.0017],
        [ 0.0054, -0.0017, -0.0021,  ...,  0.0139,  0.0062, -0.0022],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5344.4966, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9453, device='cuda:0')



h[100].sum tensor(-0.1324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-16.8592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0000, 0.0000,  ..., 0.0468, 0.0191, 0.0000],
        [0.0395, 0.0000, 0.0000,  ..., 0.0816, 0.0399, 0.0000],
        [0.0177, 0.0000, 0.0000,  ..., 0.0473, 0.0193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84626.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0401, 0.0000,  ..., 0.2911, 0.0000, 0.0100],
        [0.0000, 0.0629, 0.0000,  ..., 0.3514, 0.0000, 0.0191],
        [0.0000, 0.0365, 0.0000,  ..., 0.2811, 0.0000, 0.0097],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(891833.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(564.7801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2005.7273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.4391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-553.9355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0359e-02],
        [ 1.6372e-01],
        [-3.0530e-03],
        ...,
        [-3.0831e+00],
        [-3.0764e+00],
        [-3.0751e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275122.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0352],
        [1.0350],
        ...,
        [0.9984],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371477.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0352],
        [1.0350],
        ...,
        [0.9983],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371484.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046, -0.0017, -0.0021,  ...,  0.0128,  0.0055, -0.0019],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5484.0664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0211, device='cuda:0')



h[100].sum tensor(-0.1413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-18.1641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0000, 0.0000,  ..., 0.0475, 0.0196, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0272, 0.0074, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87624.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0382, 0.0000,  ..., 0.2842, 0.0000, 0.0102],
        [0.0000, 0.0145, 0.0000,  ..., 0.2034, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.1451, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(905242.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(577.5068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1916.1558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.2345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.3691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [-0.5416],
        [-1.1143],
        ...,
        [-3.0544],
        [-3.0682],
        [-3.0725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251025.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0352],
        [1.0350],
        ...,
        [0.9983],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371484.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0353],
        [1.0350],
        ...,
        [0.9982],
        [0.9967],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371490.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0017, -0.0021,  ...,  0.0124,  0.0053, -0.0018],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5530.6509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0318, device='cuda:0')



h[100].sum tensor(-0.1435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-18.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0104, 0.0000, 0.0000,  ..., 0.0389, 0.0143, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0336, 0.0111, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90253.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.2403, 0.0000, 0.0024],
        [0.0000, 0.0124, 0.0000,  ..., 0.2039, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.1454, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927712., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.3370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1911.0947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.3797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.1324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2739],
        [-0.6698],
        [-1.2463],
        ...,
        [-3.1013],
        [-3.0945],
        [-3.0934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265233.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0353],
        [1.0350],
        ...,
        [0.9982],
        [0.9967],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371490.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0347],
        [1.0353],
        [1.0351],
        ...,
        [0.9982],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371497.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0239, -0.0020, -0.0025,  ...,  0.0397,  0.0215, -0.0080],
        [ 0.0392, -0.0023, -0.0028,  ...,  0.0611,  0.0343, -0.0129],
        [ 0.0275, -0.0021, -0.0025,  ...,  0.0447,  0.0245, -0.0092],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5080.2534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7835, device='cuda:0')



h[100].sum tensor(-0.1072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-14.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1066, 0.0000, 0.0000,  ..., 0.1749, 0.0955, 0.0000],
        [0.1392, 0.0000, 0.0000,  ..., 0.2205, 0.1226, 0.0000],
        [0.1479, 0.0000, 0.0000,  ..., 0.2326, 0.1298, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80699.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2519, 0.0000,  ..., 0.8289, 0.0000, 0.1028],
        [0.0000, 0.3271, 0.0000,  ..., 1.0219, 0.0000, 0.1360],
        [0.0000, 0.3386, 0.0000,  ..., 1.0520, 0.0000, 0.1409],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(872878.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(553.1280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2056.5005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.2179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.1724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3733],
        [ 0.4161],
        [ 0.4322],
        ...,
        [-3.1144],
        [-3.1076],
        [-3.1065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294903.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0347],
        [1.0353],
        [1.0351],
        ...,
        [0.9982],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371497.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0354],
        [1.0352],
        ...,
        [0.9981],
        [0.9966],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371503.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107, -0.0018, -0.0022,  ...,  0.0213,  0.0105, -0.0038],
        [ 0.0044, -0.0017, -0.0021,  ...,  0.0126,  0.0053, -0.0018],
        [ 0.0049, -0.0017, -0.0021,  ...,  0.0132,  0.0057, -0.0020],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5724.2056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1382, device='cuda:0')



h[100].sum tensor(-0.1541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-20.3678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0000, 0.0000,  ..., 0.0470, 0.0191, 0.0000],
        [0.0374, 0.0000, 0.0000,  ..., 0.0788, 0.0380, 0.0000],
        [0.0178, 0.0000, 0.0000,  ..., 0.0475, 0.0193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92145.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0382, 0.0000,  ..., 0.2847, 0.0000, 0.0085],
        [0.0000, 0.0584, 0.0000,  ..., 0.3382, 0.0000, 0.0163],
        [0.0000, 0.0335, 0.0000,  ..., 0.2706, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(930687.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.4294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1872.1605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.8455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.6871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1659],
        [-0.1426],
        [-0.5477],
        ...,
        [-3.1161],
        [-3.1094],
        [-3.1083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248075.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0354],
        [1.0352],
        ...,
        [0.9981],
        [0.9966],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371503.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0353],
        ...,
        [0.9981],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371510.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181, -0.0019, -0.0024,  ...,  0.0316,  0.0167, -0.0062],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5878.4229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2140, device='cuda:0')



h[100].sum tensor(-0.1623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-21.6642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0696, 0.0000, 0.0000,  ..., 0.1213, 0.0635, 0.0000],
        [0.0462, 0.0000, 0.0000,  ..., 0.0889, 0.0441, 0.0000],
        [0.0259, 0.0000, 0.0000,  ..., 0.0607, 0.0273, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93794.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2059, 0.0000,  ..., 0.7102, 0.0000, 0.0813],
        [0.0000, 0.1683, 0.0000,  ..., 0.6164, 0.0000, 0.0646],
        [0.0000, 0.1304, 0.0000,  ..., 0.5209, 0.0000, 0.0477],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(936009.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(604.6052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1880.0549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.9690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-541.2512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3939],
        [ 0.4103],
        [ 0.4223],
        ...,
        [-3.1136],
        [-3.1069],
        [-3.1058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235579.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0353],
        ...,
        [0.9981],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371510.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0353],
        ...,
        [0.9981],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371510.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5712.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1369, device='cuda:0')



h[100].sum tensor(-0.1498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-19.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0278, 0.0076, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91208.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1128, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1316, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.1838, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(926051.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(594.2180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2013.4897, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.3807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-546.1818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9724],
        [-1.4064],
        [-0.7780],
        ...,
        [-3.1136],
        [-3.1069],
        [-3.1058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270029.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0353],
        ...,
        [0.9981],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371510.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0354],
        ...,
        [0.9980],
        [0.9965],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371517.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060, -0.0017, -0.0021,  ...,  0.0148,  0.0067, -0.0023],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0060, -0.0017, -0.0021,  ...,  0.0148,  0.0067, -0.0023],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7553.5449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1173, device='cuda:0')



h[100].sum tensor(-0.2845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0221, device='cuda:0')



h[200].sum tensor(-38.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0404, 0.0153, 0.0000],
        [0.0270, 0.0000, 0.0000,  ..., 0.0641, 0.0293, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0274, 0.0074, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(127130.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0304, 0.0000,  ..., 0.2628, 0.0000, 0.0051],
        [0.0000, 0.0388, 0.0000,  ..., 0.2873, 0.0000, 0.0072],
        [0.0000, 0.0166, 0.0000,  ..., 0.2169, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1136685.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(737.4811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2120.9497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.1166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-457.5259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0810],
        [ 0.0729],
        [-0.2229],
        ...,
        [-3.1124],
        [-3.1057],
        [-3.1048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267684.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0354],
        ...,
        [0.9980],
        [0.9965],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371517.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0355],
        ...,
        [0.9980],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371523.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0017, -0.0021,  ...,  0.0125,  0.0053, -0.0018],
        [ 0.0044, -0.0017, -0.0021,  ...,  0.0125,  0.0053, -0.0018],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5819.4619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1669, device='cuda:0')



h[100].sum tensor(-0.1519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-20.6605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0000, 0.0000,  ..., 0.0447, 0.0179, 0.0000],
        [0.0107, 0.0000, 0.0000,  ..., 0.0393, 0.0147, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0336, 0.0113, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91736.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0331, 0.0000,  ..., 0.2759, 0.0000, 0.0057],
        [0.0000, 0.0238, 0.0000,  ..., 0.2510, 0.0000, 0.0033],
        [0.0000, 0.0113, 0.0000,  ..., 0.2145, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927196.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.2623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2116.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.1880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-544.0085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2523],
        [ 0.0393],
        [-0.3504],
        ...,
        [-3.1067],
        [-3.1000],
        [-3.0988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228786.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0355],
        ...,
        [0.9980],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371523.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0359],
        [1.0356],
        ...,
        [0.9980],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371529.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0135, -0.0018, -0.0023,  ...,  0.0253,  0.0130, -0.0047],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5469.7622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9626, device='cuda:0')



h[100].sum tensor(-0.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-17.1158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0000, 0.0000,  ..., 0.0633, 0.0291, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0362, 0.0128, 0.0000],
        [0.0336, 0.0000, 0.0000,  ..., 0.0733, 0.0349, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86577.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0442, 0.0000,  ..., 0.3078, 0.0000, 0.0119],
        [0.0000, 0.0366, 0.0000,  ..., 0.2890, 0.0000, 0.0088],
        [0.0000, 0.0585, 0.0000,  ..., 0.3470, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(903952.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(571.6650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2169.9832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.8900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.8773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5224],
        [ 0.5194],
        [ 0.5325],
        ...,
        [-3.1151],
        [-3.1076],
        [-3.1047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239834.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0359],
        [1.0356],
        ...,
        [0.9980],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371529.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0355],
        [1.0360],
        [1.0357],
        ...,
        [0.9979],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371535.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0123, -0.0018, -0.0022,  ...,  0.0235,  0.0119, -0.0043],
        [ 0.0107, -0.0018, -0.0022,  ...,  0.0212,  0.0105, -0.0038],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5797.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1651, device='cuda:0')



h[100].sum tensor(-0.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-20.4600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0508, 0.0000, 0.0000,  ..., 0.0972, 0.0492, 0.0000],
        [0.0215, 0.0000, 0.0000,  ..., 0.0525, 0.0224, 0.0000],
        [0.0110, 0.0000, 0.0000,  ..., 0.0360, 0.0125, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89787.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0713, 0.0000,  ..., 0.3790, 0.0000, 0.0254],
        [0.0000, 0.0385, 0.0000,  ..., 0.2900, 0.0000, 0.0120],
        [0.0000, 0.0169, 0.0000,  ..., 0.2138, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(908710., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(587.7614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2240.2463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.9052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-550.4344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4922],
        [-0.7428],
        [-1.1072],
        ...,
        [-3.1355],
        [-3.1289],
        [-3.1280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273782.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0355],
        [1.0360],
        [1.0357],
        ...,
        [0.9979],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371535.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0361],
        [1.0357],
        ...,
        [0.9979],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371541.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0017, -0.0021,  ...,  0.0121,  0.0050, -0.0017],
        [ 0.0119, -0.0018, -0.0022,  ...,  0.0230,  0.0116, -0.0042],
        [ 0.0058, -0.0017, -0.0021,  ...,  0.0145,  0.0065, -0.0022],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5346.7485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8955, device='cuda:0')



h[100].sum tensor(-0.1137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-15.9145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0339, 0.0000, 0.0000,  ..., 0.0736, 0.0350, 0.0000],
        [0.0186, 0.0000, 0.0000,  ..., 0.0525, 0.0224, 0.0000],
        [0.0347, 0.0000, 0.0000,  ..., 0.0750, 0.0358, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84572.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0444, 0.0000,  ..., 0.3102, 0.0000, 0.0129],
        [0.0000, 0.0471, 0.0000,  ..., 0.3198, 0.0000, 0.0142],
        [0.0000, 0.0663, 0.0000,  ..., 0.3712, 0.0000, 0.0231],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(894527.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(566.3026, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2209.7893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.1232, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.1439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7982],
        [-0.3789],
        [-0.1458],
        ...,
        [-2.6835],
        [-2.9104],
        [-3.0549]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259954.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0361],
        [1.0357],
        ...,
        [0.9979],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371541.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0362],
        [1.0358],
        ...,
        [0.9979],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371547.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5146.2876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7818, device='cuda:0')



h[100].sum tensor(-0.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-13.8409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81810.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1058, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(881212.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(554.9355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2261.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-574.2648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8754],
        [-2.5729],
        [-2.1456],
        ...,
        [-3.1406],
        [-3.1266],
        [-3.1180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274404.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0362],
        [1.0358],
        ...,
        [0.9979],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371547.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0362],
        [1.0358],
        ...,
        [0.9978],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371553.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5323.4858, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8752, device='cuda:0')



h[100].sum tensor(-0.1090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-15.5436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83929.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(889951.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(562.1492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2422.8198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.2793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-569.6093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8522],
        [-3.0272],
        [-3.1561],
        ...,
        [-3.1558],
        [-3.1495],
        [-3.1490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276636.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0362],
        [1.0358],
        ...,
        [0.9978],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371553.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0363],
        [1.0358],
        ...,
        [0.9978],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371559.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0113, -0.0018, -0.0022,  ...,  0.0221,  0.0110, -0.0039],
        [ 0.0175, -0.0019, -0.0023,  ...,  0.0308,  0.0162, -0.0059],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5158.6660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7792, device='cuda:0')



h[100].sum tensor(-0.0958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0081, device='cuda:0')



h[200].sum tensor(-13.7846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0311, 0.0000, 0.0000,  ..., 0.0694, 0.0325, 0.0000],
        [0.0275, 0.0000, 0.0000,  ..., 0.0608, 0.0273, 0.0000],
        [0.0264, 0.0000, 0.0000,  ..., 0.0593, 0.0264, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81335.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0425, 0.0000,  ..., 0.3064, 0.0000, 0.0123],
        [0.0000, 0.0561, 0.0000,  ..., 0.3434, 0.0000, 0.0187],
        [0.0000, 0.0671, 0.0000,  ..., 0.3734, 0.0000, 0.0239],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1117, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(879398.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(548.6040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2581.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.0383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-575.0012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0974],
        [ 0.3725],
        [ 0.4888],
        ...,
        [-3.1588],
        [-3.1521],
        [-3.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277375.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0363],
        [1.0358],
        ...,
        [0.9978],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371559.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0363],
        [1.0359],
        ...,
        [0.9978],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371564.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5470.4966, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9428, device='cuda:0')



h[100].sum tensor(-0.1155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-16.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88793.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1305, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1389, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(926442.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(576.1987, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2673.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.2849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.0358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4969],
        [-1.4712],
        [-1.3194],
        ...,
        [-3.1573],
        [-3.1507],
        [-3.1499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255070.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0363],
        [1.0359],
        ...,
        [0.9978],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371564.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 190.0 event: 950 loss: tensor(932.1797, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0364],
        [1.0359],
        ...,
        [0.9977],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371570.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0163, -0.0019, -0.0023,  ...,  0.0290,  0.0152, -0.0055],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5507.1650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9591, device='cuda:0')



h[100].sum tensor(-0.1165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-17.0931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0537, 0.0000, 0.0000,  ..., 0.0970, 0.0492, 0.0000],
        [0.0304, 0.0000, 0.0000,  ..., 0.0647, 0.0298, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88268.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1092, 0.0000,  ..., 0.4859, 0.0000, 0.0452],
        [0.0000, 0.0663, 0.0000,  ..., 0.3733, 0.0000, 0.0250],
        [0.0000, 0.0174, 0.0000,  ..., 0.2425, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1126, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(922109.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(573.5409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2666.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.6136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2761],
        [ 0.2128],
        [ 0.1522],
        ...,
        [-3.1660],
        [-3.1593],
        [-3.1585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245242.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0364],
        [1.0359],
        ...,
        [0.9977],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371570.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0365],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371575.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0187, -0.0019, -0.0024,  ...,  0.0323,  0.0172, -0.0062],
        [ 0.0076, -0.0017, -0.0021,  ...,  0.0170,  0.0080, -0.0028],
        [ 0.0102, -0.0018, -0.0022,  ...,  0.0205,  0.0101, -0.0036],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5989.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2419, device='cuda:0')



h[100].sum tensor(-0.1479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-21.9031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0538, 0.0000, 0.0000,  ..., 0.1011, 0.0516, 0.0000],
        [0.0544, 0.0000, 0.0000,  ..., 0.1020, 0.0520, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0452, 0.0181, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95677.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1815, 0.0000,  ..., 0.6828, 0.0000, 0.0804],
        [0.0000, 0.1402, 0.0000,  ..., 0.5741, 0.0000, 0.0603],
        [0.0000, 0.0787, 0.0000,  ..., 0.4094, 0.0000, 0.0303],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1352, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(954154.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(604.2322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2646.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.1486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-540.6855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5617],
        [ 0.5539],
        [ 0.5395],
        ...,
        [-2.9805],
        [-2.8260],
        [-2.7264]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240243.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0365],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371575.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0365],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371581.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5549.5820, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9976, device='cuda:0')



h[100].sum tensor(-0.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-17.5337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86958.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.1880, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.1318, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1123, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1123, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(905991., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(568.9380, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2928.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-563.8068, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5997],
        [-1.2900],
        [-1.7826],
        ...,
        [-3.1991],
        [-3.1922],
        [-3.1914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285807.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0365],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371581.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371588.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092, -0.0018, -0.0022,  ...,  0.0191,  0.0093, -0.0032],
        [ 0.0064, -0.0017, -0.0021,  ...,  0.0152,  0.0069, -0.0024],
        [ 0.0041, -0.0017, -0.0021,  ...,  0.0119,  0.0050, -0.0017],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5333.3936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8739, device='cuda:0')



h[100].sum tensor(-0.1012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-15.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0324, 0.0000, 0.0000,  ..., 0.0709, 0.0334, 0.0000],
        [0.0222, 0.0000, 0.0000,  ..., 0.0569, 0.0250, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0494, 0.0204, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83504.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0519, 0.0000,  ..., 0.3293, 0.0000, 0.0155],
        [0.0000, 0.0517, 0.0000,  ..., 0.3313, 0.0000, 0.0153],
        [0.0000, 0.0444, 0.0000,  ..., 0.3127, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1122, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1122, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(893512.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(552.4677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2974.2815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.1542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-574.5978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2592],
        [ 0.4047],
        [ 0.3729],
        ...,
        [-3.2038],
        [-3.1968],
        [-3.1960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280152.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0360],
        ...,
        [0.9977],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371588.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0360],
        ...,
        [0.9976],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371594.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0139, -0.0019, -0.0023,  ...,  0.0256,  0.0131, -0.0047],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6234.1113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3534, device='cuda:0')



h[100].sum tensor(-0.1575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-24.0183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0020, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0403, 0.0152, 0.0000],
        [0.0496, 0.0000, 0.0000,  ..., 0.0912, 0.0455, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99933.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.1661, 0.0000, 0.0014],
        [0.0000, 0.0578, 0.0000,  ..., 0.3252, 0.0000, 0.0225],
        [0.0000, 0.1536, 0.0000,  ..., 0.5920, 0.0000, 0.0627],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1125, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1125, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982305.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(615.9506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3051.6614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.5986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-534.0809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7266],
        [-0.2756],
        [ 0.1168],
        ...,
        [-3.2025],
        [-3.1955],
        [-3.1947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262715.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0360],
        ...,
        [0.9976],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371594.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0361],
        ...,
        [0.9976],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371600.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5581.4429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0088, device='cuda:0')



h[100].sum tensor(-0.1131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-17.4124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88099.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1129, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1129, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(922306.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(564.7515, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3070.7700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.5775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-562.1224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0263],
        [-2.8146],
        [-2.4648],
        ...,
        [-3.1957],
        [-3.1877],
        [-3.1859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235153.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0361],
        ...,
        [0.9976],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371600.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0361],
        ...,
        [0.9976],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371606.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0167, -0.0019, -0.0023,  ...,  0.0294,  0.0155, -0.0055],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5078.1758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7063, device='cuda:0')



h[100].sum tensor(-0.0798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0074, device='cuda:0')



h[200].sum tensor(-12.4034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0482, 0.0000, 0.0000,  ..., 0.0907, 0.0456, 0.0000],
        [0.0313, 0.0000, 0.0000,  ..., 0.0654, 0.0304, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80107.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1117, 0.0000,  ..., 0.4824, 0.0000, 0.0421],
        [0.0000, 0.0623, 0.0000,  ..., 0.3532, 0.0000, 0.0219],
        [0.0000, 0.0119, 0.0000,  ..., 0.1869, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(881503.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(529.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3591.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.0963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-578.7072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2023],
        [-0.2338],
        [-0.9503],
        ...,
        [-3.2020],
        [-3.1952],
        [-3.1945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259419.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0361],
        ...,
        [0.9976],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371606.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0048, -0.0017, -0.0021,  ...,  0.0128,  0.0056, -0.0019],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7539.5127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1034, device='cuda:0')



h[100].sum tensor(-0.2333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0219, device='cuda:0')



h[200].sum tensor(-36.6149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0355, 0.0000, 0.0000,  ..., 0.0749, 0.0363, 0.0000],
        [0.0038, 0.0000, 0.0000,  ..., 0.0253, 0.0066, 0.0000],
        [0.0356, 0.0000, 0.0000,  ..., 0.0734, 0.0353, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(119502.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1030, 0.0000,  ..., 0.4627, 0.0000, 0.0377],
        [0.0000, 0.0882, 0.0000,  ..., 0.4251, 0.0000, 0.0309],
        [0.0000, 0.1398, 0.0000,  ..., 0.5588, 0.0000, 0.0544],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1073823., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(690.4955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3223.4556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-347.6052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.8590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2543],
        [ 0.2371],
        [ 0.2167],
        ...,
        [-3.2093],
        [-3.2031],
        [-3.2032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221316.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0219, -0.0020, -0.0024,  ...,  0.0367,  0.0199, -0.0071],
        [ 0.0239, -0.0020, -0.0025,  ...,  0.0394,  0.0215, -0.0077],
        [ 0.0222, -0.0020, -0.0024,  ...,  0.0370,  0.0201, -0.0072],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5979.4512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2206, device='cuda:0')



h[100].sum tensor(-0.1357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-21.2930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0854, 0.0000, 0.0000,  ..., 0.1442, 0.0776, 0.0000],
        [0.0999, 0.0000, 0.0000,  ..., 0.1647, 0.0898, 0.0000],
        [0.1043, 0.0000, 0.0000,  ..., 0.1708, 0.0934, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95953.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2734, 0.0000,  ..., 0.9012, 0.0000, 0.1161],
        [0.0000, 0.2820, 0.0000,  ..., 0.9249, 0.0000, 0.1200],
        [0.0000, 0.2664, 0.0000,  ..., 0.8851, 0.0000, 0.1127],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(964038.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(594.4998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3265.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.2559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-541.4959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2560],
        [ 0.2619],
        [ 0.2742],
        ...,
        [-3.2132],
        [-3.2064],
        [-3.2057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218417.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0253, -0.0020, -0.0025,  ...,  0.0414,  0.0226, -0.0081],
        [ 0.0173, -0.0019, -0.0023,  ...,  0.0302,  0.0160, -0.0057],
        [ 0.0049, -0.0017, -0.0021,  ...,  0.0130,  0.0057, -0.0019],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5758.0137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0984, device='cuda:0')



h[100].sum tensor(-0.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-19.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0940, 0.0000, 0.0000,  ..., 0.1562, 0.0848, 0.0000],
        [0.0504, 0.0000, 0.0000,  ..., 0.0958, 0.0487, 0.0000],
        [0.0425, 0.0000, 0.0000,  ..., 0.0849, 0.0421, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90992.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2155, 0.0000,  ..., 0.7532, 0.0000, 0.0896],
        [0.0000, 0.1559, 0.0000,  ..., 0.6014, 0.0000, 0.0619],
        [0.0000, 0.1181, 0.0000,  ..., 0.5046, 0.0000, 0.0442],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(936867.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(573.9166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3352.4744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.4666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-553.5338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4170],
        [ 0.4410],
        [ 0.4428],
        ...,
        [-3.2132],
        [-3.2064],
        [-3.2057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224005.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0366],
        [1.0362],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371611.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 200.0 event: 1000 loss: tensor(389.1881, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0367],
        [1.0363],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371616.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0059, -0.0017, -0.0021,  ...,  0.0144,  0.0065, -0.0022],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5523.4238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9769, device='cuda:0')



h[100].sum tensor(-0.1074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-17.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.0022, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0285, 0.0084, 0.0000],
        [0.0124, 0.0000, 0.0000,  ..., 0.0393, 0.0148, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89427.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1470, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.1843, 0.0000, 0.0006],
        [0.0000, 0.0192, 0.0000,  ..., 0.2390, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(937127.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(569.6830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3525.1143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.0691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-559.3214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1978],
        [-0.6775],
        [-0.1711],
        ...,
        [-3.2402],
        [-3.2332],
        [-3.2324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248156.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0367],
        [1.0363],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371616.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0367],
        [1.0363],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371621.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0187, -0.0019, -0.0024,  ...,  0.0323,  0.0172, -0.0061],
        [ 0.0173, -0.0019, -0.0023,  ...,  0.0303,  0.0159, -0.0057],
        [ 0.0034, -0.0017, -0.0020,  ...,  0.0110,  0.0044, -0.0014],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5890.0508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1916, device='cuda:0')



h[100].sum tensor(-0.1303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-20.8448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0879, 0.0000, 0.0000,  ..., 0.1480, 0.0796, 0.0000],
        [0.0636, 0.0000, 0.0000,  ..., 0.1144, 0.0594, 0.0000],
        [0.0822, 0.0000, 0.0000,  ..., 0.1403, 0.0749, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92460.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2307, 0.0000,  ..., 0.7937, 0.0000, 0.0957],
        [0.0000, 0.2133, 0.0000,  ..., 0.7505, 0.0000, 0.0877],
        [0.0000, 0.2142, 0.0000,  ..., 0.7534, 0.0000, 0.0879],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1135, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(942199.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(586.3926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3512.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.7526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.3063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2591],
        [ 0.2528],
        [ 0.2505],
        ...,
        [-3.2676],
        [-3.2604],
        [-3.2596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272826.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0367],
        [1.0363],
        ...,
        [0.9975],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371621.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0364],
        ...,
        [0.9975],
        [0.9960],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371626.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0062, -0.0017, -0.0021,  ...,  0.0150,  0.0068, -0.0023],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5666.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0882, device='cuda:0')



h[100].sum tensor(-0.1168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-18.8676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0291, 0.0085, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0274, 0.0074, 0.0000],
        [0.0274, 0.0000, 0.0000,  ..., 0.0642, 0.0294, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91719.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.2403e-03, 0.0000e+00,  ..., 1.7209e-01, 0.0000e+00,
         8.6841e-05],
        [0.0000e+00, 9.4626e-03, 0.0000e+00,  ..., 2.0683e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8389e-02, 0.0000e+00,  ..., 2.7498e-01, 0.0000e+00,
         3.1946e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1342e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1337e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1341e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(949951.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(586.5167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3538.9905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.1166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-560.4794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1696],
        [-0.6737],
        [-0.3554],
        ...,
        [-3.2984],
        [-3.2912],
        [-3.2905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300772.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0364],
        ...,
        [0.9975],
        [0.9960],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371626.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0365],
        ...,
        [0.9975],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371631.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5833.0894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1805, device='cuda:0')



h[100].sum tensor(-0.1260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-20.5559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90153.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1111, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1138, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927962.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(580.5714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3597.1748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.0885, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-563.8383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5666],
        [-2.7848],
        [-2.8241],
        ...,
        [-3.3047],
        [-3.2982],
        [-3.2982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300627.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0365],
        ...,
        [0.9975],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371631.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0365],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371637., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0046, -0.0017, -0.0021,  ...,  0.0127,  0.0054, -0.0018],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5424.5142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9495, device='cuda:0')



h[100].sum tensor(-0.1006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-16.5706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0269, 0.0072, 0.0000],
        [0.0178, 0.0000, 0.0000,  ..., 0.0472, 0.0193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87213.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0257, 0.0000,  ..., 0.2572, 0.0000, 0.0052],
        [0.0000, 0.0349, 0.0000,  ..., 0.2870, 0.0000, 0.0086],
        [0.0000, 0.0499, 0.0000,  ..., 0.3366, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1327, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.1746, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927668.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(568.8484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3523.2405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.1247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-570.5765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1870],
        [ 0.2510],
        [ 0.2958],
        ...,
        [-2.9476],
        [-2.3892],
        [-1.6326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278611.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0367],
        [1.0365],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371637., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0368],
        [1.0366],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371642.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0017, -0.0021,  ...,  0.0132,  0.0057, -0.0019],
        [ 0.0045, -0.0017, -0.0021,  ...,  0.0127,  0.0054, -0.0018],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5548.6973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0145, device='cuda:0')



h[100].sum tensor(-0.1069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-17.7819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0282, 0.0000, 0.0000,  ..., 0.0654, 0.0303, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0418, 0.0161, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0269, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88519.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3021e-02, 0.0000e+00,  ..., 2.9236e-01, 0.0000e+00,
         6.8494e-03],
        [0.0000e+00, 1.8114e-02, 0.0000e+00,  ..., 2.4252e-01, 0.0000e+00,
         3.9275e-03],
        [0.0000e+00, 3.3949e-03, 0.0000e+00,  ..., 1.8010e-01, 0.0000e+00,
         2.1488e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1436e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1432e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1436e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(932196.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(576.3110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3497.7051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.9854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-567.2265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2217],
        [-0.4969],
        [-1.0297],
        ...,
        [-3.3098],
        [-3.3035],
        [-3.3039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300255.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0368],
        [1.0366],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371642.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0368],
        [1.0367],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371649., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5428.8467, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9523, device='cuda:0')



h[100].sum tensor(-0.0988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-16.5971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86628.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1333, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1310, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1340, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(922280.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(570.5081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3414.4187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.6306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-575.0560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3791],
        [-1.4656],
        [-1.4203],
        ...,
        [-3.3214],
        [-3.3141],
        [-3.3135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292295.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0368],
        [1.0367],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371649., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0368],
        [1.0367],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371655.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6521.5869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5882, device='cuda:0')



h[100].sum tensor(-0.1595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-27.0534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(104856.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1087, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1096, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1017004.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(645.3992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3289.5227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-320.3719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-531.6082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1303],
        [-3.3095],
        [-3.4199],
        ...,
        [-3.3164],
        [-3.3103],
        [-3.3105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286090.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0368],
        [1.0367],
        ...,
        [0.9974],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371655.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0368],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371661.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057, -0.0017, -0.0021,  ...,  0.0142,  0.0064, -0.0021],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5368.7783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9065, device='cuda:0')



h[100].sum tensor(-0.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-15.7232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0124, 0.0000, 0.0000,  ..., 0.0393, 0.0147, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0285, 0.0082, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84725.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0210, 0.0000,  ..., 0.2460, 0.0000, 0.0052],
        [0.0000, 0.0057, 0.0000,  ..., 0.1820, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.1308, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1145, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(910134.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(563.6052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3262.0288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.3182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.6745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7167],
        [-1.4951],
        [-2.2680],
        ...,
        [-3.3129],
        [-3.3056],
        [-3.3049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280693.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0368],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371661.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0368],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371667.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0115, -0.0018, -0.0022,  ...,  0.0223,  0.0112, -0.0039],
        [ 0.0098, -0.0018, -0.0022,  ...,  0.0199,  0.0098, -0.0033],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5594.1309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0255, device='cuda:0')



h[100].sum tensor(-0.1027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-17.7559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0000,  ..., 0.0366, 0.0130, 0.0000],
        [0.0197, 0.0000, 0.0000,  ..., 0.0497, 0.0208, 0.0000],
        [0.0651, 0.0000, 0.0000,  ..., 0.1168, 0.0609, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86836.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.2184, 0.0000, 0.0055],
        [0.0000, 0.0445, 0.0000,  ..., 0.3128, 0.0000, 0.0152],
        [0.0000, 0.1094, 0.0000,  ..., 0.4901, 0.0000, 0.0429],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1689, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1554, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1280, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(912949.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(573.1389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3076.7505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.5852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-576.8384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5307],
        [-0.0855],
        [ 0.2470],
        ...,
        [-1.9505],
        [-2.2668],
        [-2.7121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273837.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0368],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371667.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 210.0 event: 1050 loss: tensor(473.9295, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0369],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371673.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5006.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.8989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6894, device='cuda:0')



h[100].sum tensor(-0.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0072, device='cuda:0')



h[200].sum tensor(-11.9102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0181, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80121.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1169, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1307, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(889844.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(545.6564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3032.7026, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.3912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.7957, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6540],
        [-1.7256],
        [-1.5655],
        ...,
        [-3.3028],
        [-3.2956],
        [-3.2948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271226.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0369],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371673.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0369],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371679.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0047, -0.0017, -0.0021,  ...,  0.0128,  0.0055, -0.0018],
        [ 0.0048, -0.0017, -0.0021,  ...,  0.0130,  0.0056, -0.0018],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5716.6831, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0830, device='cuda:0')



h[100].sum tensor(-0.1059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-18.6650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0268, 0.0071, 0.0000],
        [0.0180, 0.0000, 0.0000,  ..., 0.0473, 0.0193, 0.0000],
        [0.0441, 0.0000, 0.0000,  ..., 0.0876, 0.0434, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91057.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.1994, 0.0000, 0.0026],
        [0.0000, 0.0363, 0.0000,  ..., 0.2915, 0.0000, 0.0115],
        [0.0000, 0.0707, 0.0000,  ..., 0.3896, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(935564.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.6740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2738.2065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.4326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-569.4541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4766],
        [-0.0237],
        [ 0.2620],
        ...,
        [-3.3041],
        [-3.2969],
        [-3.2961]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262669.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0369],
        ...,
        [0.9974],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371679.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0369],
        ...,
        [0.9973],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371685.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5801.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1354, device='cuda:0')



h[100].sum tensor(-0.1089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-19.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93129.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(955239.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.5884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2690.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.4076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-563.9094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5599],
        [-2.9812],
        [-3.2126],
        ...,
        [-3.3050],
        [-3.2978],
        [-3.2971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267256.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0369],
        ...,
        [0.9973],
        [0.9959],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371685.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0364],
        [1.0371],
        [1.0370],
        ...,
        [0.9973],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371690.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0105, -0.0018, -0.0022,  ...,  0.0210,  0.0104, -0.0035],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5600.5137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0096, device='cuda:0')



h[100].sum tensor(-0.0967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-17.3854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0000, 0.0000,  ..., 0.0353, 0.0122, 0.0000],
        [0.0185, 0.0000, 0.0000,  ..., 0.0460, 0.0186, 0.0000],
        [0.0889, 0.0000, 0.0000,  ..., 0.1500, 0.0806, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88184.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0616, 0.0000,  ..., 0.3627, 0.0000, 0.0233],
        [0.0000, 0.1107, 0.0000,  ..., 0.4982, 0.0000, 0.0463],
        [0.0000, 0.2000, 0.0000,  ..., 0.7385, 0.0000, 0.0900],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(920376.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(581.4144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2646.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.8773, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-575.0536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3609],
        [ 0.3634],
        [ 0.3649],
        ...,
        [-3.3052],
        [-3.2981],
        [-3.2973]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250366.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0364],
        [1.0371],
        [1.0370],
        ...,
        [0.9973],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371690.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0370],
        ...,
        [0.9973],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371696.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0556, -0.0026, -0.0031,  ...,  0.0837,  0.0479, -0.0169],
        [ 0.0212, -0.0020, -0.0024,  ...,  0.0359,  0.0193, -0.0067],
        [ 0.0105, -0.0018, -0.0022,  ...,  0.0209,  0.0104, -0.0035],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5768.4990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0894, device='cuda:0')



h[100].sum tensor(-0.1041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-18.9042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0933, 0.0000, 0.0000,  ..., 0.1560, 0.0843, 0.0000],
        [0.1213, 0.0000, 0.0000,  ..., 0.1951, 0.1076, 0.0000],
        [0.0681, 0.0000, 0.0000,  ..., 0.1212, 0.0635, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92509.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2734, 0.0000,  ..., 0.9386, 0.0000, 0.1276],
        [0.0000, 0.2808, 0.0000,  ..., 0.9602, 0.0000, 0.1312],
        [0.0000, 0.2021, 0.0000,  ..., 0.7512, 0.0000, 0.0926],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1147, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1146, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(946847.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.4322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2470.6750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.9842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-563.4031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4009],
        [ 0.4149],
        [ 0.4346],
        ...,
        [-3.3039],
        [-3.2968],
        [-3.2960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233597.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0370],
        ...,
        [0.9973],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371696.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0365],
        [1.0372],
        [1.0371],
        ...,
        [0.9973],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371701.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0307, -0.0021, -0.0026,  ...,  0.0490,  0.0272, -0.0095],
        [ 0.0216, -0.0020, -0.0024,  ...,  0.0365,  0.0197, -0.0068],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0105, -0.0018, -0.0022,  ...,  0.0210,  0.0104, -0.0035]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6705.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6230, device='cuda:0')



h[100].sum tensor(-0.1516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-27.8023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1193, 0.0000, 0.0000,  ..., 0.1923, 0.1061, 0.0000],
        [0.0565, 0.0000, 0.0000,  ..., 0.1031, 0.0528, 0.0000],
        [0.0302, 0.0000, 0.0000,  ..., 0.0645, 0.0296, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0365, 0.0127, 0.0000],
        [0.0200, 0.0000, 0.0000,  ..., 0.0508, 0.0213, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108912.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2768, 0.0000,  ..., 0.9534, 0.0000, 0.1305],
        [0.0000, 0.1570, 0.0000,  ..., 0.6310, 0.0000, 0.0710],
        [0.0000, 0.0686, 0.0000,  ..., 0.3847, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1567, 0.0000, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.2268, 0.0000, 0.0057],
        [0.0000, 0.0417, 0.0000,  ..., 0.3193, 0.0000, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1040406.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(666.5880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2351.3022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-319.4019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-522.2800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3560],
        [ 0.1518],
        [-0.4355],
        ...,
        [-1.7177],
        [-1.0823],
        [-0.3728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216229.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0365],
        [1.0372],
        [1.0371],
        ...,
        [0.9973],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371701.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371707.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6289.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3887, device='cuda:0')



h[100].sum tensor(-0.1286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-23.8161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0182, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97110.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1497, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1510, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1509, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(955999.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(618.5398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2532.2224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.3543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-550.9837, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0196],
        [-0.8678],
        [-0.7097],
        ...,
        [-3.3175],
        [-3.3103],
        [-3.3095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244330.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371707.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371707.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [ 0.0144, -0.0019, -0.0023,  ...,  0.0265,  0.0137, -0.0047],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6556.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4924, device='cuda:0')



h[100].sum tensor(-0.1424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-26.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0535, 0.0000, 0.0000,  ..., 0.1009, 0.0514, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0371, 0.0132, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0413, 0.0157, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(106084.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0437, 0.0000,  ..., 0.3243, 0.0000, 0.0151],
        [0.0000, 0.0144, 0.0000,  ..., 0.2367, 0.0000, 0.0037],
        [0.0000, 0.0108, 0.0000,  ..., 0.2097, 0.0000, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1149, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1022204.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(654.6295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2384.8726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.6880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-530.3875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9660],
        [-1.2509],
        [-1.6780],
        ...,
        [-3.3152],
        [-3.3079],
        [-3.3069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214195.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371707.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371712.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054, -0.0017, -0.0021,  ...,  0.0139,  0.0061, -0.0020],
        [ 0.0121, -0.0018, -0.0022,  ...,  0.0232,  0.0117, -0.0040],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5259.3022, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8201, device='cuda:0')



h[100].sum tensor(-0.0753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-14.0864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0311, 0.0000, 0.0000,  ..., 0.0677, 0.0314, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0443, 0.0174, 0.0000],
        [0.0450, 0.0000, 0.0000,  ..., 0.0895, 0.0443, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83225.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0894, 0.0000,  ..., 0.4443, 0.0000, 0.0347],
        [0.0000, 0.0485, 0.0000,  ..., 0.3351, 0.0000, 0.0149],
        [0.0000, 0.0453, 0.0000,  ..., 0.3270, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(900496.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(566.6085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2559.9949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.9065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.4000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4112],
        [ 0.3141],
        [ 0.0945],
        ...,
        [-3.3444],
        [-3.3371],
        [-3.3362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285122.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0371],
        ...,
        [0.9972],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371712.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0372],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371718.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5957.0215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1920, device='cuda:0')



h[100].sum tensor(-0.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-20.7775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96597.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1085, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1093, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1140, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1141, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(973376.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(625.7305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2084.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.0721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-565.2182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5175],
        [-3.4972],
        [-3.4395],
        ...,
        [-3.2774],
        [-3.3300],
        [-3.3505]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237230.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0372],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371718.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 220.0 event: 1100 loss: tensor(442.7890, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0373],
        [1.0372],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371723.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115, -0.0018, -0.0022,  ...,  0.0225,  0.0112, -0.0038],
        [ 0.0118, -0.0018, -0.0022,  ...,  0.0228,  0.0114, -0.0039],
        [ 0.0161, -0.0019, -0.0023,  ...,  0.0288,  0.0150, -0.0051],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5730.2808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0898, device='cuda:0')



h[100].sum tensor(-0.0976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-18.6106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0545, 0.0000, 0.0000,  ..., 0.1024, 0.0519, 0.0000],
        [0.0517, 0.0000, 0.0000,  ..., 0.0989, 0.0497, 0.0000],
        [0.0573, 0.0000, 0.0000,  ..., 0.1045, 0.0531, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89875.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1002, 0.0000,  ..., 0.4641, 0.0000, 0.0343],
        [0.0000, 0.1254, 0.0000,  ..., 0.5320, 0.0000, 0.0454],
        [0.0000, 0.1377, 0.0000,  ..., 0.5636, 0.0000, 0.0507],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(928496.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(598.9809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2508.3645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.1273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.1497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3525],
        [ 0.4102],
        [ 0.4148],
        ...,
        [-3.3794],
        [-3.3719],
        [-3.3710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305273.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0373],
        [1.0372],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371723.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371728.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0198, -0.0020, -0.0024,  ...,  0.0340,  0.0181, -0.0062],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [ 0.0076, -0.0017, -0.0021,  ...,  0.0170,  0.0080, -0.0026],
        [ 0.0185, -0.0019, -0.0024,  ...,  0.0322,  0.0170, -0.0058],
        [ 0.0127, -0.0018, -0.0022,  ...,  0.0242,  0.0122, -0.0041]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5953.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1992, device='cuda:0')



h[100].sum tensor(-0.1072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-20.6618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0767, 0.0000, 0.0000,  ..., 0.1313, 0.0692, 0.0000],
        [0.0483, 0.0000, 0.0000,  ..., 0.0920, 0.0457, 0.0000],
        [0.0209, 0.0000, 0.0000,  ..., 0.0518, 0.0217, 0.0000],
        ...,
        [0.0360, 0.0000, 0.0000,  ..., 0.0734, 0.0344, 0.0000],
        [0.0625, 0.0000, 0.0000,  ..., 0.1145, 0.0589, 0.0000],
        [0.0745, 0.0000, 0.0000,  ..., 0.1312, 0.0689, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92977.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2507, 0.0000,  ..., 0.8515, 0.0000, 0.0968],
        [0.0000, 0.1831, 0.0000,  ..., 0.6792, 0.0000, 0.0677],
        [0.0000, 0.1412, 0.0000,  ..., 0.5714, 0.0000, 0.0496],
        ...,
        [0.0000, 0.0762, 0.0000,  ..., 0.4090, 0.0000, 0.0219],
        [0.0000, 0.1215, 0.0000,  ..., 0.5279, 0.0000, 0.0402],
        [0.0000, 0.1259, 0.0000,  ..., 0.5387, 0.0000, 0.0421]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(945151.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2725.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.3121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-572.6909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1094],
        [ 0.1336],
        [ 0.1509],
        ...,
        [-0.2749],
        [ 0.2506],
        [ 0.1993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304027.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9972],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371728.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9971],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371734.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0270, -0.0021, -0.0025,  ...,  0.0439,  0.0241, -0.0083],
        [ 0.0292, -0.0021, -0.0026,  ...,  0.0471,  0.0259, -0.0089],
        [ 0.0267, -0.0021, -0.0025,  ...,  0.0436,  0.0239, -0.0082],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5600.4277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0013, device='cuda:0')



h[100].sum tensor(-0.0880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-17.1212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0975, 0.0000, 0.0000,  ..., 0.1622, 0.0879, 0.0000],
        [0.1010, 0.0000, 0.0000,  ..., 0.1672, 0.0908, 0.0000],
        [0.0928, 0.0000, 0.0000,  ..., 0.1560, 0.0841, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88275.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2193, 0.0000,  ..., 0.7700, 0.0000, 0.0805],
        [0.0000, 0.2111, 0.0000,  ..., 0.7496, 0.0000, 0.0769],
        [0.0000, 0.1819, 0.0000,  ..., 0.6750, 0.0000, 0.0645],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1156, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1156, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(928156.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(585.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2772.8647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.8086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-581.9927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3951],
        [ 0.3609],
        [ 0.2563],
        ...,
        [-3.3792],
        [-3.3719],
        [-3.3711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258222.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9971],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371734.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9971],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371739.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059, -0.0017, -0.0021,  ...,  0.0146,  0.0066, -0.0021],
        [ 0.0100, -0.0018, -0.0022,  ...,  0.0204,  0.0100, -0.0033],
        [ 0.0229, -0.0020, -0.0025,  ...,  0.0382,  0.0207, -0.0071],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5808.5059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1116, device='cuda:0')



h[100].sum tensor(-0.0967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-18.9946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0000, 0.0000,  ..., 0.0561, 0.0247, 0.0000],
        [0.0567, 0.0000, 0.0000,  ..., 0.1057, 0.0542, 0.0000],
        [0.0736, 0.0000, 0.0000,  ..., 0.1293, 0.0683, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92017.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0653, 0.0000,  ..., 0.3786, 0.0000, 0.0167],
        [0.0000, 0.1329, 0.0000,  ..., 0.5538, 0.0000, 0.0425],
        [0.0000, 0.1801, 0.0000,  ..., 0.6750, 0.0000, 0.0617],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(947695.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(598.7626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2945.3132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.0761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.2158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3024],
        [ 0.4405],
        [ 0.4620],
        ...,
        [-3.3827],
        [-3.3753],
        [-3.3742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250729.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0373],
        ...,
        [0.9971],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371739.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0374],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371744.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096, -0.0018, -0.0022,  ...,  0.0198,  0.0097, -0.0032],
        [ 0.0191, -0.0019, -0.0024,  ...,  0.0331,  0.0176, -0.0060],
        [ 0.0037, -0.0017, -0.0021,  ...,  0.0116,  0.0048, -0.0015],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5611.7358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0005, device='cuda:0')



h[100].sum tensor(-0.0866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-17.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0807, 0.0000, 0.0000,  ..., 0.1389, 0.0740, 0.0000],
        [0.0414, 0.0000, 0.0000,  ..., 0.0846, 0.0415, 0.0000],
        [0.0385, 0.0000, 0.0000,  ..., 0.0806, 0.0391, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88864.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1445, 0.0000,  ..., 0.5786, 0.0000, 0.0452],
        [0.0000, 0.1145, 0.0000,  ..., 0.5055, 0.0000, 0.0331],
        [0.0000, 0.0897, 0.0000,  ..., 0.4438, 0.0000, 0.0232],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1162, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(935226.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(586.7656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3071.2415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.1320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4908],
        [ 0.4974],
        [ 0.4793],
        ...,
        [-3.4019],
        [-3.3945],
        [-3.3936]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275830.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0374],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371744.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0374],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371749.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0258, -0.0021, -0.0025,  ...,  0.0423,  0.0230, -0.0079],
        [ 0.0268, -0.0021, -0.0025,  ...,  0.0437,  0.0239, -0.0082],
        [ 0.0112, -0.0018, -0.0022,  ...,  0.0221,  0.0110, -0.0037],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5749.5684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0931, device='cuda:0')



h[100].sum tensor(-0.0925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-18.5500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0800, 0.0000, 0.0000,  ..., 0.1380, 0.0733, 0.0000],
        [0.0809, 0.0000, 0.0000,  ..., 0.1394, 0.0741, 0.0000],
        [0.0770, 0.0000, 0.0000,  ..., 0.1341, 0.0709, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91313.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1535, 0.0000,  ..., 0.5939, 0.0000, 0.0466],
        [0.0000, 0.1557, 0.0000,  ..., 0.6016, 0.0000, 0.0474],
        [0.0000, 0.1311, 0.0000,  ..., 0.5402, 0.0000, 0.0378],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1160, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1161, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(947920.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.2817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2914.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-576.4905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3985],
        [ 0.4139],
        [ 0.3741],
        ...,
        [-3.4226],
        [-3.4094],
        [-3.3849]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261960.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0374],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371749.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0375],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371755.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048, -0.0017, -0.0021,  ...,  0.0132,  0.0056, -0.0018],
        [ 0.0044, -0.0017, -0.0021,  ...,  0.0126,  0.0053, -0.0017],
        [ 0.0106, -0.0018, -0.0022,  ...,  0.0213,  0.0105, -0.0035],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5190.1553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7863, device='cuda:0')



h[100].sum tensor(-0.0658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-13.3209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0175, 0.0000, 0.0000,  ..., 0.0471, 0.0189, 0.0000],
        [0.0373, 0.0000, 0.0000,  ..., 0.0790, 0.0379, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0476, 0.0192, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82466.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0317, 0.0000,  ..., 0.2786, 0.0000, 0.0038],
        [0.0000, 0.0558, 0.0000,  ..., 0.3482, 0.0000, 0.0069],
        [0.0000, 0.0359, 0.0000,  ..., 0.2965, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(904438.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(565.6014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3240.8960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.6013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.9566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0774],
        [ 0.0916],
        [-0.1471],
        ...,
        [-3.4438],
        [-3.4362],
        [-3.4350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333804.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0375],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371755.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0376],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371761.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6012.6729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2309, device='cuda:0')



h[100].sum tensor(-0.1025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-20.9620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95219.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1239, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1379, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(963890.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.9816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3101.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.0698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-568.3405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6090],
        [-2.4242],
        [-2.2415],
        ...,
        [-3.0084],
        [-3.2618],
        [-3.3844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291253.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0376],
        ...,
        [0.9971],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371761.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0376],
        ...,
        [0.9970],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371766.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111, -0.0018, -0.0022,  ...,  0.0219,  0.0109, -0.0036],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5407.3867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.6748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8971, device='cuda:0')



h[100].sum tensor(-0.0730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-15.0713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0000, 0.0000,  ..., 0.0464, 0.0187, 0.0000],
        [0.0115, 0.0000, 0.0000,  ..., 0.0367, 0.0128, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86893.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.0176e-02, 0.0000e+00,  ..., 3.7148e-01, 0.0000e+00,
         1.3536e-02],
        [0.0000e+00, 4.5051e-02, 0.0000e+00,  ..., 2.9761e-01, 0.0000e+00,
         4.1621e-03],
        [0.0000e+00, 2.2974e-02, 0.0000e+00,  ..., 2.3464e-01, 0.0000e+00,
         5.0884e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1671e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1664e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1667e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927641.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(578.6146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3516.9614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.2553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-581.2632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0962],
        [ 0.0444],
        [ 0.0080],
        ...,
        [-3.4373],
        [-3.4298],
        [-3.4287]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310069.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0373],
        [1.0376],
        ...,
        [0.9970],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371766.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0374],
        [1.0377],
        ...,
        [0.9970],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371772.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0017, -0.0021,  ...,  0.0145,  0.0065, -0.0021],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5805.5181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0983, device='cuda:0')



h[100].sum tensor(-0.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-18.5860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0350, 0.0121, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0290, 0.0084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95231.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.2048, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.1734, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1437, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1177, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(984387.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(608.3897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3408.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.9759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-558.3129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8266],
        [-1.9594],
        [-1.9229],
        ...,
        [-3.4220],
        [-3.4146],
        [-3.4135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255058.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0374],
        [1.0377],
        ...,
        [0.9970],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371772.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 230.0 event: 1150 loss: tensor(463.3015, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0374],
        [1.0378],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371777.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104, -0.0018, -0.0022,  ...,  0.0210,  0.0104, -0.0034],
        [ 0.0192, -0.0019, -0.0024,  ...,  0.0332,  0.0177, -0.0059],
        [ 0.0052, -0.0017, -0.0021,  ...,  0.0137,  0.0061, -0.0019],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0006,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0006,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0044,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5819.4277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0886, device='cuda:0')



h[100].sum tensor(-0.0881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-18.5705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0910, 0.0000, 0.0000,  ..., 0.1534, 0.0827, 0.0000],
        [0.0616, 0.0000, 0.0000,  ..., 0.1127, 0.0584, 0.0000],
        [0.0336, 0.0000, 0.0000,  ..., 0.0695, 0.0327, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94263.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1896, 0.0000,  ..., 0.6835, 0.0000, 0.0536],
        [0.0000, 0.1523, 0.0000,  ..., 0.5926, 0.0000, 0.0400],
        [0.0000, 0.0889, 0.0000,  ..., 0.4334, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1185, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1184, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1184, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(972138.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.5267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3399.3892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.2614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-558.0709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4029],
        [ 0.3467],
        [ 0.1669],
        ...,
        [-3.4154],
        [-3.4081],
        [-3.4070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216341.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0374],
        [1.0378],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371777.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0375],
        [1.0379],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371782.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0098, -0.0018, -0.0022,  ...,  0.0202,  0.0099, -0.0032],
        [ 0.0205, -0.0020, -0.0024,  ...,  0.0351,  0.0188, -0.0063],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6096.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2518, device='cuda:0')



h[100].sum tensor(-0.0996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-21.2113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0478, 0.0197, 0.0000],
        [0.0369, 0.0000, 0.0000,  ..., 0.0763, 0.0366, 0.0000],
        [0.0563, 0.0000, 0.0000,  ..., 0.1055, 0.0539, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97955.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0381, 0.0000,  ..., 0.2929, 0.0000, 0.0059],
        [0.0000, 0.0947, 0.0000,  ..., 0.4461, 0.0000, 0.0201],
        [0.0000, 0.1505, 0.0000,  ..., 0.5875, 0.0000, 0.0383],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1368, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1182, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(984739.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(617.2450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3519.1255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.5031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.3277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2075],
        [ 0.3033],
        [ 0.3630],
        ...,
        [-2.7986],
        [-3.1695],
        [-3.3467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234460.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0375],
        [1.0379],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371782.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0376],
        [1.0380],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371787., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0017, -0.0021,  ...,  0.0135,  0.0058, -0.0018],
        [ 0.0151, -0.0019, -0.0023,  ...,  0.0276,  0.0142, -0.0047],
        [ 0.0188, -0.0019, -0.0024,  ...,  0.0327,  0.0173, -0.0058],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6244.6519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3394, device='cuda:0')



h[100].sum tensor(-0.1058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0140, device='cuda:0')



h[200].sum tensor(-22.7454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0000, 0.0000,  ..., 0.0669, 0.0307, 0.0000],
        [0.0584, 0.0000, 0.0000,  ..., 0.1085, 0.0554, 0.0000],
        [0.0883, 0.0000, 0.0000,  ..., 0.1501, 0.0802, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99489.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1062, 0.0000,  ..., 0.4727, 0.0000, 0.0220],
        [0.0000, 0.1439, 0.0000,  ..., 0.5688, 0.0000, 0.0353],
        [0.0000, 0.1716, 0.0000,  ..., 0.6383, 0.0000, 0.0451],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(983895.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(625.3786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3667.1606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.8333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-546.3555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0785],
        [ 0.0991],
        [ 0.1206],
        ...,
        [-3.4625],
        [-3.4549],
        [-3.4537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281253.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0376],
        [1.0380],
        ...,
        [0.9970],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371787., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0376],
        [1.0380],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371791.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5368.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.3796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8760, device='cuda:0')



h[100].sum tensor(-0.0676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-14.6852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85592.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1113, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1126, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1172, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(920418.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(567.7230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3905.9587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.0730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-581.1062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6532],
        [-3.6623],
        [-3.6543],
        ...,
        [-3.4762],
        [-3.4714],
        [-3.4706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302387.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0376],
        [1.0380],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371791.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0377],
        [1.0381],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371797.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5330.4658, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.1291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8400, device='cuda:0')



h[100].sum tensor(-0.0650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-14.2599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84618.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1129, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1175, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1174, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1174, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(912508.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(562.0153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4015.8647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.1038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.8325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3852],
        [-3.5046],
        [-3.5906],
        ...,
        [-3.4849],
        [-3.4772],
        [-3.4759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315776.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0377],
        [1.0381],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371797.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0378],
        [1.0382],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371802.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0053, -0.0017, -0.0021,  ...,  0.0140,  0.0061, -0.0019],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5634.9868, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.2952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9975, device='cuda:0')



h[100].sum tensor(-0.0762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-16.8867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0000, 0.0000,  ..., 0.0332, 0.0106, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0286, 0.0078, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0269, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89283.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.2164, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1875, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.1765, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(935336.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(577.6562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3989.3083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.7275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-564.6232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5406],
        [-1.0345],
        [-1.5396],
        ...,
        [-3.4701],
        [-3.4624],
        [-3.4612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284105.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0378],
        [1.0382],
        ...,
        [0.9969],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371802.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0378],
        [1.0383],
        ...,
        [0.9969],
        [0.9954],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371807.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037, -0.0017, -0.0021,  ...,  0.0117,  0.0048, -0.0015],
        [ 0.0037, -0.0017, -0.0021,  ...,  0.0117,  0.0048, -0.0015],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6221.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3244, device='cuda:0')



h[100].sum tensor(-0.0986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-22.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0000, 0.0000,  ..., 0.0457, 0.0183, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0392, 0.0144, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0324, 0.0103, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101351.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0233, 0.0000,  ..., 0.2670, 0.0000, 0.0000],
        [0.0000, 0.0181, 0.0000,  ..., 0.2503, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.2130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1010934.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(624.1907, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3845.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.1637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-529.7004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2325],
        [-0.3543],
        [-0.8281],
        ...,
        [-3.4500],
        [-3.4426],
        [-3.4415]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253974.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0378],
        [1.0383],
        ...,
        [0.9969],
        [0.9954],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371807.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0379],
        [1.0383],
        ...,
        [0.9969],
        [0.9954],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371812.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5351.6377, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8160, device='cuda:0')



h[100].sum tensor(-0.0612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-13.8477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86559.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1160, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(930532.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(561.5383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3860.9617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.9405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.2895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5798],
        [-2.9328],
        [-3.1710],
        ...,
        [-3.4295],
        [-3.4220],
        [-3.4207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226423.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0379],
        [1.0383],
        ...,
        [0.9969],
        [0.9954],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371812.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371816.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        ...,
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0044,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5595.1826, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.8208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9535, device='cuda:0')



h[100].sum tensor(-0.0703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-16.0532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0183, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89573.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1376, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(943500.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(572.8950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3704.2778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.5566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.6885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6756],
        [-2.2292],
        [-1.6188],
        ...,
        [-3.4417],
        [-3.4344],
        [-3.4333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212056.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371816.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371821.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7334.5044, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8778, device='cuda:0')



h[100].sum tensor(-0.1394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0196, device='cuda:0')



h[200].sum tensor(-32.1737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0184, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(121248.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1300, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1260, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1118869.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(703.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3438.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.9554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.7242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6254],
        [-2.2455],
        [-2.4960],
        ...,
        [-3.4631],
        [-3.4557],
        [-3.4546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221090.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371821.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371821.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071, -0.0017, -0.0021,  ...,  0.0166,  0.0077, -0.0024],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6198.2461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2778, device='cuda:0')



h[100].sum tensor(-0.0941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-21.7107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0195, 0.0000, 0.0000,  ..., 0.0500, 0.0209, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0311, 0.0096, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100690.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0552, 0.0000,  ..., 0.3391, 0.0000, 0.0100],
        [0.0000, 0.0164, 0.0000,  ..., 0.2154, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.1431, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1002749.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.0098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3664.6255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-293.4552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-524.8339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2281],
        [-0.9090],
        [-1.6221],
        ...,
        [-3.4631],
        [-3.4557],
        [-3.4546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259722.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0380],
        [1.0384],
        ...,
        [0.9969],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371821.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0381],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371825.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0062, -0.0017, -0.0021,  ...,  0.0153,  0.0069, -0.0022],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6671.8252, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5646, device='cuda:0')



h[100].sum tensor(-0.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0163, device='cuda:0')



h[200].sum tensor(-26.1969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0019, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0299, 0.0086, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0279, 0.0074, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(106591.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1282, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.1636, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.1899, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1029161.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(645.4741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3232.4595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.4500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-513.8295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6861],
        [-2.0338],
        [-1.2890],
        ...,
        [-3.4827],
        [-3.4752],
        [-3.4742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245335.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0381],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371825.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0385],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371829.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094, -0.0018, -0.0022,  ...,  0.0197,  0.0095, -0.0031],
        [ 0.0179, -0.0019, -0.0023,  ...,  0.0315,  0.0165, -0.0054],
        [ 0.0335, -0.0022, -0.0027,  ...,  0.0532,  0.0294, -0.0098],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5753.8018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0678, device='cuda:0')



h[100].sum tensor(-0.0759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-17.8783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1480, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0336, 0.0000, 0.0000,  ..., 0.0698, 0.0323, 0.0000],
        [0.0807, 0.0000, 0.0000,  ..., 0.1398, 0.0739, 0.0000],
        [0.1010, 0.0000, 0.0000,  ..., 0.1680, 0.0907, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92053., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0949, 0.0000,  ..., 0.4411, 0.0000, 0.0202],
        [0.0000, 0.1869, 0.0000,  ..., 0.6742, 0.0000, 0.0496],
        [0.0000, 0.2556, 0.0000,  ..., 0.8463, 0.0000, 0.0736],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1481, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(950936.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(585.3687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3449.4751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.3203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-547.9352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2304],
        [ 0.3476],
        [ 0.3710],
        ...,
        [-2.1696],
        [-2.9214],
        [-3.3068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287334.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0385],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371829.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0385],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371829.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0082, -0.0018, -0.0021,  ...,  0.0180,  0.0085, -0.0027],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [ 0.0082, -0.0018, -0.0021,  ...,  0.0180,  0.0085, -0.0027],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6040.6377, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2323, device='cuda:0')



h[100].sum tensor(-0.0871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-20.5110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0000, 0.0000,  ..., 0.0300, 0.0086, 0.0000],
        [0.0302, 0.0000, 0.0000,  ..., 0.0696, 0.0321, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0303, 0.0087, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97959.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.1903, 0.0000, 0.0000],
        [0.0000, 0.0233, 0.0000,  ..., 0.2619, 0.0000, 0.0000],
        [0.0000, 0.0151, 0.0000,  ..., 0.2393, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1187, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(990843.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(608.9366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3324.8545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.1790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-534.5244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2996],
        [-0.5631],
        [-0.0718],
        ...,
        [-3.5116],
        [-3.5040],
        [-3.5028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262303.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0385],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371829.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371831.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [ 0.0044, -0.0017, -0.0021,  ...,  0.0128,  0.0054, -0.0017],
        [ 0.0041, -0.0017, -0.0021,  ...,  0.0124,  0.0051, -0.0016],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5499.1357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9299, device='cuda:0')



h[100].sum tensor(-0.0657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-15.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0271, 0.0069, 0.0000],
        [0.0170, 0.0000, 0.0000,  ..., 0.0469, 0.0186, 0.0000],
        [0.0358, 0.0000, 0.0000,  ..., 0.0775, 0.0368, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88454.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.2003, 0.0000, 0.0000],
        [0.0000, 0.0312, 0.0000,  ..., 0.2795, 0.0000, 0.0018],
        [0.0000, 0.0549, 0.0000,  ..., 0.3474, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(936255.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(571.2606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3274.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.8143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-554.9991, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8227],
        [-0.2146],
        [ 0.0853],
        ...,
        [-3.5257],
        [-3.5180],
        [-3.5168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281046.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371831.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371834.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6097.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2675, device='cuda:0')



h[100].sum tensor(-0.0880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-21.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95655.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1133, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(964048.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(602.5648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3073.4866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.0438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-535.9128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1481],
        [-3.4179],
        [-3.5813],
        ...,
        [-3.5357],
        [-3.5280],
        [-3.5267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268602.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371834.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9953],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371837.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6597.1924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5276, device='cuda:0')



h[100].sum tensor(-0.1060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0159, device='cuda:0')



h[200].sum tensor(-25.7391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(107268.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1526, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1461, 0.0000, 0.0000],
        [0.0000, 0.0215, 0.0000,  ..., 0.2224, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1046582.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(652.7516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2686.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.9999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-507.0434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5021],
        [-0.3859],
        [-0.0996],
        ...,
        [-3.5419],
        [-3.5341],
        [-3.5328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246009.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0382],
        [1.0384],
        ...,
        [0.9968],
        [0.9953],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371837.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0384],
        ...,
        [0.9968],
        [0.9953],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371840.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0128, -0.0018, -0.0022,  ...,  0.0245,  0.0124, -0.0040],
        [ 0.0195, -0.0020, -0.0024,  ...,  0.0338,  0.0180, -0.0058],
        [ 0.0128, -0.0018, -0.0022,  ...,  0.0245,  0.0124, -0.0040],
        ...,
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0016, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6468.7524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4704, device='cuda:0')



h[100].sum tensor(-0.1002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-24.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0730, 0.0000, 0.0000,  ..., 0.1293, 0.0681, 0.0000],
        [0.0792, 0.0000, 0.0000,  ..., 0.1380, 0.0732, 0.0000],
        [0.1403, 0.0000, 0.0000,  ..., 0.2232, 0.1240, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103305.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1969, 0.0000,  ..., 0.7179, 0.0000, 0.0579],
        [0.0000, 0.2389, 0.0000,  ..., 0.8276, 0.0000, 0.0734],
        [0.0000, 0.3226, 0.0000,  ..., 1.0428, 0.0000, 0.1039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1293, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1008602., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(639.3366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2660.7686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.9892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-515.6638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5614],
        [ 0.5666],
        [ 0.5565],
        ...,
        [-3.4956],
        [-3.3768],
        [-3.1621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257063.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0384],
        ...,
        [0.9968],
        [0.9953],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371840.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0383],
        ...,
        [0.9968],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371844.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6022.1118, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2140, device='cuda:0')



h[100].sum tensor(-0.0826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-20.4711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97008.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1172, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1152, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982159.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(615.4573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2508.1128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.0248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-534.3517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0776],
        [-2.6975],
        [-3.1041],
        ...,
        [-3.5504],
        [-3.5424],
        [-3.5410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256596.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0383],
        ...,
        [0.9968],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371844.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0383],
        ...,
        [0.9968],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371844.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0299, -0.0021, -0.0026,  ...,  0.0483,  0.0266, -0.0087],
        [ 0.0330, -0.0022, -0.0027,  ...,  0.0526,  0.0292, -0.0096],
        [ 0.0205, -0.0020, -0.0024,  ...,  0.0351,  0.0188, -0.0061],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5295.2329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-34.8297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8332, device='cuda:0')



h[100].sum tensor(-0.0559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-13.8535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1441, 0.0000, 0.0000,  ..., 0.2280, 0.1269, 0.0000],
        [0.1201, 0.0000, 0.0000,  ..., 0.1949, 0.1072, 0.0000],
        [0.0899, 0.0000, 0.0000,  ..., 0.1529, 0.0821, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84693.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3270, 0.0000,  ..., 1.0461, 0.0000, 0.1051],
        [0.0000, 0.2957, 0.0000,  ..., 0.9680, 0.0000, 0.0935],
        [0.0000, 0.2396, 0.0000,  ..., 0.8256, 0.0000, 0.0729],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(919912.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(563.3256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2693.6660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.3101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-562.7816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5583],
        [ 0.5508],
        [ 0.5449],
        ...,
        [-3.5572],
        [-3.5493],
        [-3.5479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285262.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0383],
        [1.0383],
        ...,
        [0.9968],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371844.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 250.0 event: 1250 loss: tensor(836.3500, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371848.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0017, -0.0020,  ...,  0.0111,  0.0044, -0.0013],
        [ 0.0032, -0.0017, -0.0020,  ...,  0.0111,  0.0044, -0.0013],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5556.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9810, device='cuda:0')



h[100].sum tensor(-0.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-16.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0000, 0.0000,  ..., 0.0309, 0.0094, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0312, 0.0095, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0312, 0.0095, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87938.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.1862, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.1943, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.1887, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(935008.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(579.0382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2379.0356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.5815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-560.8047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3160],
        [-1.1231],
        [-1.1661],
        ...,
        [-3.5703],
        [-3.5622],
        [-3.5608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259645.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371848.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371852.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0097, -0.0018, -0.0022,  ...,  0.0200,  0.0097, -0.0031],
        [ 0.0198, -0.0020, -0.0024,  ...,  0.0342,  0.0182, -0.0059],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5215.1362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.6614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7983, device='cuda:0')



h[100].sum tensor(-0.0520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-13.1638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0000, 0.0000,  ..., 0.0346, 0.0115, 0.0000],
        [0.0373, 0.0000, 0.0000,  ..., 0.0750, 0.0355, 0.0000],
        [0.0936, 0.0000, 0.0000,  ..., 0.1577, 0.0848, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83441.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0340, 0.0000,  ..., 0.2584, 0.0000, 0.0057],
        [0.0000, 0.1015, 0.0000,  ..., 0.4560, 0.0000, 0.0243],
        [0.0000, 0.2103, 0.0000,  ..., 0.7355, 0.0000, 0.0605],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1190, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(915251.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(563.3848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2440.7427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.1316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-575.9763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0363],
        [-0.2525],
        [ 0.2376],
        ...,
        [-3.5848],
        [-3.5766],
        [-3.5752]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308706.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371852.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371856.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061, -0.0017, -0.0021,  ...,  0.0151,  0.0068, -0.0021],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [ 0.0109, -0.0018, -0.0022,  ...,  0.0217,  0.0108, -0.0034],
        ...,
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0015, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5818.7744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1246, device='cuda:0')



h[100].sum tensor(-0.0728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-18.6070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0358, 0.0000, 0.0000,  ..., 0.0748, 0.0355, 0.0000],
        [0.0316, 0.0000, 0.0000,  ..., 0.0713, 0.0333, 0.0000],
        [0.0232, 0.0000, 0.0000,  ..., 0.0555, 0.0239, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0021, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0431, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91525.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1152, 0.0000,  ..., 0.4911, 0.0000, 0.0256],
        [0.0000, 0.0833, 0.0000,  ..., 0.4117, 0.0000, 0.0137],
        [0.0000, 0.0734, 0.0000,  ..., 0.3861, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.1591, 0.0000, 0.0000],
        [0.0000, 0.0291, 0.0000,  ..., 0.2571, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(952050.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.0537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2280.0972, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.0710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.9228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5086],
        [ 0.5190],
        [ 0.5209],
        ...,
        [-3.2511],
        [-2.7054],
        [-1.8786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302440.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371856.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371860.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0119, -0.0018, -0.0022,  ...,  0.0231,  0.0116, -0.0037],
        [ 0.0102, -0.0018, -0.0022,  ...,  0.0207,  0.0102, -0.0032],
        [ 0.0099, -0.0018, -0.0022,  ...,  0.0203,  0.0100, -0.0031],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5807.0127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0897, device='cuda:0')



h[100].sum tensor(-0.0708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-18.2952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0437, 0.0000, 0.0000,  ..., 0.0878, 0.0434, 0.0000],
        [0.0600, 0.0000, 0.0000,  ..., 0.1106, 0.0569, 0.0000],
        [0.0571, 0.0000, 0.0000,  ..., 0.1067, 0.0546, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92608.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1253, 0.0000,  ..., 0.5179, 0.0000, 0.0291],
        [0.0000, 0.1662, 0.0000,  ..., 0.6238, 0.0000, 0.0440],
        [0.0000, 0.1720, 0.0000,  ..., 0.6390, 0.0000, 0.0459],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(961045.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(602.9781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2259.3315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.3606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.1368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3468],
        [ 0.4753],
        [ 0.5155],
        ...,
        [-3.4669],
        [-3.3725],
        [-3.3118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273195.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371860.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371865.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0017, -0.0021,  ...,  0.0125,  0.0054, -0.0016],
        [ 0.0045, -0.0017, -0.0021,  ...,  0.0127,  0.0055, -0.0016],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5495.7168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9192, device='cuda:0')



h[100].sum tensor(-0.0584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-15.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0358, 0.0000, 0.0000,  ..., 0.0765, 0.0369, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0469, 0.0192, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0273, 0.0074, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86591.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0575, 0.0000,  ..., 0.3448, 0.0000, 0.0045],
        [0.0000, 0.0335, 0.0000,  ..., 0.2833, 0.0000, 0.0023],
        [0.0000, 0.0114, 0.0000,  ..., 0.2130, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(924883.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.7209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2452.5671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.6331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-575.9430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2680],
        [ 0.1970],
        [ 0.0463],
        ...,
        [-3.5633],
        [-3.5549],
        [-3.5527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278245.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371865.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371870.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6123.8198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2409, device='cuda:0')



h[100].sum tensor(-0.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-20.6695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0299, 0.0092, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0244, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97045.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1693, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1542, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1356, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(983144.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(617.7472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2372.2974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.8684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-553.7820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4330],
        [-2.6645],
        [-2.9316],
        ...,
        [-3.5509],
        [-3.5428],
        [-3.5412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246215.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371870.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371875.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5566.1924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9297, device='cuda:0')



h[100].sum tensor(-0.0583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-15.5381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0186, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90123.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1154, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(952644.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.4802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2388.2490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.0118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-576.5485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5241],
        [-3.5983],
        [-3.6497],
        ...,
        [-3.5383],
        [-3.5327],
        [-3.5326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243148.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371875.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5438.6025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8679, device='cuda:0')



h[100].sum tensor(-0.0534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-14.3589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0273, 0.0075, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86520.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0177, 0.0000,  ..., 0.2102, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.1502, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1402, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(927972., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(578.4413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2514.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.8030, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.4224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7017],
        [-1.3121],
        [-1.6240],
        ...,
        [-3.5624],
        [-3.5542],
        [-3.5525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282087.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5873.6914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1126, device='cuda:0')



h[100].sum tensor(-0.0679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-18.2687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1542, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91436.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1143, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(945447.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.0034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2357.4167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.9263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.7715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6290],
        [-3.6640],
        [-3.6784],
        ...,
        [-3.5593],
        [-3.5506],
        [-3.5486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248153.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [ 0.0173, -0.0019, -0.0023,  ...,  0.0305,  0.0161, -0.0051],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5697.0635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0061, device='cuda:0')



h[100].sum tensor(-0.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-16.6815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0024, 0.0000],
        [0.0325, 0.0000, 0.0000,  ..., 0.0679, 0.0317, 0.0000],
        [0.0666, 0.0000, 0.0000,  ..., 0.1174, 0.0612, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90140.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0244, 0.0000,  ..., 0.2305, 0.0000, 0.0016],
        [0.0000, 0.0860, 0.0000,  ..., 0.4096, 0.0000, 0.0181],
        [0.0000, 0.1598, 0.0000,  ..., 0.5955, 0.0000, 0.0398],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(943711.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(593.5915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2441.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.1518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.2443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1964],
        [ 0.1278],
        [ 0.3139],
        ...,
        [-3.5624],
        [-3.5542],
        [-3.5525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263362.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371879.7812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 260.0 event: 1300 loss: tensor(373.3324, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0386],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371884.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0017, -0.0021,  ...,  0.0119,  0.0049, -0.0014],
        [ 0.0051, -0.0017, -0.0021,  ...,  0.0135,  0.0059, -0.0018],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6268.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3236, device='cuda:0')



h[100].sum tensor(-0.0802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-21.8148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0382, 0.0000, 0.0000,  ..., 0.0796, 0.0386, 0.0000],
        [0.0175, 0.0000, 0.0000,  ..., 0.0472, 0.0192, 0.0000],
        [0.0105, 0.0000, 0.0000,  ..., 0.0375, 0.0134, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102222.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 6.4322e-02, 0.0000e+00,  ..., 3.5534e-01, 0.0000e+00,
         5.1858e-03],
        [0.0000e+00, 3.9974e-02, 0.0000e+00,  ..., 2.9541e-01, 0.0000e+00,
         2.4393e-03],
        [0.0000e+00, 2.2413e-02, 0.0000e+00,  ..., 2.5145e-01, 0.0000e+00,
         1.3214e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1913e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1903e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1905e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1024910.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(646.2549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2359.2271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.1270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-557.8139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2590],
        [ 0.1830],
        [ 0.0261],
        ...,
        [-3.5740],
        [-3.5656],
        [-3.5639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280755.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0386],
        [1.0383],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371884.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0386],
        [1.0384],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371888.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6427.8892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3874, device='cuda:0')



h[100].sum tensor(-0.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-23.2168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103099.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1131, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1260, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1487, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1014373.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(649.8414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2507.1023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-302.5891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-557.7758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0508],
        [-2.9177],
        [-2.7060],
        ...,
        [-3.4988],
        [-3.3101],
        [-2.8978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288318.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0386],
        [1.0384],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371888.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0384],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371891.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0054, -0.0017, -0.0021,  ...,  0.0140,  0.0062, -0.0018],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5614.7285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9659, device='cuda:0')



h[100].sum tensor(-0.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-15.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0288, 0.0083, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0272, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91093.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1293, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.1568, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.1694, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(958411., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.3962, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2455.7629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.5082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-587.7316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3023],
        [-2.5762],
        [-2.6860],
        ...,
        [-3.5829],
        [-3.5745],
        [-3.5727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259495.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0384],
        ...,
        [0.9967],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371891.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0384],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371895.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6493.7432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4434, device='cuda:0')



h[100].sum tensor(-0.0848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-23.7940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103053.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1385e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1467e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1525e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.3495e-03, 0.0000e+00,  ..., 1.8013e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.7944e-04, 0.0000e+00,  ..., 1.7699e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.9341e-05, 0.0000e+00,  ..., 1.6271e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1020040.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(649.9593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2488.5835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.4193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-559.0051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5328],
        [-3.5959],
        [-3.6184],
        ...,
        [-1.3948],
        [-1.4223],
        [-1.5379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248897.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0384],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371895.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0387],
        [1.0384],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371898.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0071, -0.0017, -0.0021,  ...,  0.0165,  0.0077, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5803.5566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0765, device='cuda:0')



h[100].sum tensor(-0.0626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-17.7350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0025, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0324, 0.0101, 0.0000],
        [0.0060, 0.0000, 0.0000,  ..., 0.0301, 0.0087, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92806.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1355, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1365, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.1766, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.1929, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(961079.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(609.9875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2680.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.1111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.9562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2496],
        [-1.5502],
        [-1.6293],
        ...,
        [-3.2682],
        [-2.9692],
        [-2.7127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272745.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0387],
        [1.0384],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371898.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371902.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5593.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9649, device='cuda:0')



h[100].sum tensor(-0.0556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-15.9396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89468.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1449, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1280, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(947563.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.1204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2597.2095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.9768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-598.4362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0433],
        [-1.4452],
        [-1.6697],
        ...,
        [-3.6214],
        [-3.6128],
        [-3.6108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267301.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371902.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371905.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5278.7705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7999, device='cuda:0')



h[100].sum tensor(-0.0457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0083, device='cuda:0')



h[200].sum tensor(-13.2219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84765.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1134, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(926653.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(580.6312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2756.7759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.8587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.5165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5315],
        [-3.3796],
        [-3.1159],
        ...,
        [-3.6399],
        [-3.6311],
        [-3.6291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303032.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371905.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371909.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0331, -0.0022, -0.0027,  ...,  0.0526,  0.0291, -0.0092],
        [ 0.0257, -0.0021, -0.0025,  ...,  0.0424,  0.0230, -0.0072],
        [ 0.0217, -0.0020, -0.0024,  ...,  0.0368,  0.0197, -0.0062],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6068.6699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2394, device='cuda:0')



h[100].sum tensor(-0.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-20.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0991, 0.0000, 0.0000,  ..., 0.1653, 0.0892, 0.0000],
        [0.1264, 0.0000, 0.0000,  ..., 0.2035, 0.1119, 0.0000],
        [0.1339, 0.0000, 0.0000,  ..., 0.2140, 0.1182, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93747.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2913, 0.0000,  ..., 0.9520, 0.0000, 0.0910],
        [0.0000, 0.3568, 0.0000,  ..., 1.1201, 0.0000, 0.1147],
        [0.0000, 0.3838, 0.0000,  ..., 1.1893, 0.0000, 0.1244],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(953752.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(618.9555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2879.6079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-287.3511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.9361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4259],
        [ 0.3934],
        [ 0.3652],
        ...,
        [-3.6507],
        [-3.6419],
        [-3.6399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334797.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371909.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371913.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0265, -0.0021, -0.0025,  ...,  0.0435,  0.0237, -0.0074],
        [ 0.0173, -0.0019, -0.0023,  ...,  0.0308,  0.0161, -0.0050],
        [ 0.0365, -0.0022, -0.0027,  ...,  0.0575,  0.0320, -0.0101],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6435.1978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4309, device='cuda:0')



h[100].sum tensor(-0.0789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-23.3242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0769, 0.0000, 0.0000,  ..., 0.1345, 0.0708, 0.0000],
        [0.1175, 0.0000, 0.0000,  ..., 0.1912, 0.1045, 0.0000],
        [0.0618, 0.0000, 0.0000,  ..., 0.1138, 0.0584, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101468.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1905, 0.0000,  ..., 0.6964, 0.0000, 0.0544],
        [0.0000, 0.2386, 0.0000,  ..., 0.8207, 0.0000, 0.0717],
        [0.0000, 0.1817, 0.0000,  ..., 0.6761, 0.0000, 0.0508],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1001345., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(650.7066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2721.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.1363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-573.2086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4887],
        [ 0.5188],
        [ 0.4537],
        ...,
        [-3.6378],
        [-3.6287],
        [-3.6259]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274250.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371913.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371918.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0176, -0.0019, -0.0023,  ...,  0.0312,  0.0164, -0.0051],
        [ 0.0285, -0.0021, -0.0026,  ...,  0.0463,  0.0254, -0.0080],
        [ 0.0326, -0.0022, -0.0027,  ...,  0.0519,  0.0287, -0.0090],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5729.9014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0358, device='cuda:0')



h[100].sum tensor(-0.0564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-16.8501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0880, 0.0000, 0.0000,  ..., 0.1498, 0.0801, 0.0000],
        [0.0918, 0.0000, 0.0000,  ..., 0.1554, 0.0834, 0.0000],
        [0.1048, 0.0000, 0.0000,  ..., 0.1736, 0.0942, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89319.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2531, 0.0000,  ..., 0.8592, 0.0000, 0.0778],
        [0.0000, 0.2471, 0.0000,  ..., 0.8455, 0.0000, 0.0755],
        [0.0000, 0.2338, 0.0000,  ..., 0.8112, 0.0000, 0.0704],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(938360., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(594.8906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3126.0920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-595.6437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5011],
        [ 0.4953],
        [ 0.4537],
        ...,
        [-3.6258],
        [-3.6175],
        [-3.6162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284289.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371918.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 270.0 event: 1350 loss: tensor(496.0511, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371923.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5630.1201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9578, device='cuda:0')



h[100].sum tensor(-0.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-15.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89068.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1177, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1482, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.1911, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(939227., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.3084, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3338.8042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.3708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2477],
        [-1.5450],
        [-0.7865],
        ...,
        [-3.6086],
        [-3.5998],
        [-3.5943]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272376.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0385],
        ...,
        [0.9966],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371923.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0385],
        ...,
        [0.9965],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371928.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5397.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8199, device='cuda:0')



h[100].sum tensor(-0.0443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-13.4985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0279, 0.0077, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0287, 0.0081, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85716.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1561, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1605, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(928234.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(573.6553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3470.6763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.3586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.1005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7196],
        [-2.9358],
        [-2.9408],
        ...,
        [-3.6098],
        [-3.6013],
        [-3.5993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269152.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0385],
        ...,
        [0.9965],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371928.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0387],
        [1.0386],
        ...,
        [0.9965],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371931.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6131.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2317, device='cuda:0')



h[100].sum tensor(-0.0646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-19.9159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95762.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1170, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1306, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(970588.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.9836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3336.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.6843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-572.8690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9418],
        [-2.6623],
        [-2.2790],
        ...,
        [-3.6158],
        [-3.6072],
        [-3.6051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243634.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0387],
        [1.0386],
        ...,
        [0.9965],
        [0.9952],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371931.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0387],
        [1.0386],
        ...,
        [0.9965],
        [0.9951],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371935.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054, -0.0017, -0.0021,  ...,  0.0141,  0.0062, -0.0018],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [ 0.0054, -0.0017, -0.0021,  ...,  0.0141,  0.0062, -0.0018],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6838.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6128, device='cuda:0')



h[100].sum tensor(-0.0838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0168, device='cuda:0')



h[200].sum tensor(-26.1096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0272, 0.0071, 0.0000],
        [0.0198, 0.0000, 0.0000,  ..., 0.0550, 0.0236, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0275, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108154.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.1937, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.2235, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.1971, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1037880.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.7695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3137.1646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-316.2396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-544.8510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6684],
        [-0.7237],
        [-0.6326],
        ...,
        [-3.6329],
        [-3.6242],
        [-3.6222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246592.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0387],
        [1.0386],
        ...,
        [0.9965],
        [0.9951],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371935.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371939.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [ 0.0057, -0.0017, -0.0021,  ...,  0.0146,  0.0065, -0.0019],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5647.6890, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9602, device='cuda:0')



h[100].sum tensor(-0.0497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-15.6563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0176, 0.0000, 0.0000,  ..., 0.0497, 0.0204, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0464, 0.0184, 0.0000],
        [0.0252, 0.0000, 0.0000,  ..., 0.0626, 0.0280, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89268.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.6294e-02, 0.0000e+00,  ..., 2.6880e-01, 0.0000e+00,
         7.1276e-05],
        [0.0000e+00, 2.7891e-02, 0.0000e+00,  ..., 2.7798e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.9020e-02, 0.0000e+00,  ..., 2.8207e-01, 0.0000e+00,
         7.1904e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2017e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2006e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2008e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(941725.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.3103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3379.3308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-592.0960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4470],
        [-0.2161],
        [-0.3354],
        ...,
        [-3.6450],
        [-3.6363],
        [-3.6342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277663.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371939.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371943.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0206, -0.0020, -0.0024,  ...,  0.0352,  0.0188, -0.0058],
        [ 0.0131, -0.0018, -0.0022,  ...,  0.0248,  0.0126, -0.0038],
        [ 0.0156, -0.0019, -0.0023,  ...,  0.0282,  0.0146, -0.0044],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5722.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0061, device='cuda:0')



h[100].sum tensor(-0.0513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-16.3142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0574, 0.0000, 0.0000,  ..., 0.1070, 0.0546, 0.0000],
        [0.0672, 0.0000, 0.0000,  ..., 0.1208, 0.0627, 0.0000],
        [0.0605, 0.0000, 0.0000,  ..., 0.1116, 0.0572, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92544.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1519, 0.0000,  ..., 0.5938, 0.0000, 0.0398],
        [0.0000, 0.1523, 0.0000,  ..., 0.5963, 0.0000, 0.0399],
        [0.0000, 0.1371, 0.0000,  ..., 0.5582, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(967413.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.2172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3344.9351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.4938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-584.4878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5214],
        [ 0.5199],
        [ 0.5166],
        ...,
        [-3.6573],
        [-3.6485],
        [-3.6464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280473.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371943.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371946.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5515.6738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8865, device='cuda:0')



h[100].sum tensor(-0.0450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-14.4670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0262, 0.0065, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87198.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.2065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1634, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1326, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(933959.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(584.2388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3600.4724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.9047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-595.4978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4638],
        [-1.1764],
        [-1.8701],
        ...,
        [-3.6633],
        [-3.6546],
        [-3.6525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287942.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0387],
        [1.0387],
        ...,
        [0.9964],
        [0.9951],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371946.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0386],
        [1.0387],
        ...,
        [0.9963],
        [0.9950],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371950.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7212.2793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8308, device='cuda:0')



h[100].sum tensor(-0.0899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0191, device='cuda:0')



h[200].sum tensor(-29.2024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0000, 0.0000,  ..., 0.0634, 0.0288, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(111759.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0946, 0.0000,  ..., 0.4404, 0.0000, 0.0237],
        [0.0000, 0.0206, 0.0000,  ..., 0.2157, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.1425, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1047524.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(686.1122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3390.0828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-325.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-533.8043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1100],
        [-0.2828],
        [-0.7373],
        ...,
        [-3.6659],
        [-3.6571],
        [-3.6550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256750.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0386],
        [1.0387],
        ...,
        [0.9963],
        [0.9950],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371950.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0386],
        [1.0387],
        ...,
        [0.9963],
        [0.9950],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371954.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0017, -0.0021,  ...,  0.0142,  0.0063, -0.0018],
        [ 0.0146, -0.0019, -0.0023,  ...,  0.0268,  0.0138, -0.0042],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5891.3623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0991, device='cuda:0')



h[100].sum tensor(-0.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-17.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0721, 0.0000, 0.0000,  ..., 0.1273, 0.0670, 0.0000],
        [0.0362, 0.0000, 0.0000,  ..., 0.0756, 0.0361, 0.0000],
        [0.0380, 0.0000, 0.0000,  ..., 0.0781, 0.0376, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91338.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1656, 0.0000,  ..., 0.6333, 0.0000, 0.0458],
        [0.0000, 0.1106, 0.0000,  ..., 0.4935, 0.0000, 0.0255],
        [0.0000, 0.0793, 0.0000,  ..., 0.4131, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(950166.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(602.6102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3504.9360, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.8843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.5651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6040],
        [ 0.5064],
        [ 0.1550],
        ...,
        [-3.6708],
        [-3.6620],
        [-3.6599]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258047.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0386],
        [1.0387],
        ...,
        [0.9963],
        [0.9950],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371954.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0387],
        ...,
        [0.9962],
        [0.9950],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371958.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5426.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8397, device='cuda:0')



h[100].sum tensor(-0.0409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-13.5880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0454, 0.0181, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86648.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.1931, 0.0000, 0.0008],
        [0.0000, 0.0535, 0.0000,  ..., 0.3314, 0.0000, 0.0110],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(938594.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(586.4687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3466.7756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.3148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.9362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0852],
        [-0.5911],
        [-0.0511],
        ...,
        [-3.6853],
        [-3.6764],
        [-3.6743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270345.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0386],
        [1.0387],
        ...,
        [0.9962],
        [0.9950],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371958.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 280.0 event: 1400 loss: tensor(480.0016, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0387],
        ...,
        [0.9962],
        [0.9950],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371962.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5699.7998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9813, device='cuda:0')



h[100].sum tensor(-0.0476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-15.9537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1360, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0024, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0451, 0.0179, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90513.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1431, 0.0000, 0.0000],
        [0.0000, 0.0173, 0.0000,  ..., 0.2169, 0.0000, 0.0000],
        [0.0000, 0.0531, 0.0000,  ..., 0.3342, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(953984.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(604.8564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3396.6296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-287.2230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-587.0179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9418],
        [-0.2938],
        [ 0.1496],
        ...,
        [-3.6469],
        [-3.6591],
        [-3.6693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297017.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0387],
        ...,
        [0.9962],
        [0.9950],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371962.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0388],
        ...,
        [0.9962],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371966.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5840.6240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0539, device='cuda:0')



h[100].sum tensor(-0.0505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-17.1211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91993.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.1884, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.2004, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.2007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(958307.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(612.2368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3360.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.7926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-584.9456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4953],
        [-0.2047],
        [-0.0186],
        ...,
        [-3.7128],
        [-3.7039],
        [-3.7018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302381.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0388],
        ...,
        [0.9962],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371966.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0388],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371971.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0017, -0.0021,  ...,  0.0140,  0.0062, -0.0018],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0055, -0.0017, -0.0021,  ...,  0.0140,  0.0062, -0.0018],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5793.6538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0258, device='cuda:0')



h[100].sum tensor(-0.0485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-16.6029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0338, 0.0113, 0.0000],
        [0.0303, 0.0000, 0.0000,  ..., 0.0688, 0.0322, 0.0000],
        [0.0095, 0.0000, 0.0000,  ..., 0.0342, 0.0115, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91370.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0144, 0.0000,  ..., 0.2265, 0.0000, 0.0000],
        [0.0000, 0.0347, 0.0000,  ..., 0.2895, 0.0000, 0.0000],
        [0.0000, 0.0160, 0.0000,  ..., 0.2338, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(958611.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.1860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3241.0522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.9949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-588.4546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9005],
        [-0.4950],
        [-0.5905],
        ...,
        [-3.7124],
        [-3.7032],
        [-3.6978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275218.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0388],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371971.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0389],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371975.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138, -0.0019, -0.0023,  ...,  0.0256,  0.0131, -0.0039],
        [ 0.0056, -0.0017, -0.0021,  ...,  0.0142,  0.0064, -0.0018],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5496.7207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8609, device='cuda:0')



h[100].sum tensor(-0.0402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-13.9377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0000, 0.0000,  ..., 0.0466, 0.0190, 0.0000],
        [0.0189, 0.0000, 0.0000,  ..., 0.0490, 0.0204, 0.0000],
        [0.0301, 0.0000, 0.0000,  ..., 0.0686, 0.0320, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86177.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0534, 0.0000,  ..., 0.3332, 0.0000, 0.0055],
        [0.0000, 0.0474, 0.0000,  ..., 0.3200, 0.0000, 0.0035],
        [0.0000, 0.0581, 0.0000,  ..., 0.3478, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1204, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(933002., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(589.0848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3351.6311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.3696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.8896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5434],
        [ 0.5569],
        [ 0.5658],
        ...,
        [-3.7203],
        [-3.7114],
        [-3.7094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290211.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0389],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371975.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0389],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371980.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0018, -0.0022,  ...,  0.0182,  0.0087, -0.0025],
        [ 0.0119, -0.0018, -0.0022,  ...,  0.0229,  0.0116, -0.0034],
        [ 0.0117, -0.0018, -0.0022,  ...,  0.0226,  0.0114, -0.0033],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6836.8486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5752, device='cuda:0')



h[100].sum tensor(-0.0725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0164, device='cuda:0')



h[200].sum tensor(-25.3799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0000, 0.0000,  ..., 0.0492, 0.0207, 0.0000],
        [0.0308, 0.0000, 0.0000,  ..., 0.0675, 0.0315, 0.0000],
        [0.0711, 0.0000, 0.0000,  ..., 0.1254, 0.0660, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(107633.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0644, 0.0000,  ..., 0.3613, 0.0000, 0.0076],
        [0.0000, 0.1050, 0.0000,  ..., 0.4637, 0.0000, 0.0205],
        [0.0000, 0.1685, 0.0000,  ..., 0.6220, 0.0000, 0.0429],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1039280.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(676.3997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3407.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-324.9605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.6806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4172],
        [ 0.4862],
        [ 0.4756],
        ...,
        [-3.7169],
        [-3.7080],
        [-3.7061]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285073.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0389],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371980.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371985., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [ 0.0102, -0.0018, -0.0022,  ...,  0.0206,  0.0102, -0.0030],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5567.3706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8931, device='cuda:0')



h[100].sum tensor(-0.0404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-14.2776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0026, 0.0000],
        [0.0106, 0.0000, 0.0000,  ..., 0.0355, 0.0126, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0326, 0.0109, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87082.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1434, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.2038, 0.0000, 0.0012],
        [0.0000, 0.0345, 0.0000,  ..., 0.2773, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1214, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940125.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(589.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3611.4294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.3391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.2168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9475],
        [-1.6027],
        [-0.9180],
        ...,
        [-3.7059],
        [-3.6969],
        [-3.6884]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279461.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371985., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371989.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0161, -0.0019, -0.0023,  ...,  0.0287,  0.0151, -0.0045],
        [ 0.0390, -0.0023, -0.0028,  ...,  0.0606,  0.0340, -0.0103],
        [ 0.0152, -0.0019, -0.0023,  ...,  0.0274,  0.0143, -0.0042],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5984.4697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0994, device='cuda:0')



h[100].sum tensor(-0.0497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-17.7845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0822, 0.0000, 0.0000,  ..., 0.1404, 0.0752, 0.0000],
        [0.0640, 0.0000, 0.0000,  ..., 0.1154, 0.0603, 0.0000],
        [0.0668, 0.0000, 0.0000,  ..., 0.1174, 0.0614, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93295.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1729, 0.0000,  ..., 0.6304, 0.0000, 0.0440],
        [0.0000, 0.1694, 0.0000,  ..., 0.6233, 0.0000, 0.0428],
        [0.0000, 0.1481, 0.0000,  ..., 0.5705, 0.0000, 0.0353],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(967418.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.8630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3476.7759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.9641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-584.8333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4787],
        [ 0.4000],
        [ 0.1419],
        ...,
        [-3.7108],
        [-3.7020],
        [-3.7002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252477.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371989.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371993.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0114, -0.0018, -0.0022,  ...,  0.0222,  0.0112, -0.0033],
        [ 0.0060, -0.0017, -0.0021,  ...,  0.0147,  0.0067, -0.0019],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5696.0605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9534, device='cuda:0')



h[100].sum tensor(-0.0423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-15.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0384, 0.0000, 0.0000,  ..., 0.0796, 0.0389, 0.0000],
        [0.0213, 0.0000, 0.0000,  ..., 0.0522, 0.0225, 0.0000],
        [0.0063, 0.0000, 0.0000,  ..., 0.0295, 0.0089, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89116.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0660, 0.0000,  ..., 0.3646, 0.0000, 0.0064],
        [0.0000, 0.0393, 0.0000,  ..., 0.2937, 0.0000, 0.0037],
        [0.0000, 0.0127, 0.0000,  ..., 0.2073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(949096.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.7281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3610.7935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-290.8379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-598.2147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2628],
        [-0.0908],
        [-0.6626],
        ...,
        [-3.7232],
        [-3.7144],
        [-3.7126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270282.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0385],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371993.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371997.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5556.4551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8714, device='cuda:0')



h[100].sum tensor(-0.0385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-14.0739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86396.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1146, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1161, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(934252.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.3680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3623.0964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.8722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-609.2007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3415],
        [-2.9142],
        [-3.2717],
        ...,
        [-3.7381],
        [-3.7293],
        [-3.7274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285636.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371997.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372002.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5894.5098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0682, device='cuda:0')



h[100].sum tensor(-0.0459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-16.9474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92904.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1145, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1166, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1398, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1463, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(971392.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(621.1995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3499.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.7733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.4888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5390],
        [-3.3291],
        [-2.9951],
        ...,
        [-3.4974],
        [-3.3088],
        [-3.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268972.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372002.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 290.0 event: 1450 loss: tensor(470.6791, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372006.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0078, -0.0017, -0.0021,  ...,  0.0172,  0.0081, -0.0023],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5272.8306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7206, device='cuda:0')



h[100].sum tensor(-0.0310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0075, device='cuda:0')



h[200].sum tensor(-11.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0026, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0321, 0.0104, 0.0000],
        [0.0289, 0.0000, 0.0000,  ..., 0.0630, 0.0288, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83651.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1718, 0.0000, 0.0000],
        [0.0000, 0.0246, 0.0000,  ..., 0.2393, 0.0000, 0.0029],
        [0.0000, 0.0817, 0.0000,  ..., 0.4028, 0.0000, 0.0171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1210, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(930005.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(582.8333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3794.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.6398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.1534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6395],
        [-0.3732],
        [-0.0146],
        ...,
        [-3.7542],
        [-3.7453],
        [-3.7434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304487.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372006.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372010.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0262, -0.0021, -0.0025,  ...,  0.0428,  0.0234, -0.0070],
        [ 0.0239, -0.0020, -0.0025,  ...,  0.0396,  0.0215, -0.0064],
        [ 0.0223, -0.0020, -0.0024,  ...,  0.0374,  0.0202, -0.0060],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6068.8398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1446, device='cuda:0')



h[100].sum tensor(-0.0485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-18.3134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0553, 0.0000, 0.0000,  ..., 0.1013, 0.0517, 0.0000],
        [0.0791, 0.0000, 0.0000,  ..., 0.1364, 0.0726, 0.0000],
        [0.0863, 0.0000, 0.0000,  ..., 0.1465, 0.0786, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94577.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1900, 0.0000,  ..., 0.6722, 0.0000, 0.0487],
        [0.0000, 0.2180, 0.0000,  ..., 0.7434, 0.0000, 0.0583],
        [0.0000, 0.2268, 0.0000,  ..., 0.7662, 0.0000, 0.0611],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1214, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1213, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1213, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(980028.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(628.0424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3685.9333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.9719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.3005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3714],
        [ 0.3736],
        [ 0.3782],
        ...,
        [-3.7553],
        [-3.7465],
        [-3.7446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277136.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372010.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372014., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0125, -0.0018, -0.0022,  ...,  0.0238,  0.0121, -0.0035],
        [ 0.0047, -0.0017, -0.0021,  ...,  0.0129,  0.0056, -0.0015],
        [ 0.0045, -0.0017, -0.0021,  ...,  0.0127,  0.0055, -0.0015],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5644.2529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9150, device='cuda:0')



h[100].sum tensor(-0.0384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-14.6387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0000, 0.0000,  ..., 0.0577, 0.0258, 0.0000],
        [0.0332, 0.0000, 0.0000,  ..., 0.0729, 0.0348, 0.0000],
        [0.0265, 0.0000, 0.0000,  ..., 0.0635, 0.0292, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88533.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0723, 0.0000,  ..., 0.3886, 0.0000, 0.0084],
        [0.0000, 0.0789, 0.0000,  ..., 0.4062, 0.0000, 0.0105],
        [0.0000, 0.0662, 0.0000,  ..., 0.3751, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(950797.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.8318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3932.9358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.3242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-607.1368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5963],
        [ 0.5916],
        [ 0.5848],
        ...,
        [-3.7545],
        [-3.7457],
        [-3.7438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273810.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372014., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372017.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0182, -0.0019, -0.0024,  ...,  0.0317,  0.0168, -0.0049],
        [ 0.0075, -0.0017, -0.0021,  ...,  0.0168,  0.0079, -0.0022],
        [ 0.0084, -0.0018, -0.0021,  ...,  0.0181,  0.0087, -0.0025],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5742.1934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9675, device='cuda:0')



h[100].sum tensor(-0.0401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-15.4687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0440, 0.0000, 0.0000,  ..., 0.0878, 0.0437, 0.0000],
        [0.0488, 0.0000, 0.0000,  ..., 0.0947, 0.0477, 0.0000],
        [0.0280, 0.0000, 0.0000,  ..., 0.0638, 0.0293, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89713.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0788, 0.0000,  ..., 0.4051, 0.0000, 0.0113],
        [0.0000, 0.0800, 0.0000,  ..., 0.4093, 0.0000, 0.0116],
        [0.0000, 0.0597, 0.0000,  ..., 0.3591, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1219, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(956148.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(608.0148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3921.2622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.9635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.5785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5374],
        [ 0.4367],
        [ 0.1986],
        ...,
        [-3.7609],
        [-3.7520],
        [-3.7502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261914.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9961],
        [0.9949],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372017.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372021.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089, -0.0018, -0.0022,  ...,  0.0189,  0.0091, -0.0026],
        [ 0.0053, -0.0017, -0.0021,  ...,  0.0139,  0.0061, -0.0017],
        [ 0.0038, -0.0017, -0.0021,  ...,  0.0118,  0.0049, -0.0013],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6374.5410, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3250, device='cuda:0')



h[100].sum tensor(-0.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-20.9079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0321, 0.0000, 0.0000,  ..., 0.0695, 0.0326, 0.0000],
        [0.0269, 0.0000, 0.0000,  ..., 0.0625, 0.0283, 0.0000],
        [0.0494, 0.0000, 0.0000,  ..., 0.0957, 0.0481, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98851.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1363, 0.0000,  ..., 0.5517, 0.0000, 0.0318],
        [0.0000, 0.1034, 0.0000,  ..., 0.4712, 0.0000, 0.0201],
        [0.0000, 0.1166, 0.0000,  ..., 0.5057, 0.0000, 0.0246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(993769.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(648.4149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3892.9282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-312.6324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.9827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3968],
        [ 0.4306],
        [ 0.4498],
        ...,
        [-3.7754],
        [-3.7666],
        [-3.7647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292965.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372021.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372025.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5392.5288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7855, device='cuda:0')



h[100].sum tensor(-0.0319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-12.5689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0347, 0.0000, 0.0000,  ..., 0.0713, 0.0336, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84851.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0686, 0.0000,  ..., 0.3791, 0.0000, 0.0135],
        [0.0000, 0.0146, 0.0000,  ..., 0.2109, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1549, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(929843.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3995.8562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.7888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-620.3469, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0105],
        [-0.6437],
        [-1.4699],
        ...,
        [-3.7790],
        [-3.7692],
        [-3.7655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306668.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0384],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372025.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372029.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6327.9346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2816, device='cuda:0')



h[100].sum tensor(-0.0512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-20.4014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0025, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0458, 0.0183, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99824.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1439, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.1723, 0.0000, 0.0000],
        [0.0000, 0.0320, 0.0000,  ..., 0.2793, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1006941.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(653.1663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3510.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.9055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-584.5015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9203],
        [-1.5716],
        [-0.8373],
        ...,
        [-3.7800],
        [-3.7714],
        [-3.7695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247125.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0385],
        [1.0390],
        ...,
        [0.9960],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372029.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372034.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0017, -0.0021,  ...,  0.0128,  0.0055, -0.0015],
        [ 0.0048, -0.0017, -0.0021,  ...,  0.0132,  0.0057, -0.0015],
        [ 0.0107, -0.0018, -0.0022,  ...,  0.0213,  0.0106, -0.0030],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6594.0371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4142, device='cuda:0')



h[100].sum tensor(-0.0559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-22.5243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0178, 0.0000, 0.0000,  ..., 0.0477, 0.0195, 0.0000],
        [0.0364, 0.0000, 0.0000,  ..., 0.0778, 0.0374, 0.0000],
        [0.0284, 0.0000, 0.0000,  ..., 0.0667, 0.0308, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(104220.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0620, 0.0000,  ..., 0.3685, 0.0000, 0.0061],
        [0.0000, 0.0684, 0.0000,  ..., 0.3873, 0.0000, 0.0082],
        [0.0000, 0.0734, 0.0000,  ..., 0.4007, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1218, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1031847.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(669.2495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3691.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-322.6901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-570.9074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5862],
        [ 0.6025],
        [ 0.5980],
        ...,
        [-3.7739],
        [-3.7643],
        [-3.7616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271079.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372034.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372039.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0102, -0.0018, -0.0022,  ...,  0.0206,  0.0102, -0.0029],
        [ 0.0236, -0.0020, -0.0025,  ...,  0.0393,  0.0213, -0.0062],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5957.8691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0710, device='cuda:0')



h[100].sum tensor(-0.0417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-16.9562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.0000, 0.0000,  ..., 0.0607, 0.0274, 0.0000],
        [0.0484, 0.0000, 0.0000,  ..., 0.0924, 0.0462, 0.0000],
        [0.0376, 0.0000, 0.0000,  ..., 0.0774, 0.0373, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92740.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0804, 0.0000,  ..., 0.4145, 0.0000, 0.0135],
        [0.0000, 0.1141, 0.0000,  ..., 0.5012, 0.0000, 0.0243],
        [0.0000, 0.1042, 0.0000,  ..., 0.4774, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(966218.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(618.2156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3769.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.0609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.8992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5085],
        [ 0.5153],
        [ 0.4877],
        ...,
        [-3.7644],
        [-3.7559],
        [-3.7541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251087.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372039.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372044.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0099, -0.0018, -0.0022,  ...,  0.0203,  0.0100, -0.0028],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6548.5186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3804, device='cuda:0')



h[100].sum tensor(-0.0530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-21.8075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0367, 0.0000, 0.0000,  ..., 0.0780, 0.0378, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0324, 0.0106, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0355, 0.0123, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100719.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.9458e-02, 0.0000e+00,  ..., 3.1143e-01, 0.0000e+00,
         1.5018e-03],
        [0.0000e+00, 1.0552e-02, 0.0000e+00,  ..., 2.3727e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.4043e-02, 0.0000e+00,  ..., 2.7409e-01, 0.0000e+00,
         1.9548e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2228e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2221e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2223e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1001272.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(650.3528, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3735.1343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.4670, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-573.4542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4055],
        [ 0.3861],
        [ 0.4131],
        ...,
        [-3.7600],
        [-3.7516],
        [-3.7497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249514.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372044.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 300.0 event: 1500 loss: tensor(480.5051, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372048.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6110.5034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1391, device='cuda:0')



h[100].sum tensor(-0.0433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-18.0254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95589.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1158, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1167, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1342, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(981363.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(629.6593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3690.2415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.0618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.1970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3835],
        [-3.5631],
        [-3.6900],
        ...,
        [-3.7312],
        [-3.6581],
        [-3.5238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245004.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0391],
        ...,
        [0.9959],
        [0.9948],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372048.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372053.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0190, -0.0019, -0.0024,  ...,  0.0329,  0.0175, -0.0051],
        [ 0.0108, -0.0018, -0.0022,  ...,  0.0215,  0.0107, -0.0030],
        [ 0.0046, -0.0017, -0.0021,  ...,  0.0129,  0.0056, -0.0015],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5564.3760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8449, device='cuda:0')



h[100].sum tensor(-0.0319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-13.4014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0550, 0.0000, 0.0000,  ..., 0.1033, 0.0529, 0.0000],
        [0.0536, 0.0000, 0.0000,  ..., 0.1015, 0.0518, 0.0000],
        [0.0467, 0.0000, 0.0000,  ..., 0.0920, 0.0461, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86589.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1785, 0.0000,  ..., 0.6599, 0.0000, 0.0463],
        [0.0000, 0.1596, 0.0000,  ..., 0.6143, 0.0000, 0.0396],
        [0.0000, 0.1439, 0.0000,  ..., 0.5757, 0.0000, 0.0338],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(937287.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(593.5164, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3793.4653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-285.9498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-607.7095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4683],
        [ 0.4971],
        [ 0.5140],
        ...,
        [-3.7719],
        [-3.7635],
        [-3.7617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262937.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372053.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372057.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151, -0.0019, -0.0023,  ...,  0.0274,  0.0142, -0.0041],
        [ 0.0159, -0.0019, -0.0023,  ...,  0.0286,  0.0149, -0.0043],
        [ 0.0222, -0.0020, -0.0024,  ...,  0.0373,  0.0201, -0.0058],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6453.1768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3115, device='cuda:0')



h[100].sum tensor(-0.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-20.8561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0880, 0.0000, 0.0000,  ..., 0.1491, 0.0802, 0.0000],
        [0.0659, 0.0000, 0.0000,  ..., 0.1166, 0.0607, 0.0000],
        [0.0352, 0.0000, 0.0000,  ..., 0.0720, 0.0341, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0026, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0305, 0.0092, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100414.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2596, 0.0000,  ..., 0.8593, 0.0000, 0.0742],
        [0.0000, 0.1894, 0.0000,  ..., 0.6850, 0.0000, 0.0497],
        [0.0000, 0.1137, 0.0000,  ..., 0.4960, 0.0000, 0.0232],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1442, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.2082, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1005601.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(654.9896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3332.3308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.2404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-577.0709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4249],
        [ 0.4581],
        [ 0.4807],
        ...,
        [-3.4389],
        [-2.8310],
        [-1.8465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235691.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372057.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372061.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0160, -0.0019, -0.0023,  ...,  0.0287,  0.0150, -0.0043],
        [ 0.0087, -0.0018, -0.0022,  ...,  0.0186,  0.0090, -0.0025],
        [ 0.0046, -0.0017, -0.0021,  ...,  0.0129,  0.0056, -0.0015],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5776.0249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9609, device='cuda:0')



h[100].sum tensor(-0.0354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-15.2249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0468, 0.0000, 0.0000,  ..., 0.0919, 0.0460, 0.0000],
        [0.0373, 0.0000, 0.0000,  ..., 0.0789, 0.0382, 0.0000],
        [0.0192, 0.0000, 0.0000,  ..., 0.0538, 0.0232, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89939.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0811, 0.0000,  ..., 0.4145, 0.0000, 0.0123],
        [0.0000, 0.0698, 0.0000,  ..., 0.3882, 0.0000, 0.0082],
        [0.0000, 0.0510, 0.0000,  ..., 0.3423, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(951996.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(615.1587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3406.8022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-293.2648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-602.5897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5708],
        [ 0.5896],
        [ 0.5412],
        ...,
        [-3.7971],
        [-3.7888],
        [-3.7870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277838., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0385],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372061.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0384],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372065.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0150, -0.0019, -0.0023,  ...,  0.0274,  0.0142, -0.0040],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5946.5757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0607, device='cuda:0')



h[100].sum tensor(-0.0383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-16.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0025, 0.0000],
        [0.0209, 0.0000, 0.0000,  ..., 0.0521, 0.0222, 0.0000],
        [0.0422, 0.0000, 0.0000,  ..., 0.0838, 0.0411, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91783.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.1752, 0.0000, 0.0000],
        [0.0000, 0.0472, 0.0000,  ..., 0.3176, 0.0000, 0.0083],
        [0.0000, 0.1072, 0.0000,  ..., 0.4821, 0.0000, 0.0226],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(962857.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(625.7064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3219.0210, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.7457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.7652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9892],
        [-1.0897],
        [-0.2915],
        ...,
        [-3.8069],
        [-3.7983],
        [-3.7962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268666.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0384],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372065.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0384],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372069.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0125, -0.0018, -0.0022,  ...,  0.0238,  0.0121, -0.0034],
        [ 0.0110, -0.0018, -0.0022,  ...,  0.0218,  0.0109, -0.0030],
        [ 0.0111, -0.0018, -0.0022,  ...,  0.0220,  0.0110, -0.0031],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6299.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2425, device='cuda:0')



h[100].sum tensor(-0.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-19.5458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1722, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0504, 0.0000, 0.0000,  ..., 0.0970, 0.0492, 0.0000],
        [0.0321, 0.0000, 0.0000,  ..., 0.0717, 0.0341, 0.0000],
        [0.0335, 0.0000, 0.0000,  ..., 0.0738, 0.0353, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99610.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1197, 0.0000,  ..., 0.5157, 0.0000, 0.0262],
        [0.0000, 0.0785, 0.0000,  ..., 0.4136, 0.0000, 0.0116],
        [0.0000, 0.0617, 0.0000,  ..., 0.3719, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1010543.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(656.4364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3238.6819, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.5501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.0548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 3.2808e-01],
        [ 2.0188e-01],
        [-3.5657e-03],
        ...,
        [-3.8116e+00],
        [-3.8032e+00],
        [-3.8013e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257414.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0384],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372069.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372073.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0017, -0.0021,  ...,  0.0146,  0.0067, -0.0018],
        [ 0.0034, -0.0017, -0.0020,  ...,  0.0112,  0.0046, -0.0012],
        [ 0.0106, -0.0018, -0.0022,  ...,  0.0213,  0.0106, -0.0030],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5555.2910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.5997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8454, device='cuda:0')



h[100].sum tensor(-0.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-13.3142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0176, 0.0000, 0.0000,  ..., 0.0474, 0.0197, 0.0000],
        [0.0467, 0.0000, 0.0000,  ..., 0.0921, 0.0463, 0.0000],
        [0.0266, 0.0000, 0.0000,  ..., 0.0622, 0.0285, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86738.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0428, 0.0000,  ..., 0.3168, 0.0000, 0.0054],
        [0.0000, 0.0805, 0.0000,  ..., 0.4219, 0.0000, 0.0125],
        [0.0000, 0.0652, 0.0000,  ..., 0.3833, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(944496.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(603.7446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3443.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.0756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.8304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0658],
        [ 0.3704],
        [ 0.4037],
        ...,
        [-3.8183],
        [-3.8100],
        [-3.8081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285810.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372073.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372078.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0129, -0.0018, -0.0022,  ...,  0.0245,  0.0125, -0.0035],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6600.8521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3847, device='cuda:0')



h[100].sum tensor(-0.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-21.9478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0000, 0.0000,  ..., 0.0357, 0.0127, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0396, 0.0150, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102789.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0400, 0.0000,  ..., 0.3030, 0.0000, 0.0052],
        [0.0000, 0.0174, 0.0000,  ..., 0.2413, 0.0000, 0.0014],
        [0.0000, 0.0037, 0.0000,  ..., 0.1969, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1025968., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(672.0726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3142.6624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-320.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.8960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3533],
        [ 0.2097],
        [ 0.0803],
        ...,
        [-3.8285],
        [-3.8201],
        [-3.8182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267247.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372078.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372078.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5995.8521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0649, device='cuda:0')



h[100].sum tensor(-0.0377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-16.9173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0027, 0.0000],
        [0.0046, 0.0000, 0.0000,  ..., 0.0274, 0.0077, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93824.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1184, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1354, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.1772, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(976587.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(635.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3150.4546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-302.6605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.9631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4247],
        [-2.8511],
        [-2.0133],
        ...,
        [-3.8283],
        [-3.8201],
        [-3.8182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259352.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0383],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372078.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0382],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372083., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0107, -0.0018, -0.0022,  ...,  0.0213,  0.0106, -0.0029],
        [ 0.0057, -0.0017, -0.0021,  ...,  0.0144,  0.0065, -0.0017],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6316.0615, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2503, device='cuda:0')



h[100].sum tensor(-0.0429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-19.4904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0000, 0.0000,  ..., 0.0596, 0.0269, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0435, 0.0172, 0.0000],
        [0.0406, 0.0000, 0.0000,  ..., 0.0834, 0.0410, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99378.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0299, 0.0000,  ..., 0.2860, 0.0000, 0.0012],
        [0.0000, 0.0372, 0.0000,  ..., 0.3064, 0.0000, 0.0027],
        [0.0000, 0.0887, 0.0000,  ..., 0.4384, 0.0000, 0.0148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1007013.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(661.4999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3008.5842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.7353, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-583.9387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5264],
        [ 0.5459],
        [ 0.5936],
        ...,
        [-3.8382],
        [-3.8298],
        [-3.8279]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260861.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0382],
        [1.0392],
        ...,
        [0.9959],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372083., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 310.0 event: 1550 loss: tensor(446.7466, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0392],
        ...,
        [0.9958],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372087.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5569.6748, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8368, device='cuda:0')



h[100].sum tensor(-0.0287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-13.1658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86403.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1165, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1171, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940705.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(606.0620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3352.4626, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-287.7929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.4103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6619],
        [-2.6190],
        [-2.4172],
        ...,
        [-3.8394],
        [-3.8311],
        [-3.8291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292794.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0392],
        ...,
        [0.9958],
        [0.9947],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372087.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0392],
        ...,
        [0.9958],
        [0.9946],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372092.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7530.6577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.8940, device='cuda:0')



h[100].sum tensor(-0.0630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0198, device='cuda:0')



h[200].sum tensor(-29.2386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(114660.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1152, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1161, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1167, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1076382., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(719.0740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3339.4675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.4868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-547.1484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6333],
        [-3.6825],
        [-3.6609],
        ...,
        [-3.8353],
        [-3.8271],
        [-3.8253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274951.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0392],
        ...,
        [0.9958],
        [0.9946],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372092.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372096.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [ 0.0048, -0.0017, -0.0021,  ...,  0.0131,  0.0058, -0.0015],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5377.1064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7287, device='cuda:0')



h[100].sum tensor(-0.0241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-11.3196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0187, 0.0028, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0278, 0.0081, 0.0000],
        [0.0171, 0.0000, 0.0000,  ..., 0.0467, 0.0193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83763.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4978e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5952e-02, 0.0000e+00,  ..., 2.2088e-01, 0.0000e+00,
         1.2151e-04],
        [0.0000e+00, 4.3820e-02, 0.0000e+00,  ..., 3.1597e-01, 0.0000e+00,
         3.9844e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2194e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2190e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2192e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(931341.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(588.7946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3740.9834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.5633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-622.1304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9175],
        [-0.3060],
        [ 0.1673],
        ...,
        [-3.8321],
        [-3.8239],
        [-3.8221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279426.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372096.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372100.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0250, -0.0020, -0.0025,  ...,  0.0411,  0.0225, -0.0064],
        [ 0.0211, -0.0020, -0.0024,  ...,  0.0356,  0.0192, -0.0054],
        [ 0.0201, -0.0020, -0.0024,  ...,  0.0343,  0.0184, -0.0052],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5859.3140, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9714, device='cuda:0')



h[100].sum tensor(-0.0321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-15.2249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0664, 0.0000, 0.0000,  ..., 0.1186, 0.0624, 0.0000],
        [0.0947, 0.0000, 0.0000,  ..., 0.1582, 0.0859, 0.0000],
        [0.0644, 0.0000, 0.0000,  ..., 0.1162, 0.0608, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90798.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1434, 0.0000,  ..., 0.5713, 0.0000, 0.0322],
        [0.0000, 0.1751, 0.0000,  ..., 0.6530, 0.0000, 0.0432],
        [0.0000, 0.1435, 0.0000,  ..., 0.5736, 0.0000, 0.0319],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(962317.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.8584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3625.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.7858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-604.7475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6064],
        [ 0.5787],
        [ 0.5445],
        ...,
        [-3.8299],
        [-3.8219],
        [-3.8201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260746.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372100.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372104.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6491.1470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2944, device='cuda:0')



h[100].sum tensor(-0.0425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-20.4029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0028, 0.0000],
        [0.0125, 0.0000, 0.0000,  ..., 0.0402, 0.0154, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101862.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4323e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.6298e-03, 0.0000e+00,  ..., 1.8872e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1030e-02, 0.0000e+00,  ..., 2.7426e-01, 0.0000e+00,
         1.4609e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2186e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2182e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2184e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1023056.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(663.5140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3231.3354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-319.3937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.3254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4281],
        [-0.6872],
        [-0.0775],
        ...,
        [-3.8356],
        [-3.8273],
        [-3.8252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216783.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372104.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372108.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181, -0.0019, -0.0024,  ...,  0.0315,  0.0167, -0.0047],
        [ 0.0098, -0.0018, -0.0022,  ...,  0.0200,  0.0098, -0.0027],
        [ 0.0145, -0.0019, -0.0023,  ...,  0.0265,  0.0138, -0.0038],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0083, -0.0018, -0.0021,  ...,  0.0179,  0.0086, -0.0023],
        [ 0.0083, -0.0018, -0.0021,  ...,  0.0179,  0.0086, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5906.5615, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9933, device='cuda:0')



h[100].sum tensor(-0.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-15.6095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0237, 0.0000, 0.0000,  ..., 0.0575, 0.0258, 0.0000],
        [0.0547, 0.0000, 0.0000,  ..., 0.1027, 0.0527, 0.0000],
        [0.0304, 0.0000, 0.0000,  ..., 0.0670, 0.0314, 0.0000],
        ...,
        [0.0159, 0.0000, 0.0000,  ..., 0.0455, 0.0183, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0455, 0.0183, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0455, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90555.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0645, 0.0000,  ..., 0.3696, 0.0000, 0.0049],
        [0.0000, 0.0866, 0.0000,  ..., 0.4273, 0.0000, 0.0118],
        [0.0000, 0.0643, 0.0000,  ..., 0.3717, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0155, 0.0000,  ..., 0.2362, 0.0000, 0.0000],
        [0.0000, 0.0206, 0.0000,  ..., 0.2603, 0.0000, 0.0000],
        [0.0000, 0.0206, 0.0000,  ..., 0.2604, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(958416.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(616.4348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3727.8511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.9562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.4663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4821],
        [ 0.4865],
        [ 0.4454],
        ...,
        [-2.2192],
        [-1.7611],
        [-1.7605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279200.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9946],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372108.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372112.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5607.4009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8411, device='cuda:0')



h[100].sum tensor(-0.0268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-13.1659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1166, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86605.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1215, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(942606.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.6326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3802.8013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.8643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.8815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5027],
        [-3.1652],
        [-2.6005],
        ...,
        [-3.8646],
        [-3.8564],
        [-3.8546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298038.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9958],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372112.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372116.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [ 0.0036, -0.0017, -0.0020,  ...,  0.0113,  0.0047, -0.0012],
        [ 0.0067, -0.0017, -0.0021,  ...,  0.0157,  0.0073, -0.0019],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5900.4614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9977, device='cuda:0')



h[100].sum tensor(-0.0314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-15.5498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        ...,
        [0.0413, 0.0000, 0.0000,  ..., 0.0849, 0.0418, 0.0000],
        [0.0137, 0.0000, 0.0000,  ..., 0.0426, 0.0165, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0316, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93312.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.1598, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1247, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1166, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0751, 0.0000,  ..., 0.4070, 0.0000, 0.0068],
        [0.0000, 0.0392, 0.0000,  ..., 0.3054, 0.0000, 0.0021],
        [0.0000, 0.0132, 0.0000,  ..., 0.2150, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(984549.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(628.7783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3665.0737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.9868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-604.3449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3873],
        [-2.2998],
        [-3.0239],
        ...,
        [ 0.1219],
        [-0.5983],
        [-1.7173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278159.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0373],
        [1.0382],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372116.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0374],
        [1.0381],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372120.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5442.6582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7559, device='cuda:0')



h[100].sum tensor(-0.0235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0079, device='cuda:0')



h[200].sum tensor(-11.7669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1048, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84845.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1169, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1507, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1315, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1219, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(937560.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.0920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3889.2219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-287.9886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-624.8834, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3750],
        [-3.1247],
        [-2.6783],
        ...,
        [-2.9466],
        [-3.3582],
        [-3.6510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280475.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0374],
        [1.0381],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372120.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0374],
        [1.0381],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372124.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0129, -0.0018, -0.0022,  ...,  0.0244,  0.0125, -0.0034]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7344.7334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.7561, device='cuda:0')



h[100].sum tensor(-0.0538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0183, device='cuda:0')



h[200].sum tensor(-27.2925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0408, 0.0155, 0.0000],
        [0.0249, 0.0000, 0.0000,  ..., 0.0582, 0.0259, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(114648.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1154, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1351, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.1988, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1584, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.2530, 0.0000, 0.0031],
        [0.0000, 0.0604, 0.0000,  ..., 0.3685, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1088789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(717.7729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3426.0825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-347.8961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-554.0994, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9159],
        [-2.1765],
        [-1.1674],
        ...,
        [-2.9401],
        [-2.0127],
        [-0.9621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248257.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0374],
        [1.0381],
        [1.0393],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372124.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 320.0 event: 1600 loss: tensor(479.5124, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0374],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372128.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5386.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.7756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7250, device='cuda:0')



h[100].sum tensor(-0.0220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0076, device='cuda:0')



h[200].sum tensor(-11.2939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0293, 0.0089, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0243, 0.0059, 0.0000],
        [0.0303, 0.0000, 0.0000,  ..., 0.0651, 0.0302, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83369.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.1873, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.2246, 0.0000, 0.0000],
        [0.0000, 0.0593, 0.0000,  ..., 0.3643, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(929668.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(585.4516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4173.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.5982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-628.3382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5188],
        [-0.6404],
        [ 0.0457],
        ...,
        [-3.8853],
        [-3.8770],
        [-3.8751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309093., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0374],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372128.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372131.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7080.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6200, device='cuda:0')



h[100].sum tensor(-0.0484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-25.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0000, 0.0000,  ..., 0.0469, 0.0194, 0.0000],
        [0.0181, 0.0000, 0.0000,  ..., 0.0482, 0.0201, 0.0000],
        [0.0058, 0.0000, 0.0000,  ..., 0.0293, 0.0088, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108076.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0223, 0.0000,  ..., 0.2657, 0.0000, 0.0000],
        [0.0000, 0.0224, 0.0000,  ..., 0.2682, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.2135, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1055473., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(690.6675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3421.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-337.0234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-572.7112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9360],
        [-1.0300],
        [-1.5556],
        ...,
        [-3.8965],
        [-3.8881],
        [-3.8859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235180.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372131.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372136., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0017, -0.0021,  ...,  0.0133,  0.0058, -0.0015],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6950.9497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5357, device='cuda:0')



h[100].sum tensor(-0.0458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0160, device='cuda:0')



h[200].sum tensor(-24.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0000, 0.0000,  ..., 0.0485, 0.0203, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0282, 0.0081, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(110701.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0343, 0.0000,  ..., 0.2934, 0.0000, 0.0015],
        [0.0000, 0.0117, 0.0000,  ..., 0.2308, 0.0000, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.2066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1080417.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(701.2549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3672.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-341.4516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.1082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1523],
        [-0.1281],
        [-0.3634],
        ...,
        [-3.8980],
        [-3.8894],
        [-3.8872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286498.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372136., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372140., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101, -0.0018, -0.0022,  ...,  0.0206,  0.0102, -0.0027],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5951.4365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0138, device='cuda:0')



h[100].sum tensor(-0.0299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-15.8432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0503, 0.0000, 0.0000,  ..., 0.0948, 0.0479, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0472, 0.0194, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93633.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1465, 0.0000,  ..., 0.5876, 0.0000, 0.0337],
        [0.0000, 0.0913, 0.0000,  ..., 0.4476, 0.0000, 0.0163],
        [0.0000, 0.0530, 0.0000,  ..., 0.3497, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1222, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(984720.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(631.3450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3571.1685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.6151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-609.4636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5290],
        [ 0.5288],
        [ 0.5248],
        ...,
        [-3.9053],
        [-3.8968],
        [-3.8947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254070.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372140., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372144.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6998.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5614, device='cuda:0')



h[100].sum tensor(-0.0453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0163, device='cuda:0')



h[200].sum tensor(-24.2857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(112352.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1176, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1172, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1223, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1100052.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(707.6391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3648.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-345.1516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-563.4260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4659],
        [-2.9301],
        [-3.3067],
        ...,
        [-3.9084],
        [-3.8998],
        [-3.8977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284145.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372144.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372148.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037, -0.0017, -0.0021,  ...,  0.0116,  0.0048, -0.0012],
        [ 0.0126, -0.0018, -0.0022,  ...,  0.0241,  0.0123, -0.0033],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5973.5273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0350, device='cuda:0')



h[100].sum tensor(-0.0293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-15.9040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0716, 0.0000, 0.0000,  ..., 0.1265, 0.0667, 0.0000],
        [0.0186, 0.0000, 0.0000,  ..., 0.0491, 0.0205, 0.0000],
        [0.0131, 0.0000, 0.0000,  ..., 0.0396, 0.0148, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91998.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1142, 0.0000,  ..., 0.5068, 0.0000, 0.0223],
        [0.0000, 0.0523, 0.0000,  ..., 0.3443, 0.0000, 0.0075],
        [0.0000, 0.0220, 0.0000,  ..., 0.2466, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1224, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1224, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(972570.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(622.2635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3696.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.4720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.4868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4421],
        [-0.0221],
        [-0.7787],
        ...,
        [-3.8957],
        [-3.8966],
        [-3.8963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268445.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372148.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372153.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0064, -0.0017, -0.0021,  ...,  0.0154,  0.0071, -0.0018],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6519.3560, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3300, device='cuda:0')



h[100].sum tensor(-0.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-20.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0235, 0.0000, 0.0000,  ..., 0.0597, 0.0268, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0285, 0.0082, 0.0000],
        [0.0243, 0.0000, 0.0000,  ..., 0.0571, 0.0252, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97958.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 6.8370e-03, 0.0000e+00,  ..., 2.2999e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5410e-02, 0.0000e+00,  ..., 2.4382e-01, 0.0000e+00,
         3.1719e-04],
        [0.0000e+00, 5.6585e-02, 0.0000e+00,  ..., 3.5920e-01, 0.0000e+00,
         8.1692e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2237e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2230e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2231e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(990551.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(646.7984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3687.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-316.8059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.5284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6412],
        [-0.9301],
        [-0.2041],
        ...,
        [-3.9101],
        [-3.9015],
        [-3.8993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270371.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9945],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372153.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372158.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0017, -0.0021,  ...,  0.0142,  0.0064, -0.0016],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5832.7998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9483, device='cuda:0')



h[100].sum tensor(-0.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-14.6284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0000, 0.0000,  ..., 0.0497, 0.0209, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0292, 0.0087, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91371.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0755, 0.0000,  ..., 0.4026, 0.0000, 0.0123],
        [0.0000, 0.0331, 0.0000,  ..., 0.2769, 0.0000, 0.0010],
        [0.0000, 0.0087, 0.0000,  ..., 0.2079, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(971836.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(617.0549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3812.9126, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.7000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.0521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3317],
        [ 0.2397],
        [ 0.1678],
        ...,
        [-3.8975],
        [-3.8887],
        [-3.8864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266750.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0394],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372158.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0395],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372162.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6217.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1414, device='cuda:0')



h[100].sum tensor(-0.0315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-17.6758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0096, 0.0000, 0.0000,  ..., 0.0365, 0.0130, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0245, 0.0058, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0288, 0.0084, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97840.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0120, 0.0000,  ..., 0.2313, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.2153, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.2258, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1228, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1007832.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(643.8155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3629.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-316.6702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-602.0839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3615],
        [-0.1163],
        [ 0.0577],
        ...,
        [-3.9095],
        [-3.9008],
        [-3.8986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257933.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0395],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372162.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0395],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372167.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0039, -0.0017, -0.0021,  ...,  0.0120,  0.0050, -0.0012],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6418.6113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2406, device='cuda:0')



h[100].sum tensor(-0.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-19.2533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0027, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0400, 0.0150, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0469, 0.0191, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99030.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.2431, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.2645, 0.0000, 0.0000],
        [0.0000, 0.0348, 0.0000,  ..., 0.3047, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1006087.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(648.8358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3618.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-318.3431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.0096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4584],
        [ 0.4664],
        [ 0.5029],
        ...,
        [-3.9181],
        [-3.9094],
        [-3.9071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274408.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0382],
        [1.0395],
        ...,
        [0.9957],
        [0.9944],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372167.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 330.0 event: 1650 loss: tensor(486.4176, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9944],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372171.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0256, -0.0021, -0.0025,  ...,  0.0421,  0.0230, -0.0063],
        [ 0.0168, -0.0019, -0.0023,  ...,  0.0299,  0.0157, -0.0042],
        [ 0.0082, -0.0018, -0.0021,  ...,  0.0180,  0.0086, -0.0022],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6155.5908, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1023, device='cuda:0')



h[100].sum tensor(-0.0297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-17.0390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0788, 0.0000, 0.0000,  ..., 0.1366, 0.0727, 0.0000],
        [0.0779, 0.0000, 0.0000,  ..., 0.1356, 0.0720, 0.0000],
        [0.0383, 0.0000, 0.0000,  ..., 0.0767, 0.0369, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95665.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1833, 0.0000,  ..., 0.6836, 0.0000, 0.0460],
        [0.0000, 0.1704, 0.0000,  ..., 0.6522, 0.0000, 0.0414],
        [0.0000, 0.1107, 0.0000,  ..., 0.5002, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1231, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1230, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(993098.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(632.0484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3768.4272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-311.5101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-605.5560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5162],
        [ 0.4891],
        [ 0.4174],
        ...,
        [-3.9156],
        [-3.9069],
        [-3.9046]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264469., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9944],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372171.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9944],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372175.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6393.6152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2367, device='cuda:0')



h[100].sum tensor(-0.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-18.8472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98275.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1177, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1183, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1234, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1234, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1003611.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(639.5942, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3762.1089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-316.3280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.8547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8782],
        [-3.7624],
        [-3.5454],
        ...,
        [-3.9128],
        [-3.9041],
        [-3.9019]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257180.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9944],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372175.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372179.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5845.7354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9268, device='cuda:0')



h[100].sum tensor(-0.0245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-14.3799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91595.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1462, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1185, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(973782.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.3027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3937.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.0311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-611.9451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1071],
        [-2.7591],
        [-3.1019],
        ...,
        [-3.9146],
        [-3.9056],
        [-3.9032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264058.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0382],
        [1.0395],
        ...,
        [0.9956],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372179.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0377],
        [1.0382],
        [1.0396],
        ...,
        [0.9956],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372184.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0079, -0.0018, -0.0021,  ...,  0.0176,  0.0084, -0.0022],
        [ 0.0095, -0.0018, -0.0022,  ...,  0.0197,  0.0096, -0.0025],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6544.1030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2914, device='cuda:0')



h[100].sum tensor(-0.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-19.9298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0907, 0.0000, 0.0000,  ..., 0.1532, 0.0825, 0.0000],
        [0.0309, 0.0000, 0.0000,  ..., 0.0663, 0.0307, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0350, 0.0120, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100666.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2757, 0.0000,  ..., 0.9154, 0.0000, 0.0770],
        [0.0000, 0.1435, 0.0000,  ..., 0.5820, 0.0000, 0.0342],
        [0.0000, 0.0593, 0.0000,  ..., 0.3533, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1014998.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(647.3402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3784.8777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-321.6362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.9593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2073],
        [ 0.1918],
        [ 0.1354],
        ...,
        [-3.9111],
        [-3.9027],
        [-3.9012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252532.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0377],
        [1.0382],
        [1.0396],
        ...,
        [0.9956],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372184.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0377],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372188.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6096.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0590, device='cuda:0')



h[100].sum tensor(-0.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-16.3205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0330, 0.0108, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96192.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0203, 0.0000,  ..., 0.2432, 0.0000, 0.0000],
        [0.0000, 0.0363, 0.0000,  ..., 0.2956, 0.0000, 0.0011],
        [0.0000, 0.0568, 0.0000,  ..., 0.3588, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(998809.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(629.3766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3743.2114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.3406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-602.6494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0488],
        [ 0.1821],
        [ 0.2598],
        ...,
        [-3.9355],
        [-3.9265],
        [-3.9240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249028.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0377],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9943],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372188.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0378],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372192.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0172, -0.0019, -0.0023,  ...,  0.0305,  0.0161, -0.0043],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5833.8643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9306, device='cuda:0')



h[100].sum tensor(-0.0234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-14.1958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        [0.0179, 0.0000, 0.0000,  ..., 0.0461, 0.0187, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0415, 0.0159, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89189.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.0000],
        [0.0000, 0.0195, 0.0000,  ..., 0.2420, 0.0000, 0.0029],
        [0.0000, 0.0328, 0.0000,  ..., 0.2939, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1236, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1235, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(957017.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4015.4060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.8132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.4385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0071],
        [-2.2264],
        [-1.3977],
        ...,
        [-3.9464],
        [-3.9377],
        [-3.9355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268591.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0378],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372192.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0378],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372196.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0141, -0.0019, -0.0023,  ...,  0.0261,  0.0135, -0.0036],
        [ 0.0107, -0.0018, -0.0022,  ...,  0.0215,  0.0107, -0.0028],
        [ 0.0322, -0.0022, -0.0026,  ...,  0.0513,  0.0284, -0.0077],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6321.8350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1666, device='cuda:0')



h[100].sum tensor(-0.0293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-18.0173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0329, 0.0000, 0.0000,  ..., 0.0688, 0.0323, 0.0000],
        [0.0916, 0.0000, 0.0000,  ..., 0.1546, 0.0833, 0.0000],
        [0.0859, 0.0000, 0.0000,  ..., 0.1467, 0.0786, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96915.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0957, 0.0000,  ..., 0.4581, 0.0000, 0.0197],
        [0.0000, 0.1927, 0.0000,  ..., 0.7061, 0.0000, 0.0479],
        [0.0000, 0.2300, 0.0000,  ..., 0.8019, 0.0000, 0.0603],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1238, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(996723.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(631.2541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3951.6899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.2079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.7236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0859],
        [ 0.3509],
        [ 0.4673],
        ...,
        [-3.9560],
        [-3.9472],
        [-3.9448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281041.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0378],
        [1.0381],
        [1.0396],
        ...,
        [0.9955],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372196.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0379],
        [1.0381],
        [1.0397],
        ...,
        [0.9954],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372201.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0078, -0.0017, -0.0021,  ...,  0.0173,  0.0082, -0.0021],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0034, -0.0017, -0.0020,  ...,  0.0113,  0.0046, -0.0011],
        ...,
        [ 0.0145, -0.0019, -0.0023,  ...,  0.0267,  0.0138, -0.0036],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6987.3760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4992, device='cuda:0')



h[100].sum tensor(-0.0374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-23.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0000, 0.0000,  ..., 0.0669, 0.0312, 0.0000],
        [0.0205, 0.0000, 0.0000,  ..., 0.0558, 0.0245, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0387, 0.0143, 0.0000],
        ...,
        [0.0125, 0.0000, 0.0000,  ..., 0.0392, 0.0144, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0435, 0.0169, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(105390.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0955, 0.0000,  ..., 0.4623, 0.0000, 0.0158],
        [0.0000, 0.0626, 0.0000,  ..., 0.3815, 0.0000, 0.0057],
        [0.0000, 0.0337, 0.0000,  ..., 0.3088, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0172, 0.0000,  ..., 0.2606, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.2301, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.1571, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1036391.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(665.5477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3912.1147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-332.3400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.4353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5849],
        [ 0.6072],
        [ 0.6059],
        ...,
        [-1.9870],
        [-2.5548],
        [-3.1977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266242.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0379],
        [1.0381],
        [1.0397],
        ...,
        [0.9954],
        [0.9942],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372201.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0379],
        [1.0381],
        [1.0397],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372204.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6372.6426, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2000, device='cuda:0')



h[100].sum tensor(-0.0291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-18.3011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99892.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1182, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1022388.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(644.0524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3778.0515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-321.5331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.0102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0257],
        [-4.0269],
        [-4.0016],
        ...,
        [-3.9667],
        [-3.9578],
        [-3.9555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268319.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0379],
        [1.0381],
        [1.0397],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372204.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0380],
        [1.0382],
        [1.0398],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372208.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6976.5752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5048, device='cuda:0')



h[100].sum tensor(-0.0364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(-23.1378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(107128.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1188, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1050967.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(673.3846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3662.2937, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-335.1609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-574.6617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8335],
        [-3.6659],
        [-3.3960],
        ...,
        [-3.9786],
        [-3.9696],
        [-3.9673]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260256.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0380],
        [1.0382],
        [1.0398],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372208.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0380],
        [1.0382],
        [1.0398],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372208.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [ 0.0068, -0.0017, -0.0021,  ...,  0.0161,  0.0075, -0.0019],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6670.0093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3569, device='cuda:0')



h[100].sum tensor(-0.0326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-20.7123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0239, 0.0000, 0.0000,  ..., 0.0573, 0.0250, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0322, 0.0101, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103862.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3225e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7308e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.5597e-03, 0.0000e+00,  ..., 2.2411e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.2078e-02, 0.0000e+00,  ..., 4.1478e-01, 0.0000e+00,
         1.1906e-02],
        [0.0000e+00, 2.8508e-02, 0.0000e+00,  ..., 2.7240e-01, 0.0000e+00,
         2.6085e-03],
        [0.0000e+00, 2.8455e-05, 0.0000e+00,  ..., 1.6961e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1046900.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(660.6498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3528.9370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-328.8228, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.9785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5558],
        [-0.8236],
        [-0.1685],
        ...,
        [-0.1071],
        [-1.1477],
        [-2.3940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257971.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0380],
        [1.0382],
        [1.0398],
        ...,
        [0.9954],
        [0.9941],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372208.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0381],
        [1.0382],
        [1.0398],
        ...,
        [0.9953],
        [0.9941],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372211.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5813.9731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9287, device='cuda:0')



h[100].sum tensor(-0.0218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-14.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0000, 0.0000,  ..., 0.0284, 0.0081, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88760.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.1885, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1392, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(956753.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(598.2867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3844.5190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.1031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.3903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3727],
        [-2.1347],
        [-2.5233],
        ...,
        [-3.9899],
        [-3.9809],
        [-3.9785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289115.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0381],
        [1.0382],
        [1.0398],
        ...,
        [0.9953],
        [0.9941],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372211.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0381],
        [1.0382],
        [1.0398],
        ...,
        [0.9953],
        [0.9940],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372215.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072, -0.0017, -0.0021,  ...,  0.0167,  0.0078, -0.0020],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [ 0.0233, -0.0020, -0.0025,  ...,  0.0390,  0.0211, -0.0056],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6347.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1993, device='cuda:0')



h[100].sum tensor(-0.0280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-18.2212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0000,  ..., 0.0294, 0.0087, 0.0000],
        [0.0434, 0.0000, 0.0000,  ..., 0.0881, 0.0436, 0.0000],
        [0.0218, 0.0000, 0.0000,  ..., 0.0539, 0.0232, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98812.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0511, 0.0000,  ..., 0.3563, 0.0000, 0.0039],
        [0.0000, 0.0839, 0.0000,  ..., 0.4446, 0.0000, 0.0132],
        [0.0000, 0.0817, 0.0000,  ..., 0.4392, 0.0000, 0.0137],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1010674.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(640.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3540.0439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-318.6452, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-595.3170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6035],
        [ 0.6120],
        [ 0.6151],
        ...,
        [-3.9972],
        [-3.9882],
        [-3.9857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252781.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0381],
        [1.0382],
        [1.0398],
        ...,
        [0.9953],
        [0.9940],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372215.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0381],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9940],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372219.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102, -0.0018, -0.0022,  ...,  0.0209,  0.0103, -0.0026],
        [ 0.0059, -0.0017, -0.0021,  ...,  0.0149,  0.0067, -0.0017],
        [ 0.0043, -0.0017, -0.0021,  ...,  0.0126,  0.0054, -0.0013],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5739.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.3703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8773, device='cuda:0')



h[100].sum tensor(-0.0203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0091, device='cuda:0')



h[200].sum tensor(-13.3557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0316, 0.0000, 0.0000,  ..., 0.0713, 0.0336, 0.0000],
        [0.0282, 0.0000, 0.0000,  ..., 0.0668, 0.0309, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0369, 0.0130, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88406.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0461, 0.0000,  ..., 0.3443, 0.0000, 0.0015],
        [0.0000, 0.0315, 0.0000,  ..., 0.3072, 0.0000, 0.0015],
        [0.0000, 0.0112, 0.0000,  ..., 0.2280, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1241, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1241, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(957920., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(597.8951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3759.8735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.7523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.0947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3504],
        [-0.2320],
        [-1.1659],
        ...,
        [-3.9995],
        [-3.9909],
        [-3.9892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294281.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0381],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9940],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372219.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0382],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372223.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6298.9810, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1524, device='cuda:0')



h[100].sum tensor(-0.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-17.5978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0026, 0.0000],
        [0.0232, 0.0000, 0.0000,  ..., 0.0558, 0.0243, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97605.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1364, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.1903, 0.0000, 0.0000],
        [0.0000, 0.0556, 0.0000,  ..., 0.3546, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1242, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1001630., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(634.2221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3590.3501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.4850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.7171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0913],
        [-1.0982],
        [-0.2263],
        ...,
        [-3.9982],
        [-3.9892],
        [-3.9867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268841.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0382],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372223.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0382],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372228.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047, -0.0017, -0.0021,  ...,  0.0131,  0.0057, -0.0014],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5816.4043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8991, device='cuda:0')



h[100].sum tensor(-0.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-13.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0478, 0.0198, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0417, 0.0161, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91578.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0248, 0.0000,  ..., 0.2793, 0.0000, 0.0000],
        [0.0000, 0.0156, 0.0000,  ..., 0.2450, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1716, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1244, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1244, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(979793.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.6856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3710.2625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.4500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.6610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1234],
        [-0.5111],
        [-1.1592],
        ...,
        [-3.9919],
        [-3.9830],
        [-3.9803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284838.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0382],
        [1.0383],
        [1.0399],
        ...,
        [0.9952],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372228.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0383],
        [1.0383],
        [1.0399],
        ...,
        [0.9951],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372233.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6399.3325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1887, device='cuda:0')



h[100].sum tensor(-0.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-17.9855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0297, 0.0091, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0364, 0.0130, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0430, 0.0169, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98723.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.2262, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.2375, 0.0000, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.2505, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1247, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1246, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1008732.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(634.2834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3764.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.1854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.7451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0950],
        [-0.0634],
        [-0.3108],
        ...,
        [-3.9858],
        [-3.9768],
        [-3.9741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262841.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0383],
        [1.0383],
        [1.0399],
        ...,
        [0.9951],
        [0.9939],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372233.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0383],
        [1.0383],
        [1.0398],
        ...,
        [0.9951],
        [0.9939],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372238.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0017, -0.0021,  ...,  0.0145,  0.0066, -0.0016],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0207, -0.0020, -0.0024,  ...,  0.0352,  0.0189, -0.0050],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6014.5918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9721, device='cuda:0')



h[100].sum tensor(-0.0215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-14.7838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0273, 0.0078, 0.0000],
        [0.0275, 0.0000, 0.0000,  ..., 0.0612, 0.0279, 0.0000],
        [0.0175, 0.0000, 0.0000,  ..., 0.0453, 0.0184, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91824.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0140, 0.0000,  ..., 0.2498, 0.0000, 0.0000],
        [0.0000, 0.0398, 0.0000,  ..., 0.3164, 0.0000, 0.0037],
        [0.0000, 0.0389, 0.0000,  ..., 0.3141, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(970505.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(603.4724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3859.3542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.2787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.7913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3078],
        [ 0.3769],
        [ 0.2015],
        ...,
        [-3.9783],
        [-3.9694],
        [-3.9666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251916.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0383],
        [1.0383],
        [1.0398],
        ...,
        [0.9951],
        [0.9939],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372238.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0383],
        [1.0398],
        ...,
        [0.9951],
        [0.9938],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372242.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0017, -0.0021,  ...,  0.0141,  0.0064, -0.0015],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0045,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6030.9121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9754, device='cuda:0')



h[100].sum tensor(-0.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-14.7793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0000, 0.0000,  ..., 0.0397, 0.0152, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0289, 0.0087, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93225.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0425, 0.0000,  ..., 0.3077, 0.0000, 0.0042],
        [0.0000, 0.0101, 0.0000,  ..., 0.2053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1464, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(981554.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.5582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3875.4507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.1355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-607.9776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1699],
        [-0.9036],
        [-1.7468],
        ...,
        [-3.9775],
        [-3.9686],
        [-3.9657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240409.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0383],
        [1.0398],
        ...,
        [0.9951],
        [0.9938],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372242.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0383],
        [1.0398],
        ...,
        [0.9950],
        [0.9938],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372246.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0017, -0.0021,  ...,  0.0133,  0.0059, -0.0014],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5860.9834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8849, device='cuda:0')



h[100].sum tensor(-0.0191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-13.4199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0000, 0.0000,  ..., 0.0409, 0.0159, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0281, 0.0082, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91132.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0262, 0.0000,  ..., 0.2658, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.2081, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1734, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(975084.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.4890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3807.8472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.8542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-613.6636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0042],
        [-0.3340],
        [-0.5371],
        ...,
        [-3.9858],
        [-3.9772],
        [-3.9745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235866.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0383],
        [1.0398],
        ...,
        [0.9950],
        [0.9938],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372246.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 350.0 event: 1750 loss: tensor(919.1901, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0398],
        ...,
        [0.9950],
        [0.9938],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372250.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0242, -0.0020, -0.0025,  ...,  0.0400,  0.0218, -0.0057],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0145, -0.0019, -0.0023,  ...,  0.0266,  0.0138, -0.0035],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6662.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3113, device='cuda:0')



h[100].sum tensor(-0.0277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-19.7317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0682, 0.0000, 0.0000,  ..., 0.1192, 0.0625, 0.0000],
        [0.0807, 0.0000, 0.0000,  ..., 0.1386, 0.0739, 0.0000],
        [0.0173, 0.0000, 0.0000,  ..., 0.0469, 0.0194, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101518.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2430, 0.0000,  ..., 0.8193, 0.0000, 0.0617],
        [0.0000, 0.1871, 0.0000,  ..., 0.6813, 0.0000, 0.0429],
        [0.0000, 0.0901, 0.0000,  ..., 0.4395, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1247, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1246, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1019817.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(644.3160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3802.8135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-322.1306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-588.1314, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2785],
        [ 0.3496],
        [ 0.4118],
        ...,
        [-4.0085],
        [-3.9994],
        [-3.9964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263303.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0398],
        ...,
        [0.9950],
        [0.9938],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372250.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0397],
        ...,
        [0.9950],
        [0.9937],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372253.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [ 0.0133, -0.0018, -0.0023,  ...,  0.0249,  0.0128, -0.0033],
        [ 0.0133, -0.0018, -0.0023,  ...,  0.0249,  0.0128, -0.0032],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6203.9082, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0803, device='cuda:0')



h[100].sum tensor(-0.0225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-16.1862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1497, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0000, 0.0000,  ..., 0.0398, 0.0152, 0.0000],
        [0.0250, 0.0000, 0.0000,  ..., 0.0575, 0.0257, 0.0000],
        [0.0801, 0.0000, 0.0000,  ..., 0.1378, 0.0735, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93868.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0241, 0.0000,  ..., 0.2479, 0.0000, 0.0027],
        [0.0000, 0.0613, 0.0000,  ..., 0.3633, 0.0000, 0.0091],
        [0.0000, 0.1385, 0.0000,  ..., 0.5610, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1248, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(986112.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(612.8137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3810.5056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.6156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.5760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0127],
        [-0.3974],
        [ 0.1625],
        ...,
        [-4.0210],
        [-4.0120],
        [-4.0088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251281., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0397],
        ...,
        [0.9950],
        [0.9937],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372253.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0397],
        ...,
        [0.9949],
        [0.9937],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372257.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0081, -0.0018, -0.0021,  ...,  0.0176,  0.0085, -0.0021],
        [ 0.0298, -0.0021, -0.0026,  ...,  0.0478,  0.0264, -0.0069],
        [ 0.0132, -0.0018, -0.0022,  ...,  0.0248,  0.0127, -0.0032],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5741.7271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8458, device='cuda:0')



h[100].sum tensor(-0.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-12.6657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0904, 0.0000, 0.0000,  ..., 0.1519, 0.0820, 0.0000],
        [0.0664, 0.0000, 0.0000,  ..., 0.1189, 0.0623, 0.0000],
        [0.0679, 0.0000, 0.0000,  ..., 0.1209, 0.0635, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89168.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1543, 0.0000,  ..., 0.6021, 0.0000, 0.0323],
        [0.0000, 0.1627, 0.0000,  ..., 0.6256, 0.0000, 0.0349],
        [0.0000, 0.1609, 0.0000,  ..., 0.6218, 0.0000, 0.0341],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(968751.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(594.6020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4049.8872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.7698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.6406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3924],
        [ 0.4277],
        [ 0.4082],
        ...,
        [-4.0361],
        [-4.0270],
        [-4.0238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273215.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0384],
        [1.0397],
        ...,
        [0.9949],
        [0.9937],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372257.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0396],
        ...,
        [0.9949],
        [0.9937],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372260.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6897.4365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4483, device='cuda:0')



h[100].sum tensor(-0.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-21.6916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0000, 0.0000,  ..., 0.0285, 0.0085, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0028, 0.0000],
        [0.0239, 0.0000, 0.0000,  ..., 0.0562, 0.0249, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(105735.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.2519, 0.0000, 0.0000],
        [0.0000, 0.0306, 0.0000,  ..., 0.2873, 0.0000, 0.0015],
        [0.0000, 0.1089, 0.0000,  ..., 0.4911, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1053759.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(664.7825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3826.1335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.7568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-578.0144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3960],
        [ 0.3429],
        [ 0.3123],
        ...,
        [-4.0501],
        [-4.0410],
        [-4.0377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271687.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0396],
        ...,
        [0.9949],
        [0.9937],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372260.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0396],
        ...,
        [0.9949],
        [0.9936],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372264.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0098, -0.0018, -0.0022,  ...,  0.0200,  0.0098, -0.0024],
        [ 0.0041, -0.0017, -0.0021,  ...,  0.0121,  0.0051, -0.0012],
        [ 0.0162, -0.0019, -0.0023,  ...,  0.0289,  0.0152, -0.0039],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6311.8452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1540, device='cuda:0')



h[100].sum tensor(-0.0231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-17.2096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0319, 0.0000, 0.0000,  ..., 0.0709, 0.0337, 0.0000],
        [0.0430, 0.0000, 0.0000,  ..., 0.0866, 0.0429, 0.0000],
        [0.0297, 0.0000, 0.0000,  ..., 0.0683, 0.0320, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95757.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0872, 0.0000,  ..., 0.4403, 0.0000, 0.0101],
        [0.0000, 0.0948, 0.0000,  ..., 0.4608, 0.0000, 0.0125],
        [0.0000, 0.0890, 0.0000,  ..., 0.4469, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1252, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(991481.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(626.0067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3960.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.8730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-604.2249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6210],
        [ 0.5838],
        [ 0.5263],
        ...,
        [-4.0628],
        [-4.0540],
        [-4.0508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282782.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0396],
        ...,
        [0.9949],
        [0.9936],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372264.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372268.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0143, -0.0019, -0.0023,  ...,  0.0263,  0.0136, -0.0034],
        [ 0.0209, -0.0020, -0.0024,  ...,  0.0355,  0.0191, -0.0049],
        [ 0.0182, -0.0019, -0.0024,  ...,  0.0317,  0.0168, -0.0043],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6487.0098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2485, device='cuda:0')



h[100].sum tensor(-0.0246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-18.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0584, 0.0000, 0.0000,  ..., 0.1078, 0.0556, 0.0000],
        [0.0720, 0.0000, 0.0000,  ..., 0.1269, 0.0669, 0.0000],
        [0.0756, 0.0000, 0.0000,  ..., 0.1320, 0.0699, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96879.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1242, 0.0000,  ..., 0.5349, 0.0000, 0.0230],
        [0.0000, 0.1579, 0.0000,  ..., 0.6223, 0.0000, 0.0341],
        [0.0000, 0.1682, 0.0000,  ..., 0.6491, 0.0000, 0.0374],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1327, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1362, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1326, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(994149.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(632.2828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3960.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-318.3190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.0103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4741],
        [ 0.5393],
        [ 0.5449],
        ...,
        [-3.7387],
        [-3.6698],
        [-3.6708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290824.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0384],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372268.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0383],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372272.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0307, -0.0021, -0.0026,  ...,  0.0491,  0.0271, -0.0070],
        [ 0.0110, -0.0018, -0.0022,  ...,  0.0218,  0.0109, -0.0027],
        [ 0.0158, -0.0019, -0.0023,  ...,  0.0284,  0.0148, -0.0038],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6142.6445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0610, device='cuda:0')



h[100].sum tensor(-0.0209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-15.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1471, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0579, 0.0000, 0.0000,  ..., 0.1074, 0.0553, 0.0000],
        [0.0729, 0.0000, 0.0000,  ..., 0.1282, 0.0677, 0.0000],
        [0.0658, 0.0000, 0.0000,  ..., 0.1185, 0.0619, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93811., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1572, 0.0000,  ..., 0.6189, 0.0000, 0.0341],
        [0.0000, 0.1548, 0.0000,  ..., 0.6149, 0.0000, 0.0332],
        [0.0000, 0.1371, 0.0000,  ..., 0.5720, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1257, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(986065.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.3600, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4007.5210, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-312.7040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.9319, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5728],
        [ 0.5967],
        [ 0.6081],
        ...,
        [-4.0716],
        [-4.0622],
        [-4.0590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281889.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0383],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372272.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0383],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372276.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0161, -0.0019, -0.0023,  ...,  0.0290,  0.0151, -0.0038],
        [ 0.0131, -0.0018, -0.0022,  ...,  0.0247,  0.0126, -0.0032],
        [ 0.0159, -0.0019, -0.0023,  ...,  0.0286,  0.0150, -0.0038],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5858.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9237, device='cuda:0')



h[100].sum tensor(-0.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-13.7624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0481, 0.0000, 0.0000,  ..., 0.0939, 0.0472, 0.0000],
        [0.0647, 0.0000, 0.0000,  ..., 0.1170, 0.0609, 0.0000],
        [0.0735, 0.0000, 0.0000,  ..., 0.1294, 0.0682, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89468.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1758, 0.0000,  ..., 0.6696, 0.0000, 0.0401],
        [0.0000, 0.2176, 0.0000,  ..., 0.7772, 0.0000, 0.0539],
        [0.0000, 0.2262, 0.0000,  ..., 0.7992, 0.0000, 0.0566],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(966055.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(602.9730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3957.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.6399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.2371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3546],
        [ 0.3540],
        [ 0.3542],
        ...,
        [-4.0816],
        [-4.0723],
        [-4.0689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279728.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0383],
        [1.0395],
        ...,
        [0.9948],
        [0.9936],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372276.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372279.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0017, -0.0021,  ...,  0.0136,  0.0059, -0.0014],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7280.2231, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.6768, device='cuda:0')



h[100].sum tensor(-0.0317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0175, device='cuda:0')



h[200].sum tensor(-24.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0000,  ..., 0.0397, 0.0148, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0288, 0.0082, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(115407.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0190, 0.0000,  ..., 0.2536, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.1921, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1480, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1618, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1114949., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(710.5424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3651.7432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-356.2685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.7280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3814],
        [-1.2547],
        [-2.1284],
        ...,
        [-3.9798],
        [-3.7403],
        [-3.2658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280370.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372279.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5871.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9369, device='cuda:0')



h[100].sum tensor(-0.0176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-13.9568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89852.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1181, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(967804.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(608.3948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3920.5964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.5844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-631.8751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1627],
        [-4.1841],
        [-4.1824],
        ...,
        [-4.1026],
        [-4.0934],
        [-4.0901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308191.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [ 0.0037, -0.0017, -0.0021,  ...,  0.0117,  0.0048, -0.0011],
        [ 0.0037, -0.0017, -0.0021,  ...,  0.0117,  0.0048, -0.0011],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5846.8564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9276, device='cuda:0')



h[100].sum tensor(-0.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-13.7651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0000, 0.0000,  ..., 0.0326, 0.0104, 0.0000],
        [0.0066, 0.0000, 0.0000,  ..., 0.0328, 0.0105, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0329, 0.0106, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90308.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.2025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1913, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1881, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(973858.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.5303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3999.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.2642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.1677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5149],
        [-2.1746],
        [-2.7275],
        ...,
        [-4.1039],
        [-4.0944],
        [-4.0908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325181.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6382.3647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1924, device='cuda:0')



h[100].sum tensor(-0.0226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-17.8768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99266.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.1992, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.1925, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.1795, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1020447.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(647.7969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3830.0046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-323.9955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.8468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3552],
        [-0.4521],
        [-0.6406],
        ...,
        [-3.7692],
        [-3.7351],
        [-3.6199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312238.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0017, -0.0020,  ...,  0.0114,  0.0046, -0.0010],
        [ 0.0057, -0.0017, -0.0021,  ...,  0.0146,  0.0065, -0.0015],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6815.8115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4021, device='cuda:0')



h[100].sum tensor(-0.0268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-21.2048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0348, 0.0000, 0.0000,  ..., 0.0757, 0.0361, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0406, 0.0151, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0299, 0.0088, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(105869.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0569, 0.0000,  ..., 0.3678, 0.0000, 0.0025],
        [0.0000, 0.0282, 0.0000,  ..., 0.2807, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.1997, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1323, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1359, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1322, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1050996., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(674.7794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3731.5776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-337.2627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.1021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2887],
        [-0.2919],
        [-1.1842],
        ...,
        [-3.7731],
        [-3.7130],
        [-3.7607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299996.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0382],
        [1.0394],
        ...,
        [0.9947],
        [0.9935],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372283.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0381],
        [1.0394],
        ...,
        [0.9946],
        [0.9934],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372287.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0074, -0.0017, -0.0021,  ...,  0.0169,  0.0079, -0.0019],
        [ 0.0141, -0.0019, -0.0023,  ...,  0.0262,  0.0134, -0.0034],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6103.6284, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0508, device='cuda:0')



h[100].sum tensor(-0.0196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-15.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1456, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0136, 0.0000, 0.0000,  ..., 0.0423, 0.0163, 0.0000],
        [0.0248, 0.0000, 0.0000,  ..., 0.0601, 0.0268, 0.0000],
        [0.0448, 0.0000, 0.0000,  ..., 0.0899, 0.0445, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92856.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0298, 0.0000,  ..., 0.2764, 0.0000, 0.0022],
        [0.0000, 0.0687, 0.0000,  ..., 0.3999, 0.0000, 0.0094],
        [0.0000, 0.1175, 0.0000,  ..., 0.5289, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(977087., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.8137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4187.4893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-309.8014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-622.6374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8758],
        [-0.0522],
        [ 0.4164],
        ...,
        [-4.0993],
        [-4.0899],
        [-4.0865]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329980.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0381],
        [1.0394],
        ...,
        [0.9946],
        [0.9934],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372287.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0381],
        [1.0394],
        ...,
        [0.9946],
        [0.9934],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372291.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111, -0.0018, -0.0022,  ...,  0.0221,  0.0110, -0.0027],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0084, -0.0018, -0.0022,  ...,  0.0183,  0.0088, -0.0021],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5584.5283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.2771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7667, device='cuda:0')



h[100].sum tensor(-0.0142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0080, device='cuda:0')



h[200].sum tensor(-11.5142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0000, 0.0000,  ..., 0.0479, 0.0197, 0.0000],
        [0.0388, 0.0000, 0.0000,  ..., 0.0816, 0.0397, 0.0000],
        [0.0586, 0.0000, 0.0000,  ..., 0.1071, 0.0548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86635.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0758, 0.0000,  ..., 0.4237, 0.0000, 0.0077],
        [0.0000, 0.1407, 0.0000,  ..., 0.5920, 0.0000, 0.0289],
        [0.0000, 0.2161, 0.0000,  ..., 0.7848, 0.0000, 0.0538],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1261, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1261, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(953223.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.8122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4217.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.1925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-633.1437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5534],
        [ 0.5350],
        [ 0.4824],
        ...,
        [-4.0839],
        [-4.0747],
        [-4.0713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308335.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0381],
        [1.0394],
        ...,
        [0.9946],
        [0.9934],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372291.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0386],
        [1.0381],
        [1.0394],
        ...,
        [0.9945],
        [0.9933],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372295.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0076, -0.0017, -0.0021,  ...,  0.0172,  0.0081, -0.0020],
        [ 0.0047, -0.0017, -0.0021,  ...,  0.0132,  0.0057, -0.0013],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5837.3838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8943, device='cuda:0')



h[100].sum tensor(-0.0162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-13.2776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0324, 0.0105, 0.0000],
        [0.0111, 0.0000, 0.0000,  ..., 0.0391, 0.0144, 0.0000],
        [0.0494, 0.0000, 0.0000,  ..., 0.0964, 0.0485, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87953.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.2256, 0.0000, 0.0000],
        [0.0000, 0.0392, 0.0000,  ..., 0.3133, 0.0000, 0.0024],
        [0.0000, 0.0914, 0.0000,  ..., 0.4652, 0.0000, 0.0125],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(953657.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(593.5185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4404.0864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.2741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-628.4413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0430],
        [ 0.2354],
        [ 0.4588],
        ...,
        [-4.0704],
        [-4.0610],
        [-4.0543]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288316.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0386],
        [1.0381],
        [1.0394],
        ...,
        [0.9945],
        [0.9933],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372295.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0387],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9933],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372299.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5822.7754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8635, device='cuda:0')



h[100].sum tensor(-0.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-12.9950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91985.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1536, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(987500.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.6408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4160.0146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.2019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.6500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8079],
        [-2.3653],
        [-1.6327],
        ...,
        [-4.0574],
        [-4.0488],
        [-4.0457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255829.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0387],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9933],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372299.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0387],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9933],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372303.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6150.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0468, device='cuda:0')



h[100].sum tensor(-0.0183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-15.3534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94962., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1197, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1205, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1211, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(994112.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(618.1212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4253.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.2434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-611.0369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9111],
        [-3.8587],
        [-3.7579],
        ...,
        [-4.0527],
        [-4.0435],
        [-4.0401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256312.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0387],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9933],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372303.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0388],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9932],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372308.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5901.9053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8969, device='cuda:0')



h[100].sum tensor(-0.0158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-13.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91279.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1195, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1203, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(976617.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.4490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4158.4150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-621.9376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6968],
        [-3.8304],
        [-3.9407],
        ...,
        [-4.0482],
        [-4.0388],
        [-4.0351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244248.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0388],
        [1.0380],
        [1.0393],
        ...,
        [0.9945],
        [0.9932],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372308.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0388],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372312.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6027.1260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9606, device='cuda:0')



h[100].sum tensor(-0.0166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-14.2608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93871.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1194, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1202, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1263, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(994172.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.3799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4323.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.9871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.0665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7368],
        [-3.9034],
        [-3.9942],
        ...,
        [-4.0546],
        [-4.0454],
        [-4.0421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258223.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0388],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372312.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 370.0 event: 1850 loss: tensor(776.6440, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372316.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0054, -0.0017, -0.0021,  ...,  0.0140,  0.0062, -0.0014],
        [ 0.0036, -0.0017, -0.0021,  ...,  0.0116,  0.0048, -0.0011],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5702.6309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7839, device='cuda:0')



h[100].sum tensor(-0.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-11.7761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0000, 0.0000,  ..., 0.0289, 0.0085, 0.0000],
        [0.0124, 0.0000, 0.0000,  ..., 0.0405, 0.0154, 0.0000],
        [0.0466, 0.0000, 0.0000,  ..., 0.0920, 0.0460, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89690.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.2090, 0.0000, 0.0000],
        [0.0000, 0.0376, 0.0000,  ..., 0.3047, 0.0000, 0.0014],
        [0.0000, 0.0950, 0.0000,  ..., 0.4662, 0.0000, 0.0120],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(972072.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4502.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-623.5701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3346],
        [ 0.0802],
        [ 0.3696],
        ...,
        [-4.0576],
        [-4.0484],
        [-4.0450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281040.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372316.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372319.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052, -0.0017, -0.0021,  ...,  0.0137,  0.0061, -0.0014],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6613.6816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2695, device='cuda:0')



h[100].sum tensor(-0.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-18.6574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0174, 0.0000, 0.0000,  ..., 0.0474, 0.0195, 0.0000],
        [0.0323, 0.0000, 0.0000,  ..., 0.0701, 0.0330, 0.0000],
        [0.0275, 0.0000, 0.0000,  ..., 0.0615, 0.0279, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100452.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0599, 0.0000,  ..., 0.3753, 0.0000, 0.0023],
        [0.0000, 0.0874, 0.0000,  ..., 0.4468, 0.0000, 0.0082],
        [0.0000, 0.0926, 0.0000,  ..., 0.4602, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1023762.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(635.4038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4272.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-312.7391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.4648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5163],
        [ 0.4862],
        [ 0.4570],
        ...,
        [-4.0615],
        [-4.0524],
        [-4.0490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247296.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0393],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372319.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0392],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372323.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0176, -0.0019, -0.0023,  ...,  0.0309,  0.0163, -0.0040],
        [ 0.0256, -0.0021, -0.0025,  ...,  0.0421,  0.0230, -0.0058],
        [ 0.0181, -0.0019, -0.0024,  ...,  0.0316,  0.0167, -0.0041],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6670.9810, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2821, device='cuda:0')



h[100].sum tensor(-0.0214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-19.0717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0980, 0.0000, 0.0000,  ..., 0.1631, 0.0885, 0.0000],
        [0.0852, 0.0000, 0.0000,  ..., 0.1455, 0.0780, 0.0000],
        [0.0698, 0.0000, 0.0000,  ..., 0.1242, 0.0653, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103027.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2599, 0.0000,  ..., 0.8891, 0.0000, 0.0650],
        [0.0000, 0.2414, 0.0000,  ..., 0.8437, 0.0000, 0.0589],
        [0.0000, 0.2120, 0.0000,  ..., 0.7700, 0.0000, 0.0490],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1276, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1275, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1033544.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(644.8392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4313.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-318.6434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-587.9073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2840],
        [ 0.2830],
        [ 0.2876],
        ...,
        [-3.7889],
        [-3.7006],
        [-3.6974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231297.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0392],
        ...,
        [0.9944],
        [0.9932],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372323.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0392],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372326.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0046,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6542.4229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2179, device='cuda:0')



h[100].sum tensor(-0.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-18.1450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0030, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100247.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.2220, 0.0000, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.2295, 0.0000, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.2422, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1279, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1278, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1278, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1019739.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(633.2096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4186.4146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-314.8484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.1174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1154],
        [ 0.1396],
        [ 0.1881],
        ...,
        [-4.0755],
        [-4.0662],
        [-4.0629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203872.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0379],
        [1.0392],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372326.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0391],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372329.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5590.0137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7522, device='cuda:0')



h[100].sum tensor(-0.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0078, device='cuda:0')



h[200].sum tensor(-11.1256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0305, 0.0094, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85398.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1449, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.2054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1273, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1273, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(945442.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(574.4503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4761.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.4506, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-629.8907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1895],
        [-2.4379],
        [-1.4877],
        ...,
        [-4.1087],
        [-4.0993],
        [-4.0958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294450.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0391],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372329.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0391],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372332.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0169, -0.0019, -0.0023,  ...,  0.0300,  0.0157, -0.0039],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6060.9180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0099, device='cuda:0')



h[100].sum tensor(-0.0161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-14.7951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0028, 0.0000],
        [0.0175, 0.0000, 0.0000,  ..., 0.0458, 0.0184, 0.0000],
        [0.0142, 0.0000, 0.0000,  ..., 0.0413, 0.0157, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92620.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1553, 0.0000, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.2356, 0.0000, 0.0006],
        [0.0000, 0.0208, 0.0000,  ..., 0.2710, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982569.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.6939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4399.4956, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.1577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.7592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8997],
        [-1.9455],
        [-1.0251],
        ...,
        [-4.1319],
        [-4.1225],
        [-4.1195]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286954.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0391],
        ...,
        [0.9943],
        [0.9931],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372332.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372336.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0049, -0.0017, -0.0021,  ...,  0.0134,  0.0058, -0.0013],
        [ 0.0043, -0.0017, -0.0021,  ...,  0.0125,  0.0053, -0.0012],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5826.2476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8919, device='cuda:0')



h[100].sum tensor(-0.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-13.0561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0283, 0.0081, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0350, 0.0120, 0.0000],
        [0.0229, 0.0000, 0.0000,  ..., 0.0591, 0.0263, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90550.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.2302, 0.0000, 0.0000],
        [0.0000, 0.0262, 0.0000,  ..., 0.2848, 0.0000, 0.0000],
        [0.0000, 0.0428, 0.0000,  ..., 0.3406, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(976685.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(599.4407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4424.5679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.7778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-623.0347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2046],
        [ 0.2030],
        [ 0.4554],
        ...,
        [-4.1509],
        [-4.1413],
        [-4.1377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300285., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372336.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0389],
        ...,
        [0.9942],
        [0.9931],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372340.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0240, -0.0020, -0.0025,  ...,  0.0399,  0.0217, -0.0053],
        [ 0.0329, -0.0022, -0.0027,  ...,  0.0522,  0.0290, -0.0072],
        [ 0.0250, -0.0020, -0.0025,  ...,  0.0413,  0.0225, -0.0056],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6383.3379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1659, device='cuda:0')



h[100].sum tensor(-0.0182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-17.1727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1133, 0.0000, 0.0000,  ..., 0.1843, 0.1010, 0.0000],
        [0.1290, 0.0000, 0.0000,  ..., 0.2062, 0.1140, 0.0000],
        [0.1268, 0.0000, 0.0000,  ..., 0.2033, 0.1122, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99021.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2878, 0.0000,  ..., 0.9662, 0.0000, 0.0733],
        [0.0000, 0.3266, 0.0000,  ..., 1.0664, 0.0000, 0.0858],
        [0.0000, 0.3208, 0.0000,  ..., 1.0525, 0.0000, 0.0837],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1021921.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(633.0806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4319.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-317.1547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-600.7453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3305],
        [ 0.3125],
        [ 0.2939],
        ...,
        [-4.1519],
        [-4.1422],
        [-4.1385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275403.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0389],
        [1.0380],
        [1.0389],
        ...,
        [0.9942],
        [0.9931],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372340.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9942],
        [0.9930],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372344.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5931.5244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9425, device='cuda:0')



h[100].sum tensor(-0.0144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-13.7193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0000, 0.0000,  ..., 0.0319, 0.0104, 0.0000],
        [0.0032, 0.0000, 0.0000,  ..., 0.0257, 0.0067, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0287, 0.0084, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91620.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7739e-04, 0.0000e+00,  ..., 1.9793e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8734e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0077e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2757e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2747e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2741e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982628.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(601.6353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4498.9976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-302.7769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-615.3689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4393],
        [-1.2376],
        [-0.9801],
        ...,
        [-4.1481],
        [-4.1386],
        [-4.1352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267083.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9942],
        [0.9930],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372344.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9941],
        [0.9930],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372348.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097, -0.0018, -0.0022,  ...,  0.0199,  0.0098, -0.0023],
        [ 0.0064, -0.0017, -0.0021,  ...,  0.0154,  0.0071, -0.0016],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6125.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0270, device='cuda:0')



h[100].sum tensor(-0.0156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-15.0977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0393, 0.0000, 0.0000,  ..., 0.0815, 0.0400, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0443, 0.0179, 0.0000],
        [0.0066, 0.0000, 0.0000,  ..., 0.0305, 0.0096, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93395.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0637, 0.0000,  ..., 0.3925, 0.0000, 0.0037],
        [0.0000, 0.0323, 0.0000,  ..., 0.3081, 0.0000, 0.0007],
        [0.0000, 0.0124, 0.0000,  ..., 0.2417, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1277, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1276, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(988220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.8464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4471.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.2594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.6624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5063],
        [ 0.2643],
        [-0.1710],
        ...,
        [-4.1487],
        [-4.1391],
        [-4.1354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254723.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9941],
        [0.9930],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372348.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 380.0 event: 1900 loss: tensor(467.6360, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9941],
        [0.9930],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372352.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5882.0420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8962, device='cuda:0')



h[100].sum tensor(-0.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-13.2568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0193, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0030, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90269.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1253, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(973977.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(595.8541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4598.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.4574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.5010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9798],
        [-3.7795],
        [-3.4220],
        ...,
        [-3.8292],
        [-4.0552],
        [-4.1202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279537.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0388],
        ...,
        [0.9941],
        [0.9930],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372352.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0387],
        ...,
        [0.9941],
        [0.9929],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372357.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6640.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.2957, device='cuda:0')



h[100].sum tensor(-0.0191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-18.8667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1796, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101652.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1821, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1537, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1439, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1031114.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(643.0969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4175.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-321.5507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.9052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7661],
        [-1.4143],
        [-1.9245],
        ...,
        [-4.1638],
        [-4.1461],
        [-4.0971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252713.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0390],
        [1.0380],
        [1.0387],
        ...,
        [0.9941],
        [0.9929],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372357.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0390],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9929],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372359.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0114, -0.0018, -0.0022,  ...,  0.0223,  0.0112, -0.0026],
        [ 0.0063, -0.0017, -0.0021,  ...,  0.0152,  0.0070, -0.0016],
        [ 0.0156, -0.0019, -0.0023,  ...,  0.0281,  0.0146, -0.0035],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6885.7148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3965, device='cuda:0')



h[100].sum tensor(-0.0207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-20.6902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0000, 0.0000,  ..., 0.0633, 0.0290, 0.0000],
        [0.0386, 0.0000, 0.0000,  ..., 0.0788, 0.0381, 0.0000],
        [0.0196, 0.0000, 0.0000,  ..., 0.0507, 0.0213, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103928.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.7026e-02, 0.0000e+00,  ..., 3.7314e-01, 0.0000e+00,
         2.5123e-04],
        [0.0000e+00, 6.8304e-02, 0.0000e+00,  ..., 4.0240e-01, 0.0000e+00,
         3.5151e-03],
        [0.0000e+00, 5.1462e-02, 0.0000e+00,  ..., 3.5879e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2655e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2646e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2639e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1035792.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(652.9670, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4333.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-325.5143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-587.7087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5712],
        [ 0.6529],
        [ 0.6591],
        ...,
        [-4.1489],
        [-4.1268],
        [-4.1176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276778.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0390],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9929],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372359.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0391],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9929],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372362.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047, -0.0017, -0.0021,  ...,  0.0130,  0.0056, -0.0012],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5776.5332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8470, device='cuda:0')



h[100].sum tensor(-0.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0088, device='cuda:0')



h[200].sum tensor(-12.4060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0183, 0.0000, 0.0000,  ..., 0.0486, 0.0202, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0282, 0.0080, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89750.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0488, 0.0000,  ..., 0.3438, 0.0000, 0.0031],
        [0.0000, 0.0171, 0.0000,  ..., 0.2405, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(975096.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(592.6967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4402.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.2909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.3716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2455],
        [-0.2224],
        [-0.9409],
        ...,
        [-4.1766],
        [-4.1669],
        [-4.1631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253826.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0391],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9929],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372362.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0391],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9928],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372365.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [ 0.0103, -0.0018, -0.0022,  ...,  0.0209,  0.0104, -0.0024],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5644.2769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7819, device='cuda:0')



h[100].sum tensor(-0.0111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-11.4153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0433, 0.0171, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0420, 0.0162, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87442.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.2416, 0.0000, 0.0000],
        [0.0000, 0.0235, 0.0000,  ..., 0.2803, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1271, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(962300.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(582.7115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4651.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.9527, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.4689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3209],
        [-0.1708],
        [ 0.1505],
        ...,
        [-4.1574],
        [-4.1605],
        [-4.1583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292465., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0391],
        [1.0381],
        [1.0387],
        ...,
        [0.9940],
        [0.9928],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372365.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0392],
        [1.0381],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372368.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6029.9189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9843, device='cuda:0')



h[100].sum tensor(-0.0137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-14.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91829.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1287, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1299, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1357, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1330, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(980586.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(600.4978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4628.1064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.1942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.1232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5403],
        [-2.5186],
        [-2.1893],
        ...,
        [-3.9078],
        [-4.0819],
        [-4.1466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286269.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0392],
        [1.0381],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372368.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0392],
        [1.0381],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372370.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6251.9385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0900, device='cuda:0')



h[100].sum tensor(-0.0151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-15.8897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95820.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1273, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(997714.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(617.4836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4376.3340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.0594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.8149, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7022],
        [-3.2387],
        [-3.5513],
        ...,
        [-4.1802],
        [-4.1707],
        [-4.1670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264414.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0392],
        [1.0381],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372370.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0393],
        [1.0382],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372373.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0047,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5914.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9261, device='cuda:0')



h[100].sum tensor(-0.0126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-13.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90637.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1207, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1213, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1270, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1269, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(975753.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(597.9705, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4262.3950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.9368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-607.1218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1464],
        [-4.0298],
        [-3.8283],
        ...,
        [-4.1902],
        [-4.1806],
        [-4.1769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272861.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0393],
        [1.0382],
        [1.0387],
        ...,
        [0.9939],
        [0.9928],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372373.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0394],
        [1.0382],
        [1.0387],
        ...,
        [0.9939],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372375.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [ 0.0109, -0.0018, -0.0022,  ...,  0.0218,  0.0108, -0.0025],
        [ 0.0117, -0.0018, -0.0022,  ...,  0.0229,  0.0115, -0.0027],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6269.0083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.1119, device='cuda:0')



h[100].sum tensor(-0.0150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-16.0888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0000, 0.0000,  ..., 0.0373, 0.0133, 0.0000],
        [0.0212, 0.0000, 0.0000,  ..., 0.0532, 0.0227, 0.0000],
        [0.0660, 0.0000, 0.0000,  ..., 0.1196, 0.0621, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96044.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.4351e-02, 0.0000e+00,  ..., 2.2719e-01, 0.0000e+00,
         3.8258e-04],
        [0.0000e+00, 4.0945e-02, 0.0000e+00,  ..., 3.2683e-01, 0.0000e+00,
         3.2480e-03],
        [0.0000e+00, 1.0171e-01, 0.0000e+00,  ..., 4.9859e-01, 0.0000e+00,
         1.3975e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2638e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2629e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2621e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1005618.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(623.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3987.6333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.9584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-598.3992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4498],
        [-0.7356],
        [-0.0184],
        ...,
        [-4.2035],
        [-4.1938],
        [-4.1901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276120.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0394],
        [1.0382],
        [1.0387],
        ...,
        [0.9939],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372375.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0394],
        [1.0383],
        [1.0387],
        ...,
        [0.9938],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372379.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6794.5571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.3798, device='cuda:0')



h[100].sum tensor(-0.0183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0144, device='cuda:0')



h[200].sum tensor(-19.9620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(105782.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1229, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1376, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1260, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1259, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1258, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1053996.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(664.9724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3822.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-328.5112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.1917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9377],
        [-2.6883],
        [-2.1128],
        ...,
        [-4.1976],
        [-4.1934],
        [-4.1910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256747.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0394],
        [1.0383],
        [1.0387],
        ...,
        [0.9938],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372379.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 390.0 event: 1950 loss: tensor(480.8515, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0395],
        [1.0383],
        [1.0387],
        ...,
        [0.9938],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372382.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6152.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0538, device='cuda:0')



h[100].sum tensor(-0.0138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-15.2242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(97298.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1186, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1254, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1019665.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(630.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4241.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-311.6767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-600.5947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5108],
        [-3.6712],
        [-3.7611],
        ...,
        [-4.2078],
        [-4.1982],
        [-4.1945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313109.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0395],
        [1.0383],
        [1.0387],
        ...,
        [0.9938],
        [0.9927],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372382.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0396],
        [1.0384],
        [1.0387],
        ...,
        [0.9938],
        [0.9926],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372386.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6219.5439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0722, device='cuda:0')



h[100].sum tensor(-0.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-15.6448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95531.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1208, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1324, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1612, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1255, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(998839.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(623.7867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3986.0435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-309.0302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.9424, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1200],
        [-2.5465],
        [-1.8269],
        ...,
        [-4.1985],
        [-4.1889],
        [-4.1852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257357.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0396],
        [1.0384],
        [1.0387],
        ...,
        [0.9938],
        [0.9926],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372386.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0397],
        [1.0384],
        [1.0386],
        ...,
        [0.9938],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372390.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0118, -0.0018, -0.0022,  ...,  0.0231,  0.0116, -0.0027],
        [ 0.0228, -0.0020, -0.0024,  ...,  0.0383,  0.0207, -0.0049],
        [ 0.0279, -0.0021, -0.0026,  ...,  0.0454,  0.0249, -0.0059],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5971.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9491, device='cuda:0')



h[100].sum tensor(-0.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-13.7407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0820, 0.0000, 0.0000,  ..., 0.1413, 0.0752, 0.0000],
        [0.1112, 0.0000, 0.0000,  ..., 0.1819, 0.0993, 0.0000],
        [0.1110, 0.0000, 0.0000,  ..., 0.1818, 0.0992, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93141.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2310, 0.0000,  ..., 0.8246, 0.0000, 0.0546],
        [0.0000, 0.2735, 0.0000,  ..., 0.9355, 0.0000, 0.0684],
        [0.0000, 0.2878, 0.0000,  ..., 0.9737, 0.0000, 0.0731],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1261, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1260, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(994196.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(610.8895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4273.6030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.4501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-613.9757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5727],
        [ 0.5649],
        [ 0.5662],
        ...,
        [-4.1813],
        [-4.1715],
        [-4.1675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249379.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0397],
        [1.0384],
        [1.0386],
        ...,
        [0.9938],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372390.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0398],
        [1.0385],
        [1.0386],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372394., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101, -0.0018, -0.0022,  ...,  0.0206,  0.0102, -0.0023],
        [ 0.0079, -0.0018, -0.0021,  ...,  0.0176,  0.0084, -0.0019],
        [ 0.0197, -0.0020, -0.0024,  ...,  0.0340,  0.0181, -0.0043],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5326.2681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.6238, device='cuda:0')



h[100].sum tensor(-0.0078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0065, device='cuda:0')



h[200].sum tensor(-8.9554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0463, 0.0188, 0.0000],
        [0.0555, 0.0000, 0.0000,  ..., 0.1047, 0.0535, 0.0000],
        [0.0580, 0.0000, 0.0000,  ..., 0.1082, 0.0556, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82921.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0303, 0.0000,  ..., 0.2906, 0.0000, 0.0011],
        [0.0000, 0.0809, 0.0000,  ..., 0.4359, 0.0000, 0.0079],
        [0.0000, 0.1021, 0.0000,  ..., 0.4925, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(940947.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(564.8046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4943.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-284.3008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-633.0807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9875],
        [-0.0746],
        [ 0.4622],
        ...,
        [-4.1708],
        [-4.1614],
        [-4.1578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277157.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0398],
        [1.0385],
        [1.0386],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372394., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0398],
        [1.0385],
        [1.0385],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372396.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046, -0.0017, -0.0021,  ...,  0.0130,  0.0056, -0.0012],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [ 0.0046, -0.0017, -0.0021,  ...,  0.0130,  0.0056, -0.0012],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5841.4131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8856, device='cuda:0')



h[100].sum tensor(-0.0110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-12.7447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0267, 0.0071, 0.0000],
        [0.0169, 0.0000, 0.0000,  ..., 0.0512, 0.0216, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0270, 0.0072, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0205, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89722.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1672, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1691, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1268, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(969596.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(593.4395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4819.0347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.0288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.4309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7438],
        [-2.7950],
        [-3.0644],
        ...,
        [-4.1766],
        [-4.1672],
        [-4.1636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261425.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0398],
        [1.0385],
        [1.0385],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372396.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0399],
        [1.0386],
        [1.0384],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372400.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [ 0.0069, -0.0017, -0.0021,  ...,  0.0162,  0.0075, -0.0017],
        [ 0.0292, -0.0021, -0.0026,  ...,  0.0472,  0.0259, -0.0062],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(8376.9717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.1924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.1951, device='cuda:0')



h[100].sum tensor(-0.0267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0229, device='cuda:0')



h[200].sum tensor(-31.2745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0000, 0.0000,  ..., 0.0509, 0.0214, 0.0000],
        [0.0621, 0.0000, 0.0000,  ..., 0.1119, 0.0576, 0.0000],
        [0.0835, 0.0000, 0.0000,  ..., 0.1436, 0.0765, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(129006.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0958, 0.0000,  ..., 0.4613, 0.0000, 0.0160],
        [0.0000, 0.2043, 0.0000,  ..., 0.7513, 0.0000, 0.0444],
        [0.0000, 0.2955, 0.0000,  ..., 0.9875, 0.0000, 0.0737],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1263, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1174383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(757.1774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4274.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-377.3687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-527.4528, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1748],
        [ 0.2783],
        [ 0.2954],
        ...,
        [-4.1904],
        [-4.1809],
        [-4.1773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253926.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0399],
        [1.0386],
        [1.0384],
        ...,
        [0.9937],
        [0.9926],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372400.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0400],
        [1.0386],
        [1.0384],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372402.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        ...,
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0013, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5469.1001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.7083, device='cuda:0')



h[100].sum tensor(-0.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0074, device='cuda:0')



h[200].sum tensor(-10.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.0982, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0206, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85128.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1263, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1261, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(948990.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(577.2119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4988.2202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.3237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.2988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9586],
        [-3.7157],
        [-3.2553],
        ...,
        [-4.2036],
        [-4.1941],
        [-4.1905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316476.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0400],
        [1.0386],
        [1.0384],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372402.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0401],
        [1.0387],
        [1.0383],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372404.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6008.3633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.9909, device='cuda:0')



h[100].sum tensor(-0.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-14.1228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.1374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0199, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92315.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1193, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1206, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1263, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1261, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(982882.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(607.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4625.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.0927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.4893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9847],
        [-4.0258],
        [-4.0090],
        ...,
        [-4.2138],
        [-4.2043],
        [-4.2006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272178.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0401],
        [1.0387],
        [1.0383],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372404.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0401],
        [1.0387],
        [1.0382],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(372407.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0306, -0.0021, -0.0026,  ...,  0.0491,  0.0270, -0.0064],
        [ 0.0197, -0.0020, -0.0024,  ...,  0.0341,  0.0181, -0.0042],
        [ 0.0096, -0.0018, -0.0022,  ...,  0.0200,  0.0097, -0.0022],
        ...,
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000],
        [-0.0014, -0.0016, -0.0019,  ...,  0.0048,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(7006.2227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4722, device='cuda:0')



h[100].sum tensor(-0.0176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-21.3725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0923, 0.0000, 0.0000,  ..., 0.1559, 0.0837, 0.0000],
        [0.0662, 0.0000, 0.0000,  ..., 0.1198, 0.0622, 0.0000],
        [0.0360, 0.0000, 0.0000,  ..., 0.0760, 0.0361, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0207, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108119.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2150, 0.0000,  ..., 0.7862, 0.0000, 0.0484],
        [0.0000, 0.1782, 0.0000,  ..., 0.6922, 0.0000, 0.0362],
        [0.0000, 0.1300, 0.0000,  ..., 0.5677, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.1264, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(1068372., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(673.1003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4143.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-339.5519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.5560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6192],
        [ 0.6440],
        [ 0.6675],
        ...,
        [-4.2218],
        [-4.2122],
        [-4.2085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244943., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0401],
        [1.0387],
        [1.0382],
        ...,
        [0.9937],
        [0.9925],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(372407.3125, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:01:01.199404
evaluation loss: 480.3573913574219
epoch: 1 mean loss: 458.0138244628906
=> saveing checkpoint at epoch 1
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 138, in <module>
    writer.add_hparams({'Lr': LrVal, 'weight_decay': weight_decay_val, 'BatchSize':BatchSize},\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 313, in add_hparams
    w_hp.add_scalar(k, v)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 354, in add_scalar
    summary = scalar(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py", line 250, in scalar
    assert scalar.squeeze().ndim == 0, "scalar should be 0D"
AssertionError: scalar should be 0D

real	2m5.100s
user	1m2.931s
sys	0m28.949s
