0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        465.19.01
supported:      external
license:        NVIDIA
firmware:       nvidia/465.19.01/gsp.bin
retpoline:      Y
rhelversion:    7.8
srcversion:     976AD09EB9C3B8943CBA8C4
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           nv_cap_enable_devfs:Enable (1) or disable (0) nv-caps devfs support. Default: 1 (int)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jul 18 17:52:03 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.505273344

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2af8a2e77fa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.829s
user	0m2.609s
sys	0m0.932s
[17:52:10] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.3809],
        [ 0.2288],
        [-0.0560],
        ...,
        [-0.7913],
        [-0.6073],
        [ 0.4647]], device='cuda:0', requires_grad=True) 
node features sum: tensor(93.9918, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-9.1280e-02, -1.2761e-01,  1.3682e-01,  6.0686e-02,  9.8856e-03,
          7.7431e-03, -5.2902e-02,  1.2908e-01,  7.5176e-03, -3.2869e-02,
          6.4113e-02, -4.3625e-02,  1.6004e-02, -5.7271e-02, -8.5391e-02,
          1.1889e-01,  6.3801e-03,  1.0640e-02,  1.3463e-01,  6.8015e-03,
         -2.7946e-02,  1.5014e-01,  1.0490e-01, -1.4328e-01, -7.9426e-02,
         -1.3876e-02,  7.9527e-02,  9.6561e-02,  6.9503e-02,  1.1113e-01,
          3.0889e-02,  2.9780e-02, -6.8132e-02,  4.5986e-02, -2.0252e-02,
         -8.6905e-02, -1.3369e-01,  9.3991e-02, -5.6888e-02,  1.0700e-01,
          1.0219e-01,  5.6928e-02, -7.0345e-02,  7.7510e-02, -6.2233e-02,
          3.3551e-02,  6.1930e-06,  8.6531e-02,  1.3781e-01,  2.2136e-02,
          9.5486e-02, -5.0483e-02, -1.1959e-01,  1.2812e-01,  2.3957e-02,
         -5.7172e-02, -3.1882e-03,  8.0324e-02, -2.4522e-02,  1.2789e-01,
         -1.2300e-01, -3.0406e-02, -1.2123e-01,  9.5531e-02,  5.5960e-03,
         -2.9392e-02,  6.4124e-02, -1.4986e-01, -2.9249e-02, -9.3729e-02,
         -1.0145e-01, -1.2488e-01, -1.4708e-01, -8.9422e-02,  5.2424e-02,
         -1.1388e-01, -1.0108e-01,  2.6294e-02, -3.6643e-02, -4.9438e-02,
          1.0033e-01, -6.8542e-02,  8.1665e-03, -5.6124e-02, -6.3539e-02,
          8.3944e-02, -1.5924e-02, -1.2134e-01, -1.1499e-01,  1.3979e-01,
          9.1425e-02, -7.5203e-02,  3.9610e-03,  1.1730e-02, -8.9185e-02,
         -1.0754e-01,  4.0050e-03, -1.2129e-02,  3.8605e-02, -9.4386e-02,
          3.9470e-02, -1.3525e-03, -1.4092e-01,  1.2312e-01,  1.0560e-01,
          7.0418e-02, -1.3041e-01,  9.6420e-02, -1.2489e-01, -7.7560e-02,
          1.1106e-01, -1.3171e-01,  5.4220e-02, -1.2974e-01, -2.1264e-02,
          1.9163e-02,  3.2412e-02, -1.3162e-01, -1.0645e-01,  6.8637e-02,
         -5.5717e-02,  1.0811e-01, -1.1407e-01,  1.1688e-01,  7.3136e-02,
         -1.9309e-02,  8.5605e-02,  1.3842e-01, -1.2335e-01, -1.0776e-01,
         -7.7193e-02,  4.2846e-02,  6.5172e-02, -4.2994e-02, -1.3324e-01,
         -5.2791e-02,  1.3054e-01,  1.0198e-01, -1.3289e-01, -6.0057e-02,
          5.0254e-02,  1.3137e-01,  8.7904e-03, -1.4920e-01,  1.1549e-02,
          6.6646e-02,  6.4144e-02, -9.8935e-02,  2.5849e-02, -1.4317e-01,
         -1.0326e-01,  4.2160e-03, -7.4270e-02,  1.3156e-01, -4.6378e-02,
          1.3414e-01,  1.1371e-01,  7.9367e-02,  4.3933e-02, -3.7315e-02,
          7.5801e-02, -6.6227e-02,  1.0358e-01,  1.5449e-02, -1.0346e-01,
         -2.8919e-02,  5.5950e-02,  8.2008e-04,  8.2884e-02,  6.9770e-02,
         -1.4973e-01, -1.8882e-02, -3.8862e-02,  1.0119e-01, -1.2921e-01,
          1.9596e-02, -7.9755e-02, -1.5068e-01,  1.0846e-01, -1.4044e-01,
         -7.1521e-02,  7.6145e-02,  1.4171e-01,  1.0513e-01,  1.0014e-01,
         -9.4734e-02, -3.1108e-02, -7.3701e-02, -1.3798e-01,  1.0693e-01,
          1.3967e-01, -1.5106e-01,  1.3443e-01,  5.8449e-02,  3.1600e-03,
         -7.1125e-02, -1.2853e-01, -4.2551e-02,  1.1220e-01, -2.3870e-02,
          8.8357e-02, -4.5783e-02, -1.0107e-01,  1.0083e-01, -8.2189e-02,
          5.5646e-02, -1.9602e-02,  1.4542e-01, -2.1098e-02, -3.7868e-02,
          2.3510e-02,  7.4246e-02, -3.2241e-02, -1.8326e-02, -5.2824e-02,
         -9.0560e-03, -4.5252e-02,  1.2047e-01, -1.2811e-01, -5.6017e-02,
          1.4143e-01, -1.8194e-02, -2.1934e-02,  1.0565e-02, -1.3194e-01,
          1.3684e-02, -2.4974e-02, -1.3632e-01,  7.5093e-02,  1.4369e-01,
          1.4046e-01,  1.0067e-01, -6.4322e-03, -4.5311e-02,  1.0154e-01,
          1.1515e-01, -1.5059e-02,  6.0075e-02, -1.3008e-01,  1.2356e-01,
         -1.5135e-01, -1.5365e-03, -3.2836e-02,  1.2267e-01,  7.0245e-03,
         -1.4549e-01,  1.3694e-01,  2.3779e-02,  2.8411e-02,  2.8732e-02,
         -1.3506e-01,  9.7123e-02,  2.9855e-03, -1.0878e-01,  1.3747e-01,
          2.3671e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-9.1280e-02, -1.2761e-01,  1.3682e-01,  6.0686e-02,  9.8856e-03,
          7.7431e-03, -5.2902e-02,  1.2908e-01,  7.5176e-03, -3.2869e-02,
          6.4113e-02, -4.3625e-02,  1.6004e-02, -5.7271e-02, -8.5391e-02,
          1.1889e-01,  6.3801e-03,  1.0640e-02,  1.3463e-01,  6.8015e-03,
         -2.7946e-02,  1.5014e-01,  1.0490e-01, -1.4328e-01, -7.9426e-02,
         -1.3876e-02,  7.9527e-02,  9.6561e-02,  6.9503e-02,  1.1113e-01,
          3.0889e-02,  2.9780e-02, -6.8132e-02,  4.5986e-02, -2.0252e-02,
         -8.6905e-02, -1.3369e-01,  9.3991e-02, -5.6888e-02,  1.0700e-01,
          1.0219e-01,  5.6928e-02, -7.0345e-02,  7.7510e-02, -6.2233e-02,
          3.3551e-02,  6.1930e-06,  8.6531e-02,  1.3781e-01,  2.2136e-02,
          9.5486e-02, -5.0483e-02, -1.1959e-01,  1.2812e-01,  2.3957e-02,
         -5.7172e-02, -3.1882e-03,  8.0324e-02, -2.4522e-02,  1.2789e-01,
         -1.2300e-01, -3.0406e-02, -1.2123e-01,  9.5531e-02,  5.5960e-03,
         -2.9392e-02,  6.4124e-02, -1.4986e-01, -2.9249e-02, -9.3729e-02,
         -1.0145e-01, -1.2488e-01, -1.4708e-01, -8.9422e-02,  5.2424e-02,
         -1.1388e-01, -1.0108e-01,  2.6294e-02, -3.6643e-02, -4.9438e-02,
          1.0033e-01, -6.8542e-02,  8.1665e-03, -5.6124e-02, -6.3539e-02,
          8.3944e-02, -1.5924e-02, -1.2134e-01, -1.1499e-01,  1.3979e-01,
          9.1425e-02, -7.5203e-02,  3.9610e-03,  1.1730e-02, -8.9185e-02,
         -1.0754e-01,  4.0050e-03, -1.2129e-02,  3.8605e-02, -9.4386e-02,
          3.9470e-02, -1.3525e-03, -1.4092e-01,  1.2312e-01,  1.0560e-01,
          7.0418e-02, -1.3041e-01,  9.6420e-02, -1.2489e-01, -7.7560e-02,
          1.1106e-01, -1.3171e-01,  5.4220e-02, -1.2974e-01, -2.1264e-02,
          1.9163e-02,  3.2412e-02, -1.3162e-01, -1.0645e-01,  6.8637e-02,
         -5.5717e-02,  1.0811e-01, -1.1407e-01,  1.1688e-01,  7.3136e-02,
         -1.9309e-02,  8.5605e-02,  1.3842e-01, -1.2335e-01, -1.0776e-01,
         -7.7193e-02,  4.2846e-02,  6.5172e-02, -4.2994e-02, -1.3324e-01,
         -5.2791e-02,  1.3054e-01,  1.0198e-01, -1.3289e-01, -6.0057e-02,
          5.0254e-02,  1.3137e-01,  8.7904e-03, -1.4920e-01,  1.1549e-02,
          6.6646e-02,  6.4144e-02, -9.8935e-02,  2.5849e-02, -1.4317e-01,
         -1.0326e-01,  4.2160e-03, -7.4270e-02,  1.3156e-01, -4.6378e-02,
          1.3414e-01,  1.1371e-01,  7.9367e-02,  4.3933e-02, -3.7315e-02,
          7.5801e-02, -6.6227e-02,  1.0358e-01,  1.5449e-02, -1.0346e-01,
         -2.8919e-02,  5.5950e-02,  8.2008e-04,  8.2884e-02,  6.9770e-02,
         -1.4973e-01, -1.8882e-02, -3.8862e-02,  1.0119e-01, -1.2921e-01,
          1.9596e-02, -7.9755e-02, -1.5068e-01,  1.0846e-01, -1.4044e-01,
         -7.1521e-02,  7.6145e-02,  1.4171e-01,  1.0513e-01,  1.0014e-01,
         -9.4734e-02, -3.1108e-02, -7.3701e-02, -1.3798e-01,  1.0693e-01,
          1.3967e-01, -1.5106e-01,  1.3443e-01,  5.8449e-02,  3.1600e-03,
         -7.1125e-02, -1.2853e-01, -4.2551e-02,  1.1220e-01, -2.3870e-02,
          8.8357e-02, -4.5783e-02, -1.0107e-01,  1.0083e-01, -8.2189e-02,
          5.5646e-02, -1.9602e-02,  1.4542e-01, -2.1098e-02, -3.7868e-02,
          2.3510e-02,  7.4246e-02, -3.2241e-02, -1.8326e-02, -5.2824e-02,
         -9.0560e-03, -4.5252e-02,  1.2047e-01, -1.2811e-01, -5.6017e-02,
          1.4143e-01, -1.8194e-02, -2.1934e-02,  1.0565e-02, -1.3194e-01,
          1.3684e-02, -2.4974e-02, -1.3632e-01,  7.5093e-02,  1.4369e-01,
          1.4046e-01,  1.0067e-01, -6.4322e-03, -4.5311e-02,  1.0154e-01,
          1.1515e-01, -1.5059e-02,  6.0075e-02, -1.3008e-01,  1.2356e-01,
         -1.5135e-01, -1.5365e-03, -3.2836e-02,  1.2267e-01,  7.0245e-03,
         -1.4549e-01,  1.3694e-01,  2.3779e-02,  2.8411e-02,  2.8732e-02,
         -1.3506e-01,  9.7123e-02,  2.9855e-03, -1.0878e-01,  1.3747e-01,
          2.3671e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0878, -0.0841,  0.1148,  ...,  0.0722,  0.0730,  0.0218],
        [-0.0265, -0.0351, -0.0859,  ...,  0.0861, -0.0092,  0.0908],
        [ 0.0510,  0.0253, -0.1197,  ...,  0.0609, -0.0487, -0.1107],
        ...,
        [-0.0891, -0.0791,  0.0822,  ...,  0.0144,  0.0340,  0.0565],
        [-0.1017,  0.0859,  0.1050,  ..., -0.0988,  0.0300, -0.1138],
        [-0.1248,  0.0416,  0.1011,  ..., -0.0028,  0.0529,  0.0826]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0878, -0.0841,  0.1148,  ...,  0.0722,  0.0730,  0.0218],
        [-0.0265, -0.0351, -0.0859,  ...,  0.0861, -0.0092,  0.0908],
        [ 0.0510,  0.0253, -0.1197,  ...,  0.0609, -0.0487, -0.1107],
        ...,
        [-0.0891, -0.0791,  0.0822,  ...,  0.0144,  0.0340,  0.0565],
        [-0.1017,  0.0859,  0.1050,  ..., -0.0988,  0.0300, -0.1138],
        [-0.1248,  0.0416,  0.1011,  ..., -0.0028,  0.0529,  0.0826]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1106,  0.1201,  0.1712,  ...,  0.1754, -0.0812, -0.1272],
        [-0.0956,  0.1036,  0.0439,  ..., -0.1162,  0.0904, -0.1439],
        [-0.0383,  0.0665,  0.1557,  ..., -0.1768,  0.1635,  0.0358],
        ...,
        [-0.1020, -0.1476, -0.0427,  ...,  0.0819,  0.0587, -0.1425],
        [ 0.1301,  0.0598, -0.0670,  ...,  0.0713, -0.0494,  0.1647],
        [-0.1288, -0.0206, -0.0457,  ...,  0.0683,  0.1704, -0.1113]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1106,  0.1201,  0.1712,  ...,  0.1754, -0.0812, -0.1272],
        [-0.0956,  0.1036,  0.0439,  ..., -0.1162,  0.0904, -0.1439],
        [-0.0383,  0.0665,  0.1557,  ..., -0.1768,  0.1635,  0.0358],
        ...,
        [-0.1020, -0.1476, -0.0427,  ...,  0.0819,  0.0587, -0.1425],
        [ 0.1301,  0.0598, -0.0670,  ...,  0.0713, -0.0494,  0.1647],
        [-0.1288, -0.0206, -0.0457,  ...,  0.0683,  0.1704, -0.1113]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0517, -0.1271,  0.2062,  ..., -0.1365,  0.1087, -0.1482],
        [ 0.1997, -0.2317, -0.2165,  ..., -0.2349, -0.0265, -0.0923],
        [ 0.1197, -0.0776,  0.2485,  ...,  0.0615,  0.2263, -0.0172],
        ...,
        [-0.2382,  0.2129,  0.1786,  ...,  0.2147, -0.2180, -0.0367],
        [-0.1234, -0.0675, -0.0358,  ..., -0.1831, -0.1776,  0.0560],
        [ 0.1522, -0.2123,  0.2252,  ..., -0.1192,  0.1876, -0.0206]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0517, -0.1271,  0.2062,  ..., -0.1365,  0.1087, -0.1482],
        [ 0.1997, -0.2317, -0.2165,  ..., -0.2349, -0.0265, -0.0923],
        [ 0.1197, -0.0776,  0.2485,  ...,  0.0615,  0.2263, -0.0172],
        ...,
        [-0.2382,  0.2129,  0.1786,  ...,  0.2147, -0.2180, -0.0367],
        [-0.1234, -0.0675, -0.0358,  ..., -0.1831, -0.1776,  0.0560],
        [ 0.1522, -0.2123,  0.2252,  ..., -0.1192,  0.1876, -0.0206]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3553],
        [-0.1312],
        [ 0.1228],
        [-0.2293],
        [-0.2198],
        [-0.0305],
        [-0.0792],
        [ 0.1199],
        [-0.2170],
        [ 0.3094],
        [ 0.2175],
        [-0.2326],
        [ 0.0849],
        [ 0.2053],
        [ 0.2475],
        [-0.3283],
        [ 0.2298],
        [ 0.2056],
        [ 0.1923],
        [ 0.1088],
        [ 0.0628],
        [ 0.1648],
        [-0.0777],
        [-0.1453],
        [ 0.0642],
        [-0.3485],
        [-0.0927],
        [-0.2950],
        [ 0.2376],
        [-0.1156],
        [ 0.2563],
        [-0.0948]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3553],
        [-0.1312],
        [ 0.1228],
        [-0.2293],
        [-0.2198],
        [-0.0305],
        [-0.0792],
        [ 0.1199],
        [-0.2170],
        [ 0.3094],
        [ 0.2175],
        [-0.2326],
        [ 0.0849],
        [ 0.2053],
        [ 0.2475],
        [-0.3283],
        [ 0.2298],
        [ 0.2056],
        [ 0.1923],
        [ 0.1088],
        [ 0.0628],
        [ 0.1648],
        [-0.0777],
        [-0.1453],
        [ 0.0642],
        [-0.3485],
        [-0.0927],
        [-0.2950],
        [ 0.2376],
        [-0.1156],
        [ 0.2563],
        [-0.0948]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0385, -0.0638, -0.0002, -0.1232, -0.0333, -0.1443,  0.0673, -0.1416,
          0.0862, -0.0380, -0.0733, -0.1297, -0.0854,  0.1114, -0.0536, -0.0684,
          0.0767, -0.0111, -0.0521, -0.1165, -0.0087,  0.1156, -0.0338, -0.1338,
         -0.0991, -0.0019, -0.0338, -0.1238, -0.0998,  0.1456,  0.0990, -0.0566,
          0.1448,  0.1304,  0.1516, -0.1093,  0.1464,  0.0918, -0.0617, -0.1182,
          0.1234, -0.0150, -0.0090, -0.1071,  0.0900, -0.1290, -0.1276,  0.0444,
         -0.1081,  0.0116,  0.1042, -0.0296, -0.1251, -0.1398,  0.0157,  0.0772,
         -0.0484, -0.1159,  0.1252,  0.0286, -0.0331, -0.0372, -0.1061,  0.1270,
         -0.1487, -0.1295, -0.0695, -0.0943,  0.1194, -0.0363, -0.1064,  0.0357,
         -0.1158,  0.0494, -0.0376,  0.0376,  0.0840, -0.0236,  0.0420, -0.0080,
          0.1202,  0.0752,  0.0465, -0.0722, -0.0642, -0.0194,  0.0830,  0.0797,
         -0.0775, -0.1216,  0.1167, -0.1493, -0.1174,  0.0366, -0.1458,  0.0080,
         -0.0577, -0.1318,  0.0587,  0.0938, -0.0866,  0.1052,  0.0512, -0.1455,
         -0.0426,  0.0736, -0.1247,  0.0092,  0.0664, -0.0618,  0.1486,  0.0326,
          0.0696, -0.0519, -0.0503, -0.0245, -0.0629, -0.1239, -0.0714,  0.1274,
         -0.0523, -0.0145, -0.0971, -0.0416, -0.1085,  0.0338, -0.0043,  0.0798,
          0.0061,  0.1269, -0.0644, -0.1453,  0.0360,  0.0281, -0.0756,  0.0248,
         -0.0041,  0.1488, -0.1086, -0.1261, -0.1163,  0.1475,  0.1344,  0.0829,
          0.1409, -0.0279,  0.1048,  0.1077, -0.0936,  0.1203,  0.1268, -0.1336,
          0.1042,  0.0203,  0.0762, -0.1092, -0.1359,  0.0548,  0.0704, -0.0480,
          0.1287,  0.1152, -0.0745, -0.1313,  0.0225, -0.0495, -0.0664,  0.0784,
          0.1070,  0.1510, -0.0109, -0.0228,  0.1527,  0.0666, -0.1508, -0.0233,
         -0.0872, -0.0574,  0.1270,  0.0081,  0.0307,  0.0105,  0.0720,  0.1382,
         -0.1208,  0.1467, -0.1435,  0.0889,  0.0479, -0.0328, -0.0174,  0.0493,
         -0.0457,  0.0278,  0.0150,  0.0229, -0.0894, -0.0313, -0.1060,  0.1230,
         -0.1199,  0.1030, -0.0462,  0.0230, -0.1110, -0.0560,  0.0270,  0.0540,
          0.1064, -0.0162,  0.0281,  0.0172,  0.1273,  0.0918, -0.0329,  0.0048,
         -0.0348, -0.1372, -0.0362,  0.1161, -0.0067,  0.0086,  0.1335, -0.1462,
          0.0817, -0.0587, -0.0093,  0.0967, -0.0428,  0.0271, -0.1234,  0.1438,
          0.0166, -0.0012,  0.0044,  0.0849, -0.0279, -0.1435, -0.0395,  0.0653,
         -0.0410,  0.0628, -0.1355,  0.1208,  0.0660,  0.0492, -0.0866, -0.0873,
         -0.0505,  0.0727,  0.1254, -0.0420,  0.0757,  0.0907,  0.1156, -0.1273]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0385, -0.0638, -0.0002, -0.1232, -0.0333, -0.1443,  0.0673, -0.1416,
          0.0862, -0.0380, -0.0733, -0.1297, -0.0854,  0.1114, -0.0536, -0.0684,
          0.0767, -0.0111, -0.0521, -0.1165, -0.0087,  0.1156, -0.0338, -0.1338,
         -0.0991, -0.0019, -0.0338, -0.1238, -0.0998,  0.1456,  0.0990, -0.0566,
          0.1448,  0.1304,  0.1516, -0.1093,  0.1464,  0.0918, -0.0617, -0.1182,
          0.1234, -0.0150, -0.0090, -0.1071,  0.0900, -0.1290, -0.1276,  0.0444,
         -0.1081,  0.0116,  0.1042, -0.0296, -0.1251, -0.1398,  0.0157,  0.0772,
         -0.0484, -0.1159,  0.1252,  0.0286, -0.0331, -0.0372, -0.1061,  0.1270,
         -0.1487, -0.1295, -0.0695, -0.0943,  0.1194, -0.0363, -0.1064,  0.0357,
         -0.1158,  0.0494, -0.0376,  0.0376,  0.0840, -0.0236,  0.0420, -0.0080,
          0.1202,  0.0752,  0.0465, -0.0722, -0.0642, -0.0194,  0.0830,  0.0797,
         -0.0775, -0.1216,  0.1167, -0.1493, -0.1174,  0.0366, -0.1458,  0.0080,
         -0.0577, -0.1318,  0.0587,  0.0938, -0.0866,  0.1052,  0.0512, -0.1455,
         -0.0426,  0.0736, -0.1247,  0.0092,  0.0664, -0.0618,  0.1486,  0.0326,
          0.0696, -0.0519, -0.0503, -0.0245, -0.0629, -0.1239, -0.0714,  0.1274,
         -0.0523, -0.0145, -0.0971, -0.0416, -0.1085,  0.0338, -0.0043,  0.0798,
          0.0061,  0.1269, -0.0644, -0.1453,  0.0360,  0.0281, -0.0756,  0.0248,
         -0.0041,  0.1488, -0.1086, -0.1261, -0.1163,  0.1475,  0.1344,  0.0829,
          0.1409, -0.0279,  0.1048,  0.1077, -0.0936,  0.1203,  0.1268, -0.1336,
          0.1042,  0.0203,  0.0762, -0.1092, -0.1359,  0.0548,  0.0704, -0.0480,
          0.1287,  0.1152, -0.0745, -0.1313,  0.0225, -0.0495, -0.0664,  0.0784,
          0.1070,  0.1510, -0.0109, -0.0228,  0.1527,  0.0666, -0.1508, -0.0233,
         -0.0872, -0.0574,  0.1270,  0.0081,  0.0307,  0.0105,  0.0720,  0.1382,
         -0.1208,  0.1467, -0.1435,  0.0889,  0.0479, -0.0328, -0.0174,  0.0493,
         -0.0457,  0.0278,  0.0150,  0.0229, -0.0894, -0.0313, -0.1060,  0.1230,
         -0.1199,  0.1030, -0.0462,  0.0230, -0.1110, -0.0560,  0.0270,  0.0540,
          0.1064, -0.0162,  0.0281,  0.0172,  0.1273,  0.0918, -0.0329,  0.0048,
         -0.0348, -0.1372, -0.0362,  0.1161, -0.0067,  0.0086,  0.1335, -0.1462,
          0.0817, -0.0587, -0.0093,  0.0967, -0.0428,  0.0271, -0.1234,  0.1438,
          0.0166, -0.0012,  0.0044,  0.0849, -0.0279, -0.1435, -0.0395,  0.0653,
         -0.0410,  0.0628, -0.1355,  0.1208,  0.0660,  0.0492, -0.0866, -0.0873,
         -0.0505,  0.0727,  0.1254, -0.0420,  0.0757,  0.0907,  0.1156, -0.1273]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 4.9323e-02,  1.2120e-01,  1.0173e-01,  ..., -1.1049e-01,
         -7.7278e-02, -6.5084e-02],
        [ 1.0157e-02,  6.9875e-02,  1.2108e-01,  ..., -4.4490e-02,
          3.1376e-02,  4.1852e-02],
        [ 8.1383e-02,  9.0529e-02, -7.1398e-02,  ...,  1.0850e-02,
         -2.0533e-02,  8.3931e-02],
        ...,
        [ 1.3476e-02, -1.2431e-01,  1.8649e-02,  ...,  1.1694e-04,
         -8.7981e-02, -3.6579e-03],
        [ 6.2339e-02, -4.4324e-02,  3.5428e-02,  ...,  4.6646e-02,
         -2.8379e-02, -7.5290e-02],
        [ 1.7172e-02,  5.6961e-02,  3.6717e-02,  ...,  6.0527e-02,
          5.5530e-02, -5.1318e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 4.9323e-02,  1.2120e-01,  1.0173e-01,  ..., -1.1049e-01,
         -7.7278e-02, -6.5084e-02],
        [ 1.0157e-02,  6.9875e-02,  1.2108e-01,  ..., -4.4490e-02,
          3.1376e-02,  4.1852e-02],
        [ 8.1383e-02,  9.0529e-02, -7.1398e-02,  ...,  1.0850e-02,
         -2.0533e-02,  8.3931e-02],
        ...,
        [ 1.3476e-02, -1.2431e-01,  1.8649e-02,  ...,  1.1694e-04,
         -8.7981e-02, -3.6579e-03],
        [ 6.2339e-02, -4.4324e-02,  3.5428e-02,  ...,  4.6646e-02,
         -2.8379e-02, -7.5290e-02],
        [ 1.7172e-02,  5.6961e-02,  3.6717e-02,  ...,  6.0527e-02,
          5.5530e-02, -5.1318e-02]], device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0573, -0.0239,  0.0484,  ...,  0.1581, -0.0175,  0.0524],
        [-0.1137,  0.0993,  0.0115,  ..., -0.0494,  0.0477, -0.0928],
        [ 0.0189,  0.1339,  0.0024,  ...,  0.0105,  0.1358,  0.1408],
        ...,
        [-0.0843, -0.0015,  0.0587,  ..., -0.0196, -0.1477,  0.0829],
        [ 0.0945,  0.1494,  0.1225,  ..., -0.1152, -0.1236,  0.0092],
        [ 0.1584, -0.0047, -0.1688,  ..., -0.0406,  0.1366,  0.1586]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0573, -0.0239,  0.0484,  ...,  0.1581, -0.0175,  0.0524],
        [-0.1137,  0.0993,  0.0115,  ..., -0.0494,  0.0477, -0.0928],
        [ 0.0189,  0.1339,  0.0024,  ...,  0.0105,  0.1358,  0.1408],
        ...,
        [-0.0843, -0.0015,  0.0587,  ..., -0.0196, -0.1477,  0.0829],
        [ 0.0945,  0.1494,  0.1225,  ..., -0.1152, -0.1236,  0.0092],
        [ 0.1584, -0.0047, -0.1688,  ..., -0.0406,  0.1366,  0.1586]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0549,  0.0681, -0.1729,  ...,  0.1462, -0.0294, -0.0894],
        [-0.2043, -0.0167,  0.1144,  ...,  0.0234, -0.1967, -0.2070],
        [-0.0047, -0.1728,  0.0787,  ...,  0.1706,  0.1636, -0.1089],
        ...,
        [ 0.0025, -0.0056, -0.2283,  ...,  0.0448, -0.0107, -0.0347],
        [-0.1753,  0.1628, -0.1977,  ..., -0.1353, -0.0645,  0.0009],
        [ 0.0300,  0.1176, -0.1979,  ..., -0.0932,  0.0293,  0.1444]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0549,  0.0681, -0.1729,  ...,  0.1462, -0.0294, -0.0894],
        [-0.2043, -0.0167,  0.1144,  ...,  0.0234, -0.1967, -0.2070],
        [-0.0047, -0.1728,  0.0787,  ...,  0.1706,  0.1636, -0.1089],
        ...,
        [ 0.0025, -0.0056, -0.2283,  ...,  0.0448, -0.0107, -0.0347],
        [-0.1753,  0.1628, -0.1977,  ..., -0.1353, -0.0645,  0.0009],
        [ 0.0300,  0.1176, -0.1979,  ..., -0.0932,  0.0293,  0.1444]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.1418],
        [-0.3649],
        [ 0.1900],
        [-0.3202],
        [-0.2210],
        [-0.2774],
        [-0.3764],
        [-0.3594],
        [-0.0628],
        [ 0.2623],
        [-0.2231],
        [ 0.0816],
        [-0.1134],
        [ 0.1475],
        [-0.1602],
        [-0.3087],
        [-0.3178],
        [-0.0882],
        [-0.1201],
        [ 0.1720],
        [ 0.2212],
        [-0.3818],
        [ 0.1531],
        [-0.0015],
        [-0.0542],
        [-0.3921],
        [-0.4175],
        [-0.3575],
        [-0.1049],
        [ 0.0301],
        [ 0.0924],
        [-0.2606]], device='cuda:0') 
 Parameter containing:
tensor([[-0.1418],
        [-0.3649],
        [ 0.1900],
        [-0.3202],
        [-0.2210],
        [-0.2774],
        [-0.3764],
        [-0.3594],
        [-0.0628],
        [ 0.2623],
        [-0.2231],
        [ 0.0816],
        [-0.1134],
        [ 0.1475],
        [-0.1602],
        [-0.3087],
        [-0.3178],
        [-0.0882],
        [-0.1201],
        [ 0.1720],
        [ 0.2212],
        [-0.3818],
        [ 0.1531],
        [-0.0015],
        [-0.0542],
        [-0.3921],
        [-0.4175],
        [-0.3575],
        [-0.1049],
        [ 0.0301],
        [ 0.0924],
        [-0.2606]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-51.7052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.8336, device='cuda:0')



h[100].sum tensor(0.2241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.2300, device='cuda:0')



h[200].sum tensor(1.3233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(1.3582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2734.0876, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0004, 0.0025, 0.0014],
        [0.0000, 0.0000, 0.0010,  ..., 0.0019, 0.0132, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(10684.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-14.4955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(685.8567, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(54.8688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-9.0285, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0480],
        [0.0587],
        [0.0847],
        ...,
        [0.0135],
        [0.0136],
        [0.0107]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1210.4941, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0480],
        [0.0587],
        [0.0847],
        ...,
        [0.0135],
        [0.0136],
        [0.0107]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(76.8457, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8142, device='cuda:0')



h[100].sum tensor(5.1762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1343, device='cuda:0')



h[200].sum tensor(-16.2498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(14525.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0026, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0039, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(66236.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1525.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(107.3822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1188.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.6564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(584.5383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.1395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0376],
        [0.0231],
        [0.0141],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(2206.8325, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0480],
        [0.0587],
        [0.0847],
        ...,
        [0.0135],
        [0.0136],
        [0.0107]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



load_model False 
TraEvN 1998 
BatchSize 5 
EpochNum 30 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0895,  0.0966, -0.0186,  0.0746,  0.0075,  0.1079,  0.0526,  0.1097,
         -0.0756,  0.0048,  0.0024,  0.0084, -0.1325, -0.0683, -0.0675, -0.0685,
          0.0187,  0.1253, -0.0810, -0.0629,  0.0173, -0.0513, -0.1068, -0.1440,
          0.0223,  0.1488,  0.0638, -0.1434,  0.0715, -0.0985,  0.0977, -0.0667,
          0.0243,  0.1225,  0.0293, -0.0455, -0.1341,  0.1123, -0.0291,  0.1148,
         -0.0959, -0.1253,  0.0794,  0.1505,  0.0763,  0.0732,  0.1275, -0.0743,
          0.0870, -0.1514, -0.0839,  0.0636,  0.0454,  0.0665,  0.0830,  0.1147,
         -0.0698,  0.0455,  0.1035,  0.0752, -0.0963, -0.0526, -0.0493,  0.0714,
          0.0214,  0.0565, -0.0329, -0.1269, -0.0376,  0.1322, -0.1066, -0.0556,
          0.0370,  0.1292,  0.0696, -0.0242,  0.0385,  0.0002,  0.0182,  0.0034,
          0.0771, -0.0874, -0.0890, -0.0216,  0.1092, -0.0725,  0.0858,  0.0739,
         -0.0556, -0.0386,  0.0388, -0.1274,  0.0724, -0.1230, -0.0011,  0.1131,
          0.0364, -0.0005,  0.0220, -0.0635,  0.0362,  0.1228, -0.0447,  0.0158,
         -0.1390, -0.0609,  0.0149,  0.0607, -0.0394, -0.1356, -0.0743, -0.0641,
          0.0634,  0.1479, -0.0038, -0.0421, -0.1133, -0.0080,  0.1029, -0.0105,
         -0.0648,  0.1464,  0.0201, -0.0632, -0.0947, -0.0378,  0.0073, -0.0686,
         -0.0655,  0.0755,  0.0561, -0.1273,  0.0773, -0.1199, -0.1168, -0.0057,
         -0.0415, -0.0780,  0.0527, -0.1269,  0.1099,  0.0091, -0.0280,  0.1412,
         -0.1380, -0.1056, -0.0018,  0.0122,  0.1486, -0.1437, -0.1423,  0.0749,
          0.1250,  0.0210,  0.0356, -0.0532,  0.1210,  0.1426, -0.0820, -0.0618,
         -0.1500, -0.1107,  0.1186, -0.0204,  0.0633,  0.0262,  0.0900, -0.0953,
          0.0217,  0.0427, -0.0874,  0.1526, -0.0125,  0.1370,  0.0681, -0.0081,
          0.0213, -0.1223, -0.1446, -0.0335, -0.1393,  0.0766,  0.0358, -0.0877,
         -0.1247,  0.0426, -0.1297, -0.0200, -0.1418, -0.0057, -0.1507, -0.1384,
          0.1031,  0.0944, -0.0159,  0.0521, -0.0836,  0.0433, -0.0692,  0.1271,
         -0.0320, -0.0097,  0.1294,  0.0650,  0.0054, -0.0135,  0.1013, -0.0120,
          0.0666,  0.1513,  0.1124, -0.1516,  0.0719,  0.1221,  0.1094,  0.0813,
          0.1013,  0.0238, -0.0341,  0.1175,  0.1256, -0.0503, -0.0203, -0.0310,
         -0.0043, -0.0500,  0.1116, -0.0582,  0.0915,  0.1353, -0.0018,  0.1351,
          0.1063,  0.0474, -0.0253,  0.1058,  0.1468, -0.0654,  0.0834,  0.1131,
          0.1493, -0.1354,  0.0984,  0.1235,  0.0343, -0.0599, -0.0425, -0.1095,
         -0.1326,  0.0647,  0.0678, -0.0223, -0.1305,  0.1274, -0.0726, -0.0866]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0456,  0.0132, -0.0455,  ..., -0.0125,  0.0609, -0.0648],
        [ 0.0562,  0.0711, -0.0886,  ..., -0.0304, -0.0360, -0.1156],
        [ 0.0762, -0.0588, -0.0238,  ...,  0.0624,  0.0201, -0.0932],
        ...,
        [-0.0543,  0.0287, -0.0116,  ..., -0.0768, -0.0391,  0.0662],
        [-0.0638, -0.0604,  0.0052,  ..., -0.0078,  0.0322, -0.0273],
        [ 0.0724,  0.0573,  0.0294,  ..., -0.0488,  0.0002,  0.1101]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0635, -0.0953,  0.1246,  ...,  0.0675,  0.0356, -0.1767],
        [ 0.0976,  0.1130,  0.1589,  ..., -0.1288, -0.0948,  0.0634],
        [ 0.1553, -0.1572, -0.0077,  ...,  0.1502, -0.0292,  0.0883],
        ...,
        [ 0.1231,  0.1054,  0.0722,  ..., -0.0332,  0.1260, -0.0099],
        [ 0.0063,  0.1320,  0.0115,  ...,  0.1114,  0.0520, -0.0791],
        [-0.1518, -0.1414, -0.0926,  ...,  0.0983,  0.0959, -0.0497]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2254, -0.2296,  0.1891,  ..., -0.0142, -0.1704, -0.1284],
        [-0.2083, -0.1646, -0.2078,  ...,  0.0079, -0.1173, -0.2275],
        [-0.2202, -0.1641,  0.0574,  ..., -0.1490,  0.2045, -0.1142],
        ...,
        [-0.1036,  0.1843,  0.1176,  ...,  0.0684,  0.1829, -0.0767],
        [ 0.1148,  0.1342,  0.0526,  ...,  0.1889, -0.1482,  0.0212],
        [ 0.1435,  0.1961, -0.2456,  ...,  0.1659,  0.0054, -0.2125]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1171],
        [ 0.0385],
        [ 0.0284],
        [-0.2308],
        [ 0.2981],
        [-0.0378],
        [-0.2559],
        [-0.4134],
        [-0.2591],
        [ 0.2220],
        [-0.1318],
        [-0.2497],
        [ 0.1364],
        [ 0.2929],
        [ 0.1590],
        [ 0.0912],
        [-0.4078],
        [ 0.1329],
        [ 0.3728],
        [ 0.3205],
        [ 0.3816],
        [-0.3828],
        [ 0.0427],
        [-0.3798],
        [-0.1639],
        [-0.1787],
        [ 0.2406],
        [ 0.2871],
        [-0.1412],
        [-0.0464],
        [-0.2263],
        [-0.2698]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0895,  0.0966, -0.0186,  0.0746,  0.0075,  0.1079,  0.0526,  0.1097,
         -0.0756,  0.0048,  0.0024,  0.0084, -0.1325, -0.0683, -0.0675, -0.0685,
          0.0187,  0.1253, -0.0810, -0.0629,  0.0173, -0.0513, -0.1068, -0.1440,
          0.0223,  0.1488,  0.0638, -0.1434,  0.0715, -0.0985,  0.0977, -0.0667,
          0.0243,  0.1225,  0.0293, -0.0455, -0.1341,  0.1123, -0.0291,  0.1148,
         -0.0959, -0.1253,  0.0794,  0.1505,  0.0763,  0.0732,  0.1275, -0.0743,
          0.0870, -0.1514, -0.0839,  0.0636,  0.0454,  0.0665,  0.0830,  0.1147,
         -0.0698,  0.0455,  0.1035,  0.0752, -0.0963, -0.0526, -0.0493,  0.0714,
          0.0214,  0.0565, -0.0329, -0.1269, -0.0376,  0.1322, -0.1066, -0.0556,
          0.0370,  0.1292,  0.0696, -0.0242,  0.0385,  0.0002,  0.0182,  0.0034,
          0.0771, -0.0874, -0.0890, -0.0216,  0.1092, -0.0725,  0.0858,  0.0739,
         -0.0556, -0.0386,  0.0388, -0.1274,  0.0724, -0.1230, -0.0011,  0.1131,
          0.0364, -0.0005,  0.0220, -0.0635,  0.0362,  0.1228, -0.0447,  0.0158,
         -0.1390, -0.0609,  0.0149,  0.0607, -0.0394, -0.1356, -0.0743, -0.0641,
          0.0634,  0.1479, -0.0038, -0.0421, -0.1133, -0.0080,  0.1029, -0.0105,
         -0.0648,  0.1464,  0.0201, -0.0632, -0.0947, -0.0378,  0.0073, -0.0686,
         -0.0655,  0.0755,  0.0561, -0.1273,  0.0773, -0.1199, -0.1168, -0.0057,
         -0.0415, -0.0780,  0.0527, -0.1269,  0.1099,  0.0091, -0.0280,  0.1412,
         -0.1380, -0.1056, -0.0018,  0.0122,  0.1486, -0.1437, -0.1423,  0.0749,
          0.1250,  0.0210,  0.0356, -0.0532,  0.1210,  0.1426, -0.0820, -0.0618,
         -0.1500, -0.1107,  0.1186, -0.0204,  0.0633,  0.0262,  0.0900, -0.0953,
          0.0217,  0.0427, -0.0874,  0.1526, -0.0125,  0.1370,  0.0681, -0.0081,
          0.0213, -0.1223, -0.1446, -0.0335, -0.1393,  0.0766,  0.0358, -0.0877,
         -0.1247,  0.0426, -0.1297, -0.0200, -0.1418, -0.0057, -0.1507, -0.1384,
          0.1031,  0.0944, -0.0159,  0.0521, -0.0836,  0.0433, -0.0692,  0.1271,
         -0.0320, -0.0097,  0.1294,  0.0650,  0.0054, -0.0135,  0.1013, -0.0120,
          0.0666,  0.1513,  0.1124, -0.1516,  0.0719,  0.1221,  0.1094,  0.0813,
          0.1013,  0.0238, -0.0341,  0.1175,  0.1256, -0.0503, -0.0203, -0.0310,
         -0.0043, -0.0500,  0.1116, -0.0582,  0.0915,  0.1353, -0.0018,  0.1351,
          0.1063,  0.0474, -0.0253,  0.1058,  0.1468, -0.0654,  0.0834,  0.1131,
          0.1493, -0.1354,  0.0984,  0.1235,  0.0343, -0.0599, -0.0425, -0.1095,
         -0.1326,  0.0647,  0.0678, -0.0223, -0.1305,  0.1274, -0.0726, -0.0866]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0456,  0.0132, -0.0455,  ..., -0.0125,  0.0609, -0.0648],
        [ 0.0562,  0.0711, -0.0886,  ..., -0.0304, -0.0360, -0.1156],
        [ 0.0762, -0.0588, -0.0238,  ...,  0.0624,  0.0201, -0.0932],
        ...,
        [-0.0543,  0.0287, -0.0116,  ..., -0.0768, -0.0391,  0.0662],
        [-0.0638, -0.0604,  0.0052,  ..., -0.0078,  0.0322, -0.0273],
        [ 0.0724,  0.0573,  0.0294,  ..., -0.0488,  0.0002,  0.1101]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0635, -0.0953,  0.1246,  ...,  0.0675,  0.0356, -0.1767],
        [ 0.0976,  0.1130,  0.1589,  ..., -0.1288, -0.0948,  0.0634],
        [ 0.1553, -0.1572, -0.0077,  ...,  0.1502, -0.0292,  0.0883],
        ...,
        [ 0.1231,  0.1054,  0.0722,  ..., -0.0332,  0.1260, -0.0099],
        [ 0.0063,  0.1320,  0.0115,  ...,  0.1114,  0.0520, -0.0791],
        [-0.1518, -0.1414, -0.0926,  ...,  0.0983,  0.0959, -0.0497]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2254, -0.2296,  0.1891,  ..., -0.0142, -0.1704, -0.1284],
        [-0.2083, -0.1646, -0.2078,  ...,  0.0079, -0.1173, -0.2275],
        [-0.2202, -0.1641,  0.0574,  ..., -0.1490,  0.2045, -0.1142],
        ...,
        [-0.1036,  0.1843,  0.1176,  ...,  0.0684,  0.1829, -0.0767],
        [ 0.1148,  0.1342,  0.0526,  ...,  0.1889, -0.1482,  0.0212],
        [ 0.1435,  0.1961, -0.2456,  ...,  0.1659,  0.0054, -0.2125]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1171],
        [ 0.0385],
        [ 0.0284],
        [-0.2308],
        [ 0.2981],
        [-0.0378],
        [-0.2559],
        [-0.4134],
        [-0.2591],
        [ 0.2220],
        [-0.1318],
        [-0.2497],
        [ 0.1364],
        [ 0.2929],
        [ 0.1590],
        [ 0.0912],
        [-0.4078],
        [ 0.1329],
        [ 0.3728],
        [ 0.3205],
        [ 0.3816],
        [-0.3828],
        [ 0.0427],
        [-0.3798],
        [-0.1639],
        [-0.1787],
        [ 0.2406],
        [ 0.2871],
        [-0.1412],
        [-0.0464],
        [-0.2263],
        [-0.2698]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0131,  0.0142, -0.0027,  ...,  0.0187, -0.0107, -0.0127],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(380.6413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.3237, device='cuda:0')



h[100].sum tensor(10.6025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6420, device='cuda:0')



h[200].sum tensor(-9.3687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.5479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0496, 0.0536, 0.0000,  ..., 0.0707, 0.0000, 0.0000],
        [0.0408, 0.0441, 0.0000,  ..., 0.0581, 0.0000, 0.0000],
        [0.0096, 0.0103, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31450.4043, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191503.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.8522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3263.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-63.2701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(577.8528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.7952, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3106e+00],
        [-2.5001e+00],
        [-2.7589e+00],
        ...,
        [-2.7765e-06],
        [-3.6481e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43063.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(91.5149, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(337.2573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.8155, device='cuda:0')



h[100].sum tensor(6.3547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7098, device='cuda:0')



h[200].sum tensor(-8.6148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.6213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31870.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.1467e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1467e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.2437e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.1376e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1376e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1376e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199115.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.8842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4364.7080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-68.2785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(489.4128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.8306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0119],
        [ 0.0013],
        [-0.0206],
        ...,
        [ 0.0068],
        [ 0.0068],
        [ 0.0068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23746.8145, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365889.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00],
        [-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00],
        [-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00],
        [-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00],
        [-3.3753e-05, -1.8145e-04,  0.0000e+00,  ...,  8.3646e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(158.2508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4775, device='cuda:0')



h[100].sum tensor(0.7406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6108, device='cuda:0')



h[200].sum tensor(-6.1053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0440, 0.0470, 0.0000,  ..., 0.0633, 0.0000, 0.0000],
        [0.0292, 0.0312, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0242, 0.0258, 0.0000,  ..., 0.0349, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25426.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163181.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.7851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4236.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-54.0763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(407.3106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-130.4614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2189e-01],
        [-5.5550e-01],
        [-4.4493e-01],
        ...,
        [ 3.3478e-04],
        [ 3.3916e-04],
        [ 3.4080e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8439.1914, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365889.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365868.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9267e-03,  4.9788e-03, -9.9723e-04,  ...,  7.0020e-03,
         -3.9384e-03, -4.7017e-03],
        [ 1.1876e-02,  1.2452e-02, -2.4175e-03,  ...,  1.6907e-02,
         -9.5474e-03, -1.1398e-02],
        [ 8.7024e-03,  9.0393e-03, -1.7689e-03,  ...,  1.2384e-02,
         -6.9859e-03, -8.3400e-03],
        ...,
        [ 4.7460e-05, -2.6852e-04,  0.0000e+00,  ...,  4.7062e-05,
          0.0000e+00,  0.0000e+00],
        [ 4.7460e-05, -2.6852e-04,  0.0000e+00,  ...,  4.7062e-05,
          0.0000e+00,  0.0000e+00],
        [ 4.7460e-05, -2.6852e-04,  0.0000e+00,  ...,  4.7062e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108.4346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.5120, device='cuda:0')



h[100].sum tensor(-0.7832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3670, device='cuda:0')



h[200].sum tensor(-6.6825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0424, 0.0443, 0.0000,  ..., 0.0604, 0.0000, 0.0000],
        [0.0274, 0.0282, 0.0000,  ..., 0.0390, 0.0000, 0.0000],
        [0.0231, 0.0238, 0.0000,  ..., 0.0328, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26951.0566, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-24.3316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4679.2568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-58.1230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(531.1412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.6454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4308],
        [-0.4003],
        [-0.3388],
        ...,
        [-0.0033],
        [-0.0033],
        [-0.0032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-408.3306, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365868.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365850.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3490e-04, -3.4288e-04,  0.0000e+00,  ..., -1.0418e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(18.8642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.9614, device='cuda:0')



h[100].sum tensor(-3.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7907, device='cuda:0')



h[200].sum tensor(-6.1807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.4574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25828.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152119.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(10.4399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4973.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-53.9103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(360.6031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-97.8773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0398],
        [ 0.0420],
        [ 0.0383],
        ...,
        [-0.0036],
        [-0.0070],
        [-0.0080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(1567.7792, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365850.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365836.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5354e-03,  8.5137e-03, -1.6791e-03,  ...,  1.1787e-02,
         -6.6850e-03, -7.9843e-03],
        [ 2.1496e-04, -4.0516e-04,  0.0000e+00,  ..., -6.0695e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1496e-04, -4.0516e-04,  0.0000e+00,  ..., -6.0695e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1496e-04, -4.0516e-04,  0.0000e+00,  ..., -6.0695e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1496e-04, -4.0516e-04,  0.0000e+00,  ..., -6.0695e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1496e-04, -4.0516e-04,  0.0000e+00,  ..., -6.0695e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.7951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.0373, device='cuda:0')



h[100].sum tensor(-2.5928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1639, device='cuda:0')



h[200].sum tensor(-8.9553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.0470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0319, 0.0325, 0.0000,  ..., 0.0441, 0.0000, 0.0000],
        [0.0092, 0.0085, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38166.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239175.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7225.9263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-73.8382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(538.5483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.1245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0160],
        [ 0.0433],
        [ 0.0418],
        ...,
        [-0.0169],
        [-0.0168],
        [-0.0168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(40.5538, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365836.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365825.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0003, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-24.0541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.3477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.5091, device='cuda:0')



h[100].sum tensor(-5.4542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8526, device='cuda:0')



h[200].sum tensor(-7.9436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35141.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214864.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.4186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7193.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-63.1142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(404.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-120.7386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0649],
        [ 0.0457],
        [ 0.0184],
        ...,
        [-0.0238],
        [-0.0237],
        [-0.0236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-7715.1143, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365825.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365817.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0157,  0.0159, -0.0031,  ...,  0.0217, -0.0123, -0.0147],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-149.9927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.8506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.0457, device='cuda:0')



h[100].sum tensor(-8.3088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8220, device='cuda:0')



h[200].sum tensor(-6.1962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.5557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0284, 0.0000,  ..., 0.0389, 0.0000, 0.0000],
        [0.0167, 0.0159, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29546.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0045,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0045,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0045,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181817.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27.4828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.6703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7168.0166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(180.2218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-76.3487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0447],
        [ 0.0892],
        [ 0.1126],
        ...,
        [-0.0324],
        [-0.0322],
        [-0.0321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-5484.9014, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365817.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365812.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0004, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-244.8636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3674, device='cuda:0')



h[100].sum tensor(-9.9094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4549, device='cuda:0')



h[200].sum tensor(-5.0379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.2640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0235, 0.0222, 0.0000,  ..., 0.0306, 0.0000, 0.0000],
        [0.0126, 0.0111, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0207, 0.0197, 0.0000,  ..., 0.0267, 0.0000, 0.0000],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26345.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168623.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(137.1624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(103.9799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7166.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.3530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(124.0443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-45.1016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1045],
        [ 0.1187],
        [ 0.1321],
        ...,
        [-0.0421],
        [-0.0419],
        [-0.0419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-13568.9902, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365812.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9995],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365810.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005, -0.0006,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0066,  0.0060, -0.0012,  ...,  0.0085, -0.0049, -0.0058],
        [ 0.0005, -0.0006,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0005, -0.0006,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0005, -0.0006,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0005, -0.0006,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-304.3428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.1410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5926, device='cuda:0')



h[100].sum tensor(-11.0712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7953, device='cuda:0')



h[200].sum tensor(-4.5526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.1932, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0060, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0106, 0.0087, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        [0.0315, 0.0293, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25677.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0001, 0.0057,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0001, 0.0057,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0001, 0.0057,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169442.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(247.6292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.9726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7503.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.6542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(63.8541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-30.8776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0677],
        [ 0.0970],
        [ 0.1239],
        ...,
        [-0.0524],
        [-0.0521],
        [-0.0518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19787.5645, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9995],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365810.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365811.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0006, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0208,  0.0210, -0.0040,  ...,  0.0285, -0.0161, -0.0192],
        [ 0.0211,  0.0213, -0.0040,  ...,  0.0289, -0.0163, -0.0195],
        ...,
        [ 0.0006, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0006, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0006, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-233.0239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.6855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.4681, device='cuda:0')



h[100].sum tensor(-8.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3507, device='cuda:0')



h[200].sum tensor(-6.5573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0272, 0.0000,  ..., 0.0373, 0.0000, 0.0000],
        [0.0393, 0.0378, 0.0000,  ..., 0.0519, 0.0000, 0.0000],
        [0.0798, 0.0803, 0.0000,  ..., 0.1092, 0.0000, 0.0000],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32428.6816, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0005, 0.0066,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0005, 0.0066,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0005, 0.0066,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207213.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(377.0789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.8576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8565.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(183.1404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-60.1611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2513],
        [ 0.2605],
        [ 0.2664],
        ...,
        [-0.0617],
        [-0.0614],
        [-0.0613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20969.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365811.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(598.6238, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365813.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0078,  0.0071, -0.0014,  ...,  0.0100, -0.0057, -0.0068],
        [ 0.0093,  0.0087, -0.0017,  ...,  0.0121, -0.0069, -0.0082],
        [ 0.0149,  0.0147, -0.0028,  ...,  0.0200, -0.0113, -0.0135],
        ...,
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-120.5764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(49.4368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.5766, device='cuda:0')



h[100].sum tensor(-3.5280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7360, device='cuda:0')



h[200].sum tensor(-9.2300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0399, 0.0376, 0.0000,  ..., 0.0522, 0.0000, 0.0000],
        [0.0428, 0.0407, 0.0000,  ..., 0.0562, 0.0000, 0.0000],
        [0.0304, 0.0275, 0.0000,  ..., 0.0387, 0.0000, 0.0000],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40889.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0007, 0.0075,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0007, 0.0075,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0007, 0.0075,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(248662.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(454.9744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(95.3857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9652.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-68.9175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(351.2564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-100.9829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2086],
        [ 0.2372],
        [ 0.2657],
        ...,
        [-0.0700],
        [-0.0697],
        [-0.0696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21231.8711, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365813.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365815.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0007, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32.8688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(62.4851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-43.9856, device='cuda:0')



h[100].sum tensor(2.0542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3480, device='cuda:0')



h[200].sum tensor(-12.6727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-51.3216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0129, 0.0101, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        ...,
        [0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52368.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0007, 0.0063,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0005, 0.0041,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0010, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0010, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0010, 0.0081,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318716.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(523.6835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(63.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11241.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-95.4806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(679.1731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.3074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0367],
        [ 0.0068],
        [ 0.0605],
        ...,
        [-0.0787],
        [-0.0784],
        [-0.0782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19748.9629, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365815.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365816.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0212,  0.0211, -0.0039,  ...,  0.0287, -0.0161, -0.0192],
        [ 0.0304,  0.0309, -0.0057,  ...,  0.0418, -0.0234, -0.0280],
        [ 0.0220,  0.0220, -0.0041,  ...,  0.0298, -0.0167, -0.0200],
        ...,
        [ 0.0008, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0008, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0008, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-136.0242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(55.3298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.1724, device='cuda:0')



h[100].sum tensor(-0.2338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9574, device='cuda:0')



h[200].sum tensor(-9.2284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.5382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0945, 0.0949, 0.0000,  ..., 0.1284, 0.0000, 0.0000],
        [0.1098, 0.1111, 0.0000,  ..., 0.1501, 0.0000, 0.0000],
        [0.0916, 0.0918, 0.0000,  ..., 0.1243, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42856.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0013, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0043, 0.0013, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0043, 0.0013, 0.0082,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(270117.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(566.1451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.2530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10168.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-84.1327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(483.9937, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-110.4168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2178],
        [ 0.2401],
        [ 0.2571],
        ...,
        [-0.0872],
        [-0.0868],
        [-0.0867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27693.6621, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365816.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365817.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0009, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-157.9547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(57.3251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.5116, device='cuda:0')



h[100].sum tensor(1.4032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3402, device='cuda:0')



h[200].sum tensor(-8.8699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.6003, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42744.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0016, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0046, 0.0016, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0046, 0.0016, 0.0083,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0016, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0045, 0.0016, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0045, 0.0016, 0.0082,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(279491.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(581.2257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(95.2808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10254.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-92.4853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(549.2308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.7178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1124],
        [-0.1029],
        [-0.0848],
        ...,
        [-0.0686],
        [-0.0859],
        [-0.0916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31281.6602, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365817.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365818.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100,  0.0090, -0.0017,  ...,  0.0125, -0.0070, -0.0084],
        [ 0.0010, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0162,  0.0156, -0.0029,  ...,  0.0213, -0.0120, -0.0144],
        ...,
        [ 0.0010, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0010, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0010, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-207.1221, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(57.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.6853, device='cuda:0')



h[100].sum tensor(2.2310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2897, device='cuda:0')



h[200].sum tensor(-7.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0072, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0428, 0.0390, 0.0000,  ..., 0.0540, 0.0000, 0.0000],
        [0.0257, 0.0214, 0.0000,  ..., 0.0300, 0.0000, 0.0000],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40039.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0019, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0019, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0019, 0.0081,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260852.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(505.8153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.4735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9910.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-97.6335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(413.3920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.6912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0168],
        [ 0.0506],
        [ 0.0709],
        ...,
        [-0.1004],
        [-0.1000],
        [-0.0998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31412.0996, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365818.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365820.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090,  0.0079, -0.0015,  ...,  0.0110, -0.0062, -0.0075],
        [ 0.0011, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0090,  0.0079, -0.0015,  ...,  0.0110, -0.0062, -0.0075],
        ...,
        [ 0.0011, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0011, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0011, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-296.2220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(55.1387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3451, device='cuda:0')



h[100].sum tensor(2.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9333, device='cuda:0')



h[200].sum tensor(-6.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0063, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0332, 0.0284, 0.0000,  ..., 0.0399, 0.0000, 0.0000],
        [0.0108, 0.0063, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33023.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0022, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0022, 0.0081,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0022, 0.0081,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224089.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(621.5669, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(116.3528, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8677.3057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-90.5556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(329.6213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-73.6494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0805],
        [-0.0696],
        [-0.0696],
        ...,
        [-0.1059],
        [-0.1054],
        [-0.1052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43577.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365820.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365823.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0184,  0.0177, -0.0032,  ...,  0.0241, -0.0135, -0.0162],
        [ 0.0171,  0.0163, -0.0030,  ...,  0.0222, -0.0125, -0.0149],
        [ 0.0257,  0.0254, -0.0046,  ...,  0.0344, -0.0192, -0.0230],
        ...,
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-231.4435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(61.6786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.6527, device='cuda:0')



h[100].sum tensor(5.6994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5343, device='cuda:0')



h[200].sum tensor(-7.2616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.9311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0732, 0.0704, 0.0000,  ..., 0.0958, 0.0000, 0.0000],
        [0.0936, 0.0918, 0.0000,  ..., 0.1245, 0.0000, 0.0000],
        [0.1119, 0.1112, 0.0000,  ..., 0.1504, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36149.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236768.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(610.5857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.1326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8887.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-105.5302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(403.2894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-93.6746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0065],
        [-0.0052],
        [-0.0149],
        ...,
        [-0.1097],
        [-0.1092],
        [-0.1091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42379.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365823.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365823.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0059, -0.0011,  ...,  0.0083, -0.0048, -0.0057],
        [ 0.0068,  0.0055, -0.0011,  ...,  0.0078, -0.0044, -0.0053],
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0012, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-24.4205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.1763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-40.0575, device='cuda:0')



h[100].sum tensor(10.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8880, device='cuda:0')



h[200].sum tensor(-11.4536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.7383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0516, 0.0475, 0.0000,  ..., 0.0653, 0.0000, 0.0000],
        [0.0154, 0.0103, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0279, 0.0229, 0.0000,  ..., 0.0321, 0.0000, 0.0000],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53150.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0053, 0.0027, 0.0082,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346989.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(537.5275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(44.9150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11372.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-141.4691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(990.4757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.3224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0581],
        [ 0.0600],
        [ 0.0652],
        ...,
        [-0.0943],
        [-0.0938],
        [-0.0965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-34325.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365823.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-156.9863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(68.7018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.1563, device='cuda:0')



h[100].sum tensor(9.5684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2081, device='cuda:0')



h[200].sum tensor(-8.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.1859, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41507.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0034, 0.0086,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(273508.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(568.7032, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.6803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9875.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-127.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(559.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.4171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1558],
        [-0.1596],
        [-0.1558],
        ...,
        [-0.1131],
        [-0.1094],
        [-0.1018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33047.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365827.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365831.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0075, -0.0014,  ...,  0.0105, -0.0059, -0.0071],
        [ 0.0093,  0.0079, -0.0015,  ...,  0.0110, -0.0062, -0.0074],
        [ 0.0089,  0.0075, -0.0014,  ...,  0.0105, -0.0059, -0.0071],
        ...,
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0013, -0.0005,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-74.5015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.9701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.5901, device='cuda:0')



h[100].sum tensor(13.6532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2277, device='cuda:0')



h[200].sum tensor(-10.0358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.5258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0404, 0.0351, 0.0000,  ..., 0.0486, 0.0000, 0.0000],
        [0.0394, 0.0340, 0.0000,  ..., 0.0472, 0.0000, 0.0000],
        [0.0195, 0.0140, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48331.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0061, 0.0041, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0041, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0041, 0.0091,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(320654.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(637.0094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.7936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10638.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-149.1639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(912.7081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.6896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0315],
        [ 0.0388],
        [ 0.0439],
        ...,
        [-0.1129],
        [-0.1117],
        [-0.1121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36386.2305, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9994],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365831.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(536.2128, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365836.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0056, -0.0011,  ...,  0.0079, -0.0045, -0.0054],
        [ 0.0014, -0.0004,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0014, -0.0004,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0014, -0.0004,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0014, -0.0004,  0.0000,  ..., -0.0002,  0.0000,  0.0000],
        [ 0.0014, -0.0004,  0.0000,  ..., -0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-224.2028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(69.1644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.7417, device='cuda:0')



h[100].sum tensor(11.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1957, device='cuda:0')



h[200].sum tensor(-6.9056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.8682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0278, 0.0224, 0.0000,  ..., 0.0308, 0.0000, 0.0000],
        [0.0114, 0.0056, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0057, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36081.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0028, 0.0002, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0052, 0.0025, 0.0048,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0065, 0.0050, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0050, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0050, 0.0096,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244982.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(754.7123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.9247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8889.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-129.5469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(490.7015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-109.9921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0108],
        [ 0.0009],
        [-0.0204],
        ...,
        [-0.1194],
        [-0.1188],
        [-0.1187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39393.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9997],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365836.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365843.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0004,  0.0000,  ..., -0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-269.5795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(68.3789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.1210, device='cuda:0')



h[100].sum tensor(11.6190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8500, device='cuda:0')



h[200].sum tensor(-5.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.6436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0198, 0.0142, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        ...,
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32378.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0032, 0.0057,  ..., 0.0000, 0.0000, 0.0000],
        [0.0060, 0.0025, 0.0044,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0058, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0058, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0058, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223591.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(895.4315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.4016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8190.6396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-125.9777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(403.7943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-97.2801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0012],
        [-0.0087],
        [-0.0139],
        ...,
        [-0.1212],
        [-0.1207],
        [-0.1205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46545.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365843.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365850.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000],
        [ 0.0015, -0.0003,  0.0000,  ..., -0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-127.2707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.3427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.6991, device='cuda:0')



h[100].sum tensor(14.6705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4098, device='cuda:0')



h[200].sum tensor(-8.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.8191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43748.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0022, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0039, 0.0067,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0055, 0.0090,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0065, 0.0104,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0065, 0.0104,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0065, 0.0104,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296389.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(953.0277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(69.7154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9957.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-159.2433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(780.3134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.3657, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0176],
        [-0.0208],
        [-0.0102],
        ...,
        [-0.1226],
        [-0.1220],
        [-0.1219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31325.9590, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365850.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365857.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5122e-03,  4.9255e-03, -8.9138e-04,  ...,  6.8666e-03,
         -3.8335e-03, -4.5975e-03],
        [ 7.8619e-03,  6.3442e-03, -1.1346e-03,  ...,  8.7621e-03,
         -4.8796e-03, -5.8520e-03],
        [ 1.5661e-03, -2.7366e-04,  0.0000e+00,  ..., -7.9528e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.6860e-03,  8.2616e-03, -1.4634e-03,  ...,  1.1324e-02,
         -6.2934e-03, -7.5476e-03],
        [ 1.5661e-03, -2.7366e-04,  0.0000e+00,  ..., -7.9528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5661e-03, -2.7366e-04,  0.0000e+00,  ..., -7.9528e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-186.6147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.6090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.5696, device='cuda:0')



h[100].sum tensor(12.3641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2467, device='cuda:0')



h[200].sum tensor(-7.6714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0531, 0.0481, 0.0000,  ..., 0.0654, 0.0000, 0.0000],
        [0.0250, 0.0192, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0126, 0.0063, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        ...,
        [0.0651, 0.0610, 0.0000,  ..., 0.0823, 0.0000, 0.0000],
        [0.0583, 0.0539, 0.0000,  ..., 0.0729, 0.0000, 0.0000],
        [0.0436, 0.0387, 0.0000,  ..., 0.0523, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38823.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0007, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266832.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1133.0789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(77.5357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9090.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-154.5955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(605.1336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.8749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0251],
        [ 0.0301],
        [ 0.0224],
        ...,
        [-0.2757],
        [-0.2037],
        [-0.1328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43005.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365857.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365865.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5893e-03, -2.3270e-04,  0.0000e+00,  ..., -5.9308e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.1186e-02,  9.8504e-03, -1.7188e-03,  ...,  1.3412e-02,
         -7.4252e-03, -8.9071e-03],
        [ 1.1186e-02,  9.8504e-03, -1.7188e-03,  ...,  1.3412e-02,
         -7.4252e-03, -8.9071e-03],
        ...,
        [ 1.5893e-03, -2.3270e-04,  0.0000e+00,  ..., -5.9308e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5893e-03, -2.3270e-04,  0.0000e+00,  ..., -5.9308e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5893e-03, -2.3270e-04,  0.0000e+00,  ..., -5.9308e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-257.8384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.7413, device='cuda:0')



h[100].sum tensor(9.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8239, device='cuda:0')



h[200].sum tensor(-6.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.7009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0238, 0.0178, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0316, 0.0258, 0.0000,  ..., 0.0352, 0.0000, 0.0000],
        [0.0430, 0.0378, 0.0000,  ..., 0.0513, 0.0000, 0.0000],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35965.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0067, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0086, 0.0111,  ..., 0.0000, 0.0000, 0.0000],
        [0.0088, 0.0086, 0.0111,  ..., 0.0000, 0.0000, 0.0000],
        [0.0088, 0.0086, 0.0111,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255458.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1281.8407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(82.2870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8684.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-155.8937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(541.7489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.8076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2180],
        [-0.2855],
        [-0.3511],
        ...,
        [-0.1391],
        [-0.1385],
        [-0.1383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55482.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365865.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365872.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6086e-03, -1.9585e-04,  0.0000e+00,  ..., -3.9689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6086e-03, -1.9585e-04,  0.0000e+00,  ..., -3.9689e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.6031e-03,  6.1002e-03, -1.0670e-03,  ...,  8.3724e-03,
         -4.6305e-03, -5.5559e-03],
        ...,
        [ 1.6086e-03, -1.9585e-04,  0.0000e+00,  ..., -3.9689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6086e-03, -1.9585e-04,  0.0000e+00,  ..., -3.9689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6086e-03, -1.9585e-04,  0.0000e+00,  ..., -3.9689e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-260.6975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.6328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.9311, device='cuda:0')



h[100].sum tensor(7.9617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2661, device='cuda:0')



h[200].sum tensor(-6.8292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.0892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0124, 0.0061, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0202, 0.0140, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38201.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0091, 0.0045, 0.0046,  ..., 0.0000, 0.0000, 0.0000],
        [0.0087, 0.0013, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0094, 0.0098, 0.0115,  ..., 0.0000, 0.0000, 0.0000],
        [0.0094, 0.0098, 0.0115,  ..., 0.0000, 0.0000, 0.0000],
        [0.0094, 0.0098, 0.0115,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(273574.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1390.2981, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.3322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9340.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-173.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(522.4886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.5185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0080],
        [-0.0004],
        [-0.0210],
        ...,
        [-0.1474],
        [-0.1467],
        [-0.1465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45183.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365872.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365881.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8377e-02,  1.7428e-02, -2.9635e-03,  ...,  2.3481e-02,
         -1.2919e-02, -1.5504e-02],
        [ 1.8970e-02,  1.8051e-02, -3.0684e-03,  ...,  2.4313e-02,
         -1.3376e-02, -1.6053e-02],
        [ 1.6269e-02,  1.5215e-02, -2.5906e-03,  ...,  2.0524e-02,
         -1.1293e-02, -1.3553e-02],
        ...,
        [ 1.6251e-03, -1.6017e-04,  0.0000e+00,  ..., -1.9212e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6251e-03, -1.6017e-04,  0.0000e+00,  ..., -1.9212e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6251e-03, -1.6017e-04,  0.0000e+00,  ..., -1.9212e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-326.8551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.2973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2396, device='cuda:0')



h[100].sum tensor(5.4142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8941, device='cuda:0')



h[200].sum tensor(-5.8655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0695, 0.0655, 0.0000,  ..., 0.0884, 0.0000, 0.0000],
        [0.0744, 0.0706, 0.0000,  ..., 0.0951, 0.0000, 0.0000],
        [0.0902, 0.0873, 0.0000,  ..., 0.1174, 0.0000, 0.0000],
        ...,
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33381.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0100, 0.0110, 0.0119,  ..., 0.0000, 0.0000, 0.0000],
        [0.0100, 0.0110, 0.0119,  ..., 0.0000, 0.0000, 0.0000],
        [0.0100, 0.0110, 0.0119,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245153.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1534.1113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.0655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8537.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-170.6658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(352.2755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.5443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0507],
        [-0.0581],
        [-0.0562],
        ...,
        [-0.1572],
        [-0.1565],
        [-0.1563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57930.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365881.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365889.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6396e-03, -1.3377e-04,  0.0000e+00,  ..., -7.9150e-07,
          0.0000e+00,  0.0000e+00],
        [ 8.0857e-03,  6.6320e-03, -1.1333e-03,  ...,  9.0397e-03,
         -4.9631e-03, -5.9577e-03],
        [ 1.6396e-03, -1.3377e-04,  0.0000e+00,  ..., -7.9150e-07,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6396e-03, -1.3377e-04,  0.0000e+00,  ..., -7.9150e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.6396e-03, -1.3377e-04,  0.0000e+00,  ..., -7.9150e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.6396e-03, -1.3377e-04,  0.0000e+00,  ..., -7.9150e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-287.0316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.5844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5052, device='cuda:0')



h[100].sum tensor(5.3466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1078, device='cuda:0')



h[200].sum tensor(-6.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0345, 0.0288, 0.0000,  ..., 0.0392, 0.0000, 0.0000],
        [0.0118, 0.0054, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0130, 0.0066, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        ...,
        [0.0066, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37017.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0104, 0.0006, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0105, 0.0025, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0106, 0.0122, 0.0121,  ..., 0.0000, 0.0000, 0.0000],
        [0.0106, 0.0122, 0.0121,  ..., 0.0000, 0.0000, 0.0000],
        [0.0105, 0.0122, 0.0120,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(268045.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1644.2809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(100.7268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9122.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-191.7654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(431.0482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.2259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0270],
        [ 0.0364],
        [ 0.0407],
        ...,
        [-0.1661],
        [-0.1655],
        [-0.1653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59135.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365889.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6565e-03, -1.0522e-04,  0.0000e+00,  ...,  2.1781e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-226.1280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.0368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.0576, device='cuda:0')



h[100].sum tensor(6.2290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7998, device='cuda:0')



h[200].sum tensor(-7.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9039, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.6204e-03, 0.0000e+00, 0.0000e+00,  ..., 8.7050e-05, 0.0000e+00,
         0.0000e+00],
        [6.6213e-03, 0.0000e+00, 0.0000e+00,  ..., 8.7062e-05, 0.0000e+00,
         0.0000e+00],
        [1.7883e-02, 1.1605e-02, 0.0000e+00,  ..., 1.5876e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.6341e-03, 0.0000e+00, 0.0000e+00,  ..., 8.7230e-05, 0.0000e+00,
         0.0000e+00],
        [6.6339e-03, 0.0000e+00, 0.0000e+00,  ..., 8.7228e-05, 0.0000e+00,
         0.0000e+00],
        [1.4725e-02, 8.3850e-03, 0.0000e+00,  ..., 1.1433e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41538.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0100, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0108, 0.0058, 0.0049,  ..., 0.0000, 0.0000, 0.0000],
        [0.0112, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0107, 0.0117, 0.0103,  ..., 0.0000, 0.0000, 0.0000],
        [0.0108, 0.0066, 0.0062,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0017, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(295055.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1661.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.2632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9884.8896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-208.3627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(518.3313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0861],
        [-0.1447],
        [-0.2525],
        ...,
        [-0.1132],
        [-0.0646],
        [-0.0337]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53494.7617, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365906.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6722e-03, -8.2681e-05,  0.0000e+00,  ...,  3.6363e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-371.0438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.7761, device='cuda:0')



h[100].sum tensor(2.9377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3501, device='cuda:0')



h[200].sum tensor(-5.4247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32182.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0129, 0.0110,  ..., 0.0000, 0.0000, 0.0000],
        [0.0109, 0.0140, 0.0126,  ..., 0.0000, 0.0000, 0.0000],
        [0.0109, 0.0141, 0.0126,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0109, 0.0140, 0.0126,  ..., 0.0000, 0.0000, 0.0000],
        [0.0109, 0.0140, 0.0126,  ..., 0.0000, 0.0000, 0.0000],
        [0.0109, 0.0140, 0.0126,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244685.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1656.7346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(116.7143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8644.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(265.4101, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-123.1869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1145],
        [-0.1618],
        [-0.2068],
        ...,
        [-0.1814],
        [-0.1807],
        [-0.1804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71871.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365906.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(549.4929, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365915.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6898e-03, -6.2653e-05,  0.0000e+00,  ...,  4.6808e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-297.6512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.5590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.8723, device='cuda:0')



h[100].sum tensor(4.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2442, device='cuda:0')



h[200].sum tensor(-6.7626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37566.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0146, 0.0119,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0148, 0.0126,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0149, 0.0130,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0149, 0.0129,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0149, 0.0129,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0149, 0.0129,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276665.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1660.9100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.9754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9617.6826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-208.3706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(369.4816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.8574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0623],
        [-0.0982],
        [-0.1312],
        ...,
        [-0.1872],
        [-0.1862],
        [-0.1859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65135.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365915.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365924.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7074e-03, -4.1077e-05,  0.0000e+00,  ...,  5.6006e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7074e-03, -4.1077e-05,  0.0000e+00,  ...,  5.6006e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.7578e-03,  7.3500e-03, -1.2091e-03,  ...,  9.9349e-03,
         -5.3933e-03, -6.4802e-03],
        ...,
        [ 1.7074e-03, -4.1077e-05,  0.0000e+00,  ...,  5.6006e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7074e-03, -4.1077e-05,  0.0000e+00,  ...,  5.6006e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7074e-03, -4.1077e-05,  0.0000e+00,  ...,  5.6006e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-244.7346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.1357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.6572, device='cuda:0')



h[100].sum tensor(5.9316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2793, device='cuda:0')



h[200].sum tensor(-7.6882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0193, 0.0130, 0.0000,  ..., 0.0177, 0.0000, 0.0000],
        [0.0380, 0.0326, 0.0000,  ..., 0.0439, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42190.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0058, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0112, 0.0156, 0.0132,  ..., 0.0000, 0.0000, 0.0000],
        [0.0112, 0.0156, 0.0132,  ..., 0.0000, 0.0000, 0.0000],
        [0.0112, 0.0156, 0.0132,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312406.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1677.3447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(103.0595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10373.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.8493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(594.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.0744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 8.0644e-04],
        [ 6.9943e-06],
        [-3.2584e-02],
        ...,
        [-1.8338e-01],
        [-1.9084e-01],
        [-1.9193e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70929.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9994],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365924.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365933.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7254e-03, -2.1444e-05,  0.0000e+00,  ...,  6.7603e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-373.3976, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.5467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6722, device='cuda:0')



h[100].sum tensor(3.5362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3115, device='cuda:0')



h[200].sum tensor(-5.3743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.9531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32780.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0113, 0.0161, 0.0136,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0161, 0.0136,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0161, 0.0136,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0161, 0.0135,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0161, 0.0135,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0161, 0.0135,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254646.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1681.8668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.1427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9280.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.6078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(203.8600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.5554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2821],
        [-0.2947],
        [-0.3044],
        ...,
        [-0.1992],
        [-0.1982],
        [-0.1979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73238.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365933.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365942.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7797e-02,  2.7289e-02, -4.4121e-03,  ...,  3.6570e-02,
         -1.9866e-02, -2.3881e-02],
        [ 2.9548e-02,  2.9124e-02, -4.7087e-03,  ...,  3.9023e-02,
         -2.1201e-02, -2.5486e-02],
        [ 2.3951e-02,  2.3260e-02, -3.7609e-03,  ...,  3.1183e-02,
         -1.6933e-02, -2.0356e-02],
        ...,
        [ 1.7448e-03, -7.8393e-06,  0.0000e+00,  ...,  7.6718e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7448e-03, -7.8393e-06,  0.0000e+00,  ...,  7.6718e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7448e-03, -7.8393e-06,  0.0000e+00,  ...,  7.6718e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-119.6025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.6122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.9518, device='cuda:0')



h[100].sum tensor(9.3929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9904, device='cuda:0')



h[200].sum tensor(-9.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.7811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1154, 0.1136, 0.0000,  ..., 0.1522, 0.0000, 0.0000],
        [0.1187, 0.1170, 0.0000,  ..., 0.1568, 0.0000, 0.0000],
        [0.1294, 0.1283, 0.0000,  ..., 0.1718, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51227.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0095, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0164, 0.0138,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0164, 0.0138,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0164, 0.0138,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380513.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1638.9363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.2793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11988.9131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.0691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1007.7615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.3279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3087],
        [-0.3543],
        [-0.3639],
        ...,
        [-0.2054],
        [-0.2046],
        [-0.2042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79491.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365942.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365949.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6388e-03,  7.2009e-03, -1.1582e-03,  ...,  9.7082e-03,
         -5.2393e-03, -6.2997e-03],
        [ 1.7578e-03, -7.6083e-06,  0.0000e+00,  ...,  6.9696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7578e-03, -7.6083e-06,  0.0000e+00,  ...,  6.9696e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7578e-03, -7.6083e-06,  0.0000e+00,  ...,  6.9696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7578e-03, -7.6083e-06,  0.0000e+00,  ...,  6.9696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7578e-03, -7.6083e-06,  0.0000e+00,  ...,  6.9696e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-358.2429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.2679, device='cuda:0')



h[100].sum tensor(5.0221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5329, device='cuda:0')



h[200].sum tensor(-5.4488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0126, 0.0059, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0139, 0.0072, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33677.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0110, 0.0056, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0118, 0.0075,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0114, 0.0167, 0.0140,  ..., 0.0000, 0.0000, 0.0000],
        [0.0114, 0.0167, 0.0140,  ..., 0.0000, 0.0000, 0.0000],
        [0.0114, 0.0167, 0.0140,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(264970.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1673.4904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.7270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9798.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-209.5955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(211.4532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.0413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1848],
        [-0.1902],
        [-0.1866],
        ...,
        [-0.2131],
        [-0.2122],
        [-0.2119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79800.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9993],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365949.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9992],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365957.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4368e-02,  1.3187e-02, -2.1056e-03,  ...,  1.7717e-02,
         -9.5703e-03, -1.1510e-02],
        [ 1.6172e-02,  1.5075e-02, -2.4073e-03,  ...,  2.0243e-02,
         -1.0941e-02, -1.3159e-02],
        [ 4.2082e-02,  4.2215e-02, -6.7414e-03,  ...,  5.6536e-02,
         -3.0641e-02, -3.6851e-02],
        ...,
        [ 1.7802e-03,  1.7255e-06,  0.0000e+00,  ...,  8.4496e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7802e-03,  1.7255e-06,  0.0000e+00,  ...,  8.4496e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7802e-03,  1.7255e-06,  0.0000e+00,  ...,  8.4496e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-176.0244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.7393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.2225, device='cuda:0')



h[100].sum tensor(9.7622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2327, device='cuda:0')



h[200].sum tensor(-8.2517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.2630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0528e-01, 1.0283e-01, 0.0000e+00,  ..., 1.3785e-01, 0.0000e+00,
         0.0000e+00],
        [1.1842e-01, 1.1659e-01, 0.0000e+00,  ..., 1.5624e-01, 0.0000e+00,
         0.0000e+00],
        [7.4858e-02, 7.0960e-02, 0.0000e+00,  ..., 9.5224e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.1357e-03, 6.9165e-06, 0.0000e+00,  ..., 3.3869e-04, 0.0000e+00,
         0.0000e+00],
        [7.1356e-03, 6.9163e-06, 0.0000e+00,  ..., 3.3868e-04, 0.0000e+00,
         0.0000e+00],
        [7.1349e-03, 6.9157e-06, 0.0000e+00,  ..., 3.3865e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46672., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0049, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0167, 0.0143,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0167, 0.0143,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0167, 0.0143,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351062.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1592.1292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(116.3923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11736.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.0972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(743.6864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.3745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2753],
        [-0.2821],
        [-0.2504],
        ...,
        [-0.2179],
        [-0.2169],
        [-0.2166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80124.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9992],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365957.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365965.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00],
        [1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00],
        [1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00],
        [1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00],
        [1.8044e-03, 8.4347e-06, 0.0000e+00,  ..., 1.0159e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-181.4030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.2726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.4184, device='cuda:0')



h[100].sum tensor(10.6588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9338, device='cuda:0')



h[200].sum tensor(-7.8194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.3248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2107e-03, 3.3706e-05, 0.0000e+00,  ..., 4.0597e-04, 0.0000e+00,
         0.0000e+00],
        [7.2118e-03, 3.3711e-05, 0.0000e+00,  ..., 4.0603e-04, 0.0000e+00,
         0.0000e+00],
        [2.4313e-02, 1.7942e-02, 0.0000e+00,  ..., 2.4359e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2336e-03, 3.3813e-05, 0.0000e+00,  ..., 4.0727e-04, 0.0000e+00,
         0.0000e+00],
        [7.2335e-03, 3.3813e-05, 0.0000e+00,  ..., 4.0726e-04, 0.0000e+00,
         0.0000e+00],
        [7.2328e-03, 3.3810e-05, 0.0000e+00,  ..., 4.0722e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44079.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0089, 0.0073,  ..., 0.0000, 0.0000, 0.0000],
        [0.0106, 0.0078, 0.0066,  ..., 0.0000, 0.0000, 0.0000],
        [0.0097, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0111, 0.0166, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
        [0.0111, 0.0166, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
        [0.0111, 0.0166, 0.0146,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331180.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1531.9624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.0405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11600.1904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.5469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(543.7411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.0974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0510],
        [-0.0127],
        [ 0.0392],
        ...,
        [-0.2218],
        [-0.2209],
        [-0.2205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72518.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365965.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365972.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.9106e-03,  7.4186e-03, -1.1700e-03,  ...,  1.0039e-02,
         -5.3685e-03, -6.4596e-03],
        [ 6.3852e-03,  4.7742e-03, -7.5278e-04,  ...,  6.5010e-03,
         -3.4541e-03, -4.1561e-03],
        [ 1.3467e-02,  1.2190e-02, -1.9228e-03,  ...,  1.6421e-02,
         -8.8226e-03, -1.0616e-02],
        ...,
        [ 1.8285e-03,  2.9439e-06,  0.0000e+00,  ...,  1.1804e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8285e-03,  2.9439e-06,  0.0000e+00,  ...,  1.1804e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8285e-03,  2.9439e-06,  0.0000e+00,  ...,  1.1804e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-305.9725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.3653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1672, device='cuda:0')



h[100].sum tensor(9.3614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4955, device='cuda:0')



h[200].sum tensor(-5.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.5308, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.1354e-02, 1.4721e-02, 0.0000e+00,  ..., 2.0149e-02, 0.0000e+00,
         0.0000e+00],
        [4.5904e-02, 4.0426e-02, 0.0000e+00,  ..., 5.4537e-02, 0.0000e+00,
         0.0000e+00],
        [3.0597e-02, 2.4396e-02, 0.0000e+00,  ..., 3.3093e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3310e-03, 1.1803e-05, 0.0000e+00,  ..., 4.7327e-04, 0.0000e+00,
         0.0000e+00],
        [7.3309e-03, 1.1803e-05, 0.0000e+00,  ..., 4.7326e-04, 0.0000e+00,
         0.0000e+00],
        [7.3302e-03, 1.1802e-05, 0.0000e+00,  ..., 4.7322e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34923.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0108, 0.0164, 0.0149,  ..., 0.0000, 0.0000, 0.0000],
        [0.0108, 0.0164, 0.0149,  ..., 0.0000, 0.0000, 0.0000],
        [0.0108, 0.0164, 0.0149,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278880.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1461.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.9237, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10275.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.7120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(310.9294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-144.4585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0568],
        [ 0.0645],
        [ 0.0616],
        ...,
        [-0.2145],
        [-0.2028],
        [-0.1935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88813.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365972.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365980.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40.8233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(97.0416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-39.3964, device='cuda:0')



h[100].sum tensor(17.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6424, device='cuda:0')



h[200].sum tensor(-10.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.9670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0112, 0.0039, 0.0000,  ..., 0.0058, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53424.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0475e-02, 1.5937e-02, 1.5173e-02,  ..., 0.0000e+00, 2.3142e-06,
         0.0000e+00],
        [1.0476e-02, 1.5940e-02, 1.5176e-02,  ..., 0.0000e+00, 2.5010e-06,
         0.0000e+00],
        [1.0392e-02, 1.3934e-02, 1.2652e-02,  ..., 0.0000e+00, 1.7255e-06,
         0.0000e+00],
        ...,
        [1.0376e-02, 1.3465e-02, 1.1310e-02,  ..., 0.0000e+00, 1.7099e-06,
         0.0000e+00],
        [1.0191e-02, 9.8096e-03, 6.5592e-03,  ..., 0.0000e+00, 9.0212e-07,
         0.0000e+00],
        [9.8891e-03, 5.2886e-03, 1.9243e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379833.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1364.6984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.9870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12581.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.5408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(929.5337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.8276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2676],
        [-0.2153],
        [-0.1410],
        ...,
        [-0.1735],
        [-0.1269],
        [-0.0858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78051.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365980.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365980.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8546e-03, -4.8994e-06,  0.0000e+00,  ...,  1.3907e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-174.9978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.7595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.0265, device='cuda:0')



h[100].sum tensor(13.5171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0449, device='cuda:0')



h[200].sum tensor(-7.0764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.5340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0212, 0.0144, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41042.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.6491e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.9683e-03, 2.6226e-03, 1.8990e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [9.6570e-03, 3.0012e-03, 1.1005e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0502e-02, 1.5962e-02, 1.5179e-02,  ..., 0.0000e+00, 3.5193e-06,
         0.0000e+00],
        [1.0501e-02, 1.5962e-02, 1.5177e-02,  ..., 0.0000e+00, 3.6197e-06,
         0.0000e+00],
        [1.0500e-02, 1.5961e-02, 1.5173e-02,  ..., 0.0000e+00, 2.5006e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312002.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1373.6730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(159.3667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11192.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.4734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(476.2634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.7888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0362],
        [ 0.0298],
        [ 0.0289],
        ...,
        [-0.2269],
        [-0.2260],
        [-0.2257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79300.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9992],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365980.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(516.0358, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365987.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6702e-03,  4.9857e-03, -7.8082e-04,  ...,  6.8603e-03,
         -3.6172e-03, -4.3545e-03],
        [ 6.6702e-03,  4.9857e-03, -7.8082e-04,  ...,  6.8603e-03,
         -3.6172e-03, -4.3545e-03],
        [ 1.8844e-03, -2.3657e-05,  0.0000e+00,  ...,  1.5598e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8844e-03, -2.3657e-05,  0.0000e+00,  ...,  1.5598e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8844e-03, -2.3657e-05,  0.0000e+00,  ...,  1.5598e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8844e-03, -2.3657e-05,  0.0000e+00,  ...,  1.5598e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-190.4616, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.1200, device='cuda:0')



h[100].sum tensor(15.0949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9646, device='cuda:0')



h[200].sum tensor(-6.3720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.1428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0090, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0162, 0.0090, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0162, 0.0090, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38578.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0080, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0085, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0089, 0.0032, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0155, 0.0154,  ..., 0.0000, 0.0008, 0.0000],
        [0.0101, 0.0155, 0.0154,  ..., 0.0000, 0.0008, 0.0000],
        [0.0101, 0.0155, 0.0154,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(295202.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1278.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.7920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10953.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-230.7361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(330.0501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.8423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0599],
        [ 0.0035],
        [-0.0736],
        ...,
        [-0.2285],
        [-0.2275],
        [-0.2272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72838.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365987.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365994.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9130e-03, -4.1626e-05,  0.0000e+00,  ...,  1.6884e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.3572e-02,  2.2625e-02, -3.5116e-03,  ...,  3.0511e-02,
         -1.6347e-02, -1.9683e-02],
        [ 1.9130e-03, -4.1626e-05,  0.0000e+00,  ...,  1.6884e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9130e-03, -4.1626e-05,  0.0000e+00,  ...,  1.6884e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9130e-03, -4.1626e-05,  0.0000e+00,  ...,  1.6884e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9130e-03, -4.1626e-05,  0.0000e+00,  ...,  1.6884e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-10.0242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.5773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.9215, device='cuda:0')



h[100].sum tensor(20.4640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2358, device='cuda:0')



h[200].sum tensor(-8.8302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.4122, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0416, 0.0355, 0.0000,  ..., 0.0483, 0.0000, 0.0000],
        [0.0500, 0.0442, 0.0000,  ..., 0.0600, 0.0000, 0.0000],
        [0.0986, 0.0950, 0.0000,  ..., 0.1281, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49115.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0098, 0.0153, 0.0155,  ..., 0.0000, 0.0016, 0.0000],
        [0.0098, 0.0153, 0.0155,  ..., 0.0000, 0.0016, 0.0000],
        [0.0098, 0.0153, 0.0155,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(361873.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1166.1376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.1532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12452.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.3728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(781.6431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.2106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0388],
        [-0.0335],
        [-0.0407],
        ...,
        [-0.2304],
        [-0.2294],
        [-0.2291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71554.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365994.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366002.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7284e-02,  3.6924e-02, -5.6948e-03,  ...,  4.9698e-02,
         -2.6638e-02, -3.2082e-02],
        [ 3.5356e-02,  3.4907e-02, -5.3842e-03,  ...,  4.6998e-02,
         -2.5185e-02, -3.0332e-02],
        [ 3.5748e-02,  3.5316e-02, -5.4473e-03,  ...,  4.7546e-02,
         -2.5480e-02, -3.0688e-02],
        ...,
        [ 1.9382e-03, -5.9183e-05,  0.0000e+00,  ...,  1.7857e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9382e-03, -5.9183e-05,  0.0000e+00,  ...,  1.7857e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9382e-03, -5.9183e-05,  0.0000e+00,  ...,  1.7857e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-73.8623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.9952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.2292, device='cuda:0')



h[100].sum tensor(20.5564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4919, device='cuda:0')



h[200].sum tensor(-7.4334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.9373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1411, 0.1393, 0.0000,  ..., 0.1876, 0.0000, 0.0000],
        [0.1530, 0.1517, 0.0000,  ..., 0.2042, 0.0000, 0.0000],
        [0.1502, 0.1488, 0.0000,  ..., 0.2002, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43476.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0095, 0.0152, 0.0157,  ..., 0.0000, 0.0023, 0.0000],
        [0.0095, 0.0152, 0.0157,  ..., 0.0000, 0.0023, 0.0000],
        [0.0095, 0.0152, 0.0157,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327161.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1091.8048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(179.5838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11891.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.3949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(523.2699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.0027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3660],
        [-0.3656],
        [-0.3419],
        ...,
        [-0.2334],
        [-0.2325],
        [-0.2321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66681.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366002.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366010.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9580e-03, -7.9662e-05,  0.0000e+00,  ...,  1.8988e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(15.1857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.5097, device='cuda:0')



h[100].sum tensor(23.8147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0828, device='cuda:0')



h[200].sum tensor(-8.4287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.9317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47435.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0080, 0.0073, 0.0070,  ..., 0.0000, 0.0009, 0.0000],
        [0.0092, 0.0133, 0.0125,  ..., 0.0000, 0.0014, 0.0000],
        [0.0093, 0.0148, 0.0146,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0092, 0.0153, 0.0158,  ..., 0.0000, 0.0030, 0.0000],
        [0.0092, 0.0153, 0.0158,  ..., 0.0000, 0.0030, 0.0000],
        [0.0092, 0.0153, 0.0158,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353794.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1010.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(154.9342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12209.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.8062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(785.8986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.0116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0007],
        [-0.0533],
        [-0.1146],
        ...,
        [-0.2359],
        [-0.2349],
        [-0.2346]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78661.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366010.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366018.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9706e-03, -9.5525e-05,  0.0000e+00,  ...,  1.9773e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9706e-03, -9.5525e-05,  0.0000e+00,  ...,  1.9773e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.1451e-02,  9.8210e-03, -1.5084e-03,  ...,  1.3483e-02,
         -7.1246e-03, -8.5849e-03],
        ...,
        [ 1.9706e-03, -9.5525e-05,  0.0000e+00,  ...,  1.9773e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9706e-03, -9.5525e-05,  0.0000e+00,  ...,  1.9773e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9706e-03, -9.5525e-05,  0.0000e+00,  ...,  1.9773e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-138.5005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.7077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9888, device='cuda:0')



h[100].sum tensor(21.6861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1725, device='cuda:0')



h[200].sum tensor(-5.7350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6561, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0244, 0.0172, 0.0000,  ..., 0.0239, 0.0000, 0.0000],
        [0.0173, 0.0098, 0.0000,  ..., 0.0141, 0.0000, 0.0000],
        [0.0357, 0.0289, 0.0000,  ..., 0.0398, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37516.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0156, 0.0160,  ..., 0.0000, 0.0037, 0.0000],
        [0.0091, 0.0156, 0.0160,  ..., 0.0000, 0.0037, 0.0000],
        [0.0091, 0.0156, 0.0160,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(298651.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(998.4208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.1963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11075.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.4852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(400.7939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.0794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0019],
        [-0.0039],
        [-0.0107],
        ...,
        [-0.2394],
        [-0.2385],
        [-0.2381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78524.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366018.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366026.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0137,  0.0122, -0.0019,  ...,  0.0166, -0.0088, -0.0106],
        [ 0.0077,  0.0058, -0.0009,  ...,  0.0082, -0.0043, -0.0051],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93.9761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.5903, device='cuda:0')



h[100].sum tensor(26.7404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8561, device='cuda:0')



h[200].sum tensor(-9.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.3594, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0580, 0.0520, 0.0000,  ..., 0.0711, 0.0000, 0.0000],
        [0.0379, 0.0312, 0.0000,  ..., 0.0429, 0.0000, 0.0000],
        [0.0136, 0.0058, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50034.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0090, 0.0159, 0.0161,  ..., 0.0000, 0.0043, 0.0000],
        [0.0090, 0.0159, 0.0161,  ..., 0.0000, 0.0043, 0.0000],
        [0.0090, 0.0159, 0.0161,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363961.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(918.4678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(167.1230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12725.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-262.5883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(787.2118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.1304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1984],
        [-0.1344],
        [-0.0768],
        ...,
        [-0.2441],
        [-0.2431],
        [-0.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69690.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366026.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366035.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-109.7595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.2434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3156, device='cuda:0')



h[100].sum tensor(22.7329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2940, device='cuda:0')



h[200].sum tensor(-5.8004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.0374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37041.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0039, 0.0037,  ..., 0.0000, 0.0012, 0.0000],
        [0.0070, 0.0080, 0.0078,  ..., 0.0000, 0.0022, 0.0000],
        [0.0087, 0.0136, 0.0126,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0090, 0.0164, 0.0163,  ..., 0.0000, 0.0049, 0.0000],
        [0.0090, 0.0164, 0.0163,  ..., 0.0000, 0.0049, 0.0000],
        [0.0090, 0.0164, 0.0163,  ..., 0.0000, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289080.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(974.1681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.2837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10971.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-230.4934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(310.5627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.9945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0034],
        [ 0.0095],
        [ 0.0234],
        ...,
        [-0.2490],
        [-0.2482],
        [-0.2480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79116.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366035.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366044.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0086,  0.0068, -0.0010,  ...,  0.0095, -0.0050, -0.0060],
        ...,
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-34.9557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.3062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.5191, device='cuda:0')



h[100].sum tensor(23.6754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8563, device='cuda:0')



h[200].sum tensor(-6.8059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0133, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        [0.0145, 0.0068, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0225, 0.0151, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40842.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0168, 0.0164,  ..., 0.0000, 0.0054, 0.0000],
        [0.0091, 0.0168, 0.0164,  ..., 0.0000, 0.0054, 0.0000],
        [0.0090, 0.0168, 0.0164,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310772.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(926.5665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.8641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11515.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-233.7717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(444.2373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.9242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0532],
        [ 0.0622],
        [ 0.0689],
        ...,
        [-0.2553],
        [-0.2542],
        [-0.2539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82010.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0000],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366044.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366053.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0020, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-77.4811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.3953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.6068, device='cuda:0')



h[100].sum tensor(21.9045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7739, device='cuda:0')



h[200].sum tensor(-6.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0178, 0.0104, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38189.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0133, 0.0113,  ..., 0.0000, 0.0042, 0.0000],
        [0.0079, 0.0088, 0.0079,  ..., 0.0000, 0.0031, 0.0000],
        [0.0049, 0.0023, 0.0013,  ..., 0.0000, 0.0013, 0.0000],
        ...,
        [0.0093, 0.0174, 0.0165,  ..., 0.0000, 0.0058, 0.0000],
        [0.0093, 0.0174, 0.0165,  ..., 0.0000, 0.0058, 0.0000],
        [0.0093, 0.0174, 0.0165,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297648.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(992.7081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.3038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11332.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.7740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(314.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.7503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0590],
        [-0.0870],
        [-0.0984],
        ...,
        [-0.2605],
        [-0.2595],
        [-0.2591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78092.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366053.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366063.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0153,  0.0139, -0.0021,  ...,  0.0190, -0.0100, -0.0120],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-84.8656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.2879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1076, device='cuda:0')



h[100].sum tensor(20.6701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5883, device='cuda:0')



h[200].sum tensor(-5.8918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0267, 0.0197, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        [0.0186, 0.0113, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0703, 0.0650, 0.0000,  ..., 0.0886, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39134.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0096, 0.0181, 0.0166,  ..., 0.0000, 0.0063, 0.0000],
        [0.0096, 0.0181, 0.0166,  ..., 0.0000, 0.0063, 0.0000],
        [0.0096, 0.0181, 0.0166,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311700.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1040.1733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.8152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11711.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.6102, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(372.1708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.3035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0586],
        [ 0.0335],
        [ 0.0045],
        ...,
        [-0.2666],
        [-0.2655],
        [-0.2651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76148.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366063.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(568.7309, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366073.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9148e-03, -9.9349e-05,  0.0000e+00,  ...,  2.0874e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9148e-03, -9.9349e-05,  0.0000e+00,  ...,  2.0874e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.0116e-02,  8.4848e-03, -1.2572e-03,  ...,  1.1720e-02,
         -6.1164e-03, -7.3808e-03],
        ...,
        [ 1.9148e-03, -9.9349e-05,  0.0000e+00,  ...,  2.0874e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9148e-03, -9.9349e-05,  0.0000e+00,  ...,  2.0874e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9148e-03, -9.9349e-05,  0.0000e+00,  ...,  2.0874e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(492.7869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(112.6388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-55.0154, device='cuda:0')



h[100].sum tensor(30.4141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.4474, device='cuda:0')



h[200].sum tensor(-14.2687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-64.1910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0203, 0.0130, 0.0000,  ..., 0.0186, 0.0000, 0.0000],
        [0.0371, 0.0305, 0.0000,  ..., 0.0422, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73472.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0064, 0.0046,  ..., 0.0000, 0.0024, 0.0000],
        [0.0036, 0.0007, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0101, 0.0188, 0.0168,  ..., 0.0000, 0.0068, 0.0000],
        [0.0101, 0.0187, 0.0168,  ..., 0.0000, 0.0068, 0.0000],
        [0.0101, 0.0187, 0.0168,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518662.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1026.2863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.1469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16342.6729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-295.5744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1733.4379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.7124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0331],
        [ 0.0388],
        [ 0.0423],
        ...,
        [-0.2725],
        [-0.2714],
        [-0.2710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67147.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366073.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366084.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9007e-03, -9.7199e-05,  0.0000e+00,  ...,  1.9995e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30.4119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.3493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.8090, device='cuda:0')



h[100].sum tensor(20.7180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7074, device='cuda:0')



h[200].sum tensor(-7.4025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.6138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47802.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0094, 0.0044,  ..., 0.0000, 0.0044, 0.0000],
        [0.0095, 0.0121, 0.0078,  ..., 0.0000, 0.0052, 0.0000],
        [0.0101, 0.0171, 0.0142,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0105, 0.0194, 0.0171,  ..., 0.0000, 0.0073, 0.0000],
        [0.0105, 0.0194, 0.0171,  ..., 0.0000, 0.0073, 0.0000],
        [0.0104, 0.0194, 0.0170,  ..., 0.0000, 0.0073, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379323.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1116.4489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.9080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12998.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(872.9780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.4196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0274],
        [-0.0530],
        [-0.0833],
        ...,
        [-0.2792],
        [-0.2781],
        [-0.2777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95328.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366084.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366093.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0184,  0.0172, -0.0025,  ...,  0.0234, -0.0123, -0.0148],
        [ 0.0102,  0.0086, -0.0013,  ...,  0.0118, -0.0062, -0.0074],
        [ 0.0102,  0.0086, -0.0013,  ...,  0.0118, -0.0062, -0.0074],
        ...,
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-5.6530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.0728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.8532, device='cuda:0')



h[100].sum tensor(19.5169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9805, device='cuda:0')



h[200].sum tensor(-6.8092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0511, 0.0452, 0.0000,  ..., 0.0619, 0.0000, 0.0000],
        [0.0526, 0.0467, 0.0000,  ..., 0.0639, 0.0000, 0.0000],
        [0.0226, 0.0155, 0.0000,  ..., 0.0218, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43470.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0107, 0.0202, 0.0173,  ..., 0.0000, 0.0079, 0.0000],
        [0.0107, 0.0202, 0.0173,  ..., 0.0000, 0.0079, 0.0000],
        [0.0107, 0.0202, 0.0173,  ..., 0.0000, 0.0079, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338987.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1188.0886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.9430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12400.8027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.5818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(490.0970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.7259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0739],
        [ 0.0697],
        [ 0.0570],
        ...,
        [-0.2869],
        [-0.2859],
        [-0.2857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84762.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366093.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366102.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0188,  0.0176, -0.0026,  ...,  0.0240, -0.0126, -0.0152],
        [ 0.0136,  0.0122, -0.0018,  ...,  0.0167, -0.0087, -0.0105],
        [ 0.0185,  0.0173, -0.0025,  ...,  0.0235, -0.0123, -0.0149],
        ...,
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-10.3450, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.1823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.1730, device='cuda:0')



h[100].sum tensor(19.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7276, device='cuda:0')



h[200].sum tensor(-6.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0496, 0.0436, 0.0000,  ..., 0.0597, 0.0000, 0.0000],
        [0.0547, 0.0489, 0.0000,  ..., 0.0668, 0.0000, 0.0000],
        [0.0369, 0.0305, 0.0000,  ..., 0.0420, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41809.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0109, 0.0207, 0.0176,  ..., 0.0000, 0.0085, 0.0000],
        [0.0109, 0.0207, 0.0176,  ..., 0.0000, 0.0085, 0.0000],
        [0.0109, 0.0207, 0.0175,  ..., 0.0000, 0.0085, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326212.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1229.4376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.4874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12134.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.6241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(407.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.6591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0505],
        [ 0.0578],
        [ 0.0625],
        ...,
        [-0.2941],
        [-0.2930],
        [-0.2925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90410.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366102.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366112.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0019, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-48.3573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.4902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.4638, device='cuda:0')



h[100].sum tensor(18.7426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3490, device='cuda:0')



h[200].sum tensor(-5.7108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2103, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38635.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0208, 0.0177,  ..., 0.0000, 0.0091, 0.0000],
        [0.0109, 0.0209, 0.0177,  ..., 0.0000, 0.0091, 0.0000],
        [0.0109, 0.0209, 0.0178,  ..., 0.0000, 0.0091, 0.0000],
        ...,
        [0.0109, 0.0209, 0.0178,  ..., 0.0000, 0.0092, 0.0000],
        [0.0109, 0.0209, 0.0178,  ..., 0.0000, 0.0092, 0.0000],
        [0.0109, 0.0209, 0.0178,  ..., 0.0000, 0.0092, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312062.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1240.8090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11629.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-209.2911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(360.1495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.6015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3359],
        [-0.3192],
        [-0.2847],
        ...,
        [-0.2972],
        [-0.2960],
        [-0.2955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100261.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366112.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366122.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9018e-03, -8.3560e-05,  0.0000e+00,  ...,  1.9702e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67.8618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.0289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.6093, device='cuda:0')



h[100].sum tensor(21.5492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2615, device='cuda:0')



h[200].sum tensor(-6.9025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41925.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0208, 0.0179,  ..., 0.0000, 0.0099, 0.0000],
        [0.0108, 0.0208, 0.0179,  ..., 0.0000, 0.0099, 0.0000],
        [0.0095, 0.0145, 0.0122,  ..., 0.0000, 0.0086, 0.0000],
        ...,
        [0.0108, 0.0209, 0.0180,  ..., 0.0000, 0.0099, 0.0000],
        [0.0108, 0.0209, 0.0180,  ..., 0.0000, 0.0099, 0.0000],
        [0.0108, 0.0209, 0.0180,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321770.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1223.6703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.6645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11724.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.9013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(461.1266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.9976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1237],
        [-0.1470],
        [-0.1447],
        ...,
        [-0.2968],
        [-0.2956],
        [-0.2952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98910.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366122.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366132.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(281.7126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(97.1549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.3452, device='cuda:0')



h[100].sum tensor(26.3891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8800, device='cuda:0')



h[200].sum tensor(-9.3877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.5736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54213.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0032, 0.0012,  ..., 0.0000, 0.0066, 0.0000],
        [0.0091, 0.0072, 0.0032,  ..., 0.0000, 0.0079, 0.0000],
        [0.0097, 0.0117, 0.0079,  ..., 0.0000, 0.0089, 0.0000],
        ...,
        [0.0108, 0.0207, 0.0184,  ..., 0.0000, 0.0107, 0.0000],
        [0.0108, 0.0207, 0.0184,  ..., 0.0000, 0.0107, 0.0000],
        [0.0108, 0.0207, 0.0184,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396763.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1094.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(203.5191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13688.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.4431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(922.3392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.0332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0654],
        [ 0.0275],
        [-0.0212],
        ...,
        [-0.2953],
        [-0.2941],
        [-0.2937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75173.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366132.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366132.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4818e-02,  2.3904e-02, -3.3772e-03,  ...,  3.2391e-02,
         -1.6937e-02, -2.0467e-02],
        [ 8.3143e-03,  6.6292e-03, -9.4318e-04,  ...,  9.2075e-03,
         -4.7301e-03, -5.7161e-03],
        [ 2.1962e-02,  2.0914e-02, -2.9559e-03,  ...,  2.8378e-02,
         -1.4824e-02, -1.7914e-02],
        ...,
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9190e-03, -6.4867e-05,  0.0000e+00,  ...,  2.2397e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(469.2115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(105.9561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-47.3906, device='cuda:0')



h[100].sum tensor(29.8924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6135, device='cuda:0')



h[200].sum tensor(-11.9739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-55.2944, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0417, 0.0355, 0.0000,  ..., 0.0487, 0.0000, 0.0000],
        [0.0915, 0.0875, 0.0000,  ..., 0.1187, 0.0000, 0.0000],
        [0.0610, 0.0556, 0.0000,  ..., 0.0758, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68304.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0108, 0.0207, 0.0184,  ..., 0.0000, 0.0107, 0.0000],
        [0.0099, 0.0134, 0.0111,  ..., 0.0000, 0.0098, 0.0000],
        [0.0071, 0.0052, 0.0043,  ..., 0.0000, 0.0073, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507483.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1089.0120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.8742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15939.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.9089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1726.6028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.6032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0281],
        [ 0.0006],
        [-0.0200],
        ...,
        [-0.2027],
        [-0.1407],
        [-0.0744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76004.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0002],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366132.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366143.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9371e-03, -4.9579e-05,  0.0000e+00,  ...,  2.3912e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(263.6428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.4820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.5674, device='cuda:0')



h[100].sum tensor(26.8917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8476, device='cuda:0')



h[200].sum tensor(-8.6847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.3326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51386.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0188, 0.0165,  ..., 0.0000, 0.0106, 0.0000],
        [0.0109, 0.0202, 0.0184,  ..., 0.0000, 0.0111, 0.0000],
        [0.0108, 0.0204, 0.0187,  ..., 0.0000, 0.0113, 0.0000],
        ...,
        [0.0109, 0.0205, 0.0187,  ..., 0.0000, 0.0113, 0.0000],
        [0.0109, 0.0205, 0.0187,  ..., 0.0000, 0.0113, 0.0000],
        [0.0109, 0.0205, 0.0187,  ..., 0.0000, 0.0113, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385882., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1112.3829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.6500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13376.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.3527, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(857.1700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.1172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1534],
        [-0.2377],
        [-0.3079],
        ...,
        [-0.2942],
        [-0.2931],
        [-0.2927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69180.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366143.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366153.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(98.3496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.3369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6898, device='cuda:0')



h[100].sum tensor(24.8265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1764, device='cuda:0')



h[200].sum tensor(-6.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.8076, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42293.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0203, 0.0188,  ..., 0.0000, 0.0121, 0.0000],
        [0.0108, 0.0203, 0.0188,  ..., 0.0000, 0.0121, 0.0000],
        [0.0108, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000],
        ...,
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000],
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000],
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334470.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1184.4694, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.2136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12099.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-214.5225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(535.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.3594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3965],
        [-0.3792],
        [-0.3446],
        ...,
        [-0.2989],
        [-0.2978],
        [-0.2974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79415.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366153.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366153.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9527e-03, -5.0127e-05,  0.0000e+00,  ...,  2.3013e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(191.7176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.6685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.6825, device='cuda:0')



h[100].sum tensor(26.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0320, device='cuda:0')



h[200].sum tensor(-7.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.6330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0142, 0.0067, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45746.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0189, 0.0171,  ..., 0.0000, 0.0118, 0.0000],
        [0.0094, 0.0101, 0.0092,  ..., 0.0000, 0.0103, 0.0000],
        [0.0068, 0.0035, 0.0028,  ..., 0.0000, 0.0071, 0.0000],
        ...,
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000],
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000],
        [0.0109, 0.0204, 0.0189,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347318.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1155.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.5020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12300.6826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-220.7588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(651.6580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.5126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0960],
        [-0.0251],
        [ 0.0442],
        ...,
        [-0.2989],
        [-0.2978],
        [-0.2974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85095.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9991],
        ...,
        [1.0003],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366153.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0003],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366163.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7284e-02,  1.5965e-02, -2.2146e-03,  ...,  2.1716e-02,
         -1.1279e-02, -1.3640e-02],
        [ 2.2210e-02,  2.1118e-02, -2.9267e-03,  ...,  2.8632e-02,
         -1.4905e-02, -1.8026e-02],
        [ 1.1970e-02,  1.0406e-02, -1.4463e-03,  ...,  1.4252e-02,
         -7.3660e-03, -8.9081e-03],
        ...,
        [ 1.9660e-03, -5.9491e-05,  0.0000e+00,  ...,  2.0231e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9660e-03, -5.9491e-05,  0.0000e+00,  ...,  2.0231e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9660e-03, -5.9491e-05,  0.0000e+00,  ...,  2.0231e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5.0044, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.9787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.9158, device='cuda:0')



h[100].sum tensor(23.8789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0304, device='cuda:0')



h[200].sum tensor(-4.7022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.0706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0993, 0.0954, 0.0000,  ..., 0.1292, 0.0000, 0.0000],
        [0.0842, 0.0796, 0.0000,  ..., 0.1080, 0.0000, 0.0000],
        [0.0715, 0.0664, 0.0000,  ..., 0.0902, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36588.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        ...,
        [0.0109, 0.0206, 0.0190,  ..., 0.0000, 0.0130, 0.0000],
        [0.0109, 0.0206, 0.0190,  ..., 0.0000, 0.0130, 0.0000],
        [0.0109, 0.0206, 0.0190,  ..., 0.0000, 0.0130, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304245.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1192.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.6414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11462.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-203.4297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(303.8217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.9946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1545],
        [-0.1392],
        [-0.0995],
        ...,
        [-0.3069],
        [-0.3058],
        [-0.3054]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83286.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0003],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366163.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366173.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8783e-03,  5.0522e-03, -7.0343e-04,  ...,  7.0458e-03,
         -3.6011e-03, -4.3561e-03],
        [ 1.3997e-02,  1.2498e-02, -1.7258e-03,  ...,  1.7043e-02,
         -8.8349e-03, -1.0687e-02],
        [ 1.9801e-03, -7.0642e-05,  0.0000e+00,  ...,  1.6750e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9801e-03, -7.0642e-05,  0.0000e+00,  ...,  1.6750e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9801e-03, -7.0642e-05,  0.0000e+00,  ...,  1.6750e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9801e-03, -7.0642e-05,  0.0000e+00,  ...,  1.6750e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(404.5135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(101.4661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-40.5742, device='cuda:0')



h[100].sum tensor(31.7653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0801, device='cuda:0')



h[200].sum tensor(-9.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-47.3412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0509, 0.0447, 0.0000,  ..., 0.0610, 0.0000, 0.0000],
        [0.0267, 0.0195, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0239, 0.0166, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54003.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0037, 0.0008, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        ...,
        [0.0109, 0.0209, 0.0193,  ..., 0.0000, 0.0139, 0.0000],
        [0.0109, 0.0209, 0.0193,  ..., 0.0000, 0.0139, 0.0000],
        [0.0109, 0.0209, 0.0192,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392447.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1115.4369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(182.3842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13080.6045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-239.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1016.4536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.6342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0439],
        [ 0.0444],
        [ 0.0260],
        ...,
        [-0.3140],
        [-0.3072],
        [-0.2936]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110329.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366173.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366182.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9925e-03, -6.9982e-05,  0.0000e+00,  ...,  1.4343e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74.2470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.3926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8741, device='cuda:0')



h[100].sum tensor(26.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1299, device='cuda:0')



h[200].sum tensor(-5.3949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38140.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0106, 0.0180, 0.0159,  ..., 0.0000, 0.0140, 0.0000],
        [0.0109, 0.0208, 0.0191,  ..., 0.0000, 0.0144, 0.0000],
        [0.0109, 0.0211, 0.0195,  ..., 0.0000, 0.0144, 0.0000],
        ...,
        [0.0110, 0.0212, 0.0195,  ..., 0.0000, 0.0145, 0.0000],
        [0.0110, 0.0212, 0.0195,  ..., 0.0000, 0.0145, 0.0000],
        [0.0110, 0.0212, 0.0195,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311329.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1162.2714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.0239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11591.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-209.5695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(351.8484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.9343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1762],
        [-0.2118],
        [-0.2373],
        ...,
        [-0.3243],
        [-0.3230],
        [-0.3226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103147.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366182.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366193.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0056e-03, -5.7208e-05,  0.0000e+00,  ...,  1.4190e-04,
          0.0000e+00,  0.0000e+00],
        [ 3.1568e-02,  3.0862e-02, -4.1894e-03,  ...,  4.1662e-02,
         -2.1671e-02, -2.6228e-02],
        [ 1.5933e-02,  1.4510e-02, -1.9737e-03,  ...,  1.9703e-02,
         -1.0210e-02, -1.2356e-02],
        ...,
        [ 1.2645e-02,  1.1071e-02, -1.5078e-03,  ...,  1.5085e-02,
         -7.7995e-03, -9.4394e-03],
        [ 9.4739e-03,  7.7539e-03, -1.0583e-03,  ...,  1.0631e-02,
         -5.4747e-03, -6.6257e-03],
        [ 1.2645e-02,  1.1071e-02, -1.5078e-03,  ...,  1.5085e-02,
         -7.7995e-03, -9.4394e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(181.7873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.1365, device='cuda:0')



h[100].sum tensor(28.6905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0858, device='cuda:0')



h[200].sum tensor(-6.5963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.6624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1070, 0.1033, 0.0000,  ..., 0.1396, 0.0000, 0.0000],
        [0.0460, 0.0396, 0.0000,  ..., 0.0540, 0.0000, 0.0000],
        [0.1079, 0.1042, 0.0000,  ..., 0.1408, 0.0000, 0.0000],
        ...,
        [0.0243, 0.0169, 0.0000,  ..., 0.0234, 0.0000, 0.0000],
        [0.0531, 0.0468, 0.0000,  ..., 0.0638, 0.0000, 0.0000],
        [0.0441, 0.0374, 0.0000,  ..., 0.0511, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41621.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326221.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1124., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.7418, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11799.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.9065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(495.7351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0956],
        [-0.0811],
        [-0.0996],
        ...,
        [-0.1027],
        [-0.0675],
        [-0.0466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111390.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366193.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366203.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0178e-03, -3.1780e-05,  0.0000e+00,  ...,  1.5609e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(108.9371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.4246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0861, device='cuda:0')



h[100].sum tensor(27.8565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2087, device='cuda:0')



h[200].sum tensor(-5.4062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7697, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39209.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0051, 0.0046,  ..., 0.0000, 0.0116, 0.0000],
        [0.0094, 0.0100, 0.0092,  ..., 0.0000, 0.0135, 0.0000],
        [0.0107, 0.0171, 0.0152,  ..., 0.0000, 0.0140, 0.0000],
        ...,
        [0.0110, 0.0212, 0.0200,  ..., 0.0000, 0.0149, 0.0000],
        [0.0110, 0.0212, 0.0200,  ..., 0.0000, 0.0149, 0.0000],
        [0.0110, 0.0212, 0.0200,  ..., 0.0000, 0.0149, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321992.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1130.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.8587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11751.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-211.6768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(466.2101, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.4812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0472],
        [-0.0539],
        [-0.0443],
        ...,
        [-0.3299],
        [-0.3287],
        [-0.3282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108345.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366203.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366214.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00],
        [2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00],
        [2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00],
        [2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00],
        [2.0292e-03, 1.2010e-06, 0.0000e+00,  ..., 1.7990e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(193.7413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.7601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.5510, device='cuda:0')



h[100].sum tensor(29.7283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4965, device='cuda:0')



h[200].sum tensor(-6.2293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.8125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.1075e-03, 4.7985e-06, 0.0000e+00,  ..., 7.1878e-04, 0.0000e+00,
         0.0000e+00],
        [8.1089e-03, 4.7993e-06, 0.0000e+00,  ..., 7.1890e-04, 0.0000e+00,
         0.0000e+00],
        [8.1113e-03, 4.8008e-06, 0.0000e+00,  ..., 7.1912e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.1639e-03, 4.8319e-06, 0.0000e+00,  ..., 7.2378e-04, 0.0000e+00,
         0.0000e+00],
        [8.1627e-03, 4.8311e-06, 0.0000e+00,  ..., 7.2367e-04, 0.0000e+00,
         0.0000e+00],
        [8.1599e-03, 4.8295e-06, 0.0000e+00,  ..., 7.2343e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43270.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0104, 0.0100,  ..., 0.0000, 0.0130, 0.0000],
        [0.0102, 0.0151, 0.0145,  ..., 0.0000, 0.0142, 0.0000],
        [0.0100, 0.0139, 0.0124,  ..., 0.0000, 0.0140, 0.0000],
        ...,
        [0.0111, 0.0210, 0.0202,  ..., 0.0000, 0.0148, 0.0000],
        [0.0111, 0.0210, 0.0202,  ..., 0.0000, 0.0148, 0.0000],
        [0.0111, 0.0210, 0.0201,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341082.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1110.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.5190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12448.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-222.3364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(543.6821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.6824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1501],
        [-0.1445],
        [-0.1172],
        ...,
        [-0.3286],
        [-0.3277],
        [-0.3274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85972.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366214.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366224.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00],
        [2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00],
        [2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00],
        [2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00],
        [2.0388e-03, 2.1237e-05, 0.0000e+00,  ..., 1.9228e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59.2749, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.3669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7256, device='cuda:0')



h[100].sum tensor(27.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5880, device='cuda:0')



h[200].sum tensor(-4.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.6819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.1460e-03, 8.4851e-05, 0.0000e+00,  ..., 7.6824e-04, 0.0000e+00,
         0.0000e+00],
        [1.2370e-02, 4.5031e-03, 0.0000e+00,  ..., 6.7017e-03, 0.0000e+00,
         0.0000e+00],
        [1.6597e-02, 8.9232e-03, 0.0000e+00,  ..., 1.2638e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.2038e-03, 8.5453e-05, 0.0000e+00,  ..., 7.7369e-04, 0.0000e+00,
         0.0000e+00],
        [8.2026e-03, 8.5441e-05, 0.0000e+00,  ..., 7.7357e-04, 0.0000e+00,
         0.0000e+00],
        [8.1998e-03, 8.5411e-05, 0.0000e+00,  ..., 7.7331e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34425.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0085, 0.0055, 0.0047,  ..., 0.0000, 0.0114, 0.0000],
        [0.0066, 0.0029, 0.0022,  ..., 0.0000, 0.0093, 0.0000],
        [0.0048, 0.0004, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        ...,
        [0.0111, 0.0211, 0.0203,  ..., 0.0000, 0.0148, 0.0000],
        [0.0111, 0.0211, 0.0203,  ..., 0.0000, 0.0148, 0.0000],
        [0.0111, 0.0211, 0.0203,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(293648.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1136.8068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.9647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11011.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-199.7650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(269.9402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.3956, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0322],
        [ 0.0624],
        [ 0.0784],
        ...,
        [-0.3302],
        [-0.3290],
        [-0.3286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109081.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366224.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366235.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3654e-02,  1.2186e-02, -1.6014e-03,  ...,  1.6510e-02,
         -8.4601e-03, -1.0249e-02],
        [ 6.8398e-03,  5.0555e-03, -6.6137e-04,  ...,  6.9343e-03,
         -3.4940e-03, -4.2327e-03],
        [ 8.8598e-03,  7.1693e-03, -9.4004e-04,  ...,  9.7730e-03,
         -4.9661e-03, -6.0162e-03],
        ...,
        [ 2.0457e-03,  3.8743e-05,  0.0000e+00,  ...,  1.9718e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0457e-03,  3.8743e-05,  0.0000e+00,  ...,  1.9718e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0457e-03,  3.8743e-05,  0.0000e+00,  ...,  1.9718e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(224.6507, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.5382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.8248, device='cuda:0')



h[100].sum tensor(30.0277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5982, device='cuda:0')



h[200].sum tensor(-6.2468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.1319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0202, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        [0.0515, 0.0455, 0.0000,  ..., 0.0616, 0.0000, 0.0000],
        [0.0274, 0.0202, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42518.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0112, 0.0214, 0.0205,  ..., 0.0000, 0.0148, 0.0000],
        [0.0112, 0.0213, 0.0204,  ..., 0.0000, 0.0148, 0.0000],
        [0.0112, 0.0213, 0.0204,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339322.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1091.3713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12041.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-216.7163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(583.2154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.9250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1011],
        [ 0.1022],
        [ 0.0969],
        ...,
        [-0.3320],
        [-0.3312],
        [-0.3311]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109873.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366235.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366246.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00],
        [2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00],
        [2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00],
        [2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00],
        [2.0555e-03, 3.7915e-05, 0.0000e+00,  ..., 1.9325e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(209.2709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.6963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5189, device='cuda:0')



h[100].sum tensor(29.7124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1129, device='cuda:0')



h[200].sum tensor(-5.8925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.6082, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0082, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0083, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0083, 0.0002, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43149.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0100, 0.0093,  ..., 0.0000, 0.0120, 0.0000],
        [0.0107, 0.0175, 0.0160,  ..., 0.0000, 0.0140, 0.0000],
        [0.0112, 0.0215, 0.0206,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0113, 0.0216, 0.0206,  ..., 0.0000, 0.0148, 0.0000],
        [0.0113, 0.0216, 0.0206,  ..., 0.0000, 0.0148, 0.0000],
        [0.0113, 0.0216, 0.0206,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347189.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1102.1268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.8694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12606.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.4418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(506.9441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.3633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0304],
        [-0.1017],
        [-0.1672],
        ...,
        [-0.3384],
        [-0.3371],
        [-0.3366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81478.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366246.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(1011.8278, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366256.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9234e-02,  1.7981e-02, -2.3362e-03,  ...,  2.4298e-02,
         -1.2474e-02, -1.5119e-02],
        [ 2.4973e-02,  2.3987e-02, -3.1173e-03,  ...,  3.2364e-02,
         -1.6644e-02, -2.0174e-02],
        [ 3.1588e-02,  3.0908e-02, -4.0174e-03,  ...,  4.1660e-02,
         -2.1451e-02, -2.5999e-02],
        ...,
        [ 2.0658e-03,  1.7992e-05,  0.0000e+00,  ...,  1.7243e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0658e-03,  1.7992e-05,  0.0000e+00,  ...,  1.7243e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0658e-03,  1.7992e-05,  0.0000e+00,  ...,  1.7243e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(404.1039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.3428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.0365, device='cuda:0')



h[100].sum tensor(33.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6503, device='cuda:0')



h[200].sum tensor(-8.2072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.7132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.5225e-02, 8.0610e-02, 0.0000e+00,  ..., 1.0886e-01, 0.0000e+00,
         0.0000e+00],
        [9.2479e-02, 8.8199e-02, 0.0000e+00,  ..., 1.1905e-01, 0.0000e+00,
         0.0000e+00],
        [1.0842e-01, 1.0488e-01, 0.0000e+00,  ..., 1.4145e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.3156e-03, 7.2424e-05, 0.0000e+00,  ..., 6.9412e-04, 0.0000e+00,
         0.0000e+00],
        [8.3143e-03, 7.2414e-05, 0.0000e+00,  ..., 6.9402e-04, 0.0000e+00,
         0.0000e+00],
        [8.3114e-03, 7.2388e-05, 0.0000e+00,  ..., 6.9378e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51149.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0220, 0.0209,  ..., 0.0000, 0.0150, 0.0000],
        [0.0113, 0.0220, 0.0209,  ..., 0.0000, 0.0150, 0.0000],
        [0.0113, 0.0220, 0.0209,  ..., 0.0000, 0.0150, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385452.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1058.0369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.1363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13433.4541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.1746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(759.8469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.4523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0303],
        [ 0.0466],
        [ 0.0588],
        ...,
        [-0.3483],
        [-0.3470],
        [-0.3465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87477.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366256.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366266.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0797e-03, -2.2391e-05,  0.0000e+00,  ...,  1.3721e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(260.5661, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.3608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.6697, device='cuda:0')



h[100].sum tensor(30.5008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9122, device='cuda:0')



h[200].sum tensor(-6.3660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.1177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0155, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44781.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0075, 0.0069, 0.0056,  ..., 0.0000, 0.0104, 0.0000],
        [0.0099, 0.0121, 0.0105,  ..., 0.0000, 0.0132, 0.0000],
        ...,
        [0.0113, 0.0225, 0.0212,  ..., 0.0000, 0.0154, 0.0000],
        [0.0113, 0.0225, 0.0212,  ..., 0.0000, 0.0154, 0.0000],
        [0.0113, 0.0225, 0.0212,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358859.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1065.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.2883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12839.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-222.6371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(514.9243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.8849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0482],
        [-0.0104],
        [-0.0848],
        ...,
        [-0.3612],
        [-0.3598],
        [-0.3588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96168.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366266.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366276.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0960e-03, -5.9640e-05,  0.0000e+00,  ...,  1.0483e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.1654e-02,  9.9353e-03, -1.2826e-03,  ...,  1.3531e-02,
         -6.9218e-03, -8.3936e-03],
        [ 9.0079e-03,  7.1681e-03, -9.2747e-04,  ...,  9.8136e-03,
         -5.0054e-03, -6.0698e-03],
        ...,
        [ 2.0960e-03, -5.9640e-05,  0.0000e+00,  ...,  1.0483e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0960e-03, -5.9640e-05,  0.0000e+00,  ...,  1.0483e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0960e-03, -5.9640e-05,  0.0000e+00,  ...,  1.0483e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(265.9735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.9658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.7414, device='cuda:0')



h[100].sum tensor(30.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9389, device='cuda:0')



h[200].sum tensor(-6.3561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.2013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0235, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        [0.0287, 0.0212, 0.0000,  ..., 0.0290, 0.0000, 0.0000],
        [0.0556, 0.0492, 0.0000,  ..., 0.0668, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46117.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0113, 0.0230, 0.0214,  ..., 0.0000, 0.0157, 0.0000],
        [0.0113, 0.0230, 0.0214,  ..., 0.0000, 0.0157, 0.0000],
        [0.0112, 0.0230, 0.0214,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372678.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1049.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.9766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13128.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.4057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(598.6449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.0561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0150],
        [-0.0036],
        [ 0.0090],
        ...,
        [-0.3716],
        [-0.3702],
        [-0.3697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105460.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366276.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366287.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1183e-03, -8.5494e-05,  0.0000e+00,  ...,  9.6600e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.6400e-03,  4.6410e-03, -6.0242e-04,  ...,  6.4461e-03,
         -3.2688e-03, -3.9648e-03],
        [ 6.6400e-03,  4.6410e-03, -6.0242e-04,  ...,  6.4461e-03,
         -3.2688e-03, -3.9648e-03],
        ...,
        [ 2.1183e-03, -8.5494e-05,  0.0000e+00,  ...,  9.6600e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1183e-03, -8.5494e-05,  0.0000e+00,  ...,  9.6600e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1183e-03, -8.5494e-05,  0.0000e+00,  ...,  9.6600e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(302.8934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.8257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.2532, device='cuda:0')



h[100].sum tensor(32.1178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5008, device='cuda:0')



h[200].sum tensor(-6.6380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.9653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0084, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0167, 0.0084, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0167, 0.0084, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45336.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0060, 0.0043,  ..., 0.0000, 0.0110, 0.0000],
        [0.0067, 0.0020, 0.0009,  ..., 0.0000, 0.0086, 0.0000],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        ...,
        [0.0114, 0.0233, 0.0217,  ..., 0.0000, 0.0160, 0.0000],
        [0.0114, 0.0233, 0.0216,  ..., 0.0000, 0.0160, 0.0000],
        [0.0114, 0.0233, 0.0216,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(361984.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1055.1570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.3298, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12941.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.9560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(498.1586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.9346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1705],
        [-0.0931],
        [-0.0340],
        ...,
        [-0.3773],
        [-0.3758],
        [-0.3753]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105217.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366287.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366298.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1433e-03, -9.9600e-05,  0.0000e+00,  ...,  1.1078e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(198.6650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.3505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.6590, device='cuda:0')



h[100].sum tensor(31.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0499, device='cuda:0')



h[200].sum tensor(-5.1364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.2713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38932.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0114, 0.0230, 0.0218,  ..., 0.0000, 0.0162, 0.0000],
        [0.0114, 0.0230, 0.0218,  ..., 0.0000, 0.0162, 0.0000],
        [0.0114, 0.0231, 0.0219,  ..., 0.0000, 0.0163, 0.0000],
        ...,
        [0.0115, 0.0232, 0.0220,  ..., 0.0000, 0.0164, 0.0000],
        [0.0115, 0.0232, 0.0220,  ..., 0.0000, 0.0164, 0.0000],
        [0.0115, 0.0232, 0.0220,  ..., 0.0000, 0.0164, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328558.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1085.7296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.1735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11737.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-209.9052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.6092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.9217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5122],
        [-0.5046],
        [-0.4883],
        ...,
        [-0.3789],
        [-0.3775],
        [-0.3770]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133817.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0007],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366298.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366308.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(297.1091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.5248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.8458, device='cuda:0')



h[100].sum tensor(34.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6060, device='cuda:0')



h[200].sum tensor(-6.0671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.1564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0271, 0.0191, 0.0000,  ..., 0.0264, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47077.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0082, 0.0093, 0.0084,  ..., 0.0000, 0.0108, 0.0000],
        [0.0110, 0.0150, 0.0140,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0118, 0.0232, 0.0224,  ..., 0.0000, 0.0166, 0.0000],
        [0.0118, 0.0232, 0.0224,  ..., 0.0000, 0.0166, 0.0000],
        [0.0118, 0.0232, 0.0224,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396417.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1153.7894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.6169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13353.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.6763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(827.8539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.4220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0120],
        [-0.0857],
        [-0.1740],
        ...,
        [-0.3778],
        [-0.3764],
        [-0.3758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111905.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9992],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366308.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9993],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366319.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(259.3114, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0334, device='cuda:0')



h[100].sum tensor(34.9824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5608, device='cuda:0')



h[200].sum tensor(-5.3908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0176, 0.0092, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42570.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0051, 0.0042,  ..., 0.0000, 0.0091, 0.0000],
        [0.0105, 0.0132, 0.0127,  ..., 0.0000, 0.0144, 0.0000],
        [0.0119, 0.0209, 0.0204,  ..., 0.0000, 0.0161, 0.0000],
        ...,
        [0.0123, 0.0232, 0.0229,  ..., 0.0000, 0.0166, 0.0000],
        [0.0123, 0.0232, 0.0229,  ..., 0.0000, 0.0166, 0.0000],
        [0.0123, 0.0232, 0.0229,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356878.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1272.8636, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.8543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12581.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(527.4205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.3931, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1156],
        [-0.2085],
        [-0.2971],
        ...,
        [-0.3735],
        [-0.3722],
        [-0.3717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99208.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9993],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366319.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9993],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366330.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(218.6975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.9680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4115, device='cuda:0')



h[100].sum tensor(35.1682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5863, device='cuda:0')



h[200].sum tensor(-4.7517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0290, 0.0209, 0.0000,  ..., 0.0290, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39439.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0115, 0.0166, 0.0167,  ..., 0.0000, 0.0153, 0.0000],
        [0.0076, 0.0107, 0.0105,  ..., 0.0000, 0.0106, 0.0000],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        ...,
        [0.0126, 0.0234, 0.0234,  ..., 0.0000, 0.0168, 0.0000],
        [0.0126, 0.0234, 0.0234,  ..., 0.0000, 0.0168, 0.0000],
        [0.0126, 0.0234, 0.0234,  ..., 0.0000, 0.0168, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335220.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1337.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.8621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12022.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-210.6308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(381.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.4472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2090],
        [-0.1169],
        [-0.0392],
        ...,
        [-0.3718],
        [-0.3705],
        [-0.3701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99147.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9992],
        [0.9993],
        ...,
        [1.0008],
        [1.0004],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366330.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9993],
        ...,
        [1.0008],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366341., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0081,  0.0060, -0.0008,  ...,  0.0084, -0.0042, -0.0051],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(227.7123, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(92.3737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.2866, device='cuda:0')



h[100].sum tensor(36.0768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5399, device='cuda:0')



h[200].sum tensor(-4.7378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0281, 0.0199, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        [0.0147, 0.0060, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0407, 0.0331, 0.0000,  ..., 0.0455, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39412.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0129, 0.0237, 0.0239,  ..., 0.0000, 0.0171, 0.0000],
        [0.0129, 0.0237, 0.0239,  ..., 0.0000, 0.0171, 0.0000],
        [0.0129, 0.0237, 0.0239,  ..., 0.0000, 0.0171, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333133.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1395.8014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.7114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12006.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-212.3345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(356.0869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.6245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0453],
        [ 0.0322],
        [ 0.0163],
        ...,
        [-0.3714],
        [-0.3703],
        [-0.3702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96760.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9993],
        ...,
        [1.0008],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366341., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9993],
        ...,
        [1.0009],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366351.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(249.4388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.5039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3974, device='cuda:0')



h[100].sum tensor(36.7505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9527, device='cuda:0')



h[200].sum tensor(-4.9548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41207.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0129, 0.0239, 0.0241,  ..., 0.0000, 0.0173, 0.0000],
        [0.0127, 0.0224, 0.0224,  ..., 0.0000, 0.0170, 0.0000],
        [0.0108, 0.0123, 0.0121,  ..., 0.0000, 0.0142, 0.0000],
        ...,
        [0.0130, 0.0241, 0.0243,  ..., 0.0000, 0.0175, 0.0000],
        [0.0130, 0.0241, 0.0243,  ..., 0.0000, 0.0175, 0.0000],
        [0.0130, 0.0241, 0.0243,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348987.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1367.5129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.4288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12184.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.6956, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(502.6090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.9808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3304],
        [-0.2303],
        [-0.1101],
        ...,
        [-0.3844],
        [-0.3831],
        [-0.3827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113756.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9993],
        ...,
        [1.0009],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366351.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(549.4536, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9994],
        ...,
        [1.0009],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366362.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0145,  0.0127, -0.0016,  ...,  0.0174, -0.0088, -0.0107],
        [ 0.0084,  0.0064, -0.0008,  ...,  0.0089, -0.0044, -0.0054],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(346.4425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(97.7042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.0506, device='cuda:0')



h[100].sum tensor(38.4351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6821, device='cuda:0')



h[200].sum tensor(-6.0678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0179, 0.0000,  ..., 0.0250, 0.0000, 0.0000],
        [0.0287, 0.0203, 0.0000,  ..., 0.0284, 0.0000, 0.0000],
        [0.0802, 0.0740, 0.0000,  ..., 0.1007, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43080.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5301e-03, 7.8240e-04, 8.5130e-05,  ..., 0.0000e+00, 5.1154e-03,
         0.0000e+00],
        [1.6421e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8308e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3241e-02, 2.4459e-02, 2.4708e-02,  ..., 0.0000e+00, 1.7798e-02,
         0.0000e+00],
        [1.3238e-02, 2.4453e-02, 2.4702e-02,  ..., 0.0000e+00, 1.7796e-02,
         0.0000e+00],
        [1.3232e-02, 2.4443e-02, 2.4691e-02,  ..., 0.0000e+00, 1.7789e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348357.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1442.2356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.5509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12433.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.9599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(422.2992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.6774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0304],
        [ 0.0533],
        [ 0.0523],
        ...,
        [-0.3936],
        [-0.3922],
        [-0.3918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100798.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9993],
        [0.9994],
        ...,
        [1.0009],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366362.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366373.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(435.9876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(101.4780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.7860, device='cuda:0')



h[100].sum tensor(39.8977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4421, device='cuda:0')



h[200].sum tensor(-7.0938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.9205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51645.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0132, 0.0247, 0.0248,  ..., 0.0000, 0.0181, 0.0000],
        [0.0132, 0.0247, 0.0248,  ..., 0.0000, 0.0181, 0.0000],
        [0.0132, 0.0247, 0.0249,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0133, 0.0249, 0.0250,  ..., 0.0000, 0.0183, 0.0000],
        [0.0133, 0.0249, 0.0250,  ..., 0.0000, 0.0183, 0.0000],
        [0.0133, 0.0249, 0.0250,  ..., 0.0000, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410200.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1378.9680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.7038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13973.2490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.6248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(833.9003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.0618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3144],
        [-0.3568],
        [-0.3940],
        ...,
        [-0.4063],
        [-0.4050],
        [-0.4044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100592.1953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366373.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366383.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0022, -0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(547.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(105.9396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.9802, device='cuda:0')



h[100].sum tensor(41.9282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3727, device='cuda:0')



h[200].sum tensor(-8.3037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.9810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0045, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0177, 0.0090, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0133, 0.0045, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54438.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.8449e-03, 9.5891e-05, 0.0000e+00,  ..., 0.0000e+00, 9.8687e-03,
         0.0000e+00],
        [9.3179e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.5730e-03,
         0.0000e+00],
        [9.3627e-03, 9.5917e-05, 0.0000e+00,  ..., 0.0000e+00, 8.9968e-03,
         0.0000e+00],
        ...,
        [1.3393e-02, 2.5371e-02, 2.5299e-02,  ..., 0.0000e+00, 1.8766e-02,
         0.0000e+00],
        [1.3391e-02, 2.5367e-02, 2.5295e-02,  ..., 0.0000e+00, 1.8764e-02,
         0.0000e+00],
        [1.3385e-02, 2.5357e-02, 2.5284e-02,  ..., 0.0000e+00, 1.8755e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423727.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1356.7262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.1003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14110.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.9142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(967.6857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-262.3020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0877],
        [ 0.0897],
        [ 0.0870],
        ...,
        [-0.4181],
        [-0.4166],
        [-0.4161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118226.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366383.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366394., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.2039e-03, -1.3491e-04,  0.0000e+00,  ...,  8.2360e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(297.0609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.4514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2125, device='cuda:0')



h[100].sum tensor(37.7414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9990, device='cuda:0')



h[200].sum tensor(-5.5120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.2507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43419.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0051, 0.0047,  ..., 0.0000, 0.0069, 0.0000],
        [0.0078, 0.0062, 0.0059,  ..., 0.0000, 0.0110, 0.0000],
        [0.0092, 0.0085, 0.0075,  ..., 0.0000, 0.0134, 0.0000],
        ...,
        [0.0135, 0.0258, 0.0255,  ..., 0.0000, 0.0192, 0.0000],
        [0.0135, 0.0258, 0.0255,  ..., 0.0000, 0.0192, 0.0000],
        [0.0135, 0.0258, 0.0255,  ..., 0.0000, 0.0192, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364186.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1380.4265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.1369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12890.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-227.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(489.3709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.8330, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0023],
        [-0.0017],
        [-0.0020],
        ...,
        [-0.4238],
        [-0.4221],
        [-0.4194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122485.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366394., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366404.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6970e-02,  3.6141e-02, -4.2837e-03,  ...,  4.8843e-02,
         -2.4694e-02, -3.0034e-02],
        [ 2.7165e-02,  2.5913e-02, -3.0758e-03,  ...,  3.5091e-02,
         -1.7731e-02, -2.1566e-02],
        [ 1.5371e-02,  1.3609e-02, -1.6228e-03,  ...,  1.8548e-02,
         -9.3548e-03, -1.1378e-02],
        ...,
        [ 2.1977e-03, -1.3219e-04,  0.0000e+00,  ...,  7.1027e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1977e-03, -1.3219e-04,  0.0000e+00,  ...,  7.1027e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1977e-03, -1.3219e-04,  0.0000e+00,  ...,  7.1027e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(292.7156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.8923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.5337, device='cuda:0')



h[100].sum tensor(37.7843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7467, device='cuda:0')



h[200].sum tensor(-5.3983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.4587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1152, 0.1105, 0.0000,  ..., 0.1495, 0.0000, 0.0000],
        [0.1065, 0.1014, 0.0000,  ..., 0.1374, 0.0000, 0.0000],
        [0.0897, 0.0839, 0.0000,  ..., 0.1138, 0.0000, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41610.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0137, 0.0262, 0.0258,  ..., 0.0000, 0.0195, 0.0000],
        [0.0137, 0.0262, 0.0258,  ..., 0.0000, 0.0195, 0.0000],
        [0.0137, 0.0262, 0.0257,  ..., 0.0000, 0.0195, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(350869.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1374.4099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.4098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12506.6357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.6954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(402.6535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.2235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0113],
        [-0.0027],
        [ 0.0060],
        ...,
        [-0.4361],
        [-0.4345],
        [-0.4340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132779.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366404.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366415.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1249e-02,  4.0627e-02, -4.7780e-03,  ...,  5.4860e-02,
         -2.7698e-02, -3.3697e-02],
        [ 1.4088e-02,  1.2292e-02, -1.4556e-03,  ...,  1.6759e-02,
         -8.4379e-03, -1.0265e-02],
        [ 2.1883e-03, -1.2137e-04,  0.0000e+00,  ...,  6.7619e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1883e-03, -1.2137e-04,  0.0000e+00,  ...,  6.7619e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1883e-03, -1.2137e-04,  0.0000e+00,  ...,  6.7619e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1883e-03, -1.2137e-04,  0.0000e+00,  ...,  6.7619e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(293.0893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.3925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.5774, device='cuda:0')



h[100].sum tensor(37.7145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7629, device='cuda:0')



h[200].sum tensor(-5.3313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0742, 0.0678, 0.0000,  ..., 0.0921, 0.0000, 0.0000],
        [0.0864, 0.0807, 0.0000,  ..., 0.1093, 0.0000, 0.0000],
        [0.0398, 0.0322, 0.0000,  ..., 0.0439, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42534.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0265, 0.0260,  ..., 0.0000, 0.0197, 0.0000],
        [0.0139, 0.0265, 0.0260,  ..., 0.0000, 0.0197, 0.0000],
        [0.0139, 0.0265, 0.0259,  ..., 0.0000, 0.0197, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359314.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1420.8268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.0087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12801.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-226.0388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(430.8918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.9257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0264],
        [-0.0152],
        [-0.0072],
        ...,
        [-0.4425],
        [-0.4410],
        [-0.4404]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126510.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366415.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366426.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1833e-03, -1.0130e-04,  0.0000e+00,  ...,  8.4527e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(571.1870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(104.9305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.5427, device='cuda:0')



h[100].sum tensor(42.6513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5817, device='cuda:0')



h[200].sum tensor(-8.1502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.6373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54860.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0141, 0.0262, 0.0258,  ..., 0.0000, 0.0196, 0.0000],
        [0.0141, 0.0262, 0.0258,  ..., 0.0000, 0.0196, 0.0000],
        [0.0141, 0.0262, 0.0259,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0142, 0.0264, 0.0261,  ..., 0.0000, 0.0197, 0.0000],
        [0.0142, 0.0264, 0.0261,  ..., 0.0000, 0.0197, 0.0000],
        [0.0142, 0.0264, 0.0261,  ..., 0.0000, 0.0197, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432318.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1443.7762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.4556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14540.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.4123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(932.1161, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.7454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5179],
        [-0.5418],
        [-0.5524],
        ...,
        [-0.4415],
        [-0.4398],
        [-0.4391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108503.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9994],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366426.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366437., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6321e-02,  1.4660e-02, -1.7046e-03,  ...,  1.9932e-02,
         -9.9937e-03, -1.2164e-02],
        [ 3.1643e-02,  3.0641e-02, -3.5519e-03,  ...,  4.1425e-02,
         -2.0825e-02, -2.5348e-02],
        [ 2.5722e-02,  2.4465e-02, -2.8380e-03,  ...,  3.3119e-02,
         -1.6639e-02, -2.0253e-02],
        ...,
        [ 2.1836e-03, -8.6965e-05,  0.0000e+00,  ...,  1.0017e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1836e-03, -8.6965e-05,  0.0000e+00,  ...,  1.0017e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1836e-03, -8.6965e-05,  0.0000e+00,  ...,  1.0017e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(517.4470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(101.9568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.2580, device='cuda:0')



h[100].sum tensor(42.5025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9892, device='cuda:0')



h[200].sum tensor(-7.3311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.6380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0810, 0.0751, 0.0000,  ..., 0.1018, 0.0000, 0.0000],
        [0.0855, 0.0798, 0.0000,  ..., 0.1081, 0.0000, 0.0000],
        [0.1037, 0.0987, 0.0000,  ..., 0.1337, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52244.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0145, 0.0264, 0.0262,  ..., 0.0000, 0.0199, 0.0000],
        [0.0145, 0.0264, 0.0262,  ..., 0.0000, 0.0198, 0.0000],
        [0.0145, 0.0263, 0.0261,  ..., 0.0000, 0.0198, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409733.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1450.4292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.9562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14162.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.0657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(751.7025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.8921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0103],
        [-0.0179],
        [-0.0278],
        ...,
        [-0.4440],
        [-0.4425],
        [-0.4419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98525.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366437., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366447.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.6907e-03,  5.6916e-03, -6.6074e-04,  ...,  7.8577e-03,
         -3.8959e-03, -4.7433e-03],
        [ 2.0459e-02,  1.9012e-02, -2.1894e-03,  ...,  2.5773e-02,
         -1.2909e-02, -1.5717e-02],
        [ 2.1718e-03, -6.6125e-05,  0.0000e+00,  ...,  1.1407e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1718e-03, -6.6125e-05,  0.0000e+00,  ...,  1.1407e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1718e-03, -6.6125e-05,  0.0000e+00,  ...,  1.1407e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1718e-03, -6.6125e-05,  0.0000e+00,  ...,  1.1407e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(723.9374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(109.3205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-41.4695, device='cuda:0')



h[100].sum tensor(46.1043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4128, device='cuda:0')



h[200].sum tensor(-9.3486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-48.3858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0673, 0.0609, 0.0000,  ..., 0.0827, 0.0000, 0.0000],
        [0.0327, 0.0249, 0.0000,  ..., 0.0342, 0.0000, 0.0000],
        [0.0481, 0.0409, 0.0000,  ..., 0.0558, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59871.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0148, 0.0264, 0.0261,  ..., 0.0000, 0.0200, 0.0000],
        [0.0148, 0.0264, 0.0261,  ..., 0.0000, 0.0200, 0.0000],
        [0.0148, 0.0264, 0.0261,  ..., 0.0000, 0.0199, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448899.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1437.0631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.9530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15092.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.8374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1044.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-278.7197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0637],
        [ 0.0636],
        [ 0.0553],
        ...,
        [-0.4447],
        [-0.4433],
        [-0.4427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94173.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366447.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366458.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1650e-03, -6.1554e-05,  0.0000e+00,  ...,  1.0971e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1650e-03, -6.1554e-05,  0.0000e+00,  ...,  1.0971e-04,
          0.0000e+00,  0.0000e+00],
        [ 8.0404e-03,  6.0679e-03, -6.9839e-04,  ...,  8.3538e-03,
         -4.1415e-03, -5.0436e-03],
        ...,
        [ 2.1650e-03, -6.1554e-05,  0.0000e+00,  ...,  1.0971e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1650e-03, -6.1554e-05,  0.0000e+00,  ...,  1.0971e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.1650e-03, -6.1554e-05,  0.0000e+00,  ...,  1.0971e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(685.4813, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(107.0436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-38.8290, device='cuda:0')



h[100].sum tensor(45.9926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.4315, device='cuda:0')



h[200].sum tensor(-8.7777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.3049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0145, 0.0061, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0285, 0.0206, 0.0000,  ..., 0.0283, 0.0000, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59387.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0129, 0.0127,  ..., 0.0000, 0.0153, 0.0000],
        [0.0078, 0.0050, 0.0045,  ..., 0.0000, 0.0080, 0.0000],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0149, 0.0265, 0.0261,  ..., 0.0000, 0.0203, 0.0000],
        [0.0149, 0.0265, 0.0261,  ..., 0.0000, 0.0203, 0.0000],
        [0.0149, 0.0265, 0.0261,  ..., 0.0000, 0.0203, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455700.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1439.6174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(242.5304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15151.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-263.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1099.3063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-275.6563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1070],
        [-0.0027],
        [ 0.0575],
        ...,
        [-0.4515],
        [-0.4500],
        [-0.4494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100166.2266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366458.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(565.5804, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366468.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(417.1353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9750, device='cuda:0')



h[100].sum tensor(42.1895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6540, device='cuda:0')



h[200].sum tensor(-5.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48644.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0203, 0.0186,  ..., 0.0000, 0.0183, 0.0000],
        [0.0147, 0.0236, 0.0222,  ..., 0.0000, 0.0191, 0.0000],
        [0.0129, 0.0138, 0.0124,  ..., 0.0000, 0.0160, 0.0000],
        ...,
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000],
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000],
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409456.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1445.2212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.4561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14095.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.5219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(752.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.7554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0421],
        [-0.0504],
        [-0.0264],
        ...,
        [-0.4613],
        [-0.4598],
        [-0.4592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103759.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366468.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366468.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1576e-03, -6.2696e-05,  0.0000e+00,  ...,  9.3186e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(344.4125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(92.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0877, device='cuda:0')



h[100].sum tensor(40.9967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5809, device='cuda:0')



h[200].sum tensor(-5.1072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41943.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0133, 0.0129,  ..., 0.0000, 0.0151, 0.0000],
        [0.0128, 0.0139, 0.0119,  ..., 0.0000, 0.0171, 0.0000],
        [0.0105, 0.0098, 0.0090,  ..., 0.0000, 0.0123, 0.0000],
        ...,
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000],
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000],
        [0.0149, 0.0268, 0.0261,  ..., 0.0000, 0.0207, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353951., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1474.9382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.3376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12615.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-226.5767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(394.5603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.0936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0902],
        [-0.0675],
        [-0.0289],
        ...,
        [-0.4589],
        [-0.4562],
        [-0.4488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123953.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366468.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366478.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5601e-03,  6.6307e-03, -7.5157e-04,  ...,  9.0836e-03,
         -4.5082e-03, -5.4929e-03],
        [ 7.2470e-03,  5.2609e-03, -5.9770e-04,  ...,  7.2408e-03,
         -3.5852e-03, -4.3684e-03],
        [ 1.3661e-02,  1.1952e-02, -1.3493e-03,  ...,  1.6242e-02,
         -8.0934e-03, -9.8613e-03],
        ...,
        [ 2.1466e-03, -6.0151e-05,  0.0000e+00,  ...,  8.2646e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1466e-03, -6.0151e-05,  0.0000e+00,  ...,  8.2646e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1466e-03, -6.0151e-05,  0.0000e+00,  ...,  8.2646e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(274.8841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.4364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.3747, device='cuda:0')



h[100].sum tensor(40.3070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2009, device='cuda:0')



h[200].sum tensor(-4.2940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.6061, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0198, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0503, 0.0433, 0.0000,  ..., 0.0589, 0.0000, 0.0000],
        [0.0277, 0.0197, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38810.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0149, 0.0271, 0.0261,  ..., 0.0000, 0.0210, 0.0000],
        [0.0149, 0.0271, 0.0261,  ..., 0.0000, 0.0210, 0.0000],
        [0.0149, 0.0271, 0.0261,  ..., 0.0000, 0.0210, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(340858.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1490.2592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.4543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12322.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-222.2615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(272.7511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.8604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0477],
        [ 0.0595],
        [ 0.0426],
        ...,
        [-0.4628],
        [-0.4525],
        [-0.4206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122742.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366478.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366488.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1300e-03, -4.8573e-05,  0.0000e+00,  ...,  8.0142e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1300e-03, -4.8573e-05,  0.0000e+00,  ...,  8.0142e-05,
          0.0000e+00,  0.0000e+00],
        [ 9.1704e-03,  7.2976e-03, -8.1921e-04,  ...,  9.9631e-03,
         -4.9423e-03, -6.0234e-03],
        ...,
        [ 2.1300e-03, -4.8573e-05,  0.0000e+00,  ...,  8.0142e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1300e-03, -4.8573e-05,  0.0000e+00,  ...,  8.0142e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1300e-03, -4.8573e-05,  0.0000e+00,  ...,  8.0142e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(526.1289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(98.4924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.9958, device='cuda:0')



h[100].sum tensor(44.4296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5201, device='cuda:0')



h[200].sum tensor(-6.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.1653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0291, 0.0215, 0.0000,  ..., 0.0293, 0.0000, 0.0000],
        [0.0155, 0.0073, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0207, 0.0126, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49757.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0151, 0.0274, 0.0262,  ..., 0.0000, 0.0212, 0.0000],
        [0.0151, 0.0274, 0.0262,  ..., 0.0000, 0.0212, 0.0000],
        [0.0151, 0.0273, 0.0262,  ..., 0.0000, 0.0212, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402799., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1406.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.7218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13687.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.6487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(737.4868, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.2584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0362],
        [-0.0179],
        [ 0.0160],
        ...,
        [-0.4771],
        [-0.4755],
        [-0.4749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128738.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9995],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366488.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366499.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 9.4617e-03,  7.6371e-03, -8.4923e-04,  ...,  1.0401e-02,
         -5.1531e-03, -6.2819e-03],
        ...,
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(584.8654, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.9082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.9871, device='cuda:0')



h[100].sum tensor(45.2549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2602, device='cuda:0')



h[200].sum tensor(-7.2792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.4888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0158, 0.0076, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0144, 0.0062, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53960.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0142, 0.0188, 0.0162,  ..., 0.0000, 0.0181, 0.0000],
        [0.0117, 0.0085, 0.0062,  ..., 0.0000, 0.0129, 0.0000],
        [0.0097, 0.0034, 0.0016,  ..., 0.0000, 0.0088, 0.0000],
        ...,
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000],
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000],
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432775.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1382.0986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.9720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14490.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.5363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(931.8450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.9998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2009],
        [-0.0829],
        [ 0.0114],
        ...,
        [-0.4842],
        [-0.4826],
        [-0.4820]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121080.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366499.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366499.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.1115e-03, -3.4020e-05,  0.0000e+00,  ...,  7.9947e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(731.6122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(105.6566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-39.2151, device='cuda:0')



h[100].sum tensor(47.6342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5750, device='cuda:0')



h[200].sum tensor(-8.7651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.7555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58613.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0273, 0.0259,  ..., 0.0000, 0.0211, 0.0000],
        [0.0152, 0.0273, 0.0259,  ..., 0.0000, 0.0211, 0.0000],
        [0.0152, 0.0274, 0.0260,  ..., 0.0000, 0.0211, 0.0000],
        ...,
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000],
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000],
        [0.0153, 0.0276, 0.0262,  ..., 0.0000, 0.0213, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447802.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1417.2605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.6216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14849.7080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-270.6912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1011.4700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.2693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6019],
        [-0.5911],
        [-0.5630],
        ...,
        [-0.4802],
        [-0.4799],
        [-0.4797]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111982.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366499.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366509.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1344e-03,  6.2931e-03, -6.9331e-04,  ...,  8.5707e-03,
         -4.2315e-03, -5.1597e-03],
        [ 2.0912e-03, -1.6013e-05,  0.0000e+00,  ...,  8.2281e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0912e-03, -1.6013e-05,  0.0000e+00,  ...,  8.2281e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0912e-03, -1.6013e-05,  0.0000e+00,  ...,  8.2281e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0912e-03, -1.6013e-05,  0.0000e+00,  ...,  8.2281e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0912e-03, -1.6013e-05,  0.0000e+00,  ...,  8.2281e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(428.5207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(92.8675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.6403, device='cuda:0')



h[100].sum tensor(42.4397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5296, device='cuda:0')



h[200].sum tensor(-5.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.9166, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0210, 0.0000,  ..., 0.0286, 0.0000, 0.0000],
        [0.0144, 0.0063, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44937.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0077, 0.0055, 0.0045,  ..., 0.0000, 0.0084, 0.0000],
        [0.0123, 0.0116, 0.0103,  ..., 0.0000, 0.0156, 0.0000],
        ...,
        [0.0156, 0.0279, 0.0263,  ..., 0.0000, 0.0214, 0.0000],
        [0.0156, 0.0279, 0.0263,  ..., 0.0000, 0.0214, 0.0000],
        [0.0156, 0.0279, 0.0263,  ..., 0.0000, 0.0214, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372554.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1532.4657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.8382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13016.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.4319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(473.6834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.4174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0294],
        [-0.0390],
        [-0.1352],
        ...,
        [-0.4917],
        [-0.4901],
        [-0.4895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124449.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366509.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366520.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7191e-02,  3.6687e-02, -4.0005e-03,  ...,  4.9442e-02,
         -2.4559e-02, -2.9954e-02],
        [ 2.0725e-03,  1.0904e-05,  0.0000e+00,  ...,  9.6267e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.2886e-02,  2.1747e-02, -2.3710e-03,  ...,  2.9342e-02,
         -1.4555e-02, -1.7753e-02],
        ...,
        [ 2.0725e-03,  1.0904e-05,  0.0000e+00,  ...,  9.6267e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0725e-03,  1.0904e-05,  0.0000e+00,  ...,  9.6267e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0725e-03,  1.0904e-05,  0.0000e+00,  ...,  9.6267e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(516.8652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.2821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.8398, device='cuda:0')



h[100].sum tensor(43.5059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7188, device='cuda:0')



h[200].sum tensor(-6.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.6498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9988e-02, 6.4487e-02, 0.0000e+00,  ..., 8.7090e-02, 0.0000e+00,
         0.0000e+00],
        [9.8130e-02, 9.3875e-02, 0.0000e+00,  ..., 1.2663e-01, 0.0000e+00,
         0.0000e+00],
        [2.5275e-02, 1.7785e-02, 0.0000e+00,  ..., 2.4255e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.3716e-03, 4.4045e-05, 0.0000e+00,  ..., 3.8886e-04, 0.0000e+00,
         0.0000e+00],
        [8.3707e-03, 4.4040e-05, 0.0000e+00,  ..., 3.8882e-04, 0.0000e+00,
         0.0000e+00],
        [8.3676e-03, 4.4024e-05, 0.0000e+00,  ..., 3.8867e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49054.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        ...,
        [0.0161, 0.0280, 0.0263,  ..., 0.0000, 0.0214, 0.0000],
        [0.0161, 0.0280, 0.0263,  ..., 0.0000, 0.0214, 0.0000],
        [0.0161, 0.0280, 0.0262,  ..., 0.0000, 0.0214, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403125.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1548.2650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.8009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13695.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.1053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(735.5507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.6694, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0117],
        [-0.0222],
        [-0.0725],
        ...,
        [-0.4963],
        [-0.4946],
        [-0.4940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128719.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366520.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366531., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0616e-03,  2.8120e-05,  0.0000e+00,  ...,  1.0817e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0616e-03,  2.8120e-05,  0.0000e+00,  ...,  1.0817e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.1906e-02,  1.0311e-02, -1.1134e-03,  ...,  1.3944e-02,
         -6.8751e-03, -8.3876e-03],
        ...,
        [ 2.0616e-03,  2.8120e-05,  0.0000e+00,  ...,  1.0817e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0616e-03,  2.8120e-05,  0.0000e+00,  ...,  1.0817e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0616e-03,  2.8120e-05,  0.0000e+00,  ...,  1.0817e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(640.5524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.1993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.5167, device='cuda:0')



h[100].sum tensor(45.4785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4570, device='cuda:0')



h[200].sum tensor(-7.4547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0181, 0.0104, 0.0000,  ..., 0.0143, 0.0000, 0.0000],
        [0.0424, 0.0358, 0.0000,  ..., 0.0485, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56236.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0093, 0.0078,  ..., 0.0000, 0.0106, 0.0000],
        [0.0052, 0.0042, 0.0033,  ..., 0.0000, 0.0053, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000],
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000],
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444305.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1589.5542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.1354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14683.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-261.9775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(984.7856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.9430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0120],
        [ 0.0204],
        [ 0.0159],
        ...,
        [-0.5002],
        [-0.4986],
        [-0.4980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115402.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366531., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366531., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00],
        [2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00],
        [2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00],
        [2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00],
        [2.0616e-03, 2.8120e-05, 0.0000e+00,  ..., 1.0817e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(300.9705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.8876, device='cuda:0')



h[100].sum tensor(40.0415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0199, device='cuda:0')



h[200].sum tensor(-4.1003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.0377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0296, 0.0224, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0083, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38841.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0153, 0.0219, 0.0198,  ..., 0.0000, 0.0198, 0.0000],
        [0.0121, 0.0139, 0.0129,  ..., 0.0000, 0.0149, 0.0000],
        [0.0059, 0.0009, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000],
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000],
        [0.0165, 0.0281, 0.0262,  ..., 0.0000, 0.0214, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343396.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1664.0708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(311.2537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12278.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.8644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(283.5151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.7358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3526],
        [-0.2129],
        [-0.0778],
        ...,
        [-0.4271],
        [-0.4778],
        [-0.4923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130610.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366531., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(368.2456, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366541.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0501e-03,  3.9971e-05,  0.0000e+00,  ...,  1.1466e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0501e-03,  3.9971e-05,  0.0000e+00,  ...,  1.1466e-04,
          0.0000e+00,  0.0000e+00],
        [ 8.0240e-03,  6.2811e-03, -6.7076e-04,  ...,  8.5127e-03,
         -4.1662e-03, -5.0840e-03],
        ...,
        [ 2.0501e-03,  3.9971e-05,  0.0000e+00,  ...,  1.1466e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0501e-03,  3.9971e-05,  0.0000e+00,  ...,  1.1466e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0501e-03,  3.9971e-05,  0.0000e+00,  ...,  1.1466e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(338.2417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.8557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0888, device='cuda:0')



h[100].sum tensor(40.4646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4664, device='cuda:0')



h[200].sum tensor(-4.3814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0002, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0142, 0.0064, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0131, 0.0053, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0002, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0083, 0.0002, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0083, 0.0002, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40093.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0217, 0.0191,  ..., 0.0000, 0.0193, 0.0000],
        [0.0141, 0.0123, 0.0100,  ..., 0.0000, 0.0148, 0.0000],
        [0.0133, 0.0062, 0.0044,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0170, 0.0281, 0.0262,  ..., 0.0000, 0.0215, 0.0000],
        [0.0170, 0.0281, 0.0262,  ..., 0.0000, 0.0215, 0.0000],
        [0.0170, 0.0276, 0.0256,  ..., 0.0000, 0.0213, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349690.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1730.6021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.5710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12520.6416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-225.0680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(303.1420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.1556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2824],
        [-0.1766],
        [-0.0655],
        ...,
        [-0.4909],
        [-0.4664],
        [-0.4333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124433.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9996],
        ...,
        [1.0010],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366541.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0010],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366551.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5157e-03,  6.8022e-03, -7.2214e-04,  ...,  9.2165e-03,
         -4.5118e-03, -5.5072e-03],
        [ 2.0373e-03,  3.3035e-05,  0.0000e+00,  ...,  1.0748e-04,
          0.0000e+00,  0.0000e+00],
        [ 8.5157e-03,  6.8022e-03, -7.2214e-04,  ...,  9.2165e-03,
         -4.5118e-03, -5.5072e-03],
        ...,
        [ 2.0373e-03,  3.3035e-05,  0.0000e+00,  ...,  1.0748e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0373e-03,  3.3035e-05,  0.0000e+00,  ...,  1.0748e-04,
          0.0000e+00,  0.0000e+00],
        [ 2.0373e-03,  3.3035e-05,  0.0000e+00,  ...,  1.0748e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(727.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(101.2887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.8853, device='cuda:0')



h[100].sum tensor(46.4601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0807, device='cuda:0')



h[200].sum tensor(-8.1413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.2039, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0281, 0.0209, 0.0000,  ..., 0.0284, 0.0000, 0.0000],
        [0.0411, 0.0346, 0.0000,  ..., 0.0468, 0.0000, 0.0000],
        [0.0182, 0.0106, 0.0000,  ..., 0.0145, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0082, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59280.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        ...,
        [0.0174, 0.0283, 0.0262,  ..., 0.0000, 0.0218, 0.0000],
        [0.0174, 0.0283, 0.0262,  ..., 0.0000, 0.0218, 0.0000],
        [0.0174, 0.0283, 0.0262,  ..., 0.0000, 0.0218, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455951.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1663.8000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.5018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15356.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(941.1545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.0863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1105],
        [ 0.0995],
        [ 0.0634],
        ...,
        [-0.5164],
        [-0.5146],
        [-0.5140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91577.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0010],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366551.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0010],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366561.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0304e-03, -5.2328e-06,  0.0000e+00,  ...,  8.0113e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(576.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.6888, device='cuda:0')



h[100].sum tensor(44.3290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4060, device='cuda:0')



h[200].sum tensor(-6.6342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.8072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0136, 0.0057, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50501.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0193, 0.0156,  ..., 0.0000, 0.0180, 0.0000],
        [0.0156, 0.0151, 0.0126,  ..., 0.0000, 0.0171, 0.0000],
        [0.0123, 0.0093, 0.0076,  ..., 0.0000, 0.0118, 0.0000],
        ...,
        [0.0172, 0.0267, 0.0242,  ..., 0.0000, 0.0217, 0.0000],
        [0.0175, 0.0286, 0.0263,  ..., 0.0000, 0.0223, 0.0000],
        [0.0175, 0.0286, 0.0263,  ..., 0.0000, 0.0223, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408612.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1735.8529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.0333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13888.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.4502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(688.2164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.6792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0623],
        [-0.0594],
        [-0.0419],
        ...,
        [-0.4770],
        [-0.5122],
        [-0.5257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130772.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0010],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366561.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366570.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0344e-03, -4.5565e-05,  0.0000e+00,  ...,  5.0688e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.1124e-02,  9.4468e-03, -9.9809e-04,  ...,  1.2827e-02,
         -6.3101e-03, -7.7062e-03],
        [ 9.8415e-03,  8.1074e-03, -8.5725e-04,  ...,  1.1024e-02,
         -5.4197e-03, -6.6188e-03],
        ...,
        [ 2.0344e-03, -4.5565e-05,  0.0000e+00,  ...,  5.0688e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0344e-03, -4.5565e-05,  0.0000e+00,  ...,  5.0688e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0344e-03, -4.5565e-05,  0.0000e+00,  ...,  5.0688e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(295.7527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.2811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6955, device='cuda:0')



h[100].sum tensor(40.6975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5769, device='cuda:0')



h[200].sum tensor(-3.8178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.6468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0094, 0.0000,  ..., 0.0130, 0.0000, 0.0000],
        [0.0297, 0.0225, 0.0000,  ..., 0.0305, 0.0000, 0.0000],
        [0.0615, 0.0556, 0.0000,  ..., 0.0753, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37860.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0173, 0.0288, 0.0263,  ..., 0.0000, 0.0227, 0.0000],
        [0.0173, 0.0288, 0.0262,  ..., 0.0000, 0.0227, 0.0000],
        [0.0173, 0.0288, 0.0262,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343020.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1726.8622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.9469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12244.4170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-228.0135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(243.5978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.8632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0325],
        [-0.0079],
        [ 0.0149],
        ...,
        [-0.5417],
        [-0.5395],
        [-0.5363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152404.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366570.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0009],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366580.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3694e-02,  1.2094e-02, -1.2697e-03,  ...,  1.6414e-02,
         -8.0750e-03, -9.8642e-03],
        [ 2.1821e-02,  2.0579e-02, -2.1553e-03,  ...,  2.7835e-02,
         -1.3707e-02, -1.6744e-02],
        [ 2.2631e-02,  2.1425e-02, -2.2436e-03,  ...,  2.8975e-02,
         -1.4269e-02, -1.7431e-02],
        ...,
        [ 2.0423e-03, -7.0010e-05,  0.0000e+00,  ...,  3.9949e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0423e-03, -7.0010e-05,  0.0000e+00,  ...,  3.9949e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0423e-03, -7.0010e-05,  0.0000e+00,  ...,  3.9949e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(428.7385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.0182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1428, device='cuda:0')



h[100].sum tensor(43.6329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6014, device='cuda:0')



h[200].sum tensor(-4.9244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.0026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0696, 0.0638, 0.0000,  ..., 0.0865, 0.0000, 0.0000],
        [0.0900, 0.0852, 0.0000,  ..., 0.1152, 0.0000, 0.0000],
        [0.0805, 0.0752, 0.0000,  ..., 0.1018, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44038.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0288, 0.0263,  ..., 0.0000, 0.0230, 0.0000],
        [0.0171, 0.0288, 0.0262,  ..., 0.0000, 0.0230, 0.0000],
        [0.0171, 0.0288, 0.0262,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383676.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1596.5438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.5181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13242.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.1825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(527.3140, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.9887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0324],
        [ 0.0226],
        [ 0.0036],
        ...,
        [-0.5481],
        [-0.5462],
        [-0.5455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149986.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9997],
        ...,
        [1.0009],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366580.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9998],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366590., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7099e-02,  1.5632e-02, -1.6279e-03,  ...,  2.1194e-02,
         -1.0415e-02, -1.2726e-02],
        [ 1.0884e-02,  9.1438e-03, -9.5571e-04,  ...,  1.2460e-02,
         -6.1145e-03, -7.4712e-03],
        [ 3.3318e-02,  3.2562e-02, -3.3819e-03,  ...,  4.3985e-02,
         -2.1637e-02, -2.6438e-02],
        ...,
        [ 2.0466e-03, -8.0621e-05,  0.0000e+00,  ...,  4.1969e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0466e-03, -8.0621e-05,  0.0000e+00,  ...,  4.1969e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0466e-03, -8.0621e-05,  0.0000e+00,  ...,  4.1969e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(474.7807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.2699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.3138, device='cuda:0')



h[100].sum tensor(45.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0366, device='cuda:0')



h[200].sum tensor(-5.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.3688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0525, 0.0460, 0.0000,  ..., 0.0625, 0.0000, 0.0000],
        [0.0862, 0.0812, 0.0000,  ..., 0.1099, 0.0000, 0.0000],
        [0.0425, 0.0356, 0.0000,  ..., 0.0485, 0.0000, 0.0000],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0083, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44340.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0288, 0.0262,  ..., 0.0000, 0.0232, 0.0000],
        [0.0171, 0.0288, 0.0262,  ..., 0.0000, 0.0232, 0.0000],
        [0.0171, 0.0288, 0.0262,  ..., 0.0000, 0.0232, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375027.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1559.5906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.2561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13193.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-251.5076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(440.3875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.0934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1564],
        [-0.1174],
        [-0.0903],
        ...,
        [-0.5524],
        [-0.5506],
        [-0.5499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137951.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9998],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366590., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9998],
        ...,
        [1.0008],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366600.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0395e-03, -7.0645e-05,  0.0000e+00,  ...,  6.1168e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(447.7020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.2814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1826, device='cuda:0')



h[100].sum tensor(44.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2445, device='cuda:0')



h[200].sum tensor(-4.7271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0073, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42247.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0069, 0.0049,  ..., 0.0000, 0.0106, 0.0000],
        [0.0151, 0.0152, 0.0128,  ..., 0.0000, 0.0182, 0.0000],
        [0.0168, 0.0257, 0.0228,  ..., 0.0000, 0.0218, 0.0000],
        ...,
        [0.0173, 0.0286, 0.0261,  ..., 0.0000, 0.0232, 0.0000],
        [0.0173, 0.0286, 0.0261,  ..., 0.0000, 0.0232, 0.0000],
        [0.0173, 0.0286, 0.0261,  ..., 0.0000, 0.0232, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360626.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1591.2421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.9353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12887.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.0168, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(346.1018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.4622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0419],
        [-0.1208],
        [-0.1885],
        ...,
        [-0.5532],
        [-0.5513],
        [-0.5506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131209.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9998],
        ...,
        [1.0008],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366600.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9999],
        ...,
        [1.0008],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366610.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9194e-03,  5.0818e-03, -5.2279e-04,  ...,  6.9865e-03,
         -3.3851e-03, -4.1383e-03],
        [ 6.9194e-03,  5.0818e-03, -5.2279e-04,  ...,  6.9865e-03,
         -3.3851e-03, -4.1383e-03],
        [ 2.0129e-03, -4.1632e-05,  0.0000e+00,  ...,  8.9267e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.0129e-03, -4.1632e-05,  0.0000e+00,  ...,  8.9267e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0129e-03, -4.1632e-05,  0.0000e+00,  ...,  8.9267e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0129e-03, -4.1632e-05,  0.0000e+00,  ...,  8.9267e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(956.1865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(105.7269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-43.7445, device='cuda:0')



h[100].sum tensor(52.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2584, device='cuda:0')



h[200].sum tensor(-9.2636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-51.0403, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0392, 0.0324, 0.0000,  ..., 0.0442, 0.0000, 0.0000],
        [0.0265, 0.0191, 0.0000,  ..., 0.0263, 0.0000, 0.0000],
        [0.0449, 0.0383, 0.0000,  ..., 0.0522, 0.0000, 0.0000],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66694.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [9.9345e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7845e-02, 2.8396e-02, 2.5960e-02,  ..., 0.0000e+00, 2.3108e-02,
         0.0000e+00],
        [1.7842e-02, 2.8392e-02, 2.5956e-02,  ..., 0.0000e+00, 2.3105e-02,
         0.0000e+00],
        [1.7836e-02, 2.8380e-02, 2.5943e-02,  ..., 0.0000e+00, 2.3096e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513199.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1548.2659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.2671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16207.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-305.0058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1527.2029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-314.2267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0518],
        [ 0.0497],
        [ 0.0441],
        ...,
        [-0.5523],
        [-0.5506],
        [-0.5500]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131006.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9995],
        [0.9999],
        ...,
        [1.0008],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366610.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366621.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9790e-03, -9.7644e-06,  0.0000e+00,  ...,  1.1294e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(645.2444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(92.5736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.6541, device='cuda:0')



h[100].sum tensor(47.2715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0214, device='cuda:0')



h[200].sum tensor(-6.2627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.5998, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0296, 0.0227, 0.0000,  ..., 0.0310, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50319.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0081, 0.0046,  ..., 0.0000, 0.0166, 0.0000],
        [0.0105, 0.0081, 0.0063,  ..., 0.0000, 0.0101, 0.0000],
        [0.0048, 0.0006, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        ...,
        [0.0185, 0.0283, 0.0258,  ..., 0.0000, 0.0230, 0.0000],
        [0.0185, 0.0283, 0.0258,  ..., 0.0000, 0.0230, 0.0000],
        [0.0185, 0.0282, 0.0258,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403905.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1731.3235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.0019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14032.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.7990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(681.3483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.7575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0041],
        [ 0.0016],
        [ 0.0021],
        ...,
        [-0.5012],
        [-0.5177],
        [-0.5333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114835.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366621.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366631.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9446e-03,  1.9551e-05,  0.0000e+00,  ...,  1.2367e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9446e-03,  1.9551e-05,  0.0000e+00,  ...,  1.2367e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9446e-03,  1.9551e-05,  0.0000e+00,  ...,  1.2367e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5194e-02,  1.3865e-02, -1.3912e-03,  ...,  1.8761e-02,
         -9.1173e-03, -1.1152e-02],
        [ 1.9446e-03,  1.9551e-05,  0.0000e+00,  ...,  1.2367e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9446e-03,  1.9551e-05,  0.0000e+00,  ...,  1.2367e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(792.9222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.7635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.5098, device='cuda:0')



h[100].sum tensor(48.7212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1978, device='cuda:0')



h[200].sum tensor(-7.5600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.4322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.7710e-03, 7.8126e-05, 0.0000e+00,  ..., 4.9420e-04, 0.0000e+00,
         0.0000e+00],
        [7.7734e-03, 7.8151e-05, 0.0000e+00,  ..., 4.9435e-04, 0.0000e+00,
         0.0000e+00],
        [7.7771e-03, 7.8188e-05, 0.0000e+00,  ..., 4.9459e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.9494e-02, 2.2679e-02, 0.0000e+00,  ..., 3.0922e-02, 0.0000e+00,
         0.0000e+00],
        [2.6705e-02, 1.9765e-02, 0.0000e+00,  ..., 2.7000e-02, 0.0000e+00,
         0.0000e+00],
        [7.8636e-03, 7.9057e-05, 0.0000e+00,  ..., 5.0009e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59127.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0179, 0.0213, 0.0186,  ..., 0.0000, 0.0205, 0.0000],
        [0.0190, 0.0280, 0.0255,  ..., 0.0000, 0.0226, 0.0000],
        [0.0190, 0.0280, 0.0255,  ..., 0.0000, 0.0227, 0.0000],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0060, 0.0007, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0139, 0.0133, 0.0119,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465436.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1809.8231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.2622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15505.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.4882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1115.4703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.1894, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1763],
        [-0.2741],
        [-0.3421],
        ...,
        [-0.0018],
        [-0.0673],
        [-0.1939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109464.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366631.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(530.4884, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366641.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8187e-03,  5.1524e-03, -5.1109e-04,  ...,  7.0200e-03,
         -3.3700e-03, -4.1230e-03],
        [ 8.2041e-03,  6.6003e-03, -6.5546e-04,  ...,  8.9690e-03,
         -4.3219e-03, -5.2877e-03],
        [ 1.9145e-03,  2.6461e-05,  0.0000e+00,  ...,  1.2018e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9145e-03,  2.6461e-05,  0.0000e+00,  ...,  1.2018e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9145e-03,  2.6461e-05,  0.0000e+00,  ...,  1.2018e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9145e-03,  2.6461e-05,  0.0000e+00,  ...,  1.2018e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(643.5225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.3090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.8201, device='cuda:0')



h[100].sum tensor(45.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0832, device='cuda:0')



h[200].sum tensor(-6.1992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0562, 0.0509, 0.0000,  ..., 0.0688, 0.0000, 0.0000],
        [0.0276, 0.0209, 0.0000,  ..., 0.0285, 0.0000, 0.0000],
        [0.0139, 0.0067, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0001, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0077, 0.0001, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0077, 0.0001, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49910.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0099, 0.0055, 0.0043,  ..., 0.0000, 0.0085, 0.0000],
        ...,
        [0.0198, 0.0285, 0.0258,  ..., 0.0000, 0.0229, 0.0000],
        [0.0198, 0.0285, 0.0258,  ..., 0.0000, 0.0229, 0.0000],
        [0.0198, 0.0285, 0.0258,  ..., 0.0000, 0.0229, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402978.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1929.4954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.2173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14220.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.3157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(640.0736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.7366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0474],
        [ 0.0210],
        [-0.0352],
        ...,
        [-0.5687],
        [-0.5670],
        [-0.5663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114151.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [0.9999],
        ...,
        [1.0007],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366641.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366651.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9048e-03,  3.4811e-06,  0.0000e+00,  ...,  1.0452e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9048e-03,  3.4811e-06,  0.0000e+00,  ...,  1.0452e-04,
          0.0000e+00,  0.0000e+00],
        [ 6.8190e-03,  5.1389e-03, -5.0817e-04,  ...,  7.0174e-03,
         -3.3711e-03, -4.1255e-03],
        ...,
        [ 1.9048e-03,  3.4811e-06,  0.0000e+00,  ...,  1.0452e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9048e-03,  3.4811e-06,  0.0000e+00,  ...,  1.0452e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.9048e-03,  3.4811e-06,  0.0000e+00,  ...,  1.0452e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(828.5418, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.8684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.4561, device='cuda:0')



h[100].sum tensor(47.8270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9212, device='cuda:0')



h[200].sum tensor(-7.8591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.7030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1595e-02, 4.1763e-03, 0.0000e+00,  ..., 6.0208e-03, 0.0000e+00,
         0.0000e+00],
        [2.8235e-02, 2.1563e-02, 0.0000e+00,  ..., 2.9426e-02, 0.0000e+00,
         0.0000e+00],
        [4.3287e-02, 3.7289e-02, 0.0000e+00,  ..., 5.0594e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.7082e-03, 1.4087e-05, 0.0000e+00,  ..., 4.2298e-04, 0.0000e+00,
         0.0000e+00],
        [7.7073e-03, 1.4086e-05, 0.0000e+00,  ..., 4.2293e-04, 0.0000e+00,
         0.0000e+00],
        [7.7045e-03, 1.4081e-05, 0.0000e+00,  ..., 4.2278e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60205.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0117, 0.0059, 0.0037,  ..., 0.0000, 0.0082, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000],
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000],
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467234.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1913.3370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.2328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15832.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-296.8998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1050.8230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.2852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0446],
        [ 0.0210],
        [ 0.0512],
        ...,
        [-0.5835],
        [-0.5817],
        [-0.5810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109916.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366651.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366651.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00],
        [1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00],
        [1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00],
        [1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00],
        [1.9048e-03, 3.4811e-06, 0.0000e+00,  ..., 1.0452e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(501.2941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.8772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.6111, device='cuda:0')



h[100].sum tensor(42.8156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7755, device='cuda:0')



h[200].sum tensor(-4.9274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.6117e-03, 1.3911e-05, 0.0000e+00,  ..., 4.1769e-04, 0.0000e+00,
         0.0000e+00],
        [7.6142e-03, 1.3916e-05, 0.0000e+00,  ..., 4.1782e-04, 0.0000e+00,
         0.0000e+00],
        [7.6179e-03, 1.3922e-05, 0.0000e+00,  ..., 4.1803e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.7082e-03, 1.4087e-05, 0.0000e+00,  ..., 4.2298e-04, 0.0000e+00,
         0.0000e+00],
        [7.7073e-03, 1.4086e-05, 0.0000e+00,  ..., 4.2293e-04, 0.0000e+00,
         0.0000e+00],
        [7.7045e-03, 1.4081e-05, 0.0000e+00,  ..., 4.2278e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44915.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0200, 0.0284, 0.0256,  ..., 0.0000, 0.0228, 0.0000],
        [0.0200, 0.0285, 0.0256,  ..., 0.0000, 0.0228, 0.0000],
        [0.0200, 0.0285, 0.0257,  ..., 0.0000, 0.0228, 0.0000],
        ...,
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000],
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000],
        [0.0202, 0.0288, 0.0260,  ..., 0.0000, 0.0231, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380750.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2037.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.9989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13536.2012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.5833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(489.0131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.0592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6443],
        [-0.5618],
        [-0.4483],
        ...,
        [-0.5835],
        [-0.5817],
        [-0.5810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135908.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366651.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366661.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8935e-03, -1.7857e-05,  0.0000e+00,  ...,  8.8689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8935e-03, -1.7857e-05,  0.0000e+00,  ...,  8.8689e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.8633e-03,  6.2198e-03, -6.1253e-04,  ...,  8.4853e-03,
         -4.0884e-03, -5.0045e-03],
        ...,
        [ 1.8935e-03, -1.7857e-05,  0.0000e+00,  ...,  8.8689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8935e-03, -1.7857e-05,  0.0000e+00,  ...,  8.8689e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8935e-03, -1.7857e-05,  0.0000e+00,  ...,  8.8689e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(622.7894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.1075, device='cuda:0')



h[100].sum tensor(44.0587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8183, device='cuda:0')



h[200].sum tensor(-6.0119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0135, 0.0062, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0165, 0.0093, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0077, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50447.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0186, 0.0176, 0.0142,  ..., 0.0000, 0.0191, 0.0000],
        [0.0142, 0.0080, 0.0058,  ..., 0.0000, 0.0111, 0.0000],
        [0.0101, 0.0008, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0207, 0.0292, 0.0261,  ..., 0.0000, 0.0233, 0.0000],
        [0.0207, 0.0292, 0.0261,  ..., 0.0000, 0.0233, 0.0000],
        [0.0206, 0.0291, 0.0261,  ..., 0.0000, 0.0233, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410202.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2051.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.2509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14544.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.5726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(594.3263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.9053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3429],
        [-0.1820],
        [-0.0567],
        ...,
        [-0.5972],
        [-0.5953],
        [-0.5947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114898.7891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366661.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366671.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(570.5847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.7688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.8684, device='cuda:0')



h[100].sum tensor(42.5785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9861, device='cuda:0')



h[200].sum tensor(-5.5367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.3495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0187, 0.0116, 0.0000,  ..., 0.0160, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46406.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0245, 0.0207,  ..., 0.0000, 0.0215, 0.0000],
        [0.0176, 0.0138, 0.0120,  ..., 0.0000, 0.0164, 0.0000],
        [0.0115, 0.0033, 0.0019,  ..., 0.0000, 0.0071, 0.0000],
        ...,
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000],
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000],
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385592.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2127.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.6009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13886.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.0090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(426.3531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.7366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4007],
        [-0.2344],
        [-0.0907],
        ...,
        [-0.5985],
        [-0.5937],
        [-0.5917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134835.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366671.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366671.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3809e-02,  3.3322e-02, -3.2506e-03,  ...,  4.4977e-02,
         -2.1830e-02, -2.6729e-02],
        [ 9.8794e-03,  8.3202e-03, -8.1427e-04,  ...,  1.1321e-02,
         -5.4683e-03, -6.6955e-03],
        [ 2.7829e-02,  2.7075e-02, -2.6418e-03,  ...,  3.6567e-02,
         -1.7742e-02, -2.1723e-02],
        ...,
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8818e-03, -3.5660e-05,  0.0000e+00,  ...,  7.2617e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(951.2772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(100.6065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-42.9212, device='cuda:0')



h[100].sum tensor(48.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9524, device='cuda:0')



h[200].sum tensor(-8.8932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.0796, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0647, 0.0596, 0.0000,  ..., 0.0807, 0.0000, 0.0000],
        [0.1361, 0.1341, 0.0000,  ..., 0.1811, 0.0000, 0.0000],
        [0.0891, 0.0850, 0.0000,  ..., 0.1150, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65030.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000],
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000],
        [0.0211, 0.0295, 0.0261,  ..., 0.0000, 0.0234, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492789.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1962.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.6984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16534.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.9748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1133.7410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.9671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0616],
        [ 0.0479],
        [ 0.0359],
        ...,
        [-0.6028],
        [-0.5957],
        [-0.5894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116698.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9996],
        [1.0000],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366671.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366681.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8734e-03, -4.9945e-05,  0.0000e+00,  ...,  5.4572e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0328e-02,  1.9227e-02, -1.8641e-03,  ...,  2.6005e-02,
         -1.2596e-02, -1.5427e-02],
        [ 1.8734e-03, -4.9945e-05,  0.0000e+00,  ...,  5.4572e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8734e-03, -4.9945e-05,  0.0000e+00,  ...,  5.4572e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8734e-03, -4.9945e-05,  0.0000e+00,  ...,  5.4572e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8734e-03, -4.9945e-05,  0.0000e+00,  ...,  5.4572e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(514.3478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.4111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5491, device='cuda:0')



h[100].sum tensor(41.1120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1241, device='cuda:0')



h[200].sum tensor(-5.0134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.6435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0192, 0.0000,  ..., 0.0261, 0.0000, 0.0000],
        [0.0226, 0.0157, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0745, 0.0698, 0.0000,  ..., 0.0945, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44939.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0020, 0.0005,  ..., 0.0000, 0.0083, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0215, 0.0299, 0.0261,  ..., 0.0000, 0.0237, 0.0000],
        [0.0214, 0.0299, 0.0261,  ..., 0.0000, 0.0237, 0.0000],
        [0.0214, 0.0299, 0.0261,  ..., 0.0000, 0.0237, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382328.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2208.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.1667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13734.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.6689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(391.5623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.0359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2872],
        [-0.2177],
        [-0.1734],
        ...,
        [-0.6221],
        [-0.6201],
        [-0.6193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149516.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366681.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366692.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8655e-03, -5.4459e-05,  0.0000e+00,  ...,  4.4351e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(538.4595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.8330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.9719, device='cuda:0')



h[100].sum tensor(40.9014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2812, device='cuda:0')



h[200].sum tensor(-5.1591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44803.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0297, 0.0258,  ..., 0.0000, 0.0236, 0.0000],
        [0.0215, 0.0294, 0.0253,  ..., 0.0000, 0.0233, 0.0000],
        [0.0216, 0.0285, 0.0242,  ..., 0.0000, 0.0227, 0.0000],
        ...,
        [0.0218, 0.0302, 0.0262,  ..., 0.0000, 0.0239, 0.0000],
        [0.0218, 0.0302, 0.0262,  ..., 0.0000, 0.0239, 0.0000],
        [0.0218, 0.0301, 0.0261,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379821., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2280.8315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.2607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13758.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.3134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(351.1019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.7014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4949],
        [-0.5330],
        [-0.5312],
        ...,
        [-0.6152],
        [-0.6277],
        [-0.6292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148131.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366692.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366702.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8561e-03, -4.0004e-05,  0.0000e+00,  ...,  5.4810e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8561e-03, -4.0004e-05,  0.0000e+00,  ...,  5.4810e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.7796e-03,  5.1014e-03, -4.8954e-04,  ...,  6.9762e-03,
         -3.3490e-03, -4.1038e-03],
        ...,
        [ 1.8561e-03, -4.0004e-05,  0.0000e+00,  ...,  5.4810e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8561e-03, -4.0004e-05,  0.0000e+00,  ...,  5.4810e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8561e-03, -4.0004e-05,  0.0000e+00,  ...,  5.4810e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(497.6195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.5109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0600, device='cuda:0')



h[100].sum tensor(39.8969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5706, device='cuda:0')



h[200].sum tensor(-4.6586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0164, 0.0093, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0201, 0.0132, 0.0000,  ..., 0.0181, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0075, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42207.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0192, 0.0142, 0.0122,  ..., 0.0000, 0.0179, 0.0000],
        [0.0141, 0.0044, 0.0029,  ..., 0.0000, 0.0081, 0.0000],
        [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0222, 0.0302, 0.0262,  ..., 0.0000, 0.0240, 0.0000],
        [0.0222, 0.0302, 0.0262,  ..., 0.0000, 0.0240, 0.0000],
        [0.0221, 0.0301, 0.0262,  ..., 0.0000, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366853.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2328.6724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.1741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13206.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-244.1240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(324.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.1518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2169],
        [-0.0838],
        [ 0.0030],
        ...,
        [-0.6373],
        [-0.6353],
        [-0.6346]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172506.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0001],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366702.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0002],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366713.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8428e-03, -1.0418e-05,  0.0000e+00,  ...,  8.0091e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8428e-03, -1.0418e-05,  0.0000e+00,  ...,  8.0091e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.7735e-03,  6.1834e-03, -5.8508e-04,  ...,  8.4177e-03,
         -4.0275e-03, -4.9365e-03],
        ...,
        [ 1.8428e-03, -1.0418e-05,  0.0000e+00,  ...,  8.0091e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7728e-02,  1.6580e-02, -1.5671e-03,  ...,  2.2413e-02,
         -1.0788e-02, -1.3223e-02],
        [ 1.8428e-03, -1.0418e-05,  0.0000e+00,  ...,  8.0091e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(719.6757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.2802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.3906, device='cuda:0')



h[100].sum tensor(42.8698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6669, device='cuda:0')



h[200].sum tensor(-6.3639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.6260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0133, 0.0062, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0271, 0.0206, 0.0000,  ..., 0.0281, 0.0000, 0.0000],
        ...,
        [0.0235, 0.0168, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0206, 0.0137, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        [0.0659, 0.0610, 0.0000,  ..., 0.0824, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52782.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0044, 0.0031,  ..., 0.0000, 0.0122, 0.0000],
        [0.0121, 0.0057, 0.0045,  ..., 0.0000, 0.0070, 0.0000],
        [0.0063, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        ...,
        [0.0118, 0.0027, 0.0013,  ..., 0.0000, 0.0095, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426454.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2311.3835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.4004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15162.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.6536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(609.4241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.4816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0400],
        [ 0.0406],
        [ 0.0474],
        ...,
        [-0.2556],
        [-0.1487],
        [-0.1092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118130.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0002],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366713.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(447.0184, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0002],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366724.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8344e-03, 1.1147e-05, 0.0000e+00,  ..., 9.4409e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(609.2244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.3394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.8457, device='cuda:0')



h[100].sum tensor(40.9278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6060, device='cuda:0')



h[200].sum tensor(-5.2314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.1563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.3312e-03, 4.4547e-05, 0.0000e+00,  ..., 3.7730e-04, 0.0000e+00,
         0.0000e+00],
        [7.3337e-03, 4.4563e-05, 0.0000e+00,  ..., 3.7743e-04, 0.0000e+00,
         0.0000e+00],
        [7.3378e-03, 4.4587e-05, 0.0000e+00,  ..., 3.7764e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4310e-03, 4.5154e-05, 0.0000e+00,  ..., 3.8243e-04, 0.0000e+00,
         0.0000e+00],
        [7.4302e-03, 4.5149e-05, 0.0000e+00,  ..., 3.8239e-04, 0.0000e+00,
         0.0000e+00],
        [7.4274e-03, 4.5132e-05, 0.0000e+00,  ..., 3.8225e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48569.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0218, 0.0251, 0.0210,  ..., 0.0000, 0.0220, 0.0000],
        [0.0225, 0.0294, 0.0257,  ..., 0.0000, 0.0235, 0.0000],
        [0.0225, 0.0295, 0.0260,  ..., 0.0000, 0.0237, 0.0000],
        ...,
        [0.0227, 0.0299, 0.0263,  ..., 0.0000, 0.0240, 0.0000],
        [0.0227, 0.0299, 0.0263,  ..., 0.0000, 0.0240, 0.0000],
        [0.0227, 0.0299, 0.0263,  ..., 0.0000, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410002.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2316.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.2375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14506.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.7793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(571.7634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.7846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4382],
        [-0.6047],
        [-0.7192],
        ...,
        [-0.6398],
        [-0.6378],
        [-0.6371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146123.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9997],
        [1.0002],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366724.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366734.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00],
        [1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00],
        [1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00],
        [1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00],
        [1.8313e-03, 2.1515e-05, 0.0000e+00,  ..., 1.0332e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(540.2473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.0799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9765, device='cuda:0')



h[100].sum tensor(39.7281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1679, device='cuda:0')



h[200].sum tensor(-4.4634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.3279e-02, 6.3097e-03, 0.0000e+00,  ..., 8.7904e-03, 0.0000e+00,
         0.0000e+00],
        [7.3212e-03, 8.6014e-05, 0.0000e+00,  ..., 4.1304e-04, 0.0000e+00,
         0.0000e+00],
        [1.9241e-02, 1.2527e-02, 0.0000e+00,  ..., 1.7159e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4192e-03, 8.7165e-05, 0.0000e+00,  ..., 4.1857e-04, 0.0000e+00,
         0.0000e+00],
        [7.4185e-03, 8.7156e-05, 0.0000e+00,  ..., 4.1853e-04, 0.0000e+00,
         0.0000e+00],
        [7.4157e-03, 8.7123e-05, 0.0000e+00,  ..., 4.1837e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44312.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0095, 0.0059,  ..., 0.0000, 0.0151, 0.0000],
        [0.0190, 0.0090, 0.0059,  ..., 0.0000, 0.0169, 0.0000],
        [0.0140, 0.0032, 0.0009,  ..., 0.0000, 0.0109, 0.0000],
        ...,
        [0.0229, 0.0298, 0.0264,  ..., 0.0000, 0.0241, 0.0000],
        [0.0229, 0.0298, 0.0264,  ..., 0.0000, 0.0240, 0.0000],
        [0.0229, 0.0298, 0.0264,  ..., 0.0000, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384350.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2386.3098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.0367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13957.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.9909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(387.2450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.8097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4541],
        [-0.4618],
        [-0.4095],
        ...,
        [-0.6423],
        [-0.6402],
        [-0.6394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138262.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366734.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366744.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(750.8896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.6462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.9817, device='cuda:0')



h[100].sum tensor(42.4634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1432, device='cuda:0')



h[200].sum tensor(-6.0705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.9820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0073, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0073, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50485.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0226, 0.0277, 0.0242,  ..., 0.0000, 0.0232, 0.0000],
        [0.0227, 0.0271, 0.0234,  ..., 0.0000, 0.0226, 0.0000],
        [0.0217, 0.0201, 0.0155,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0232, 0.0298, 0.0265,  ..., 0.0000, 0.0242, 0.0000],
        [0.0232, 0.0298, 0.0265,  ..., 0.0000, 0.0242, 0.0000],
        [0.0232, 0.0297, 0.0264,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406579.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2343.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.7007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14889.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-268.5038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(433.3911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.2620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4134],
        [-0.3013],
        [-0.1611],
        ...,
        [-0.6469],
        [-0.6450],
        [-0.6444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106391.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366744.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366744.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00],
        [1.8217e-03, 2.8656e-05, 0.0000e+00,  ..., 1.0884e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(609.5319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.6465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.3139, device='cuda:0')



h[100].sum tensor(40.3712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0367, device='cuda:0')



h[200].sum tensor(-4.8916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.3690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0590, 0.0541, 0.0000,  ..., 0.0731, 0.0000, 0.0000],
        [0.0157, 0.0089, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0073, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0074, 0.0001, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45113.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0049, 0.0021, 0.0006,  ..., 0.0000, 0.0042, 0.0000],
        ...,
        [0.0232, 0.0298, 0.0265,  ..., 0.0000, 0.0242, 0.0000],
        [0.0232, 0.0298, 0.0265,  ..., 0.0000, 0.0242, 0.0000],
        [0.0232, 0.0297, 0.0264,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381990.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2428.2395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.9669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14019.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.3964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(340.6541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.9360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0696],
        [ 0.0702],
        [ 0.0671],
        ...,
        [-0.6480],
        [-0.6460],
        [-0.6454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129401.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0003],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366744.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366754.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00],
        [1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00],
        [1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00],
        [1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00],
        [1.8110e-03, 3.5021e-06, 0.0000e+00,  ..., 9.3197e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(619.5580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6146, device='cuda:0')



h[100].sum tensor(39.8691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1484, device='cuda:0')



h[200].sum tensor(-4.9246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.7198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2378e-03, 1.3996e-05, 0.0000e+00,  ..., 3.7246e-04, 0.0000e+00,
         0.0000e+00],
        [7.2405e-03, 1.4001e-05, 0.0000e+00,  ..., 3.7260e-04, 0.0000e+00,
         0.0000e+00],
        [7.2447e-03, 1.4009e-05, 0.0000e+00,  ..., 3.7282e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3392e-03, 1.4192e-05, 0.0000e+00,  ..., 3.7768e-04, 0.0000e+00,
         0.0000e+00],
        [7.3385e-03, 1.4191e-05, 0.0000e+00,  ..., 3.7765e-04, 0.0000e+00,
         0.0000e+00],
        [7.3357e-03, 1.4186e-05, 0.0000e+00,  ..., 3.7751e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46604.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0199, 0.0154,  ..., 0.0000, 0.0206, 0.0000],
        [0.0226, 0.0273, 0.0237,  ..., 0.0000, 0.0234, 0.0000],
        [0.0229, 0.0297, 0.0263,  ..., 0.0000, 0.0243, 0.0000],
        ...,
        [0.0233, 0.0300, 0.0266,  ..., 0.0000, 0.0246, 0.0000],
        [0.0233, 0.0300, 0.0266,  ..., 0.0000, 0.0246, 0.0000],
        [0.0232, 0.0300, 0.0266,  ..., 0.0000, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395462.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2330.8875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(347.6440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14465.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-256.6437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(383.1429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.1413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3854],
        [-0.5705],
        [-0.7073],
        ...,
        [-0.6616],
        [-0.6597],
        [-0.6590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128429.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366754.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366764.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8012e-03, -4.3713e-05,  0.0000e+00,  ...,  6.2445e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(639.2828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.4145, device='cuda:0')



h[100].sum tensor(39.3608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4458, device='cuda:0')



h[200].sum tensor(-5.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.6532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45898.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0175, 0.0073, 0.0046,  ..., 0.0000, 0.0141, 0.0000],
        [0.0191, 0.0108, 0.0073,  ..., 0.0000, 0.0170, 0.0000],
        [0.0207, 0.0171, 0.0127,  ..., 0.0000, 0.0200, 0.0000],
        ...,
        [0.0233, 0.0306, 0.0268,  ..., 0.0000, 0.0250, 0.0000],
        [0.0233, 0.0306, 0.0268,  ..., 0.0000, 0.0250, 0.0000],
        [0.0233, 0.0306, 0.0268,  ..., 0.0000, 0.0250, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385915.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2359.8015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14308.5166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.2854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(299.1972, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.9629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0168],
        [-0.0297],
        [-0.0473],
        ...,
        [-0.6775],
        [-0.6759],
        [-0.6758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135885.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366764.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0005],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366775.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7918e-03, -8.2596e-05,  0.0000e+00,  ...,  3.9110e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(630.0784, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.1330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.7689, device='cuda:0')



h[100].sum tensor(38.5198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2058, device='cuda:0')



h[200].sum tensor(-4.9493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.8999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44972.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0306, 0.0266,  ..., 0.0000, 0.0251, 0.0000],
        [0.0230, 0.0306, 0.0266,  ..., 0.0000, 0.0251, 0.0000],
        [0.0230, 0.0307, 0.0267,  ..., 0.0000, 0.0251, 0.0000],
        ...,
        [0.0233, 0.0311, 0.0270,  ..., 0.0000, 0.0255, 0.0000],
        [0.0233, 0.0311, 0.0270,  ..., 0.0000, 0.0255, 0.0000],
        [0.0233, 0.0311, 0.0270,  ..., 0.0000, 0.0255, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387314.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2323.3472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(337.5111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14044.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.8282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(385.4444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.8431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6454],
        [-0.6729],
        [-0.6853],
        ...,
        [-0.6925],
        [-0.6903],
        [-0.6895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176283.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0005],
        ...,
        [1.0006],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366775.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366785.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7827e-03, -1.0702e-04,  0.0000e+00,  ...,  2.8747e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(640.3262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.8143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.7217, device='cuda:0')



h[100].sum tensor(38.0989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1882, device='cuda:0')



h[200].sum tensor(-4.9269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.8448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45864.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0223, 0.0259, 0.0213,  ..., 0.0000, 0.0238, 0.0000],
        [0.0191, 0.0123, 0.0100,  ..., 0.0000, 0.0186, 0.0000],
        [0.0155, 0.0057, 0.0043,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0235, 0.0313, 0.0272,  ..., 0.0000, 0.0258, 0.0000],
        [0.0235, 0.0313, 0.0272,  ..., 0.0000, 0.0258, 0.0000],
        [0.0235, 0.0313, 0.0272,  ..., 0.0000, 0.0258, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393074.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2281.8108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.9240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14502.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-253.1108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(339.9598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.4560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1325],
        [-0.0746],
        [-0.0209],
        ...,
        [-0.7021],
        [-0.6998],
        [-0.6990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156435.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9998],
        [1.0005],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366785.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366796.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7769e-03, -1.1726e-04,  0.0000e+00,  ...,  3.2621e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(659.1365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.9741, device='cuda:0')



h[100].sum tensor(38.1546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2821, device='cuda:0')



h[200].sum tensor(-4.8936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45383.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0233, 0.0291, 0.0244,  ..., 0.0000, 0.0243, 0.0000],
        [0.0233, 0.0292, 0.0246,  ..., 0.0000, 0.0245, 0.0000],
        [0.0227, 0.0264, 0.0216,  ..., 0.0000, 0.0238, 0.0000],
        ...,
        [0.0223, 0.0239, 0.0191,  ..., 0.0000, 0.0237, 0.0000],
        [0.0195, 0.0116, 0.0079,  ..., 0.0000, 0.0190, 0.0000],
        [0.0181, 0.0067, 0.0041,  ..., 0.0000, 0.0167, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388012.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2284.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.8265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14268.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-250.3209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(349.4439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.3364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4856],
        [-0.4187],
        [-0.3135],
        ...,
        [-0.4856],
        [-0.3449],
        [-0.2570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170869.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366796.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366807., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7713e-03, -1.2144e-04,  0.0000e+00,  ...,  4.1090e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7713e-03, -1.2144e-04,  0.0000e+00,  ...,  4.1090e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6724e-02,  1.5472e-02, -1.3719e-03,  ...,  2.1033e-02,
         -9.9967e-03, -1.2282e-02],
        ...,
        [ 1.7713e-03, -1.2144e-04,  0.0000e+00,  ...,  4.1090e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7713e-03, -1.2144e-04,  0.0000e+00,  ...,  4.1090e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7713e-03, -1.2144e-04,  0.0000e+00,  ...,  4.1090e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1141.8373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(97.3153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-43.4635, device='cuda:0')



h[100].sum tensor(44.8398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.1540, device='cuda:0')



h[200].sum tensor(-8.5407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.7124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0342, 0.0281, 0.0000,  ..., 0.0383, 0.0000, 0.0000],
        [0.0343, 0.0281, 0.0000,  ..., 0.0383, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63333.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0057, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        ...,
        [0.0238, 0.0315, 0.0274,  ..., 0.0000, 0.0263, 0.0000],
        [0.0238, 0.0315, 0.0274,  ..., 0.0000, 0.0263, 0.0000],
        [0.0238, 0.0315, 0.0274,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488586.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2141.5886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.5422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16758.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.0824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(986.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.4363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0640],
        [ 0.0602],
        [ 0.0561],
        ...,
        [-0.7106],
        [-0.7083],
        [-0.7074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144230.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0006],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366807., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(497.3445, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366817.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7528e-03,  5.0840e-03, -4.5408e-04,  ...,  7.0569e-03,
         -3.3302e-03, -4.0926e-03],
        [ 2.3667e-02,  2.2718e-02, -1.9931e-03,  ...,  3.0797e-02,
         -1.4617e-02, -1.7963e-02],
        [ 3.5754e-02,  3.5320e-02, -3.0929e-03,  ...,  4.7762e-02,
         -2.2683e-02, -2.7876e-02],
        ...,
        [ 1.7623e-03, -1.1893e-04,  0.0000e+00,  ...,  5.2544e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7623e-03, -1.1893e-04,  0.0000e+00,  ...,  5.2544e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7623e-03, -1.1893e-04,  0.0000e+00,  ...,  5.2544e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(915.5747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.3574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.3926, device='cuda:0')



h[100].sum tensor(41.2065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4109, device='cuda:0')



h[200].sum tensor(-6.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.9619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0787, 0.0742, 0.0000,  ..., 0.1008, 0.0000, 0.0000],
        [0.0873, 0.0832, 0.0000,  ..., 0.1129, 0.0000, 0.0000],
        [0.1136, 0.1106, 0.0000,  ..., 0.1498, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56159., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0240, 0.0315, 0.0274,  ..., 0.0000, 0.0265, 0.0000],
        [0.0240, 0.0315, 0.0274,  ..., 0.0000, 0.0265, 0.0000],
        [0.0240, 0.0315, 0.0274,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455364.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2196.8142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.6836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16023.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.1398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(779.0621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.1767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0841],
        [ 0.0810],
        [ 0.0730],
        ...,
        [-0.7147],
        [-0.7125],
        [-0.7118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145492.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366817.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366828.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4313e-02,  1.2981e-02, -1.1336e-03,  ...,  1.7689e-02,
         -8.3677e-03, -1.0286e-02],
        [ 1.7504e-03, -1.1315e-04,  0.0000e+00,  ...,  6.1200e-05,
          0.0000e+00,  0.0000e+00],
        [ 3.6164e-02,  3.5757e-02, -3.1054e-03,  ...,  4.8350e-02,
         -2.2922e-02, -2.8177e-02],
        ...,
        [ 1.7504e-03, -1.1315e-04,  0.0000e+00,  ...,  6.1200e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03, -1.1315e-04,  0.0000e+00,  ...,  6.1200e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03, -1.1315e-04,  0.0000e+00,  ...,  6.1200e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1152.2499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(95.3286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-41.6603, device='cuda:0')



h[100].sum tensor(44.0707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4837, device='cuda:0')



h[200].sum tensor(-8.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-48.6084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0106, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        [0.0825, 0.0783, 0.0000,  ..., 0.1062, 0.0000, 0.0000],
        [0.0774, 0.0730, 0.0000,  ..., 0.0990, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60501.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0244, 0.0315, 0.0275,  ..., 0.0000, 0.0267, 0.0000],
        [0.0230, 0.0239, 0.0191,  ..., 0.0000, 0.0241, 0.0000],
        [0.0211, 0.0137, 0.0101,  ..., 0.0000, 0.0203, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458968.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2250.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.0092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16053.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.7390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(818.9415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-281.4119, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0133],
        [ 0.0479],
        [ 0.0480],
        ...,
        [-0.6395],
        [-0.5234],
        [-0.3643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151197.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366828.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366839.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7355e-03, -1.0971e-04,  0.0000e+00,  ...,  6.1369e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7355e-03, -1.0971e-04,  0.0000e+00,  ...,  6.1369e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3941e-02,  1.2609e-02, -1.0923e-03,  ...,  1.7183e-02,
         -8.1150e-03, -9.9781e-03],
        ...,
        [ 1.7355e-03, -1.0971e-04,  0.0000e+00,  ...,  6.1369e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7355e-03, -1.0971e-04,  0.0000e+00,  ...,  6.1369e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7355e-03, -1.0971e-04,  0.0000e+00,  ...,  6.1369e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(607.5569, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.7091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5158, device='cuda:0')



h[100].sum tensor(35.7644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2534, device='cuda:0')



h[200].sum tensor(-3.8016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0236, 0.0172, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0224, 0.0158, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41260.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0148, 0.0128,  ..., 0.0000, 0.0196, 0.0000],
        [0.0129, 0.0022, 0.0005,  ..., 0.0000, 0.0082, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        ...,
        [0.0248, 0.0316, 0.0276,  ..., 0.0000, 0.0271, 0.0000],
        [0.0248, 0.0316, 0.0276,  ..., 0.0000, 0.0270, 0.0000],
        [0.0248, 0.0316, 0.0276,  ..., 0.0000, 0.0270, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368731.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2403.2185, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.8020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13977.9717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.3784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(211.6566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.3207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2006],
        [-0.0489],
        [ 0.0481],
        ...,
        [-0.7257],
        [-0.7236],
        [-0.7227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166140.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366839.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366850.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3745e-02,  2.2846e-02, -1.9552e-03,  ...,  3.0956e-02,
         -1.4620e-02, -1.7982e-02],
        [ 2.3904e-02,  2.3011e-02, -1.9692e-03,  ...,  3.1179e-02,
         -1.4726e-02, -1.8111e-02],
        [ 1.7173e-03, -1.0435e-04,  0.0000e+00,  ...,  6.0364e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7173e-03, -1.0435e-04,  0.0000e+00,  ...,  6.0364e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7173e-03, -1.0435e-04,  0.0000e+00,  ...,  6.0364e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7173e-03, -1.0435e-04,  0.0000e+00,  ...,  6.0364e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(929.4392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.7203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.0389, device='cuda:0')



h[100].sum tensor(39.8380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9078, device='cuda:0')



h[200].sum tensor(-6.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.3824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0972, 0.0937, 0.0000,  ..., 0.1269, 0.0000, 0.0000],
        [0.0719, 0.0674, 0.0000,  ..., 0.0914, 0.0000, 0.0000],
        [0.0667, 0.0619, 0.0000,  ..., 0.0841, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54106.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0253, 0.0317, 0.0277,  ..., 0.0000, 0.0274, 0.0000],
        [0.0253, 0.0317, 0.0277,  ..., 0.0000, 0.0274, 0.0000],
        [0.0253, 0.0317, 0.0277,  ..., 0.0000, 0.0274, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441506.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2394.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.1830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15789.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(665.7982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.9969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0341],
        [ 0.0390],
        [ 0.0411],
        ...,
        [-0.7337],
        [-0.7315],
        [-0.7307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148889.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366850.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366861.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4530e-03,  4.8507e-03, -4.1853e-04,  ...,  6.7198e-03,
         -3.1501e-03, -3.8754e-03],
        [ 8.1964e-03,  6.6667e-03, -5.7200e-04,  ...,  9.1647e-03,
         -4.3053e-03, -5.2965e-03],
        [ 1.6985e-03, -1.0183e-04,  0.0000e+00,  ...,  5.2507e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6985e-03, -1.0183e-04,  0.0000e+00,  ...,  5.2507e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6985e-03, -1.0183e-04,  0.0000e+00,  ...,  5.2507e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6985e-03, -1.0183e-04,  0.0000e+00,  ...,  5.2507e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(694.7406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9728, device='cuda:0')



h[100].sum tensor(36.0467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1666, device='cuda:0')



h[200].sum tensor(-4.2858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0493, 0.0439, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0207, 0.0143, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        [0.0133, 0.0067, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44000.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0089, 0.0006, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0176, 0.0096, 0.0070,  ..., 0.0000, 0.0137, 0.0000],
        ...,
        [0.0258, 0.0319, 0.0278,  ..., 0.0000, 0.0278, 0.0000],
        [0.0258, 0.0319, 0.0278,  ..., 0.0000, 0.0278, 0.0000],
        [0.0258, 0.0318, 0.0278,  ..., 0.0000, 0.0278, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385074.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2541.0835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.6151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14492.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.5400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(281.5347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.0487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0499],
        [ 0.0029],
        [-0.0754],
        ...,
        [-0.7443],
        [-0.7421],
        [-0.7412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159877.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366861.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366871.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6911e-03, -1.0975e-04,  0.0000e+00,  ...,  4.0615e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(751.1099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.0166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2199, device='cuda:0')



h[100].sum tensor(36.7730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0017, device='cuda:0')



h[200].sum tensor(-4.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.2593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0149, 0.0083, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46413.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0110, 0.0068,  ..., 0.0000, 0.0201, 0.0000],
        [0.0182, 0.0078, 0.0065,  ..., 0.0000, 0.0149, 0.0000],
        [0.0126, 0.0022, 0.0003,  ..., 0.0000, 0.0085, 0.0000],
        ...,
        [0.0260, 0.0321, 0.0279,  ..., 0.0000, 0.0283, 0.0000],
        [0.0260, 0.0321, 0.0279,  ..., 0.0000, 0.0282, 0.0000],
        [0.0259, 0.0321, 0.0279,  ..., 0.0000, 0.0282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405700.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2481.0503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.1823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14941.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.8140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(407.4259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.2225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0243],
        [-0.0018],
        [ 0.0226],
        ...,
        [-0.7545],
        [-0.7522],
        [-0.7514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164212.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366871.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366882.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6853e-03, -1.1855e-04,  0.0000e+00,  ...,  2.5464e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(755.5897, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.6208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.5219, device='cuda:0')



h[100].sum tensor(36.8089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7423, device='cuda:0')



h[200].sum tensor(-4.5470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.4449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45418.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0252, 0.0267, 0.0217,  ..., 0.0000, 0.0261, 0.0000],
        [0.0257, 0.0303, 0.0258,  ..., 0.0000, 0.0274, 0.0000],
        [0.0259, 0.0317, 0.0274,  ..., 0.0000, 0.0281, 0.0000],
        ...,
        [0.0262, 0.0323, 0.0281,  ..., 0.0000, 0.0287, 0.0000],
        [0.0262, 0.0323, 0.0281,  ..., 0.0000, 0.0287, 0.0000],
        [0.0262, 0.0323, 0.0281,  ..., 0.0000, 0.0287, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393896.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2507.3010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.1471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14652.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-246.3391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(309.0251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.6172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2217],
        [-0.3500],
        [-0.4942],
        ...,
        [-0.7584],
        [-0.7592],
        [-0.7598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171971.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366882.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366893.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2568e-02,  1.1209e-02, -9.3477e-04,  ...,  1.5272e-02,
         -7.1756e-03, -8.8348e-03],
        [ 1.3985e-02,  1.2685e-02, -1.0565e-03,  ...,  1.7259e-02,
         -8.1098e-03, -9.9850e-03],
        [ 1.4265e-02,  1.2976e-02, -1.0805e-03,  ...,  1.7651e-02,
         -8.2943e-03, -1.0212e-02],
        ...,
        [ 1.6789e-03, -1.2163e-04,  0.0000e+00,  ...,  1.4704e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6789e-03, -1.2163e-04,  0.0000e+00,  ...,  1.4704e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6789e-03, -1.2163e-04,  0.0000e+00,  ...,  1.4704e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(785.3749, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.1138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5586, device='cuda:0')



h[100].sum tensor(37.0781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1276, device='cuda:0')



h[200].sum tensor(-4.6856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.6545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.6498e-02, 4.0917e-02, 0.0000e+00,  ..., 5.5810e-02, 0.0000e+00,
         0.0000e+00],
        [5.3036e-02, 4.7718e-02, 0.0000e+00,  ..., 6.4968e-02, 0.0000e+00,
         0.0000e+00],
        [6.5269e-02, 6.0443e-02, 0.0000e+00,  ..., 8.2103e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8167e-03, 0.0000e+00, 0.0000e+00,  ..., 5.9700e-05, 0.0000e+00,
         0.0000e+00],
        [6.8158e-03, 0.0000e+00, 0.0000e+00,  ..., 5.9692e-05, 0.0000e+00,
         0.0000e+00],
        [6.8133e-03, 0.0000e+00, 0.0000e+00,  ..., 5.9671e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46600.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0266, 0.0325, 0.0282,  ..., 0.0000, 0.0290, 0.0000],
        [0.0266, 0.0325, 0.0282,  ..., 0.0000, 0.0290, 0.0000],
        [0.0266, 0.0325, 0.0282,  ..., 0.0000, 0.0290, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403611.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2543.1548, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.7918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14910.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-248.4241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(349.8005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.4235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0488],
        [ 0.0439],
        [ 0.0292],
        ...,
        [-0.7754],
        [-0.7729],
        [-0.7719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167599.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366893.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366903.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3226e-02,  1.1892e-02, -9.8301e-04,  ...,  1.6188e-02,
         -7.5959e-03, -9.3548e-03],
        [ 6.5483e-03,  4.9445e-03, -4.1450e-04,  ...,  6.8321e-03,
         -3.2030e-03, -3.9446e-03],
        [ 8.3574e-03,  6.8265e-03, -5.6851e-04,  ...,  9.3664e-03,
         -4.3930e-03, -5.4102e-03],
        ...,
        [ 1.6793e-03, -1.2099e-04,  0.0000e+00,  ...,  1.0693e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6793e-03, -1.2099e-04,  0.0000e+00,  ...,  1.0693e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6793e-03, -1.2099e-04,  0.0000e+00,  ...,  1.0693e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(738.9767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.1304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1075, device='cuda:0')



h[100].sum tensor(36.5171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2166, device='cuda:0')



h[200].sum tensor(-4.2240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.5799e-02, 1.9494e-02, 0.0000e+00,  ..., 2.6782e-02, 0.0000e+00,
         0.0000e+00],
        [4.9413e-02, 4.3936e-02, 0.0000e+00,  ..., 5.9861e-02, 0.0000e+00,
         0.0000e+00],
        [2.5824e-02, 1.9633e-02, 0.0000e+00,  ..., 2.6807e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8191e-03, 0.0000e+00, 0.0000e+00,  ..., 4.3422e-05, 0.0000e+00,
         0.0000e+00],
        [6.8182e-03, 0.0000e+00, 0.0000e+00,  ..., 4.3416e-05, 0.0000e+00,
         0.0000e+00],
        [6.8157e-03, 0.0000e+00, 0.0000e+00,  ..., 4.3401e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43428.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0096, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0269, 0.0326, 0.0283,  ..., 0.0000, 0.0293, 0.0000],
        [0.0269, 0.0326, 0.0283,  ..., 0.0000, 0.0293, 0.0000],
        [0.0269, 0.0326, 0.0283,  ..., 0.0000, 0.0293, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388195.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2642.5962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.3256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14243.5928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.8832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(328.6812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.5125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0303],
        [ 0.0324],
        [-0.0268],
        ...,
        [-0.7842],
        [-0.7817],
        [-0.7809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201691.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0009],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366903.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0001],
        [1.0009],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366914.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0077e-03,  5.4223e-03, -4.4925e-04,  ...,  7.4726e-03,
         -3.4945e-03, -4.3049e-03],
        [ 1.3716e-02,  1.2399e-02, -1.0155e-03,  ...,  1.6868e-02,
         -7.8991e-03, -9.7308e-03],
        [ 1.6856e-03, -1.1292e-04,  0.0000e+00,  ...,  1.8147e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6856e-03, -1.1292e-04,  0.0000e+00,  ...,  1.8147e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6856e-03, -1.1292e-04,  0.0000e+00,  ...,  1.8147e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6856e-03, -1.1292e-04,  0.0000e+00,  ...,  1.8147e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1129.6376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.8727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.9333, device='cuda:0')



h[100].sum tensor(42.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7269, device='cuda:0')



h[200].sum tensor(-6.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.0930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.8455e-02, 4.2937e-02, 0.0000e+00,  ..., 5.8505e-02, 0.0000e+00,
         0.0000e+00],
        [2.5733e-02, 1.9414e-02, 0.0000e+00,  ..., 2.6674e-02, 0.0000e+00,
         0.0000e+00],
        [2.3123e-02, 1.6808e-02, 0.0000e+00,  ..., 2.3012e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8459e-03, 0.0000e+00, 0.0000e+00,  ..., 7.3702e-05, 0.0000e+00,
         0.0000e+00],
        [6.8449e-03, 0.0000e+00, 0.0000e+00,  ..., 7.3692e-05, 0.0000e+00,
         0.0000e+00],
        [6.8424e-03, 0.0000e+00, 0.0000e+00,  ..., 7.3665e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63435.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0133, 0.0026, 0.0008,  ..., 0.0000, 0.0092, 0.0000],
        ...,
        [0.0273, 0.0326, 0.0285,  ..., 0.0000, 0.0295, 0.0000],
        [0.0273, 0.0325, 0.0285,  ..., 0.0000, 0.0295, 0.0000],
        [0.0272, 0.0325, 0.0285,  ..., 0.0000, 0.0295, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516313.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2424.0945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.9780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17517.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.6911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1096.0956, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.8844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0462],
        [ 0.0123],
        [-0.0749],
        ...,
        [-0.7888],
        [-0.7862],
        [-0.7851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157005.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0001],
        [1.0009],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366914.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(515.3680, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366925.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6253e-02,  2.5440e-02, -2.0556e-03,  ...,  3.4426e-02,
         -1.6097e-02, -1.9834e-02],
        [ 5.0780e-02,  5.0943e-02, -4.1085e-03,  ...,  6.8773e-02,
         -3.2172e-02, -3.9642e-02],
        [ 2.6399e-02,  2.5591e-02, -2.0678e-03,  ...,  3.4630e-02,
         -1.6192e-02, -1.9952e-02],
        ...,
        [ 1.6929e-03, -9.7411e-05,  0.0000e+00,  ...,  3.2585e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6929e-03, -9.7411e-05,  0.0000e+00,  ...,  3.2585e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6929e-03, -9.7411e-05,  0.0000e+00,  ...,  3.2585e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(753.5895, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.0290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.5530, device='cuda:0')



h[100].sum tensor(37.1392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0105, device='cuda:0')



h[200].sum tensor(-4.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.1476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.2653e-01, 1.2414e-01, 0.0000e+00,  ..., 1.6785e-01, 0.0000e+00,
         0.0000e+00],
        [1.1824e-01, 1.1551e-01, 0.0000e+00,  ..., 1.5623e-01, 0.0000e+00,
         0.0000e+00],
        [1.2810e-01, 1.2576e-01, 0.0000e+00,  ..., 1.7003e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8764e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3236e-04, 0.0000e+00,
         0.0000e+00],
        [6.8755e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3234e-04, 0.0000e+00,
         0.0000e+00],
        [6.8730e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3229e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43274.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0277, 0.0324, 0.0287,  ..., 0.0000, 0.0297, 0.0000],
        [0.0277, 0.0324, 0.0287,  ..., 0.0000, 0.0297, 0.0000],
        [0.0277, 0.0324, 0.0287,  ..., 0.0000, 0.0297, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386978.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2735.1724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.2248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14375.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-240.0190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(283.6880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.7070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0225],
        [ 0.0249],
        [ 0.0253],
        ...,
        [-0.7920],
        [-0.7897],
        [-0.7889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185098.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366925.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366936.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6969e-03, -7.8882e-05,  0.0000e+00,  ...,  5.0102e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.7385e-03,  6.2017e-03, -5.0139e-04,  ...,  8.5092e-03,
         -3.9524e-03, -4.8715e-03],
        [ 1.6969e-03, -7.8882e-05,  0.0000e+00,  ...,  5.0102e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6969e-03, -7.8882e-05,  0.0000e+00,  ...,  5.0102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6969e-03, -7.8882e-05,  0.0000e+00,  ...,  5.0102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6969e-03, -7.8882e-05,  0.0000e+00,  ...,  5.0102e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(817.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3577, device='cuda:0')



h[100].sum tensor(38.0930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6813, device='cuda:0')



h[200].sum tensor(-4.3919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0382, 0.0323, 0.0000,  ..., 0.0442, 0.0000, 0.0000],
        [0.0117, 0.0050, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0128, 0.0062, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45214.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0196, 0.0042, 0.0013,  ..., 0.0000, 0.0138, 0.0000],
        [0.0204, 0.0027, 0.0010,  ..., 0.0000, 0.0157, 0.0000],
        ...,
        [0.0281, 0.0323, 0.0289,  ..., 0.0000, 0.0298, 0.0000],
        [0.0281, 0.0323, 0.0289,  ..., 0.0000, 0.0298, 0.0000],
        [0.0281, 0.0323, 0.0289,  ..., 0.0000, 0.0298, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395714.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2774.0093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.1204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14570.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-242.6649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(346.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.4413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2366],
        [-0.3378],
        [-0.3870],
        ...,
        [-0.7939],
        [-0.7915],
        [-0.7907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181946.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366936.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366947.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6976e-03, -5.8323e-05,  0.0000e+00,  ...,  6.6881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7246e-02,  1.6103e-02, -1.2795e-03,  ...,  2.1836e-02,
         -1.0154e-02, -1.2519e-02],
        [ 2.4955e-02,  2.4116e-02, -1.9139e-03,  ...,  3.2628e-02,
         -1.5188e-02, -1.8725e-02],
        ...,
        [ 1.6976e-03, -5.8323e-05,  0.0000e+00,  ...,  6.6881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6976e-03, -5.8323e-05,  0.0000e+00,  ...,  6.6881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6976e-03, -5.8323e-05,  0.0000e+00,  ...,  6.6881e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(946.1079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7415, device='cuda:0')



h[100].sum tensor(39.8465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3106, device='cuda:0')



h[200].sum tensor(-5.1953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0362, 0.0304, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0488, 0.0435, 0.0000,  ..., 0.0591, 0.0000, 0.0000],
        [0.0711, 0.0666, 0.0000,  ..., 0.0903, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49959.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0287, 0.0321, 0.0291,  ..., 0.0000, 0.0298, 0.0000],
        [0.0287, 0.0321, 0.0291,  ..., 0.0000, 0.0298, 0.0000],
        [0.0287, 0.0321, 0.0291,  ..., 0.0000, 0.0298, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419449.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2816.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.2247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15362.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.1425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(461.1851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.3081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0567],
        [-0.0463],
        [-0.0366],
        ...,
        [-0.7937],
        [-0.7912],
        [-0.7900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155057.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366947.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366958.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6953e-03, -4.0787e-05,  0.0000e+00,  ...,  7.4726e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03, -4.0787e-05,  0.0000e+00,  ...,  7.4726e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.1700e-03,  6.6882e-03, -5.2833e-04,  ...,  9.1387e-03,
         -4.2210e-03, -5.2054e-03],
        ...,
        [ 1.6953e-03, -4.0787e-05,  0.0000e+00,  ...,  7.4726e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03, -4.0787e-05,  0.0000e+00,  ...,  7.4726e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03, -4.0787e-05,  0.0000e+00,  ...,  7.4726e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(850.8679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.7108, device='cuda:0')



h[100].sum tensor(38.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8125, device='cuda:0')



h[200].sum tensor(-4.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.6653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0133, 0.0067, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0422, 0.0367, 0.0000,  ..., 0.0498, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46435.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0113, 0.0094,  ..., 0.0000, 0.0173, 0.0000],
        [0.0131, 0.0039, 0.0025,  ..., 0.0000, 0.0091, 0.0000],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0292, 0.0321, 0.0292,  ..., 0.0000, 0.0300, 0.0000],
        [0.0292, 0.0321, 0.0292,  ..., 0.0000, 0.0300, 0.0000],
        [0.0292, 0.0321, 0.0292,  ..., 0.0000, 0.0300, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405527.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2922.6960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.8025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14993.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.0441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(373.9001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-221.2622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0427],
        [ 0.0178],
        [ 0.0615],
        ...,
        [-0.7996],
        [-0.7972],
        [-0.7964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161588.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366958.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366969.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03, -4.0239e-05,  0.0000e+00,  ...,  5.8778e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(674.8492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.1470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7375, device='cuda:0')



h[100].sum tensor(36.0936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2208, device='cuda:0')



h[200].sum tensor(-3.1274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.5290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0362, 0.0305, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40364.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0222, 0.0134, 0.0117,  ..., 0.0000, 0.0182, 0.0000],
        [0.0266, 0.0215, 0.0193,  ..., 0.0000, 0.0256, 0.0000],
        ...,
        [0.0295, 0.0324, 0.0294,  ..., 0.0000, 0.0303, 0.0000],
        [0.0295, 0.0324, 0.0294,  ..., 0.0000, 0.0303, 0.0000],
        [0.0295, 0.0324, 0.0293,  ..., 0.0000, 0.0303, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(376940.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3021.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14335.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-233.5326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(153.3550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.1129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0844],
        [-0.2871],
        [-0.5196],
        ...,
        [-0.8144],
        [-0.8121],
        [-0.8113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169473.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366969.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366979.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4406e-02,  1.3147e-02, -1.0193e-03,  ...,  1.7813e-02,
         -8.2544e-03, -1.0185e-02],
        [ 2.7333e-02,  2.6573e-02, -2.0563e-03,  ...,  3.5903e-02,
         -1.6652e-02, -2.0547e-02],
        [ 3.8859e-02,  3.8545e-02, -2.9810e-03,  ...,  5.2033e-02,
         -2.4140e-02, -2.9786e-02],
        ...,
        [ 1.7006e-03, -5.0270e-05,  0.0000e+00,  ...,  3.2528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7006e-03, -5.0270e-05,  0.0000e+00,  ...,  3.2528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7006e-03, -5.0270e-05,  0.0000e+00,  ...,  3.2528e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(723.1829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.8454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7353, device='cuda:0')



h[100].sum tensor(36.8216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9633, device='cuda:0')



h[200].sum tensor(-3.4532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.8600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.9846e-02, 8.6058e-02, 0.0000e+00,  ..., 1.1635e-01, 0.0000e+00,
         0.0000e+00],
        [1.2478e-01, 1.2234e-01, 0.0000e+00,  ..., 1.6523e-01, 0.0000e+00,
         0.0000e+00],
        [1.8112e-01, 1.8086e-01, 0.0000e+00,  ..., 2.4407e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9131e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3223e-04, 0.0000e+00,
         0.0000e+00],
        [6.9120e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3221e-04, 0.0000e+00,
         0.0000e+00],
        [6.9095e-03, 0.0000e+00, 0.0000e+00,  ..., 1.3216e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40872.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0298, 0.0327, 0.0295,  ..., 0.0000, 0.0309, 0.0000],
        [0.0298, 0.0327, 0.0294,  ..., 0.0000, 0.0308, 0.0000],
        [0.0298, 0.0327, 0.0294,  ..., 0.0000, 0.0308, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378756.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3024.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.8817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14215.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.2177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(191.0580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.8993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9150e-04],
        [-6.1029e-02],
        [-1.2263e-01],
        ...,
        [-8.3277e-01],
        [-8.3035e-01],
        [-8.2939e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199108.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366979.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366989.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7022e-03, -5.8794e-05,  0.0000e+00,  ...,  7.8078e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(864.5253, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2933, device='cuda:0')



h[100].sum tensor(38.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0290, device='cuda:0')



h[200].sum tensor(-4.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.3449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4022e-02, 7.4359e-03, 0.0000e+00,  ..., 1.0130e-02, 0.0000e+00,
         0.0000e+00],
        [6.8076e-03, 0.0000e+00, 0.0000e+00,  ..., 3.1226e-05, 0.0000e+00,
         0.0000e+00],
        [1.5733e-02, 9.2041e-03, 0.0000e+00,  ..., 1.2513e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9203e-03, 0.0000e+00, 0.0000e+00,  ..., 3.1744e-05, 0.0000e+00,
         0.0000e+00],
        [6.9192e-03, 0.0000e+00, 0.0000e+00,  ..., 3.1739e-05, 0.0000e+00,
         0.0000e+00],
        [6.9167e-03, 0.0000e+00, 0.0000e+00,  ..., 3.1727e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47368.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0244, 0.0101, 0.0062,  ..., 0.0000, 0.0209, 0.0000],
        [0.0259, 0.0119, 0.0076,  ..., 0.0000, 0.0238, 0.0000],
        [0.0213, 0.0070, 0.0043,  ..., 0.0000, 0.0163, 0.0000],
        ...,
        [0.0300, 0.0332, 0.0296,  ..., 0.0000, 0.0313, 0.0000],
        [0.0300, 0.0332, 0.0296,  ..., 0.0000, 0.0313, 0.0000],
        [0.0300, 0.0331, 0.0295,  ..., 0.0000, 0.0313, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420784.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2921.0630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.7154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15341.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-254.1951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(411.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.0296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4612],
        [-0.3782],
        [-0.2299],
        ...,
        [-0.8505],
        [-0.8479],
        [-0.8469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194318.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366989.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366999.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4529e-03,  4.8727e-03, -3.7512e-04,  ...,  6.6364e-03,
         -3.0793e-03, -3.8015e-03],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1087.6664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.6924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.1775, device='cuda:0')



h[100].sum tensor(41.3845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9593, device='cuda:0')



h[200].sum tensor(-5.9783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.5442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0234, 0.0171, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0370, 0.0312, 0.0000,  ..., 0.0422, 0.0000, 0.0000],
        [0.0572, 0.0521, 0.0000,  ..., 0.0704, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55606.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000],
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000],
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464904.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2930.1118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.0956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16403.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-274.2961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(678.1716, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.0681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0218],
        [ 0.0169],
        [ 0.0122],
        ...,
        [-0.8644],
        [-0.8617],
        [-0.8607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194943., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366999.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366999.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6968e-03, -6.4688e-05,  0.0000e+00,  ..., -1.7623e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(747.0946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.6222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6835, device='cuda:0')



h[100].sum tensor(36.7364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3157, device='cuda:0')



h[200].sum tensor(-3.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.9663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0077, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0106, 0.0039, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43266.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0258, 0.0069, 0.0033,  ..., 0.0000, 0.0204, 0.0000],
        [0.0269, 0.0142, 0.0098,  ..., 0.0000, 0.0236, 0.0000],
        [0.0284, 0.0232, 0.0183,  ..., 0.0000, 0.0273, 0.0000],
        ...,
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000],
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000],
        [0.0304, 0.0336, 0.0297,  ..., 0.0000, 0.0317, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398959.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3026.8877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.8684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14992.9189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-247.8953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(204.3710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.4130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6499],
        [-0.7037],
        [-0.7798],
        ...,
        [-0.8660],
        [-0.8632],
        [-0.8622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191473.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366999.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0013],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367010.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.6808e-03,  6.1566e-03, -4.6785e-04,  ...,  8.3377e-03,
         -3.8668e-03, -4.7751e-03],
        [ 7.5242e-03,  5.9941e-03, -4.5561e-04,  ...,  8.1188e-03,
         -3.7656e-03, -4.6501e-03],
        [ 1.3507e-02,  1.2204e-02, -9.2347e-04,  ...,  1.6488e-02,
         -7.6324e-03, -9.4252e-03],
        ...,
        [ 1.6980e-03, -5.3411e-05,  0.0000e+00,  ..., -3.1784e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6980e-03, -5.3411e-05,  0.0000e+00,  ..., -3.1784e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6980e-03, -5.3411e-05,  0.0000e+00,  ..., -3.1784e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(923.4209, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.1301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.0229, device='cuda:0')



h[100].sum tensor(39.2214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6719, device='cuda:0')



h[200].sum tensor(-4.7635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0201, 0.0000,  ..., 0.0272, 0.0000, 0.0000],
        [0.0471, 0.0417, 0.0000,  ..., 0.0563, 0.0000, 0.0000],
        [0.0263, 0.0201, 0.0000,  ..., 0.0272, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48298.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0031, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0307, 0.0339, 0.0300,  ..., 0.0000, 0.0319, 0.0000],
        [0.0307, 0.0339, 0.0300,  ..., 0.0000, 0.0319, 0.0000],
        [0.0307, 0.0339, 0.0299,  ..., 0.0000, 0.0319, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420007.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3011.6655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.6279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15508.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.3871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(326.5233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.9032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0400],
        [ 0.0487],
        [ 0.0538],
        ...,
        [-0.8753],
        [-0.8726],
        [-0.8716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189816.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0013],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367010.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(451.3090, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0013],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367021., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7011e-03, -3.1761e-05,  0.0000e+00,  ..., -3.6711e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(889.7902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6486, device='cuda:0')



h[100].sum tensor(39.1320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1611, device='cuda:0')



h[200].sum tensor(-4.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.7596, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47319.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0335, 0.0296,  ..., 0.0000, 0.0314, 0.0000],
        [0.0304, 0.0331, 0.0291,  ..., 0.0000, 0.0312, 0.0000],
        [0.0304, 0.0322, 0.0279,  ..., 0.0000, 0.0305, 0.0000],
        ...,
        [0.0310, 0.0341, 0.0302,  ..., 0.0000, 0.0320, 0.0000],
        [0.0310, 0.0341, 0.0302,  ..., 0.0000, 0.0320, 0.0000],
        [0.0310, 0.0341, 0.0302,  ..., 0.0000, 0.0320, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425639.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3106.5054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.4563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15252.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-255.1580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(463.2823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.5576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7149],
        [-0.7309],
        [-0.7281],
        ...,
        [-0.8812],
        [-0.8791],
        [-0.8785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218117.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0013],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367021., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367032.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7047e-03,  5.6782e-06,  0.0000e+00,  ..., -2.9397e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(779.8741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.6366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.4526, device='cuda:0')



h[100].sum tensor(38.2032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2299, device='cuda:0')



h[200].sum tensor(-3.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.6969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0594e-02, 3.9447e-03, 0.0000e+00,  ..., 5.2570e-03, 0.0000e+00,
         0.0000e+00],
        [1.4380e-02, 7.8708e-03, 0.0000e+00,  ..., 1.0519e-02, 0.0000e+00,
         0.0000e+00],
        [1.0606e-02, 3.9491e-03, 0.0000e+00,  ..., 5.2628e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9348e-03, 2.3100e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.9336e-03, 2.3096e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.9311e-03, 2.3087e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43871.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0264, 0.0088, 0.0043,  ..., 0.0000, 0.0201, 0.0000],
        [0.0251, 0.0044, 0.0012,  ..., 0.0000, 0.0171, 0.0000],
        [0.0267, 0.0111, 0.0068,  ..., 0.0000, 0.0209, 0.0000],
        ...,
        [0.0315, 0.0342, 0.0303,  ..., 0.0000, 0.0318, 0.0000],
        [0.0315, 0.0342, 0.0303,  ..., 0.0000, 0.0318, 0.0000],
        [0.0315, 0.0341, 0.0303,  ..., 0.0000, 0.0318, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408226.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3149.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.8633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15100.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-249.3960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(270.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.0939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2106],
        [-0.1153],
        [-0.0812],
        ...,
        [-0.8818],
        [-0.8792],
        [-0.8782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184247.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367032.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0015],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367043.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7050e-03,  4.9940e-05,  0.0000e+00,  ..., -1.7839e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7050e-03,  4.9940e-05,  0.0000e+00,  ..., -1.7839e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.4208e-02,  1.3027e-02, -9.5286e-04,  ...,  1.7474e-02,
         -8.0395e-03, -9.9360e-03],
        ...,
        [ 1.7050e-03,  4.9940e-05,  0.0000e+00,  ..., -1.7839e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7050e-03,  4.9940e-05,  0.0000e+00,  ..., -1.7839e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7050e-03,  4.9940e-05,  0.0000e+00,  ..., -1.7839e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1114.9875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.6772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.9359, device='cuda:0')



h[100].sum tensor(43.2767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4979, device='cuda:0')



h[200].sum tensor(-5.6072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.0955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0136, 0.0072, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0437, 0.0385, 0.0000,  ..., 0.0515, 0.0000, 0.0000],
        [0.0625, 0.0580, 0.0000,  ..., 0.0778, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53919.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0037, 0.0020,  ..., 0.0000, 0.0090, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0320, 0.0343, 0.0304,  ..., 0.0000, 0.0316, 0.0000],
        [0.0320, 0.0343, 0.0304,  ..., 0.0000, 0.0316, 0.0000],
        [0.0320, 0.0343, 0.0304,  ..., 0.0000, 0.0316, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450378.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3143.4009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.1674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16075.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.4150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(548.0760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-273.4495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0478],
        [ 0.0812],
        [ 0.0941],
        ...,
        [-0.8769],
        [-0.8746],
        [-0.8739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169546.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0015],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367043.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0015],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367054.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0036e-02,  8.7014e-03, -6.2880e-04,  ...,  1.1629e-02,
         -5.3422e-03, -6.6042e-03],
        [ 1.7139e-03,  6.4220e-05,  0.0000e+00,  ..., -1.4116e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7139e-03,  6.4220e-05,  0.0000e+00,  ..., -1.4116e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7139e-03,  6.4220e-05,  0.0000e+00,  ..., -1.4116e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7139e-03,  6.4220e-05,  0.0000e+00,  ..., -1.4116e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7139e-03,  6.4220e-05,  0.0000e+00,  ..., -1.4116e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(832.9344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.1748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.7697, device='cuda:0')



h[100].sum tensor(40.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3478, device='cuda:0')



h[200].sum tensor(-3.5363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0334, 0.0278, 0.0000,  ..., 0.0370, 0.0000, 0.0000],
        [0.0152, 0.0089, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0069, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43239.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0171, 0.0062, 0.0046,  ..., 0.0000, 0.0113, 0.0000],
        [0.0270, 0.0139, 0.0115,  ..., 0.0000, 0.0223, 0.0000],
        ...,
        [0.0319, 0.0347, 0.0305,  ..., 0.0000, 0.0316, 0.0000],
        [0.0319, 0.0347, 0.0305,  ..., 0.0000, 0.0316, 0.0000],
        [0.0319, 0.0347, 0.0305,  ..., 0.0000, 0.0316, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396344.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3169.9292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14718.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.1356, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(203.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.1728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0809],
        [ 0.0272],
        [-0.0546],
        ...,
        [-0.8783],
        [-0.8759],
        [-0.8750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174767.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0015],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367054.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367065.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7280e-03,  5.8989e-05,  0.0000e+00,  ..., -1.6046e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.3990e-02,  2.3158e-02, -1.6674e-03,  ...,  3.1123e-02,
         -1.4265e-02, -1.7639e-02],
        [ 2.7809e-02,  2.7121e-02, -1.9535e-03,  ...,  3.6465e-02,
         -1.6712e-02, -2.0666e-02],
        ...,
        [ 1.7280e-03,  5.8989e-05,  0.0000e+00,  ..., -1.6046e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7280e-03,  5.8989e-05,  0.0000e+00,  ..., -1.6046e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7280e-03,  5.8989e-05,  0.0000e+00,  ..., -1.6046e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(875.6320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.3982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4706, device='cuda:0')



h[100].sum tensor(42.1749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6082, device='cuda:0')



h[200].sum tensor(-3.6761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0292, 0.0233, 0.0000,  ..., 0.0311, 0.0000, 0.0000],
        [0.0559, 0.0511, 0.0000,  ..., 0.0686, 0.0000, 0.0000],
        [0.1232, 0.1209, 0.0000,  ..., 0.1626, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44945.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0924e-02, 1.9602e-03, 9.0103e-05,  ..., 0.0000e+00, 7.5573e-03,
         0.0000e+00],
        [3.0006e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.1544e-02, 3.5140e-02, 3.0662e-02,  ..., 0.0000e+00, 3.1855e-02,
         0.0000e+00],
        [3.1539e-02, 3.5132e-02, 3.0657e-02,  ..., 0.0000e+00, 3.1850e-02,
         0.0000e+00],
        [3.1528e-02, 3.5121e-02, 3.0642e-02,  ..., 0.0000e+00, 3.1837e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411883.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3008.3337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.6860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14844.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-259.6407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(360.9021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.4004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0650],
        [ 0.0922],
        [ 0.0990],
        ...,
        [-0.8831],
        [-0.8808],
        [-0.8799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191647.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367065.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367076.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7379e-03,  4.3139e-05,  0.0000e+00,  ..., -2.0223e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1172.4247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.7552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.1237, device='cuda:0')



h[100].sum tensor(47.0039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5676, device='cuda:0')



h[200].sum tensor(-5.5273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.3145, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52674.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0256, 0.0157, 0.0130,  ..., 0.0000, 0.0206, 0.0000],
        [0.0294, 0.0267, 0.0209,  ..., 0.0000, 0.0280, 0.0000],
        [0.0307, 0.0343, 0.0294,  ..., 0.0000, 0.0310, 0.0000],
        ...,
        [0.0313, 0.0357, 0.0309,  ..., 0.0000, 0.0322, 0.0000],
        [0.0313, 0.0357, 0.0309,  ..., 0.0000, 0.0322, 0.0000],
        [0.0313, 0.0357, 0.0309,  ..., 0.0000, 0.0321, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438650.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2814.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.1790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15707.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-283.8205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(442.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.1336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0619],
        [-0.2216],
        [-0.4049],
        ...,
        [-0.8881],
        [-0.8860],
        [-0.8855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160345.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367076.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0018],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367086.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7408e-03,  3.5456e-06,  0.0000e+00,  ..., -3.0615e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(935.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.3296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3301, device='cuda:0')



h[100].sum tensor(44.2333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2994, device='cuda:0')



h[200].sum tensor(-3.9527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.0543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9598e-03, 1.4175e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.9639e-03, 1.4183e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.9695e-03, 1.4195e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.0874e-03, 1.4435e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.0862e-03, 1.4433e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.0836e-03, 1.4427e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45763.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0305, 0.0358, 0.0308,  ..., 0.0000, 0.0320, 0.0000],
        [0.0296, 0.0307, 0.0253,  ..., 0.0000, 0.0304, 0.0000],
        [0.0283, 0.0240, 0.0182,  ..., 0.0000, 0.0283, 0.0000],
        ...,
        [0.0311, 0.0365, 0.0315,  ..., 0.0000, 0.0326, 0.0000],
        [0.0311, 0.0365, 0.0315,  ..., 0.0000, 0.0326, 0.0000],
        [0.0311, 0.0364, 0.0315,  ..., 0.0000, 0.0326, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409749.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2762.0601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.9222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15001.1787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.1744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(247.5768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.3243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8699],
        [-0.7908],
        [-0.6247],
        ...,
        [-0.9119],
        [-0.9094],
        [-0.9085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175169.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0018],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367086.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0018],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367096.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6213e-03,  7.1061e-03, -5.0263e-04,  ...,  9.5921e-03,
         -4.3911e-03, -5.4343e-03],
        [ 1.7996e-02,  1.6826e-02, -1.1866e-03,  ...,  2.2698e-02,
         -1.0367e-02, -1.2830e-02],
        [ 2.2283e-02,  2.1271e-02, -1.4994e-03,  ...,  2.8691e-02,
         -1.3099e-02, -1.6211e-02],
        ...,
        [ 1.7322e-03, -3.6488e-05,  0.0000e+00,  ..., -3.8598e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7322e-03, -3.6488e-05,  0.0000e+00,  ..., -3.8598e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7322e-03, -3.6488e-05,  0.0000e+00,  ..., -3.8598e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1601.5427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.1866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-45.5456, device='cuda:0')



h[100].sum tensor(53.0071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.9278, device='cuda:0')



h[200].sum tensor(-8.2772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-53.1418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0377, 0.0318, 0.0000,  ..., 0.0429, 0.0000, 0.0000],
        [0.0757, 0.0711, 0.0000,  ..., 0.0960, 0.0000, 0.0000],
        [0.0900, 0.0860, 0.0000,  ..., 0.1160, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71229.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0312, 0.0373, 0.0320,  ..., 0.0000, 0.0332, 0.0000],
        [0.0312, 0.0373, 0.0320,  ..., 0.0000, 0.0331, 0.0000],
        [0.0312, 0.0373, 0.0319,  ..., 0.0000, 0.0331, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562394.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2645.4614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.7887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18122.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-324.7281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1355.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.4003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0739],
        [ 0.0606],
        [ 0.0308],
        ...,
        [-0.9212],
        [-0.9242],
        [-0.9248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200376.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0018],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367096.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0019],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367107., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1136e-02,  2.0060e-02, -1.4040e-03,  ...,  2.7095e-02,
         -1.2352e-02, -1.5291e-02],
        [ 3.5780e-02,  3.5241e-02, -2.4631e-03,  ...,  4.7564e-02,
         -2.1670e-02, -2.6826e-02],
        [ 2.0147e-02,  1.9035e-02, -1.3324e-03,  ...,  2.5713e-02,
         -1.1723e-02, -1.4512e-02],
        ...,
        [ 1.7244e-03, -6.2781e-05,  0.0000e+00,  ..., -3.9138e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7244e-03, -6.2781e-05,  0.0000e+00,  ..., -3.9138e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7244e-03, -6.2781e-05,  0.0000e+00,  ..., -3.9138e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(914.2922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.1690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.6683, device='cuda:0')



h[100].sum tensor(43.9084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0534, device='cuda:0')



h[200].sum tensor(-3.7967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.2822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1020, 0.0983, 0.0000,  ..., 0.1328, 0.0000, 0.0000],
        [0.1052, 0.1016, 0.0000,  ..., 0.1372, 0.0000, 0.0000],
        [0.1109, 0.1075, 0.0000,  ..., 0.1451, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44640.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0314, 0.0379, 0.0323,  ..., 0.0000, 0.0335, 0.0000],
        [0.0314, 0.0379, 0.0323,  ..., 0.0000, 0.0335, 0.0000],
        [0.0314, 0.0379, 0.0323,  ..., 0.0000, 0.0335, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406981.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2764.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.3860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14802.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-269.3832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(238.5690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.7262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0101],
        [ 0.0386],
        [ 0.0551],
        ...,
        [-0.9441],
        [-0.9412],
        [-0.9402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206776.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0019],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367107., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0020],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367117.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6397e-02,  2.5510e-02, -1.7698e-03,  ...,  3.4469e-02,
         -1.5681e-02, -1.9417e-02],
        [ 4.9726e-02,  4.9693e-02, -3.4424e-03,  ...,  6.7078e-02,
         -3.0500e-02, -3.7768e-02],
        [ 4.8694e-02,  4.8623e-02, -3.3684e-03,  ...,  6.5635e-02,
         -2.9845e-02, -3.6956e-02],
        ...,
        [ 1.7123e-03, -7.7990e-05,  0.0000e+00,  ..., -3.4200e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7123e-03, -7.7990e-05,  0.0000e+00,  ..., -3.4200e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7123e-03, -7.7990e-05,  0.0000e+00,  ..., -3.4200e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(938.3302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.3747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.2502, device='cuda:0')



h[100].sum tensor(44.0874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2697, device='cuda:0')



h[200].sum tensor(-3.9059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.9612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1283, 0.1255, 0.0000,  ..., 0.1696, 0.0000, 0.0000],
        [0.1675, 0.1662, 0.0000,  ..., 0.2244, 0.0000, 0.0000],
        [0.1750, 0.1740, 0.0000,  ..., 0.2349, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46404.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0318, 0.0384, 0.0327,  ..., 0.0000, 0.0337, 0.0000],
        [0.0318, 0.0384, 0.0326,  ..., 0.0000, 0.0337, 0.0000],
        [0.0318, 0.0383, 0.0326,  ..., 0.0000, 0.0337, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422178.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2793.8540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.9741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15195.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.3398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(311.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.1042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0015],
        [ 0.0326],
        [ 0.0466],
        ...,
        [-0.9539],
        [-0.9508],
        [-0.9492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202026.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0020],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367117.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(528.6826, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367128.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1887e-02,  1.0477e-02, -7.2377e-04,  ...,  1.4213e-02,
         -6.4583e-03, -7.9993e-03],
        [ 2.6215e-02,  2.5329e-02, -1.7421e-03,  ...,  3.4240e-02,
         -1.5545e-02, -1.9254e-02],
        [ 2.2710e-02,  2.1696e-02, -1.4930e-03,  ...,  2.9341e-02,
         -1.3322e-02, -1.6501e-02],
        ...,
        [ 1.7024e-03, -7.9778e-05,  0.0000e+00,  ..., -2.1381e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7024e-03, -7.9778e-05,  0.0000e+00,  ..., -2.1381e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7024e-03, -7.9778e-05,  0.0000e+00,  ..., -2.1381e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.1067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.4825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.1081, device='cuda:0')



h[100].sum tensor(47.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9335, device='cuda:0')



h[200].sum tensor(-5.6004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.4631, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0752, 0.0705, 0.0000,  ..., 0.0955, 0.0000, 0.0000],
        [0.0747, 0.0700, 0.0000,  ..., 0.0948, 0.0000, 0.0000],
        [0.0775, 0.0730, 0.0000,  ..., 0.0988, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55033.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0323, 0.0386, 0.0329,  ..., 0.0000, 0.0338, 0.0000],
        [0.0323, 0.0386, 0.0329,  ..., 0.0000, 0.0338, 0.0000],
        [0.0323, 0.0386, 0.0329,  ..., 0.0000, 0.0338, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469704.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2781.8523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.9033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16294.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-293.2607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(611.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.6254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0768],
        [ 0.0811],
        [ 0.0837],
        ...,
        [-0.9603],
        [-0.9584],
        [-0.9578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192034.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367128.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0006],
        [1.0021],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367139.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5404e-02,  1.4135e-02, -9.6567e-04,  ...,  1.9154e-02,
         -8.6782e-03, -1.0752e-02],
        [ 1.4046e-02,  1.2728e-02, -8.7005e-04,  ...,  1.7257e-02,
         -7.8189e-03, -9.6872e-03],
        [ 1.5151e-02,  1.3873e-02, -9.4789e-04,  ...,  1.8802e-02,
         -8.5184e-03, -1.0554e-02],
        ...,
        [ 1.6931e-03, -7.3644e-05,  0.0000e+00,  ..., -4.4967e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7098e-02,  1.5891e-02, -1.0850e-03,  ...,  2.1522e-02,
         -9.7505e-03, -1.2080e-02],
        [ 1.6931e-03, -7.3644e-05,  0.0000e+00,  ..., -4.4967e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1139.2692, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.4261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.1396, device='cuda:0')



h[100].sum tensor(46.2650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8302, device='cuda:0')



h[200].sum tensor(-5.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0541, 0.0487, 0.0000,  ..., 0.0661, 0.0000, 0.0000],
        [0.0553, 0.0500, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        [0.0538, 0.0484, 0.0000,  ..., 0.0657, 0.0000, 0.0000],
        ...,
        [0.0226, 0.0162, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0197, 0.0132, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0639, 0.0587, 0.0000,  ..., 0.0796, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51724.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0213, 0.0055, 0.0029,  ..., 0.0000, 0.0160, 0.0000],
        [0.0180, 0.0000, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444095.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2908.6741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.5587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15905.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.6451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(368.0851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.5787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1264],
        [ 0.1261],
        [ 0.1263],
        ...,
        [-0.3441],
        [-0.2647],
        [-0.2302]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167716.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0006],
        [1.0021],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367139.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0022],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367150.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6862e-03, -6.0356e-05,  0.0000e+00,  ...,  1.6410e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1279.9622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.1832, device='cuda:0')



h[100].sum tensor(47.8899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3331, device='cuda:0')



h[200].sum tensor(-5.7782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.7176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.7419e-03, 0.0000e+00, 0.0000e+00,  ..., 6.5614e-05, 0.0000e+00,
         0.0000e+00],
        [6.7465e-03, 0.0000e+00, 0.0000e+00,  ..., 6.5658e-05, 0.0000e+00,
         0.0000e+00],
        [6.7524e-03, 0.0000e+00, 0.0000e+00,  ..., 6.5716e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8711e-03, 0.0000e+00, 0.0000e+00,  ..., 6.6871e-05, 0.0000e+00,
         0.0000e+00],
        [6.8700e-03, 0.0000e+00, 0.0000e+00,  ..., 6.6860e-05, 0.0000e+00,
         0.0000e+00],
        [6.8674e-03, 0.0000e+00, 0.0000e+00,  ..., 6.6835e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56316.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0189, 0.0126,  ..., 0.0000, 0.0259, 0.0000],
        [0.0318, 0.0302, 0.0241,  ..., 0.0000, 0.0301, 0.0000],
        [0.0326, 0.0342, 0.0285,  ..., 0.0000, 0.0316, 0.0000],
        ...,
        [0.0339, 0.0386, 0.0333,  ..., 0.0000, 0.0340, 0.0000],
        [0.0338, 0.0386, 0.0333,  ..., 0.0000, 0.0339, 0.0000],
        [0.0338, 0.0386, 0.0333,  ..., 0.0000, 0.0339, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467052.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2980.6614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.0433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16423.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.2797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(500.4391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.4806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1118],
        [-0.2647],
        [-0.4377],
        ...,
        [-0.9715],
        [-0.9688],
        [-0.9676]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161171.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0022],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367150.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0007],
        [1.0023],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367161.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7000e-03, -6.4324e-05,  0.0000e+00,  ...,  1.1711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7000e-03, -6.4324e-05,  0.0000e+00,  ...,  1.1711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3894e-02,  1.2564e-02, -8.4337e-04,  ...,  1.7040e-02,
         -7.6880e-03, -9.5304e-03],
        ...,
        [ 1.7000e-03, -6.4324e-05,  0.0000e+00,  ...,  1.1711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7000e-03, -6.4324e-05,  0.0000e+00,  ...,  1.1711e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7000e-03, -6.4324e-05,  0.0000e+00,  ...,  1.1711e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1243.1332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.8324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.3505, device='cuda:0')



h[100].sum tensor(47.7355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6519, device='cuda:0')



h[200].sum tensor(-5.4122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.5791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.7971e-03, 0.0000e+00, 0.0000e+00,  ..., 4.6827e-05, 0.0000e+00,
         0.0000e+00],
        [2.8964e-02, 2.2822e-02, 0.0000e+00,  ..., 3.0994e-02, 0.0000e+00,
         0.0000e+00],
        [3.7443e-02, 3.1533e-02, 0.0000e+00,  ..., 4.2827e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9282e-03, 0.0000e+00, 0.0000e+00,  ..., 4.7730e-05, 0.0000e+00,
         0.0000e+00],
        [6.9270e-03, 0.0000e+00, 0.0000e+00,  ..., 4.7722e-05, 0.0000e+00,
         0.0000e+00],
        [2.3233e-02, 1.6758e-02, 0.0000e+00,  ..., 2.2821e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53167.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0259, 0.0148, 0.0119,  ..., 0.0000, 0.0191, 0.0000],
        [0.0128, 0.0009, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        ...,
        [0.0320, 0.0276, 0.0218,  ..., 0.0000, 0.0299, 0.0000],
        [0.0278, 0.0159, 0.0130,  ..., 0.0000, 0.0213, 0.0000],
        [0.0177, 0.0018, 0.0000,  ..., 0.0000, 0.0083, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449775.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3059.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.3180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15885.0283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-289.3005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(412.9181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-304.8394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0961],
        [ 0.0097],
        [ 0.0799],
        ...,
        [-0.5990],
        [-0.3230],
        [-0.0903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167754.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0007],
        [1.0023],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367161.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0007],
        [1.0024],
        ...,
        [1.0011],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367172.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2837e-02,  1.1448e-02, -7.6257e-04,  ...,  1.5528e-02,
         -7.0015e-03, -8.6817e-03],
        [ 3.1316e-02,  3.0576e-02, -2.0289e-03,  ...,  4.1321e-02,
         -1.8629e-02, -2.3099e-02],
        [ 6.7131e-03,  5.1094e-03, -3.4290e-04,  ...,  6.9811e-03,
         -3.1484e-03, -3.9039e-03],
        ...,
        [ 1.7093e-03, -6.9889e-05,  0.0000e+00,  ..., -2.8914e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7093e-03, -6.9889e-05,  0.0000e+00,  ..., -2.8914e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7093e-03, -6.9889e-05,  0.0000e+00,  ..., -2.8914e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(987.7529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.9538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9393, device='cuda:0')



h[100].sum tensor(44.5166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1541, device='cuda:0')



h[200].sum tensor(-3.7562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1118, 0.1084, 0.0000,  ..., 0.1465, 0.0000, 0.0000],
        [0.0598, 0.0546, 0.0000,  ..., 0.0739, 0.0000, 0.0000],
        [0.0659, 0.0608, 0.0000,  ..., 0.0824, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44214.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0029, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0344, 0.0392, 0.0339,  ..., 0.0000, 0.0343, 0.0000],
        [0.0344, 0.0392, 0.0339,  ..., 0.0000, 0.0343, 0.0000],
        [0.0344, 0.0392, 0.0339,  ..., 0.0000, 0.0343, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406925.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3198.4136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.2812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14603.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-267.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(202.7917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.9362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0791],
        [ 0.0836],
        [ 0.0804],
        ...,
        [-0.9934],
        [-0.9907],
        [-0.9895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214119.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0007],
        [1.0024],
        ...,
        [1.0011],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367172.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0025],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367182.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1488e-02,  1.0040e-02, -6.6363e-04,  ...,  1.3622e-02,
         -6.1371e-03, -7.6120e-03],
        [ 2.1326e-02,  2.0218e-02, -1.3316e-03,  ...,  2.7347e-02,
         -1.2315e-02, -1.5274e-02],
        [ 1.1553e-02,  1.0107e-02, -6.6799e-04,  ...,  1.3712e-02,
         -6.1774e-03, -7.6620e-03],
        ...,
        [ 1.7151e-03, -7.1108e-05,  0.0000e+00,  ..., -1.3064e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7151e-03, -7.1108e-05,  0.0000e+00,  ..., -1.3064e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7151e-03, -7.1108e-05,  0.0000e+00,  ..., -1.3064e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1330.0992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.4754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.5943, device='cuda:0')



h[100].sum tensor(48.9012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4859, device='cuda:0')



h[200].sum tensor(-5.7843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.1972, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0715, 0.0666, 0.0000,  ..., 0.0901, 0.0000, 0.0000],
        [0.0714, 0.0665, 0.0000,  ..., 0.0900, 0.0000, 0.0000],
        [0.0708, 0.0659, 0.0000,  ..., 0.0891, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58660.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0348, 0.0397, 0.0342,  ..., 0.0000, 0.0346, 0.0000],
        [0.0348, 0.0396, 0.0342,  ..., 0.0000, 0.0346, 0.0000],
        [0.0348, 0.0396, 0.0342,  ..., 0.0000, 0.0346, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491909.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3102.4880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.5807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16680.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.1162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(702.0731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-337.7892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0240],
        [-0.0362],
        [-0.0329],
        ...,
        [-1.0065],
        [-1.0036],
        [-1.0026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192545.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0025],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367182.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0026],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367193.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7375e-03, -7.1466e-05,  0.0000e+00,  ..., -2.1236e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1047.3743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.0950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.2381, device='cuda:0')



h[100].sum tensor(45.5077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6368, device='cuda:0')



h[200].sum tensor(-3.9550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47470.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0340, 0.0365, 0.0308,  ..., 0.0000, 0.0325, 0.0000],
        [0.0343, 0.0388, 0.0334,  ..., 0.0000, 0.0338, 0.0000],
        [0.0343, 0.0392, 0.0339,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0350, 0.0399, 0.0346,  ..., 0.0000, 0.0347, 0.0000],
        [0.0350, 0.0399, 0.0346,  ..., 0.0000, 0.0347, 0.0000],
        [0.0350, 0.0399, 0.0345,  ..., 0.0000, 0.0347, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431618.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3166.8176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.3775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15386.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-278.2436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(262.0679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-277.8563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5549],
        [-0.7777],
        [-0.9565],
        ...,
        [-1.0160],
        [-1.0131],
        [-1.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193198.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0026],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367193.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0026],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367204.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7542e-03, -5.9951e-05,  0.0000e+00,  ..., -2.1069e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1010.3328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.9975, device='cuda:0')



h[100].sum tensor(45.2707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8041, device='cuda:0')



h[200].sum tensor(-3.5951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.4994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44861.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0345, 0.0393, 0.0340,  ..., 0.0000, 0.0341, 0.0000],
        [0.0345, 0.0393, 0.0340,  ..., 0.0000, 0.0341, 0.0000],
        [0.0346, 0.0394, 0.0341,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0352, 0.0401, 0.0348,  ..., 0.0000, 0.0348, 0.0000],
        [0.0352, 0.0401, 0.0348,  ..., 0.0000, 0.0348, 0.0000],
        [0.0352, 0.0401, 0.0348,  ..., 0.0000, 0.0347, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416013.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3196.2693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.9803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14933.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-273.8658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(174.3416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.4961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1182],
        [-1.1722],
        [-1.2139],
        ...,
        [-1.0222],
        [-1.0193],
        [-1.0182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201522.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0026],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367204.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0028],
        ...,
        [1.0013],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367215.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5649e-02,  1.4306e-02, -9.1705e-04,  ...,  1.9333e-02,
         -8.6668e-03, -1.0759e-02],
        [ 2.5099e-02,  2.4071e-02, -1.5412e-03,  ...,  3.2501e-02,
         -1.4565e-02, -1.8081e-02],
        [ 1.7674e-02,  1.6398e-02, -1.0508e-03,  ...,  2.2154e-02,
         -9.9305e-03, -1.2327e-02],
        ...,
        [ 1.7628e-03, -4.1185e-05,  0.0000e+00,  ..., -1.5275e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7628e-03, -4.1185e-05,  0.0000e+00,  ..., -1.5275e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7628e-03, -4.1185e-05,  0.0000e+00,  ..., -1.5275e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1905.5227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(104.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-51.9804, device='cuda:0')



h[100].sum tensor(56.9342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.3194, device='cuda:0')



h[200].sum tensor(-8.8056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-60.6498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0454, 0.0395, 0.0000,  ..., 0.0534, 0.0000, 0.0000],
        [0.0632, 0.0579, 0.0000,  ..., 0.0782, 0.0000, 0.0000],
        [0.1147, 0.1111, 0.0000,  ..., 0.1499, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82973.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0356, 0.0402, 0.0350,  ..., 0.0000, 0.0348, 0.0000],
        [0.0356, 0.0402, 0.0350,  ..., 0.0000, 0.0348, 0.0000],
        [0.0356, 0.0401, 0.0350,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680478.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2981.3228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.2499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20894.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-361.5769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1938.3347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-462.9811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0834],
        [ 0.0682],
        [ 0.0393],
        ...,
        [-1.0263],
        [-1.0234],
        [-1.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161380.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0028],
        ...,
        [1.0013],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367215.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0029],
        ...,
        [1.0013],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367226.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3953e-02,  1.2567e-02, -7.9732e-04,  ...,  1.6966e-02,
         -7.5904e-03, -9.4251e-03],
        [ 7.2930e-03,  5.6880e-03, -3.6153e-04,  ...,  7.6892e-03,
         -3.4418e-03, -4.2737e-03],
        [ 2.2963e-02,  2.1875e-02, -1.3869e-03,  ...,  2.9518e-02,
         -1.3203e-02, -1.6395e-02],
        ...,
        [ 1.7680e-03, -1.9106e-05,  0.0000e+00,  ..., -7.2166e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7680e-03, -1.9106e-05,  0.0000e+00,  ..., -7.2166e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7680e-03, -1.9106e-05,  0.0000e+00,  ..., -7.2166e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1343.5852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.2419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.0842, device='cuda:0')



h[100].sum tensor(50.0397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5530, device='cuda:0')



h[200].sum tensor(-5.2840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.2685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0343, 0.0281, 0.0000,  ..., 0.0379, 0.0000, 0.0000],
        [0.0559, 0.0503, 0.0000,  ..., 0.0679, 0.0000, 0.0000],
        [0.0345, 0.0282, 0.0000,  ..., 0.0381, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58571.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0361, 0.0402, 0.0352,  ..., 0.0000, 0.0349, 0.0000],
        [0.0360, 0.0402, 0.0352,  ..., 0.0000, 0.0349, 0.0000],
        [0.0360, 0.0402, 0.0352,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507033.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3128.6277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.9882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17229.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-310.9252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(682.2780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.8985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1010],
        [ 0.1119],
        [ 0.1162],
        ...,
        [-1.0295],
        [-1.0267],
        [-1.0255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156846.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0029],
        ...,
        [1.0013],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367226.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(561.9877, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0030],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367237.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0819e-02,  9.3271e-03, -5.8602e-04,  ...,  1.2582e-02,
         -5.6198e-03, -6.9801e-03],
        [ 1.1010e-02,  9.5248e-03, -5.9844e-04,  ...,  1.2849e-02,
         -5.7388e-03, -7.1279e-03],
        [ 1.7786e-03, -5.5665e-06,  0.0000e+00,  ..., -4.0940e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7786e-03, -5.5665e-06,  0.0000e+00,  ..., -4.0940e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7786e-03, -5.5665e-06,  0.0000e+00,  ..., -4.0940e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7786e-03, -5.5665e-06,  0.0000e+00,  ..., -4.0940e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1344.8361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.9066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.6189, device='cuda:0')



h[100].sum tensor(50.7766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3800, device='cuda:0')



h[200].sum tensor(-5.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.7256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0570, 0.0515, 0.0000,  ..., 0.0695, 0.0000, 0.0000],
        [0.0237, 0.0171, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0164, 0.0095, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56990.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0190, 0.0015, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0270, 0.0121, 0.0092,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0364, 0.0404, 0.0355,  ..., 0.0000, 0.0352, 0.0000],
        [0.0364, 0.0404, 0.0355,  ..., 0.0000, 0.0351, 0.0000],
        [0.0364, 0.0403, 0.0355,  ..., 0.0000, 0.0351, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499213.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3193.1885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.4150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16892.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.6512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(674.9227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.8160, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1120],
        [-0.3224],
        [-0.5944],
        ...,
        [-1.0359],
        [-1.0331],
        [-1.0320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172097., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0011],
        [1.0030],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367237.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0031],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367249.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7690e-03,  9.9709e-06,  0.0000e+00,  ...,  4.7220e-07,
          0.0000e+00,  0.0000e+00],
        [ 2.9458e-02,  2.8583e-02, -1.7783e-03,  ...,  3.8536e-02,
         -1.7179e-02, -2.1343e-02],
        [ 1.7690e-03,  9.9709e-06,  0.0000e+00,  ...,  4.7220e-07,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7690e-03,  9.9709e-06,  0.0000e+00,  ...,  4.7220e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.7690e-03,  9.9709e-06,  0.0000e+00,  ...,  4.7220e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.7690e-03,  9.9709e-06,  0.0000e+00,  ...,  4.7220e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1336.7235, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.7675, device='cuda:0')



h[100].sum tensor(50.6731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0636, device='cuda:0')



h[200].sum tensor(-4.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9914e-02, 6.4886e-02, 0.0000e+00,  ..., 8.7457e-02, 0.0000e+00,
         0.0000e+00],
        [2.9700e-02, 2.3383e-02, 0.0000e+00,  ..., 3.1483e-02, 0.0000e+00,
         0.0000e+00],
        [7.2624e-02, 6.7669e-02, 0.0000e+00,  ..., 9.1211e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2181e-03, 4.0685e-05, 0.0000e+00,  ..., 1.9267e-06, 0.0000e+00,
         0.0000e+00],
        [7.2168e-03, 4.0677e-05, 0.0000e+00,  ..., 1.9264e-06, 0.0000e+00,
         0.0000e+00],
        [7.2138e-03, 4.0661e-05, 0.0000e+00,  ..., 1.9256e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55615.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0063, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0371, 0.0407, 0.0359,  ..., 0.0000, 0.0356, 0.0000],
        [0.0371, 0.0407, 0.0359,  ..., 0.0000, 0.0356, 0.0000],
        [0.0370, 0.0406, 0.0359,  ..., 0.0000, 0.0356, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481474.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3333.2722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.4571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16428.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-303.6992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(579.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.5236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2159],
        [-0.2317],
        [-0.2647],
        ...,
        [-1.0390],
        [-1.0365],
        [-1.0367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192060.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0031],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367249.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367260.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.8857e-03,  7.3868e-03, -4.5408e-04,  ...,  9.9335e-03,
         -4.4190e-03, -5.4917e-03],
        [ 2.3056e-02,  2.2005e-02, -1.3558e-03,  ...,  2.9648e-02,
         -1.3194e-02, -1.6397e-02],
        [ 2.8022e-02,  2.7127e-02, -1.6718e-03,  ...,  3.6557e-02,
         -1.6269e-02, -2.0219e-02],
        ...,
        [ 1.7499e-03,  2.5589e-05,  0.0000e+00,  ...,  5.6231e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7499e-03,  2.5589e-05,  0.0000e+00,  ...,  5.6231e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7499e-03,  2.5589e-05,  0.0000e+00,  ...,  5.6231e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1214.3087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.4513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.3056, device='cuda:0')



h[100].sum tensor(48.8997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4053, device='cuda:0')



h[200].sum tensor(-4.2190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.5261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.7966e-02, 3.2049e-02, 0.0000e+00,  ..., 4.3107e-02, 0.0000e+00,
         0.0000e+00],
        [7.9927e-02, 7.5330e-02, 0.0000e+00,  ..., 1.0148e-01, 0.0000e+00,
         0.0000e+00],
        [1.2781e-01, 1.2472e-01, 0.0000e+00,  ..., 1.6809e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.1413e-03, 1.0443e-04, 0.0000e+00,  ..., 2.2947e-05, 0.0000e+00,
         0.0000e+00],
        [7.1400e-03, 1.0441e-04, 0.0000e+00,  ..., 2.2943e-05, 0.0000e+00,
         0.0000e+00],
        [7.1370e-03, 1.0437e-04, 0.0000e+00,  ..., 2.2934e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50462.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0379, 0.0409, 0.0362,  ..., 0.0000, 0.0361, 0.0000],
        [0.0379, 0.0409, 0.0362,  ..., 0.0000, 0.0361, 0.0000],
        [0.0379, 0.0409, 0.0361,  ..., 0.0000, 0.0361, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451646.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3535.7322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.6066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15988.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-292.9783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(307.2986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-301.1624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0462],
        [ 0.0359],
        [ 0.0260],
        ...,
        [-1.0638],
        [-1.0608],
        [-1.0598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174454.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367260.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367271.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2619e-02,  1.1263e-02, -6.8644e-04,  ...,  1.5150e-02,
         -6.7296e-03, -8.3656e-03],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1192.7224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6357, device='cuda:0')



h[100].sum tensor(48.3224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1563, device='cuda:0')



h[200].sum tensor(-4.0392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.7445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.8868e-02, 2.2768e-02, 0.0000e+00,  ..., 3.0547e-02, 0.0000e+00,
         0.0000e+00],
        [1.7823e-02, 1.1372e-02, 0.0000e+00,  ..., 1.5177e-02, 0.0000e+00,
         0.0000e+00],
        [6.9391e-03, 1.3900e-04, 0.0000e+00,  ..., 2.7784e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.0676e-03, 1.4157e-04, 0.0000e+00,  ..., 2.8298e-05, 0.0000e+00,
         0.0000e+00],
        [7.0663e-03, 1.4155e-04, 0.0000e+00,  ..., 2.8293e-05, 0.0000e+00,
         0.0000e+00],
        [7.0633e-03, 1.4149e-04, 0.0000e+00,  ..., 2.8281e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48586.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0173, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0267, 0.0069, 0.0038,  ..., 0.0000, 0.0149, 0.0000],
        [0.0340, 0.0176, 0.0137,  ..., 0.0000, 0.0260, 0.0000],
        ...,
        [0.0387, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000],
        [0.0387, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000],
        [0.0386, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441635.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3684.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.6571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15545.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-286.4308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(278.4811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.7900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0880],
        [-0.2006],
        [-0.3358],
        ...,
        [-1.0786],
        [-1.0757],
        [-1.0745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207805.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367271.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367271.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3563e-02,  1.2236e-02, -7.4597e-04,  ...,  1.6463e-02,
         -7.3133e-03, -9.0911e-03],
        [ 6.8874e-03,  5.3520e-03, -3.2508e-04,  ...,  7.1782e-03,
         -3.1870e-03, -3.9617e-03],
        [ 8.4070e-03,  6.9191e-03, -4.2089e-04,  ...,  9.2918e-03,
         -4.1263e-03, -5.1294e-03],
        ...,
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7316e-03,  3.4686e-05,  0.0000e+00,  ...,  6.9333e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1136.6071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.2968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.5118, device='cuda:0')



h[100].sum tensor(47.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3669, device='cuda:0')



h[200].sum tensor(-3.7165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.2664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.6478e-02, 2.0304e-02, 0.0000e+00,  ..., 2.7223e-02, 0.0000e+00,
         0.0000e+00],
        [5.0056e-02, 4.4614e-02, 0.0000e+00,  ..., 6.0010e-02, 0.0000e+00,
         0.0000e+00],
        [2.6523e-02, 2.0336e-02, 0.0000e+00,  ..., 2.7267e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.0676e-03, 1.4157e-04, 0.0000e+00,  ..., 2.8298e-05, 0.0000e+00,
         0.0000e+00],
        [7.0663e-03, 1.4155e-04, 0.0000e+00,  ..., 2.8293e-05, 0.0000e+00,
         0.0000e+00],
        [7.0633e-03, 1.4149e-04, 0.0000e+00,  ..., 2.8281e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46639.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0208, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        ...,
        [0.0387, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000],
        [0.0387, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000],
        [0.0386, 0.0410, 0.0365,  ..., 0.0000, 0.0366, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434237.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3723.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.1474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15290.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-281.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(264.7792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.4031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0226],
        [ 0.0734],
        [ 0.0540],
        ...,
        [-1.0786],
        [-1.0757],
        [-1.0745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217886.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0032],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367271.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0033],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367282.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.6195e-03,  6.1546e-03, -3.6957e-04,  ...,  8.2403e-03,
         -3.6501e-03, -4.5387e-03],
        [ 1.7046e-03,  5.4358e-05,  0.0000e+00,  ...,  1.3376e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7046e-03,  5.4358e-05,  0.0000e+00,  ...,  1.3376e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7046e-03,  5.4358e-05,  0.0000e+00,  ...,  1.3376e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7046e-03,  5.4358e-05,  0.0000e+00,  ...,  1.3376e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7046e-03,  5.4358e-05,  0.0000e+00,  ...,  1.3376e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1191.1996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.9069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.3190, device='cuda:0')



h[100].sum tensor(47.7076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0386, device='cuda:0')



h[200].sum tensor(-3.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.3749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.6806e-02, 2.0832e-02, 0.0000e+00,  ..., 2.7855e-02, 0.0000e+00,
         0.0000e+00],
        [1.2741e-02, 6.3208e-03, 0.0000e+00,  ..., 8.2844e-03, 0.0000e+00,
         0.0000e+00],
        [6.8311e-03, 2.1784e-04, 0.0000e+00,  ..., 5.3606e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9583e-03, 2.2190e-04, 0.0000e+00,  ..., 5.4605e-05, 0.0000e+00,
         0.0000e+00],
        [6.9570e-03, 2.2186e-04, 0.0000e+00,  ..., 5.4595e-05, 0.0000e+00,
         0.0000e+00],
        [6.9541e-03, 2.2176e-04, 0.0000e+00,  ..., 5.4571e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49118.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0296, 0.0103, 0.0073,  ..., 0.0000, 0.0153, 0.0000],
        [0.0355, 0.0210, 0.0172,  ..., 0.0000, 0.0274, 0.0000],
        ...,
        [0.0394, 0.0412, 0.0367,  ..., 0.0000, 0.0370, 0.0000],
        [0.0394, 0.0412, 0.0367,  ..., 0.0000, 0.0370, 0.0000],
        [0.0394, 0.0412, 0.0367,  ..., 0.0000, 0.0370, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446111.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3797.1899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.1465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15763.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.7790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(261.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.4402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0431],
        [-0.0748],
        [-0.2366],
        ...,
        [-1.0710],
        [-1.0811],
        [-1.0839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204960.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0033],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367282.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0034],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367294.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3137e-03,  5.8804e-03, -3.4894e-04,  ...,  7.8560e-03,
         -3.4720e-03, -4.3185e-03],
        [ 1.6778e-03,  6.7924e-05,  0.0000e+00,  ...,  1.7528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6778e-03,  6.7924e-05,  0.0000e+00,  ...,  1.7528e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6778e-03,  6.7924e-05,  0.0000e+00,  ...,  1.7528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6778e-03,  6.7924e-05,  0.0000e+00,  ...,  1.7528e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6778e-03,  6.7924e-05,  0.0000e+00,  ...,  1.7528e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1212.8582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.5460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5715, device='cuda:0')



h[100].sum tensor(47.3288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1324, device='cuda:0')



h[200].sum tensor(-4.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.6696, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.0758e-02, 1.4760e-02, 0.0000e+00,  ..., 1.9608e-02, 0.0000e+00,
         0.0000e+00],
        [1.6959e-02, 1.0835e-02, 0.0000e+00,  ..., 1.4315e-02, 0.0000e+00,
         0.0000e+00],
        [6.7243e-03, 2.7222e-04, 0.0000e+00,  ..., 7.0249e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8502e-03, 2.7732e-04, 0.0000e+00,  ..., 7.1564e-05, 0.0000e+00,
         0.0000e+00],
        [6.8489e-03, 2.7727e-04, 0.0000e+00,  ..., 7.1551e-05, 0.0000e+00,
         0.0000e+00],
        [6.8460e-03, 2.7715e-04, 0.0000e+00,  ..., 7.1521e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51445.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0272, 0.0013, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0307, 0.0072, 0.0048,  ..., 0.0000, 0.0138, 0.0000],
        [0.0363, 0.0202, 0.0158,  ..., 0.0000, 0.0275, 0.0000],
        ...,
        [0.0401, 0.0414, 0.0371,  ..., 0.0000, 0.0375, 0.0000],
        [0.0401, 0.0414, 0.0371,  ..., 0.0000, 0.0374, 0.0000],
        [0.0401, 0.0413, 0.0371,  ..., 0.0000, 0.0374, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469210.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3911.0674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.8450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16291.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-293.0815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(420.6008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.5367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0242],
        [-0.0858],
        [-0.2793],
        ...,
        [-1.1081],
        [-1.1049],
        [-1.1038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206264.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0034],
        ...,
        [1.0013],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367294.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0013],
        [1.0035],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367305.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6597e-02,  2.5773e-02, -1.5295e-03,  ...,  3.4682e-02,
         -1.5333e-02, -1.9076e-02],
        [ 4.1748e-02,  4.1396e-02, -2.4589e-03,  ...,  5.5751e-02,
         -2.4649e-02, -3.0667e-02],
        [ 3.0127e-02,  2.9413e-02, -1.7460e-03,  ...,  3.9590e-02,
         -1.7503e-02, -2.1776e-02],
        ...,
        [ 1.6649e-03,  6.1882e-05,  0.0000e+00,  ...,  1.0350e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6649e-03,  6.1882e-05,  0.0000e+00,  ...,  1.0350e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6649e-03,  6.1882e-05,  0.0000e+00,  ...,  1.0350e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1035.3417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.8243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6604, device='cuda:0')



h[100].sum tensor(44.5486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9354, device='cuda:0')



h[200].sum tensor(-3.0641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.7726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1429e-01, 1.1124e-01, 0.0000e+00,  ..., 1.4972e-01, 0.0000e+00,
         0.0000e+00],
        [1.4460e-01, 1.4249e-01, 0.0000e+00,  ..., 1.9186e-01, 0.0000e+00,
         0.0000e+00],
        [1.5277e-01, 1.5090e-01, 0.0000e+00,  ..., 2.0320e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.7983e-03, 2.5269e-04, 0.0000e+00,  ..., 4.2261e-05, 0.0000e+00,
         0.0000e+00],
        [6.7971e-03, 2.5264e-04, 0.0000e+00,  ..., 4.2254e-05, 0.0000e+00,
         0.0000e+00],
        [6.7942e-03, 2.5253e-04, 0.0000e+00,  ..., 4.2236e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43321.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0407, 0.0417, 0.0376,  ..., 0.0000, 0.0378, 0.0000],
        [0.0407, 0.0417, 0.0376,  ..., 0.0000, 0.0378, 0.0000],
        [0.0407, 0.0417, 0.0376,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420672.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4139.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.8956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15007.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-271.2801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(141.2748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-281.7393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0502],
        [ 0.0462],
        [ 0.0529],
        ...,
        [-1.1260],
        [-1.1227],
        [-1.1216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237588.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0013],
        [1.0035],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367305.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0036],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367316.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3556e-02,  1.2344e-02, -7.2228e-04,  ...,  1.6545e-02,
         -7.2949e-03, -9.0784e-03],
        [ 7.4116e-03,  6.0088e-03, -3.4886e-04,  ...,  8.0019e-03,
         -3.5234e-03, -4.3848e-03],
        [ 7.8158e-03,  6.4256e-03, -3.7342e-04,  ...,  8.5639e-03,
         -3.7715e-03, -4.6936e-03],
        ...,
        [ 1.6714e-03,  9.0217e-05,  0.0000e+00,  ...,  2.0988e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6714e-03,  9.0217e-05,  0.0000e+00,  ...,  2.0988e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6714e-03,  9.0217e-05,  0.0000e+00,  ...,  2.0988e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1299.9514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.4116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.1061, device='cuda:0')



h[100].sum tensor(48.0372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0745, device='cuda:0')



h[200].sum tensor(-4.4326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.6269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.6266e-02, 2.0550e-02, 0.0000e+00,  ..., 2.7308e-02, 0.0000e+00,
         0.0000e+00],
        [4.7750e-02, 4.2696e-02, 0.0000e+00,  ..., 5.7170e-02, 0.0000e+00,
         0.0000e+00],
        [2.6312e-02, 2.0584e-02, 0.0000e+00,  ..., 2.7353e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8261e-03, 3.6844e-04, 0.0000e+00,  ..., 8.5714e-05, 0.0000e+00,
         0.0000e+00],
        [6.8248e-03, 3.6838e-04, 0.0000e+00,  ..., 8.5698e-05, 0.0000e+00,
         0.0000e+00],
        [6.8219e-03, 3.6822e-04, 0.0000e+00,  ..., 8.5661e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52904.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0239, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0190, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0252, 0.0000, 0.0000,  ..., 0.0000, 0.0080, 0.0000],
        ...,
        [0.0409, 0.0421, 0.0380,  ..., 0.0000, 0.0378, 0.0000],
        [0.0409, 0.0421, 0.0380,  ..., 0.0000, 0.0378, 0.0000],
        [0.0409, 0.0421, 0.0380,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471012.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4013.8745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.4204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16480.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-298.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(363.7467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.8920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0174],
        [ 0.0098],
        [-0.1230],
        ...,
        [-1.1332],
        [-1.1300],
        [-1.1289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194889.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0036],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367316.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0037],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367326.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0938e-02,  1.9933e-02, -1.1577e-03,  ...,  2.6753e-02,
         -1.1780e-02, -1.4665e-02],
        [ 1.7056e-03,  1.1121e-04,  0.0000e+00,  ...,  2.5757e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7056e-03,  1.1121e-04,  0.0000e+00,  ...,  2.5757e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7056e-03,  1.1121e-04,  0.0000e+00,  ...,  2.5757e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7056e-03,  1.1121e-04,  0.0000e+00,  ...,  2.5757e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7056e-03,  1.1121e-04,  0.0000e+00,  ...,  2.5757e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1378.2922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.9108, device='cuda:0')



h[100].sum tensor(50.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7452, device='cuda:0')



h[200].sum tensor(-4.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.7326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0776, 0.0734, 0.0000,  ..., 0.0984, 0.0000, 0.0000],
        [0.0549, 0.0500, 0.0000,  ..., 0.0669, 0.0000, 0.0000],
        [0.0356, 0.0300, 0.0000,  ..., 0.0400, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0005, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0070, 0.0005, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0070, 0.0005, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54346.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0406, 0.0427, 0.0386,  ..., 0.0000, 0.0378, 0.0000],
        [0.0406, 0.0427, 0.0386,  ..., 0.0000, 0.0377, 0.0000],
        [0.0406, 0.0427, 0.0385,  ..., 0.0000, 0.0377, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475607.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3824.3413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.7832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16610.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.7063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(353.2287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-338.5388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0662],
        [ 0.0766],
        [ 0.0876],
        ...,
        [-1.1368],
        [-1.1337],
        [-1.1326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182681.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0037],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367326.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(501.0101, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0038],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367337.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00],
        [1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00],
        [1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00],
        [1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00],
        [1.7413e-03, 1.1248e-04, 0.0000e+00,  ..., 1.9256e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1336.2450, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.3965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.0753, device='cuda:0')



h[100].sum tensor(50.8241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0630, device='cuda:0')



h[200].sum tensor(-4.3180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.5910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9654e-03, 4.4994e-04, 0.0000e+00,  ..., 7.7026e-05, 0.0000e+00,
         0.0000e+00],
        [6.9719e-03, 4.5035e-04, 0.0000e+00,  ..., 7.7097e-05, 0.0000e+00,
         0.0000e+00],
        [1.3236e-02, 6.8948e-03, 0.0000e+00,  ..., 8.7661e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.1137e-03, 4.5951e-04, 0.0000e+00,  ..., 7.8665e-05, 0.0000e+00,
         0.0000e+00],
        [7.1123e-03, 4.5942e-04, 0.0000e+00,  ..., 7.8650e-05, 0.0000e+00,
         0.0000e+00],
        [7.1092e-03, 4.5922e-04, 0.0000e+00,  ..., 7.8616e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52408.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0390, 0.0391, 0.0348,  ..., 0.0000, 0.0351, 0.0000],
        [0.0375, 0.0287, 0.0240,  ..., 0.0000, 0.0309, 0.0000],
        [0.0329, 0.0142, 0.0119,  ..., 0.0000, 0.0188, 0.0000],
        ...,
        [0.0403, 0.0433, 0.0393,  ..., 0.0000, 0.0378, 0.0000],
        [0.0403, 0.0433, 0.0393,  ..., 0.0000, 0.0378, 0.0000],
        [0.0402, 0.0433, 0.0393,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472216.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3729.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.7306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16206.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-307.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(397.5427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.9697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5779],
        [-0.3392],
        [-0.1231],
        ...,
        [-1.1431],
        [-1.1400],
        [-1.1389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213759., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0038],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367337.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0039],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367347.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0755e-03,  7.6271e-03, -4.3145e-04,  ...,  1.0150e-02,
         -4.4570e-03, -5.5513e-03],
        [ 1.7688e-03,  1.0411e-04,  0.0000e+00,  ...,  5.8842e-06,
          0.0000e+00,  0.0000e+00],
        [ 9.0755e-03,  7.6271e-03, -4.3145e-04,  ...,  1.0150e-02,
         -4.4570e-03, -5.5513e-03],
        ...,
        [ 1.7688e-03,  1.0411e-04,  0.0000e+00,  ...,  5.8842e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7688e-03,  1.0411e-04,  0.0000e+00,  ...,  5.8842e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7688e-03,  1.0411e-04,  0.0000e+00,  ...,  5.8842e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2075.9771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(104.7097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-50.4245, device='cuda:0')



h[100].sum tensor(60.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7411, device='cuda:0')



h[200].sum tensor(-8.2539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-58.8344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.2200e-02, 1.5989e-02, 0.0000e+00,  ..., 2.1021e-02, 0.0000e+00,
         0.0000e+00],
        [3.8236e-02, 3.2493e-02, 0.0000e+00,  ..., 4.3274e-02, 0.0000e+00,
         0.0000e+00],
        [1.3068e-02, 6.5713e-03, 0.0000e+00,  ..., 8.3214e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2269e-03, 4.2538e-04, 0.0000e+00,  ..., 2.4042e-05, 0.0000e+00,
         0.0000e+00],
        [7.2255e-03, 4.2529e-04, 0.0000e+00,  ..., 2.4037e-05, 0.0000e+00,
         0.0000e+00],
        [7.2224e-03, 4.2511e-04, 0.0000e+00,  ..., 2.4027e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82925.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0230, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0296, 0.0062, 0.0047,  ..., 0.0000, 0.0132, 0.0000],
        ...,
        [0.0401, 0.0440, 0.0401,  ..., 0.0000, 0.0379, 0.0000],
        [0.0401, 0.0440, 0.0400,  ..., 0.0000, 0.0379, 0.0000],
        [0.0400, 0.0440, 0.0400,  ..., 0.0000, 0.0379, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683732.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3592.8472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(179.3116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20428.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-373.3486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1910.6539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.2384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0703],
        [ 0.0688],
        [ 0.0071],
        ...,
        [-1.1539],
        [-1.1507],
        [-1.1495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222860.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0039],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367347.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0040],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367358., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4691e-03,  5.9553e-03, -3.3233e-04,  ...,  7.8864e-03,
         -3.4591e-03, -4.3097e-03],
        [ 7.4691e-03,  5.9553e-03, -3.3233e-04,  ...,  7.8864e-03,
         -3.4591e-03, -4.3097e-03],
        [ 1.7883e-03,  1.0668e-04,  0.0000e+00,  ...,  1.7342e-07,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7883e-03,  1.0668e-04,  0.0000e+00,  ...,  1.7342e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.7883e-03,  1.0668e-04,  0.0000e+00,  ...,  1.7342e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.7883e-03,  1.0668e-04,  0.0000e+00,  ...,  1.7342e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1396.6704, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.8339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7899, device='cuda:0')



h[100].sum tensor(53.2909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3286, device='cuda:0')



h[200].sum tensor(-4.4340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.4247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.5274e-02, 1.9083e-02, 0.0000e+00,  ..., 2.5156e-02, 0.0000e+00,
         0.0000e+00],
        [2.1393e-02, 1.5080e-02, 0.0000e+00,  ..., 1.9759e-02, 0.0000e+00,
         0.0000e+00],
        [1.7507e-02, 1.1070e-02, 0.0000e+00,  ..., 1.4351e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3075e-03, 4.3592e-04, 0.0000e+00,  ..., 7.0864e-07, 0.0000e+00,
         0.0000e+00],
        [7.3061e-03, 4.3584e-04, 0.0000e+00,  ..., 7.0850e-07, 0.0000e+00,
         0.0000e+00],
        [7.3029e-03, 4.3565e-04, 0.0000e+00,  ..., 7.0819e-07, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52589.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4250e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.2342e-03,
         0.0000e+00],
        [2.6735e-02, 4.3911e-05, 0.0000e+00,  ..., 0.0000e+00, 5.3996e-03,
         0.0000e+00],
        [3.0055e-02, 2.6936e-03, 9.2615e-04,  ..., 0.0000e+00, 1.2033e-02,
         0.0000e+00],
        ...,
        [3.9981e-02, 4.4521e-02, 4.0695e-02,  ..., 0.0000e+00, 3.8045e-02,
         0.0000e+00],
        [3.9973e-02, 4.4512e-02, 4.0685e-02,  ..., 0.0000e+00, 3.8038e-02,
         0.0000e+00],
        [3.9956e-02, 4.4491e-02, 4.0662e-02,  ..., 0.0000e+00, 3.8020e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470116.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3573.3796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.2737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16234.2178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-317.1483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(331.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.4896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1053],
        [ 0.0692],
        [-0.0146],
        ...,
        [-1.1613],
        [-1.1577],
        [-1.1565]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198802.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0016],
        [1.0040],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367358., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0041],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367368.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7913e-03,  1.2335e-04,  0.0000e+00,  ...,  2.5803e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6556e-02,  1.5325e-02, -8.5576e-04,  ...,  2.0500e-02,
         -8.9752e-03, -1.1185e-02],
        [ 1.7913e-03,  1.2335e-04,  0.0000e+00,  ...,  2.5803e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7913e-03,  1.2335e-04,  0.0000e+00,  ...,  2.5803e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7913e-03,  1.2335e-04,  0.0000e+00,  ...,  2.5803e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7913e-03,  1.2335e-04,  0.0000e+00,  ...,  2.5803e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1270.3680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.8287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.9240, device='cuda:0')



h[100].sum tensor(51.9258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5201, device='cuda:0')



h[200].sum tensor(-3.6613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.7474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.8158e-02, 3.2403e-02, 0.0000e+00,  ..., 4.3036e-02, 0.0000e+00,
         0.0000e+00],
        [1.9241e-02, 1.2919e-02, 0.0000e+00,  ..., 1.6764e-02, 0.0000e+00,
         0.0000e+00],
        [4.4683e-02, 3.9106e-02, 0.0000e+00,  ..., 5.2073e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3210e-03, 5.0411e-04, 0.0000e+00,  ..., 1.0546e-05, 0.0000e+00,
         0.0000e+00],
        [7.3195e-03, 5.0401e-04, 0.0000e+00,  ..., 1.0543e-05, 0.0000e+00,
         0.0000e+00],
        [7.3163e-03, 5.0379e-04, 0.0000e+00,  ..., 1.0539e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48389.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0234, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0182, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0401, 0.0449, 0.0411,  ..., 0.0000, 0.0381, 0.0000],
        [0.0401, 0.0449, 0.0411,  ..., 0.0000, 0.0381, 0.0000],
        [0.0401, 0.0449, 0.0411,  ..., 0.0000, 0.0381, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448846.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3597.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(303.4688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15718.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-309.7352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(202.6190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-314.6387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1141],
        [ 0.1145],
        [ 0.1140],
        ...,
        [-1.1680],
        [-1.1642],
        [-1.1617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205875.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0041],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367368.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0042],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367379.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5272e-02,  1.4035e-02, -7.7457e-04,  ...,  1.8736e-02,
         -8.1858e-03, -1.0204e-02],
        [ 1.3689e-02,  1.2405e-02, -6.8367e-04,  ...,  1.6539e-02,
         -7.2251e-03, -9.0069e-03],
        [ 1.7840e-03,  1.4637e-04,  0.0000e+00,  ...,  9.5561e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7840e-03,  1.4637e-04,  0.0000e+00,  ...,  9.5561e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  1.4637e-04,  0.0000e+00,  ...,  9.5561e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  1.4637e-04,  0.0000e+00,  ...,  9.5561e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1414.9495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.5060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7464, device='cuda:0')



h[100].sum tensor(53.7190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3124, device='cuda:0')



h[200].sum tensor(-4.3624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.1409e-02, 5.6470e-02, 0.0000e+00,  ..., 7.5388e-02, 0.0000e+00,
         0.0000e+00],
        [3.0373e-02, 2.4505e-02, 0.0000e+00,  ..., 3.2289e-02, 0.0000e+00,
         0.0000e+00],
        [1.9079e-02, 1.2868e-02, 0.0000e+00,  ..., 1.6597e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2922e-03, 5.9829e-04, 0.0000e+00,  ..., 3.9061e-05, 0.0000e+00,
         0.0000e+00],
        [7.2907e-03, 5.9817e-04, 0.0000e+00,  ..., 3.9054e-05, 0.0000e+00,
         0.0000e+00],
        [7.2876e-03, 5.9791e-04, 0.0000e+00,  ..., 3.9037e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51356.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0239, 0.0000, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0308, 0.0108, 0.0088,  ..., 0.0000, 0.0165, 0.0000],
        ...,
        [0.0406, 0.0451, 0.0415,  ..., 0.0000, 0.0383, 0.0000],
        [0.0406, 0.0451, 0.0414,  ..., 0.0000, 0.0383, 0.0000],
        [0.0406, 0.0451, 0.0414,  ..., 0.0000, 0.0383, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458922.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3657.3804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.5454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15747.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.0370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(304.5729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-336.6056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2173],
        [-0.2570],
        [-0.3358],
        ...,
        [-1.1784],
        [-1.1752],
        [-1.1741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226004.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0042],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367379.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0043],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367389.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1602e-03,  5.7242e-03, -3.0649e-04,  ...,  7.5024e-03,
         -3.2638e-03, -4.0699e-03],
        [ 1.4917e-02,  1.3712e-02, -7.4779e-04,  ...,  1.8272e-02,
         -7.9634e-03, -9.9300e-03],
        [ 8.8571e-03,  7.4716e-03, -4.0303e-04,  ...,  9.8584e-03,
         -4.2919e-03, -5.3519e-03],
        ...,
        [ 1.7732e-03,  1.7677e-04,  0.0000e+00,  ...,  2.2817e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7732e-03,  1.7677e-04,  0.0000e+00,  ...,  2.2817e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7732e-03,  1.7677e-04,  0.0000e+00,  ...,  2.2817e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1245.5244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.8548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3255, device='cuda:0')



h[100].sum tensor(51.5413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9260, device='cuda:0')



h[200].sum tensor(-3.3821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.5067e-02, 3.9811e-02, 0.0000e+00,  ..., 5.2814e-02, 0.0000e+00,
         0.0000e+00],
        [3.0332e-02, 2.4630e-02, 0.0000e+00,  ..., 3.2345e-02, 0.0000e+00,
         0.0000e+00],
        [4.5725e-02, 4.0473e-02, 0.0000e+00,  ..., 5.3706e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2494e-03, 7.2268e-04, 0.0000e+00,  ..., 9.3280e-05, 0.0000e+00,
         0.0000e+00],
        [7.2480e-03, 7.2254e-04, 0.0000e+00,  ..., 9.3262e-05, 0.0000e+00,
         0.0000e+00],
        [7.2448e-03, 7.2222e-04, 0.0000e+00,  ..., 9.3221e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47055.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0233, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0190, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0410, 0.0430, 0.0393,  ..., 0.0000, 0.0376, 0.0000],
        [0.0413, 0.0452, 0.0415,  ..., 0.0000, 0.0385, 0.0000],
        [0.0413, 0.0452, 0.0415,  ..., 0.0000, 0.0384, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442730.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3816.6343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.3668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15532.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-308.3436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(163.2150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.2612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3075],
        [-0.1709],
        [-0.0888],
        ...,
        [-0.9020],
        [-1.0366],
        [-1.1282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201691.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0043],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367389.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0043],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367399.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00],
        [1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00],
        [1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00],
        [1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00],
        [1.7568e-03, 2.0547e-04, 0.0000e+00,  ..., 3.2570e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1172.8977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.8895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6178, device='cuda:0')



h[100].sum tensor(50.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9196, device='cuda:0')



h[200].sum tensor(-2.9318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.7229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0070, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0070, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0072, 0.0008, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44838.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0411, 0.0442, 0.0405,  ..., 0.0000, 0.0378, 0.0000],
        [0.0409, 0.0426, 0.0389,  ..., 0.0000, 0.0372, 0.0000],
        [0.0407, 0.0404, 0.0367,  ..., 0.0000, 0.0362, 0.0000],
        ...,
        [0.0420, 0.0452, 0.0417,  ..., 0.0000, 0.0387, 0.0000],
        [0.0420, 0.0452, 0.0416,  ..., 0.0000, 0.0387, 0.0000],
        [0.0420, 0.0452, 0.0416,  ..., 0.0000, 0.0387, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432019.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3966.2339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.7470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15262.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-304.0942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(87.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.3485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0923],
        [-0.9100],
        [-0.6706],
        ...,
        [-1.1840],
        [-1.1751],
        [-1.1677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207084.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0043],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367399.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0044],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367410.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00],
        [1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00],
        [1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00],
        [1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00],
        [1.7282e-03, 2.2139e-04, 0.0000e+00,  ..., 3.3585e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1244.2700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.9307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8436, device='cuda:0')



h[100].sum tensor(50.8348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7469, device='cuda:0')



h[200].sum tensor(-3.2817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46631.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0443, 0.0407,  ..., 0.0000, 0.0383, 0.0000],
        [0.0420, 0.0444, 0.0407,  ..., 0.0000, 0.0383, 0.0000],
        [0.0420, 0.0445, 0.0409,  ..., 0.0000, 0.0384, 0.0000],
        ...,
        [0.0429, 0.0454, 0.0418,  ..., 0.0000, 0.0392, 0.0000],
        [0.0429, 0.0454, 0.0418,  ..., 0.0000, 0.0392, 0.0000],
        [0.0429, 0.0453, 0.0418,  ..., 0.0000, 0.0391, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442586., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4137.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.3729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15432.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.0612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(191.2624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-316.4893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0851],
        [-1.2099],
        [-1.3074],
        ...,
        [-1.2049],
        [-1.2019],
        [-1.2011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213150.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0044],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367410.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0045],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367420.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6958e-03,  2.2959e-04,  0.0000e+00,  ...,  2.7785e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.4229e-02,  1.3140e-02, -6.9342e-04,  ...,  1.7433e-02,
         -7.5566e-03, -9.4308e-03],
        [ 2.0387e-02,  1.9482e-02, -1.0341e-03,  ...,  2.5984e-02,
         -1.1269e-02, -1.4064e-02],
        ...,
        [ 1.6958e-03,  2.2959e-04,  0.0000e+00,  ...,  2.7785e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6958e-03,  2.2959e-04,  0.0000e+00,  ...,  2.7785e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6958e-03,  2.2959e-04,  0.0000e+00,  ...,  2.7785e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1171.3413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.7854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5559, device='cuda:0')



h[100].sum tensor(49.2434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8966, device='cuda:0')



h[200].sum tensor(-2.9008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.6507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0420, 0.0372, 0.0000,  ..., 0.0490, 0.0000, 0.0000],
        [0.0358, 0.0308, 0.0000,  ..., 0.0403, 0.0000, 0.0000],
        [0.0347, 0.0296, 0.0000,  ..., 0.0388, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0009, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44292.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0287, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0260, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0438, 0.0457, 0.0421,  ..., 0.0000, 0.0397, 0.0000],
        [0.0438, 0.0457, 0.0421,  ..., 0.0000, 0.0397, 0.0000],
        [0.0437, 0.0456, 0.0421,  ..., 0.0000, 0.0397, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433244.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4310.1421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.4073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15228.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(109.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.7965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1059],
        [ 0.1577],
        [ 0.1773],
        ...,
        [-1.2254],
        [-1.2221],
        [-1.2209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229785.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0045],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367420.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0046],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367430.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00],
        [1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00],
        [1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00],
        [1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00],
        [1.6648e-03, 2.3767e-04, 0.0000e+00,  ..., 2.1949e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1293.3884, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.2632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.4536, device='cuda:0')



h[100].sum tensor(50.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3452, device='cuda:0')



h[200].sum tensor(-3.5200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.1984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.6607e-03, 9.5093e-04, 0.0000e+00,  ..., 8.7816e-05, 0.0000e+00,
         0.0000e+00],
        [6.6679e-03, 9.5196e-04, 0.0000e+00,  ..., 8.7912e-05, 0.0000e+00,
         0.0000e+00],
        [6.6766e-03, 9.5320e-04, 0.0000e+00,  ..., 8.8026e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8099e-03, 9.7223e-04, 0.0000e+00,  ..., 8.9784e-05, 0.0000e+00,
         0.0000e+00],
        [6.8086e-03, 9.7204e-04, 0.0000e+00,  ..., 8.9766e-05, 0.0000e+00,
         0.0000e+00],
        [6.8055e-03, 9.7161e-04, 0.0000e+00,  ..., 8.9726e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50495.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0429, 0.0408, 0.0371,  ..., 0.0000, 0.0375, 0.0000],
        [0.0420, 0.0321, 0.0283,  ..., 0.0000, 0.0338, 0.0000],
        [0.0415, 0.0277, 0.0239,  ..., 0.0000, 0.0319, 0.0000],
        ...,
        [0.0444, 0.0460, 0.0424,  ..., 0.0000, 0.0401, 0.0000],
        [0.0444, 0.0459, 0.0424,  ..., 0.0000, 0.0401, 0.0000],
        [0.0444, 0.0459, 0.0424,  ..., 0.0000, 0.0401, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477755.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4351.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16285.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-313.5580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(353.0341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.1781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3906],
        [-0.3709],
        [-0.3053],
        ...,
        [-1.2414],
        [-1.2380],
        [-1.2367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226532.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0020],
        [1.0046],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367430.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(512.7442, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0047],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367440.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9082e-02,  1.8204e-02, -9.4668e-04,  ...,  2.4228e-02,
         -1.0478e-02, -1.3084e-02],
        [ 1.6458e-03,  2.4464e-04,  0.0000e+00,  ...,  1.6075e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6458e-03,  2.4464e-04,  0.0000e+00,  ...,  1.6075e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6458e-03,  2.4464e-04,  0.0000e+00,  ...,  1.6075e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6458e-03,  2.4464e-04,  0.0000e+00,  ...,  1.6075e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6458e-03,  2.4464e-04,  0.0000e+00,  ...,  1.6075e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1311.9448, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.0081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8409, device='cuda:0')



h[100].sum tensor(50.2295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4892, device='cuda:0')



h[200].sum tensor(-3.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.0861e-02, 5.6884e-02, 0.0000e+00,  ..., 7.5431e-02, 0.0000e+00,
         0.0000e+00],
        [3.8301e-02, 3.3640e-02, 0.0000e+00,  ..., 4.4094e-02, 0.0000e+00,
         0.0000e+00],
        [6.6010e-03, 9.8122e-04, 0.0000e+00,  ..., 6.4474e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.7333e-03, 1.0009e-03, 0.0000e+00,  ..., 6.5767e-05, 0.0000e+00,
         0.0000e+00],
        [6.7320e-03, 1.0007e-03, 0.0000e+00,  ..., 6.5754e-05, 0.0000e+00,
         0.0000e+00],
        [6.7290e-03, 1.0002e-03, 0.0000e+00,  ..., 6.5725e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49994.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0260, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0350, 0.0000, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        ...,
        [0.0449, 0.0461, 0.0428,  ..., 0.0000, 0.0406, 0.0000],
        [0.0449, 0.0461, 0.0427,  ..., 0.0000, 0.0406, 0.0000],
        [0.0449, 0.0461, 0.0427,  ..., 0.0000, 0.0406, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472168.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4418.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.1684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16305.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.5089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(264.9817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.0861, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0231],
        [-0.0223],
        [-0.0084],
        ...,
        [-1.2571],
        [-1.2536],
        [-1.2523]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221700.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0047],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367440.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0048],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367450.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1474e-02,  2.0664e-02, -1.0665e-03,  ...,  2.7535e-02,
         -1.1896e-02, -1.4859e-02],
        [ 1.0549e-02,  9.4170e-03, -4.7910e-04,  ...,  1.2372e-02,
         -5.3440e-03, -6.6751e-03],
        [ 1.3078e-02,  1.2021e-02, -6.1508e-04,  ...,  1.5882e-02,
         -6.8606e-03, -8.5696e-03],
        ...,
        [ 1.6388e-03,  2.4347e-04,  0.0000e+00,  ...,  4.5673e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6388e-03,  2.4347e-04,  0.0000e+00,  ...,  4.5673e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6388e-03,  2.4347e-04,  0.0000e+00,  ...,  4.5673e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1516.6348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.4556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.5772, device='cuda:0')



h[100].sum tensor(53.1278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9929, device='cuda:0')



h[200].sum tensor(-4.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.5101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.3656e-02, 5.9757e-02, 0.0000e+00,  ..., 7.9269e-02, 0.0000e+00,
         0.0000e+00],
        [6.3874e-02, 5.9975e-02, 0.0000e+00,  ..., 7.9561e-02, 0.0000e+00,
         0.0000e+00],
        [2.4866e-02, 1.9809e-02, 0.0000e+00,  ..., 2.5408e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.7055e-03, 9.9621e-04, 0.0000e+00,  ..., 1.8689e-05, 0.0000e+00,
         0.0000e+00],
        [6.7042e-03, 9.9602e-04, 0.0000e+00,  ..., 1.8685e-05, 0.0000e+00,
         0.0000e+00],
        [6.7012e-03, 9.9557e-04, 0.0000e+00,  ..., 1.8677e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56314.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0246, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        ...,
        [0.0450, 0.0434, 0.0403,  ..., 0.0000, 0.0397, 0.0000],
        [0.0443, 0.0377, 0.0344,  ..., 0.0000, 0.0370, 0.0000],
        [0.0440, 0.0348, 0.0315,  ..., 0.0000, 0.0356, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502470.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4398.3442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.4005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17142.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.4545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(409.0512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.6037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1221],
        [ 0.1215],
        [ 0.1235],
        ...,
        [-1.1314],
        [-1.0284],
        [-0.9605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214918.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0021],
        [1.0048],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367450.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0048],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367461.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6531e-03,  2.3615e-04,  0.0000e+00,  ..., -4.6263e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1354.0970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.8315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.7589, device='cuda:0')



h[100].sum tensor(52.4830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8304, device='cuda:0')



h[200].sum tensor(-3.6412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.7215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49132.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0390, 0.0181, 0.0165,  ..., 0.0000, 0.0218, 0.0000],
        [0.0429, 0.0314, 0.0282,  ..., 0.0000, 0.0348, 0.0000],
        [0.0442, 0.0425, 0.0394,  ..., 0.0000, 0.0393, 0.0000],
        ...,
        [0.0455, 0.0465, 0.0434,  ..., 0.0000, 0.0415, 0.0000],
        [0.0454, 0.0465, 0.0434,  ..., 0.0000, 0.0415, 0.0000],
        [0.0454, 0.0464, 0.0434,  ..., 0.0000, 0.0415, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463196.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4487.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.4243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15921.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-320.5865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(278.1130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.7047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1543],
        [-0.3704],
        [-0.5431],
        ...,
        [-1.2798],
        [-1.2761],
        [-1.2750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248808.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0048],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367461.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0049],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367471.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2136e-02,  1.0995e-02, -5.5138e-04,  ...,  1.4492e-02,
         -6.2470e-03, -7.8076e-03],
        [ 9.3208e-03,  8.1019e-03, -4.0301e-04,  ...,  1.0591e-02,
         -4.5660e-03, -5.7067e-03],
        [ 7.0068e-03,  5.7234e-03, -2.8104e-04,  ...,  7.3833e-03,
         -3.1841e-03, -3.9796e-03],
        ...,
        [ 1.6749e-03,  2.4309e-04,  0.0000e+00,  ..., -7.0003e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6749e-03,  2.4309e-04,  0.0000e+00,  ..., -7.0003e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6749e-03,  2.4309e-04,  0.0000e+00,  ..., -7.0003e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1281.6198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.9409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8115, device='cuda:0')



h[100].sum tensor(52.9238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7350, device='cuda:0')



h[200].sum tensor(-3.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0430, 0.0383, 0.0000,  ..., 0.0503, 0.0000, 0.0000],
        [0.0331, 0.0281, 0.0000,  ..., 0.0366, 0.0000, 0.0000],
        [0.0279, 0.0228, 0.0000,  ..., 0.0294, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46256.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0298, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0301, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0315, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0456, 0.0465, 0.0435,  ..., 0.0000, 0.0417, 0.0000],
        [0.0456, 0.0465, 0.0435,  ..., 0.0000, 0.0416, 0.0000],
        [0.0456, 0.0464, 0.0435,  ..., 0.0000, 0.0416, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449240.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4520.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.0428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15707.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-321.1924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(159.7072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-323.9610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0629],
        [ 0.1030],
        [ 0.1076],
        ...,
        [-1.2830],
        [-1.2794],
        [-1.2783]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231760.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0049],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367471.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0050],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367482.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6953e-03,  2.4991e-04,  0.0000e+00,  ..., -7.5325e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03,  2.4991e-04,  0.0000e+00,  ..., -7.5325e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6768e-02,  1.5732e-02, -7.8662e-04,  ...,  2.0872e-02,
         -8.9825e-03, -1.1230e-02],
        ...,
        [ 1.6953e-03,  2.4991e-04,  0.0000e+00,  ..., -7.5325e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03,  2.4991e-04,  0.0000e+00,  ..., -7.5325e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6953e-03,  2.4991e-04,  0.0000e+00,  ..., -7.5325e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1662.2874, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.9944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.2318, device='cuda:0')



h[100].sum tensor(58.5647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9795, device='cuda:0')



h[200].sum tensor(-4.9547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.6075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0219, 0.0165, 0.0000,  ..., 0.0209, 0.0000, 0.0000],
        [0.0568, 0.0524, 0.0000,  ..., 0.0693, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60192.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0410, 0.0205, 0.0188,  ..., 0.0000, 0.0271, 0.0000],
        [0.0301, 0.0069, 0.0057,  ..., 0.0000, 0.0147, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0459, 0.0465, 0.0437,  ..., 0.0000, 0.0418, 0.0000],
        [0.0459, 0.0465, 0.0437,  ..., 0.0000, 0.0418, 0.0000],
        [0.0458, 0.0465, 0.0436,  ..., 0.0000, 0.0417, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534987.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4443.6538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.2218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17622.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-356.7308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(711.8824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.7426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1513],
        [-0.0673],
        [-0.0099],
        ...,
        [-1.2868],
        [-1.2835],
        [-1.2824]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221426.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0050],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367482.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0051],
        ...,
        [1.0013],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367492.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7095e-03,  2.6063e-04,  0.0000e+00,  ..., -4.8467e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1405.3763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.0249, device='cuda:0')



h[100].sum tensor(56.3096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9293, device='cuda:0')



h[200].sum tensor(-3.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50133.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.0454, 0.0426,  ..., 0.0000, 0.0408, 0.0000],
        [0.0452, 0.0450, 0.0422,  ..., 0.0000, 0.0406, 0.0000],
        [0.0452, 0.0442, 0.0413,  ..., 0.0000, 0.0398, 0.0000],
        ...,
        [0.0462, 0.0465, 0.0438,  ..., 0.0000, 0.0418, 0.0000],
        [0.0462, 0.0464, 0.0438,  ..., 0.0000, 0.0418, 0.0000],
        [0.0462, 0.0464, 0.0438,  ..., 0.0000, 0.0418, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471712.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4491.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.2068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16462.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-342.0063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(203.5532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.8246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2862],
        [-1.1453],
        [-0.9332],
        ...,
        [-1.2874],
        [-1.2831],
        [-1.2811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195415.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0051],
        ...,
        [1.0013],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367492.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0052],
        ...,
        [1.0013],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367503.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9580e-02,  1.8617e-02, -9.1499e-04,  ...,  2.4751e-02,
         -1.0615e-02, -1.3278e-02],
        [ 1.6971e-03,  2.6691e-04,  0.0000e+00,  ...,  9.1632e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.6971e-03,  2.6691e-04,  0.0000e+00,  ...,  9.1632e-07,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6971e-03,  2.6691e-04,  0.0000e+00,  ...,  9.1632e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.6971e-03,  2.6691e-04,  0.0000e+00,  ...,  9.1632e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.6971e-03,  2.6691e-04,  0.0000e+00,  ...,  9.1632e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1201.6399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.3693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8210, device='cuda:0')



h[100].sum tensor(53.9949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2518, device='cuda:0')



h[200].sum tensor(-2.5416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.7084e-02, 5.2674e-02, 0.0000e+00,  ..., 6.9609e-02, 0.0000e+00,
         0.0000e+00],
        [3.9325e-02, 3.4445e-02, 0.0000e+00,  ..., 4.5020e-02, 0.0000e+00,
         0.0000e+00],
        [6.8090e-03, 1.0708e-03, 0.0000e+00,  ..., 3.6763e-06, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9491e-03, 1.0929e-03, 0.0000e+00,  ..., 3.7520e-06, 0.0000e+00,
         0.0000e+00],
        [6.9478e-03, 1.0927e-03, 0.0000e+00,  ..., 3.7513e-06, 0.0000e+00,
         0.0000e+00],
        [6.9448e-03, 1.0922e-03, 0.0000e+00,  ..., 3.7497e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43435.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0300, 0.0000, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0409, 0.0198, 0.0187,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0469, 0.0463, 0.0440,  ..., 0.0000, 0.0421, 0.0000],
        [0.0469, 0.0463, 0.0440,  ..., 0.0000, 0.0421, 0.0000],
        [0.0468, 0.0463, 0.0439,  ..., 0.0000, 0.0420, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437075.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4716.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(336.2002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15289.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-324.8799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(106.9140, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.6978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0105],
        [-0.1058],
        [-0.3546],
        ...,
        [-1.2999],
        [-1.2968],
        [-1.2955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233137.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0052],
        ...,
        [1.0013],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367503.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0053],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367513.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6664e-03,  2.7886e-04,  0.0000e+00,  ...,  1.2021e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.7127e-03,  6.4822e-03, -3.0639e-04,  ...,  8.3790e-03,
         -3.5826e-03, -4.4828e-03],
        [ 1.6664e-03,  2.7886e-04,  0.0000e+00,  ...,  1.2021e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6664e-03,  2.7886e-04,  0.0000e+00,  ...,  1.2021e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6664e-03,  2.7886e-04,  0.0000e+00,  ...,  1.2021e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6664e-03,  2.7886e-04,  0.0000e+00,  ...,  1.2021e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2220.0542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(100.0721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-50.0942, device='cuda:0')



h[100].sum tensor(65.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6184, device='cuda:0')



h[200].sum tensor(-7.4778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-58.4490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.6098e-02, 4.1570e-02, 0.0000e+00,  ..., 5.4612e-02, 0.0000e+00,
         0.0000e+00],
        [1.1619e-02, 6.1882e-03, 0.0000e+00,  ..., 6.8878e-03, 0.0000e+00,
         0.0000e+00],
        [4.4784e-02, 4.0206e-02, 0.0000e+00,  ..., 5.2769e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8244e-03, 1.1420e-03, 0.0000e+00,  ..., 4.9228e-05, 0.0000e+00,
         0.0000e+00],
        [6.8230e-03, 1.1418e-03, 0.0000e+00,  ..., 4.9218e-05, 0.0000e+00,
         0.0000e+00],
        [6.8201e-03, 1.1413e-03, 0.0000e+00,  ..., 4.9197e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76822.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0251, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0275, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0192, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0477, 0.0461, 0.0441,  ..., 0.0000, 0.0422, 0.0000],
        [0.0477, 0.0461, 0.0441,  ..., 0.0000, 0.0422, 0.0000],
        [0.0477, 0.0460, 0.0441,  ..., 0.0000, 0.0422, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619842., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4562.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.3134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19725.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-403.8111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1209.8401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-490.5288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0832],
        [-0.0946],
        [-0.1065],
        ...,
        [-1.3080],
        [-1.3050],
        [-1.3044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197773.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0053],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367513.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4709e-02,  2.3963e-02, -1.1582e-03,  ...,  3.1953e-02,
         -1.3651e-02, -1.7086e-02],
        [ 2.6629e-02,  2.5933e-02, -1.2546e-03,  ...,  3.4610e-02,
         -1.4787e-02, -1.8507e-02],
        [ 2.4932e-02,  2.4191e-02, -1.1693e-03,  ...,  3.2261e-02,
         -1.3782e-02, -1.7250e-02],
        ...,
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1574.0974, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.7889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.0697, device='cuda:0')



h[100].sum tensor(57.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8043, device='cuda:0')



h[200].sum tensor(-4.3297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.3967e-02, 9.0851e-02, 0.0000e+00,  ..., 1.2107e-01, 0.0000e+00,
         0.0000e+00],
        [1.0735e-01, 1.0457e-01, 0.0000e+00,  ..., 1.3958e-01, 0.0000e+00,
         0.0000e+00],
        [1.1152e-01, 1.0884e-01, 0.0000e+00,  ..., 1.4533e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.6924e-03, 1.1720e-03, 0.0000e+00,  ..., 7.8100e-05, 0.0000e+00,
         0.0000e+00],
        [6.6911e-03, 1.1718e-03, 0.0000e+00,  ..., 7.8084e-05, 0.0000e+00,
         0.0000e+00],
        [6.6883e-03, 1.1713e-03, 0.0000e+00,  ..., 7.8051e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56994.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0458, 0.0442,  ..., 0.0000, 0.0424, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512258.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4879.1572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(311.3737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17440.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-360.1058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(435.7166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.8051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0655],
        [-0.0630],
        [-0.0565],
        ...,
        [-1.3239],
        [-1.3205],
        [-1.3194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188942.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8026e-02,  2.7365e-02, -1.3246e-03,  ...,  3.6543e-02,
         -1.5613e-02, -1.9541e-02],
        [ 2.0069e-02,  1.9201e-02, -9.2528e-04,  ...,  2.5531e-02,
         -1.0906e-02, -1.3650e-02],
        [ 7.7995e-03,  6.6124e-03, -3.0946e-04,  ...,  8.5516e-03,
         -3.6474e-03, -4.5651e-03],
        ...,
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1482.4880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.2082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.1584, device='cuda:0')



h[100].sum tensor(56.4275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7222, device='cuda:0')



h[200].sum tensor(-3.8875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0223e-01, 9.9333e-02, 0.0000e+00,  ..., 1.3251e-01, 0.0000e+00,
         0.0000e+00],
        [6.0054e-02, 5.6048e-02, 0.0000e+00,  ..., 7.4126e-02, 0.0000e+00,
         0.0000e+00],
        [5.2465e-02, 4.8253e-02, 0.0000e+00,  ..., 6.3610e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.6924e-03, 1.1720e-03, 0.0000e+00,  ..., 7.8100e-05, 0.0000e+00,
         0.0000e+00],
        [6.6911e-03, 1.1718e-03, 0.0000e+00,  ..., 7.8084e-05, 0.0000e+00,
         0.0000e+00],
        [6.6883e-03, 1.1713e-03, 0.0000e+00,  ..., 7.8051e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52752.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0200, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0256, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0458, 0.0442,  ..., 0.0000, 0.0424, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485733.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4935.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.6240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16810.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-350.2551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(267.6028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.9405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.0710],
        [ 0.0891],
        ...,
        [-1.3239],
        [-1.3205],
        [-1.3194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195443.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.7581e-03,  7.5959e-03, -3.5757e-04,  ...,  9.8782e-03,
         -4.2145e-03, -5.2749e-03],
        ...,
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6340e-03,  2.8615e-04,  0.0000e+00,  ...,  1.9068e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1394.3970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.7266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.2658, device='cuda:0')



h[100].sum tensor(55.3742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6471, device='cuda:0')



h[200].sum tensor(-3.4623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.5387e-03, 1.1451e-03, 0.0000e+00,  ..., 7.6306e-05, 0.0000e+00,
         0.0000e+00],
        [1.3686e-02, 8.4721e-03, 0.0000e+00,  ..., 9.9568e-03, 0.0000e+00,
         0.0000e+00],
        [2.1033e-02, 1.6003e-02, 0.0000e+00,  ..., 2.0112e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.6924e-03, 1.1720e-03, 0.0000e+00,  ..., 7.8100e-05, 0.0000e+00,
         0.0000e+00],
        [6.6911e-03, 1.1718e-03, 0.0000e+00,  ..., 7.8084e-05, 0.0000e+00,
         0.0000e+00],
        [6.6883e-03, 1.1713e-03, 0.0000e+00,  ..., 7.8051e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51660.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0454, 0.0225, 0.0210,  ..., 0.0000, 0.0316, 0.0000],
        [0.0436, 0.0120, 0.0112,  ..., 0.0000, 0.0227, 0.0000],
        [0.0408, 0.0033, 0.0025,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0459, 0.0443,  ..., 0.0000, 0.0425, 0.0000],
        [0.0486, 0.0458, 0.0442,  ..., 0.0000, 0.0424, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487262.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4949.2783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.5200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16720.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-346.2454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(336.0709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.2917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2881],
        [-0.1126],
        [ 0.0088],
        ...,
        [-1.3239],
        [-1.3205],
        [-1.3192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206154.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0024],
        [1.0054],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0055],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367534.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1514e-02,  2.0704e-02, -9.8956e-04,  ...,  2.7561e-02,
         -1.1757e-02, -1.4719e-02],
        [ 2.0066e-02,  1.9218e-02, -9.1757e-04,  ...,  2.5556e-02,
         -1.0901e-02, -1.3648e-02],
        [ 6.2854e-03,  5.0785e-03, -2.3254e-04,  ...,  6.4857e-03,
         -2.7627e-03, -3.4589e-03],
        ...,
        [ 1.6075e-03,  2.7860e-04,  0.0000e+00,  ...,  1.2100e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6075e-03,  2.7860e-04,  0.0000e+00,  ...,  1.2100e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6075e-03,  2.7860e-04,  0.0000e+00,  ...,  1.2100e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1549.1470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.4102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.3777, device='cuda:0')



h[100].sum tensor(56.6418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5471, device='cuda:0')



h[200].sum tensor(-4.2286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.6322e-02, 9.3347e-02, 0.0000e+00,  ..., 1.2444e-01, 0.0000e+00,
         0.0000e+00],
        [7.2499e-02, 6.8897e-02, 0.0000e+00,  ..., 9.1464e-02, 0.0000e+00,
         0.0000e+00],
        [9.0335e-02, 8.7189e-02, 0.0000e+00,  ..., 1.1613e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.5851e-03, 1.1413e-03, 0.0000e+00,  ..., 4.9567e-05, 0.0000e+00,
         0.0000e+00],
        [6.5838e-03, 1.1410e-03, 0.0000e+00,  ..., 4.9557e-05, 0.0000e+00,
         0.0000e+00],
        [6.5810e-03, 1.1405e-03, 0.0000e+00,  ..., 4.9536e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54388.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0114, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0494, 0.0460, 0.0447,  ..., 0.0000, 0.0428, 0.0000],
        [0.0494, 0.0460, 0.0447,  ..., 0.0000, 0.0428, 0.0000],
        [0.0494, 0.0460, 0.0447,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494295.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5082.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.5149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16915.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-350.8078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(343.4934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.6235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0244],
        [-0.0298],
        [-0.0298],
        ...,
        [-1.3398],
        [-1.3361],
        [-1.3349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215944.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0055],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367534.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0056],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367544.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5765e-03,  2.6579e-04,  0.0000e+00,  ...,  1.1240e-07,
          0.0000e+00,  0.0000e+00],
        [ 9.0874e-03,  7.9730e-03, -3.6978e-04,  ...,  1.0394e-02,
         -4.4285e-03, -5.5460e-03],
        [ 1.5765e-03,  2.6579e-04,  0.0000e+00,  ...,  1.1240e-07,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5765e-03,  2.6579e-04,  0.0000e+00,  ...,  1.1240e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.5765e-03,  2.6579e-04,  0.0000e+00,  ...,  1.1240e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.5765e-03,  2.6579e-04,  0.0000e+00,  ...,  1.1240e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1454.8427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.0286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9160, device='cuda:0')



h[100].sum tensor(54.7725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6321, device='cuda:0')



h[200].sum tensor(-3.8146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.3825e-02, 8.7761e-03, 0.0000e+00,  ..., 1.0402e-02, 0.0000e+00,
         0.0000e+00],
        [1.2462e-02, 7.3710e-03, 0.0000e+00,  ..., 8.5048e-03, 0.0000e+00,
         0.0000e+00],
        [3.7830e-02, 3.3394e-02, 0.0000e+00,  ..., 4.3598e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4590e-03, 1.0889e-03, 0.0000e+00,  ..., 4.6051e-07, 0.0000e+00,
         0.0000e+00],
        [6.4577e-03, 1.0887e-03, 0.0000e+00,  ..., 4.6042e-07, 0.0000e+00,
         0.0000e+00],
        [6.4550e-03, 1.0883e-03, 0.0000e+00,  ..., 4.6022e-07, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53956.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0463, 0.0156, 0.0152,  ..., 0.0000, 0.0257, 0.0000],
        [0.0448, 0.0066, 0.0061,  ..., 0.0000, 0.0174, 0.0000],
        [0.0416, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        ...,
        [0.0502, 0.0462, 0.0453,  ..., 0.0000, 0.0432, 0.0000],
        [0.0502, 0.0462, 0.0453,  ..., 0.0000, 0.0432, 0.0000],
        [0.0502, 0.0462, 0.0453,  ..., 0.0000, 0.0432, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505376.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5240.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.7678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17068.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-345.0498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(430.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.1552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2611],
        [-0.0829],
        [ 0.0181],
        ...,
        [-1.3686],
        [-1.3649],
        [-1.3637]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233620.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0056],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367544.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0057],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367553.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5726e-03,  2.6104e-04,  0.0000e+00,  ..., -9.0171e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1523.5300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.1138, device='cuda:0')



h[100].sum tensor(55.3309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4490, device='cuda:0')



h[200].sum tensor(-4.1419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.8027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0063, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0063, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0064, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0064, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52485.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0492, 0.0440, 0.0428,  ..., 0.0000, 0.0417, 0.0000],
        [0.0494, 0.0452, 0.0440,  ..., 0.0000, 0.0421, 0.0000],
        [0.0495, 0.0441, 0.0429,  ..., 0.0000, 0.0412, 0.0000],
        ...,
        [0.0506, 0.0468, 0.0458,  ..., 0.0000, 0.0435, 0.0000],
        [0.0506, 0.0468, 0.0458,  ..., 0.0000, 0.0435, 0.0000],
        [0.0506, 0.0467, 0.0458,  ..., 0.0000, 0.0435, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483453.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5331.8140, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.9776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16598.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-339.5816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(262.0308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-375.0324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0084],
        [-1.1312],
        [-1.1607],
        ...,
        [-1.3836],
        [-1.3804],
        [-1.3799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247188.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0025],
        [1.0057],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367553.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0058],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367563.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5800e-03,  2.5905e-04,  0.0000e+00,  ..., -1.6714e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5800e-03,  2.5905e-04,  0.0000e+00,  ..., -1.6714e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.4812e-03,  6.3155e-03, -2.8492e-04,  ...,  8.1494e-03,
         -3.4675e-03, -4.3450e-03],
        ...,
        [ 1.5800e-03,  2.5905e-04,  0.0000e+00,  ..., -1.6714e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5800e-03,  2.5905e-04,  0.0000e+00,  ..., -1.6714e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5800e-03,  2.5905e-04,  0.0000e+00,  ..., -1.6714e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1354.0354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.4564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.6126, device='cuda:0')



h[100].sum tensor(53.3895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4044, device='cuda:0')



h[200].sum tensor(-3.3276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0122, 0.0071, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0261, 0.0213, 0.0000,  ..., 0.0273, 0.0000, 0.0000],
        ...,
        [0.0065, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49956.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0428, 0.0037, 0.0030,  ..., 0.0000, 0.0116, 0.0000],
        [0.0417, 0.0021, 0.0015,  ..., 0.0000, 0.0068, 0.0000],
        [0.0396, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        ...,
        [0.0508, 0.0474, 0.0464,  ..., 0.0000, 0.0437, 0.0000],
        [0.0500, 0.0363, 0.0356,  ..., 0.0000, 0.0392, 0.0000],
        [0.0482, 0.0173, 0.0170,  ..., 0.0000, 0.0291, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480582.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5354.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.6212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16636.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-333.3531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(204.8290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.0025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0019],
        [ 0.0231],
        [ 0.0491],
        ...,
        [-1.1717],
        [-0.8841],
        [-0.5466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233239.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0058],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367563.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0059],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367573.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8619e-03,  6.6846e-03, -2.9863e-04,  ...,  8.6259e-03,
         -3.6638e-03, -4.5923e-03],
        [ 7.5007e-03,  6.3139e-03, -2.8137e-04,  ...,  8.1262e-03,
         -3.4520e-03, -4.3268e-03],
        [ 1.6142e-03,  2.7372e-04,  0.0000e+00,  ..., -1.7166e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6142e-03,  2.7372e-04,  0.0000e+00,  ..., -1.7166e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6142e-03,  2.7372e-04,  0.0000e+00,  ..., -1.7166e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6142e-03,  2.7372e-04,  0.0000e+00,  ..., -1.7166e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1417.3052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.0943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.1614, device='cuda:0')



h[100].sum tensor(54.9034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9800, device='cuda:0')



h[200].sum tensor(-3.5587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.1911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0392, 0.0347, 0.0000,  ..., 0.0452, 0.0000, 0.0000],
        [0.0226, 0.0177, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0124, 0.0072, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0066, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51129.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0436, 0.0035, 0.0029,  ..., 0.0000, 0.0119, 0.0000],
        [0.0467, 0.0165, 0.0160,  ..., 0.0000, 0.0248, 0.0000],
        ...,
        [0.0507, 0.0481, 0.0468,  ..., 0.0000, 0.0438, 0.0000],
        [0.0506, 0.0481, 0.0468,  ..., 0.0000, 0.0438, 0.0000],
        [0.0506, 0.0480, 0.0468,  ..., 0.0000, 0.0438, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489184.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5236.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.2596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16617.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-333.7850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(276.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.4384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0163],
        [-0.0709],
        [-0.2637],
        ...,
        [-1.4055],
        [-1.4022],
        [-1.4014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257759.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0059],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367573.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0026],
        [1.0060],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367583.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6606e-03,  2.9094e-04,  0.0000e+00,  ..., -1.5246e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1382.9192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.2835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.6798, device='cuda:0')



h[100].sum tensor(55.5334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4293, device='cuda:0')



h[200].sum tensor(-3.3104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.4623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49585.7852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0481, 0.0354, 0.0342,  ..., 0.0000, 0.0358, 0.0000],
        [0.0483, 0.0371, 0.0359,  ..., 0.0000, 0.0368, 0.0000],
        [0.0483, 0.0351, 0.0340,  ..., 0.0000, 0.0363, 0.0000],
        ...,
        [0.0503, 0.0485, 0.0473,  ..., 0.0000, 0.0438, 0.0000],
        [0.0503, 0.0485, 0.0473,  ..., 0.0000, 0.0438, 0.0000],
        [0.0503, 0.0485, 0.0473,  ..., 0.0000, 0.0438, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477607.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5116.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.2167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16404.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-331.9629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(169.4506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.2377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3041],
        [-0.3408],
        [-0.3236],
        ...,
        [-1.4130],
        [-1.4093],
        [-1.4082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243735.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0026],
        [1.0060],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367583.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0027],
        [1.0062],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367594.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1861.2489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.4733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.8224, device='cuda:0')



h[100].sum tensor(62.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0573, device='cuda:0')



h[200].sum tensor(-5.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.1305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65215.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0486, 0.0476, 0.0461,  ..., 0.0000, 0.0426, 0.0000],
        [0.0487, 0.0477, 0.0462,  ..., 0.0000, 0.0426, 0.0000],
        [0.0488, 0.0478, 0.0463,  ..., 0.0000, 0.0427, 0.0000],
        ...,
        [0.0499, 0.0488, 0.0475,  ..., 0.0000, 0.0437, 0.0000],
        [0.0499, 0.0488, 0.0475,  ..., 0.0000, 0.0437, 0.0000],
        [0.0498, 0.0488, 0.0475,  ..., 0.0000, 0.0436, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572667.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4864.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.0262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18384.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-367.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(809.4518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.5176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3596],
        [-1.4878],
        [-1.5772],
        ...,
        [-1.4050],
        [-1.4026],
        [-1.4025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233942.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0027],
        [1.0062],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367594.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0027],
        [1.0062],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367594.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7256e-03,  7.5198e-03, -3.2865e-04,  ...,  9.6927e-03,
         -4.0979e-03, -5.1395e-03],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7086e-03,  3.2366e-04,  0.0000e+00,  ..., -7.4836e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1371.4050, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.9164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.5882, device='cuda:0')



h[100].sum tensor(56.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0236, device='cuda:0')



h[200].sum tensor(-3.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.1888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0163, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0139, 0.0085, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0069, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47930.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0039, 0.0034,  ..., 0.0000, 0.0121, 0.0000],
        [0.0450, 0.0172, 0.0167,  ..., 0.0000, 0.0243, 0.0000],
        [0.0477, 0.0351, 0.0342,  ..., 0.0000, 0.0375, 0.0000],
        ...,
        [0.0499, 0.0488, 0.0475,  ..., 0.0000, 0.0437, 0.0000],
        [0.0499, 0.0488, 0.0475,  ..., 0.0000, 0.0437, 0.0000],
        [0.0498, 0.0488, 0.0475,  ..., 0.0000, 0.0436, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466559.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4990.5220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.2543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16036.2998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-329.6189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(148.7614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.2183, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2088],
        [-0.5501],
        [-0.9277],
        ...,
        [-1.4084],
        [-1.4052],
        [-1.4039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239336.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0027],
        [1.0062],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367594.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0063],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367604.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7729e-03,  3.4384e-04,  0.0000e+00,  ..., -5.0146e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.4598e-02,  1.3491e-02, -5.9453e-04,  ...,  1.7716e-02,
         -7.4738e-03, -9.3761e-03],
        [ 1.2893e-02,  1.1743e-02, -5.1547e-04,  ...,  1.5359e-02,
         -6.4800e-03, -8.1293e-03],
        ...,
        [ 1.7729e-03,  3.4384e-04,  0.0000e+00,  ..., -5.0146e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7729e-03,  3.4384e-04,  0.0000e+00,  ..., -5.0146e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7729e-03,  3.4384e-04,  0.0000e+00,  ..., -5.0146e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1486.0804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.4232, device='cuda:0')



h[100].sum tensor(59.3241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0773, device='cuda:0')



h[200].sum tensor(-3.5209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.4965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0199, 0.0145, 0.0000,  ..., 0.0177, 0.0000, 0.0000],
        [0.0288, 0.0236, 0.0000,  ..., 0.0299, 0.0000, 0.0000],
        [0.0751, 0.0711, 0.0000,  ..., 0.0939, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49792.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2072e-02, 1.1721e-02, 1.1711e-02,  ..., 0.0000e+00, 1.9685e-02,
         0.0000e+00],
        [3.7194e-02, 9.8332e-05, 0.0000e+00,  ..., 0.0000e+00, 9.3055e-03,
         0.0000e+00],
        [2.7888e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.6273e-02, 1.4809e-02, 1.5470e-02,  ..., 0.0000e+00, 2.9959e-02,
         0.0000e+00],
        [4.6974e-02, 2.3307e-02, 2.3489e-02,  ..., 0.0000e+00, 3.3292e-02,
         0.0000e+00],
        [4.8402e-02, 4.0637e-02, 3.9846e-02,  ..., 0.0000e+00, 4.0068e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471097.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4745.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.1768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16030.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.6315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(159.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.7091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1122],
        [-0.0067],
        [ 0.0682],
        ...,
        [-0.6328],
        [-0.7868],
        [-1.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231132.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0063],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367604.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(868.2150, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0028],
        [1.0063],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367613.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8214e-03,  3.6235e-04,  0.0000e+00,  ..., -1.2401e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1255.5393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.2185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4191, device='cuda:0')



h[100].sum tensor(57.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1024, device='cuda:0')



h[200].sum tensor(-2.3537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.1575, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44166.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0470, 0.0437, 0.0426,  ..., 0.0000, 0.0401, 0.0000],
        [0.0470, 0.0425, 0.0415,  ..., 0.0000, 0.0400, 0.0000],
        [0.0465, 0.0349, 0.0345,  ..., 0.0000, 0.0374, 0.0000],
        ...,
        [0.0487, 0.0498, 0.0486,  ..., 0.0000, 0.0435, 0.0000],
        [0.0487, 0.0497, 0.0485,  ..., 0.0000, 0.0434, 0.0000],
        [0.0486, 0.0497, 0.0485,  ..., 0.0000, 0.0434, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448394.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4637.9214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.3849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15451.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-323.4272, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40.3712, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-329.3241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4603],
        [-0.5034],
        [-0.4569],
        ...,
        [-1.3978],
        [-1.3944],
        [-1.3933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222279.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0028],
        [1.0063],
        ...,
        [1.0013],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367613.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0064],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367623.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8476e-03,  3.7423e-04,  0.0000e+00,  ...,  2.6808e-06,
          0.0000e+00,  0.0000e+00],
        [ 7.8562e-03,  6.5307e-03, -2.7290e-04,  ...,  8.2990e-03,
         -3.4872e-03, -4.3773e-03],
        [ 8.0033e-03,  6.6815e-03, -2.7958e-04,  ...,  8.5021e-03,
         -3.5726e-03, -4.4845e-03],
        ...,
        [ 1.8476e-03,  3.7423e-04,  0.0000e+00,  ...,  2.6808e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8476e-03,  3.7423e-04,  0.0000e+00,  ...,  2.6808e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8476e-03,  3.7423e-04,  0.0000e+00,  ...,  2.6808e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1561.9912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.2666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.7912, device='cuda:0')



h[100].sum tensor(61.7063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5857, device='cuda:0')



h[200].sum tensor(-3.6759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.0927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.3410e-02, 7.6601e-03, 0.0000e+00,  ..., 8.3147e-03, 0.0000e+00,
         0.0000e+00],
        [2.7420e-02, 2.2007e-02, 0.0000e+00,  ..., 2.7646e-02, 0.0000e+00,
         0.0000e+00],
        [5.5269e-02, 5.0531e-02, 0.0000e+00,  ..., 6.6080e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.5781e-03, 1.5350e-03, 0.0000e+00,  ..., 1.0996e-05, 0.0000e+00,
         0.0000e+00],
        [7.5766e-03, 1.5346e-03, 0.0000e+00,  ..., 1.0994e-05, 0.0000e+00,
         0.0000e+00],
        [7.5734e-03, 1.5340e-03, 0.0000e+00,  ..., 1.0989e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53476.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0425, 0.0156, 0.0154,  ..., 0.0000, 0.0194, 0.0000],
        [0.0374, 0.0009, 0.0009,  ..., 0.0000, 0.0087, 0.0000],
        [0.0320, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0487, 0.0503, 0.0491,  ..., 0.0000, 0.0436, 0.0000],
        [0.0487, 0.0503, 0.0491,  ..., 0.0000, 0.0436, 0.0000],
        [0.0486, 0.0502, 0.0491,  ..., 0.0000, 0.0436, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495136., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4480.7651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.2843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16536.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.1577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(256.9320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-377.6596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0234],
        [ 0.0759],
        [ 0.1326],
        ...,
        [-1.4050],
        [-1.4017],
        [-1.4006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217401.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0064],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367623.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0065],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367633.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00],
        [1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00],
        [1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00],
        [1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00],
        [1.8574e-03, 3.8397e-04, 0.0000e+00,  ..., 5.0376e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1598.5851, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.5086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.0401, device='cuda:0')



h[100].sum tensor(62.1049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0499, device='cuda:0')



h[200].sum tensor(-3.8065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.5499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.4350e-03, 1.5370e-03, 0.0000e+00,  ..., 2.0165e-05, 0.0000e+00,
         0.0000e+00],
        [7.4452e-03, 1.5391e-03, 0.0000e+00,  ..., 2.0193e-05, 0.0000e+00,
         0.0000e+00],
        [7.4582e-03, 1.5418e-03, 0.0000e+00,  ..., 2.0228e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.6194e-03, 1.5751e-03, 0.0000e+00,  ..., 2.0665e-05, 0.0000e+00,
         0.0000e+00],
        [7.6178e-03, 1.5748e-03, 0.0000e+00,  ..., 2.0661e-05, 0.0000e+00,
         0.0000e+00],
        [7.6146e-03, 1.5742e-03, 0.0000e+00,  ..., 2.0653e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55188.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0477, 0.0463,  ..., 0.0000, 0.0420, 0.0000],
        [0.0478, 0.0496, 0.0481,  ..., 0.0000, 0.0429, 0.0000],
        [0.0479, 0.0497, 0.0483,  ..., 0.0000, 0.0430, 0.0000],
        ...,
        [0.0489, 0.0508, 0.0495,  ..., 0.0000, 0.0439, 0.0000],
        [0.0489, 0.0508, 0.0495,  ..., 0.0000, 0.0439, 0.0000],
        [0.0489, 0.0508, 0.0495,  ..., 0.0000, 0.0439, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516863.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4483.3550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.5010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16968.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.3791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(383.1703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.4047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0457],
        [-1.2951],
        [-1.4585],
        ...,
        [-1.4179],
        [-1.4145],
        [-1.4134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227733.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0029],
        [1.0065],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367633.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0066],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367643.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8610e-03,  3.8259e-04,  0.0000e+00,  ..., -1.1099e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.3760e-02,  1.2572e-02, -5.2959e-04,  ...,  1.6421e-02,
         -6.8796e-03, -8.6408e-03],
        [ 1.8610e-03,  3.8259e-04,  0.0000e+00,  ..., -1.1099e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8610e-03,  3.8259e-04,  0.0000e+00,  ..., -1.1099e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.8259e-04,  0.0000e+00,  ..., -1.1099e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.8259e-04,  0.0000e+00,  ..., -1.1099e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1509.9565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.0437, device='cuda:0')



h[100].sum tensor(60.9421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9363, device='cuda:0')



h[200].sum tensor(-3.4005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.0538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0137, 0.0000,  ..., 0.0164, 0.0000, 0.0000],
        [0.0265, 0.0211, 0.0000,  ..., 0.0263, 0.0000, 0.0000],
        [0.0983, 0.0946, 0.0000,  ..., 0.1254, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50769.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0345, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0277, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0493, 0.0513, 0.0500,  ..., 0.0000, 0.0443, 0.0000],
        [0.0493, 0.0513, 0.0500,  ..., 0.0000, 0.0443, 0.0000],
        [0.0493, 0.0513, 0.0500,  ..., 0.0000, 0.0443, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481413.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4570.4673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.2126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16322.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-333.6084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(153.0224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.2017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1231],
        [ 0.1183],
        [ 0.1156],
        ...,
        [-1.4365],
        [-1.4330],
        [-1.4318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225934.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0030],
        [1.0066],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367643.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0031],
        [1.0067],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367652.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8698e-02,  5.8598e-02, -2.5041e-03,  ...,  7.8420e-02,
         -3.2799e-02, -4.1208e-02],
        [ 2.4409e-02,  2.3475e-02, -9.9338e-04,  ...,  3.1106e-02,
         -1.3011e-02, -1.6347e-02],
        [ 1.3692e-02,  1.2497e-02, -5.2120e-04,  ...,  1.6318e-02,
         -6.8268e-03, -8.5770e-03],
        ...,
        [ 1.8620e-03,  3.7997e-04,  0.0000e+00,  ..., -5.6456e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8620e-03,  3.7997e-04,  0.0000e+00,  ..., -5.6456e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8620e-03,  3.7997e-04,  0.0000e+00,  ..., -5.6456e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1579.8710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.1333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9452, device='cuda:0')



h[100].sum tensor(61.5876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6430, device='cuda:0')



h[200].sum tensor(-3.6848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1033, 0.0997, 0.0000,  ..., 0.1322, 0.0000, 0.0000],
        [0.1297, 0.1267, 0.0000,  ..., 0.1686, 0.0000, 0.0000],
        [0.0785, 0.0743, 0.0000,  ..., 0.0980, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54361.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0498, 0.0517, 0.0503,  ..., 0.0000, 0.0448, 0.0000],
        [0.0498, 0.0517, 0.0503,  ..., 0.0000, 0.0448, 0.0000],
        [0.0498, 0.0517, 0.0503,  ..., 0.0000, 0.0448, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506019.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4573.6538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.8811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17039.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-341.3413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(214.2658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-383.6331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1101],
        [ 0.1165],
        [ 0.1312],
        ...,
        [-1.4544],
        [-1.4509],
        [-1.4495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220091.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0031],
        [1.0067],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367652.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0031],
        [1.0068],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367662.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3843e-02,  3.3141e-02, -1.3950e-03,  ...,  4.4119e-02,
         -1.8424e-02, -2.3154e-02],
        [ 2.4833e-02,  2.3914e-02, -1.0021e-03,  ...,  3.1689e-02,
         -1.3234e-02, -1.6632e-02],
        [ 1.8573e-03,  3.8233e-04,  0.0000e+00,  ..., -7.1979e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8573e-03,  3.8233e-04,  0.0000e+00,  ..., -7.1979e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8573e-03,  3.8233e-04,  0.0000e+00,  ..., -7.1979e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.3774e-02,  1.2587e-02, -5.1972e-04,  ...,  1.6432e-02,
         -6.8641e-03, -8.6263e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1979.8813, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.7764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-38.6526, device='cuda:0')



h[100].sum tensor(65.8934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3659, device='cuda:0')



h[200].sum tensor(-5.4005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.0991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1285, 0.1256, 0.0000,  ..., 0.1670, 0.0000, 0.0000],
        [0.0661, 0.0616, 0.0000,  ..., 0.0809, 0.0000, 0.0000],
        [0.0394, 0.0342, 0.0000,  ..., 0.0440, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0198, 0.0141, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0297, 0.0242, 0.0000,  ..., 0.0304, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68238.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0234, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0356, 0.0010, 0.0010,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0483, 0.0256, 0.0262,  ..., 0.0000, 0.0350, 0.0000],
        [0.0447, 0.0131, 0.0134,  ..., 0.0000, 0.0218, 0.0000],
        [0.0400, 0.0009, 0.0010,  ..., 0.0000, 0.0108, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596761.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4560.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.2024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19204.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-371.4515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(679.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-457.2159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0696],
        [-0.0119],
        [-0.2376],
        ...,
        [-0.6151],
        [-0.3819],
        [-0.1392]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212722.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0031],
        [1.0068],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367662.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0032],
        [1.0069],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367671.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8491e-03,  3.7913e-04,  0.0000e+00,  ..., -9.1729e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1809.7838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.7085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.0723, device='cuda:0')



h[100].sum tensor(63.8055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2919, device='cuda:0')



h[200].sum tensor(-4.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.5881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58245.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0265, 0.0268,  ..., 0.0000, 0.0339, 0.0000],
        [0.0476, 0.0264, 0.0267,  ..., 0.0000, 0.0339, 0.0000],
        [0.0477, 0.0266, 0.0270,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0510, 0.0521, 0.0509,  ..., 0.0000, 0.0457, 0.0000],
        [0.0510, 0.0521, 0.0508,  ..., 0.0000, 0.0457, 0.0000],
        [0.0510, 0.0521, 0.0508,  ..., 0.0000, 0.0457, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520318.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4808.4678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.5313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17231.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-341.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(360.1025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.5468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2463],
        [-0.1754],
        [-0.1125],
        ...,
        [-1.4875],
        [-1.4839],
        [-1.4826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244929.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0032],
        [1.0069],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367671.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0033],
        [1.0070],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367681.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8365e-03,  3.8217e-04,  0.0000e+00,  ..., -6.4262e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7660e-02,  1.6577e-02, -6.7609e-04,  ...,  2.1805e-02,
         -9.0790e-03, -1.1417e-02],
        [ 1.8365e-03,  3.8217e-04,  0.0000e+00,  ..., -6.4262e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8365e-03,  3.8217e-04,  0.0000e+00,  ..., -6.4262e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8365e-03,  3.8217e-04,  0.0000e+00,  ..., -6.4262e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8365e-03,  3.8217e-04,  0.0000e+00,  ..., -6.4262e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1930.0557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.3903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.5427, device='cuda:0')



h[100].sum tensor(64.8875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2101, device='cuda:0')



h[200].sum tensor(-5.0820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.4706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0649, 0.0604, 0.0000,  ..., 0.0793, 0.0000, 0.0000],
        [0.0203, 0.0148, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        [0.0233, 0.0178, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65861.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0396, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0442, 0.0018, 0.0018,  ..., 0.0000, 0.0163, 0.0000],
        [0.0457, 0.0112, 0.0117,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0518, 0.0522, 0.0511,  ..., 0.0000, 0.0462, 0.0000],
        [0.0518, 0.0522, 0.0510,  ..., 0.0000, 0.0462, 0.0000],
        [0.0517, 0.0521, 0.0510,  ..., 0.0000, 0.0461, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581728.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4910.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.2700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18903.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.9001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(694.8538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.9874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5122],
        [-0.6372],
        [-0.8157],
        ...,
        [-1.5007],
        [-1.4970],
        [-1.4955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213545.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0033],
        [1.0070],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367681.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0033],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367691.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6340e-03,  7.3658e-03, -2.8815e-04,  ...,  9.3909e-03,
         -3.9020e-03, -4.9081e-03],
        [ 1.5277e-02,  1.4162e-02, -5.6907e-04,  ...,  1.8544e-02,
         -7.7060e-03, -9.6931e-03],
        [ 1.8194e-03,  3.9484e-04,  0.0000e+00,  ...,  2.0800e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8194e-03,  3.9484e-04,  0.0000e+00,  ...,  2.0800e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8194e-03,  3.9484e-04,  0.0000e+00,  ...,  2.0800e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8194e-03,  3.9484e-04,  0.0000e+00,  ...,  2.0800e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1386.4200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5310, device='cuda:0')



h[100].sum tensor(58.1308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2590, device='cuda:0')



h[200].sum tensor(-2.7052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.1675e-02, 3.6761e-02, 0.0000e+00,  ..., 4.7390e-02, 0.0000e+00,
         0.0000e+00],
        [2.5143e-02, 1.9840e-02, 0.0000e+00,  ..., 2.4598e-02, 0.0000e+00,
         0.0000e+00],
        [5.6413e-02, 5.1817e-02, 0.0000e+00,  ..., 6.7662e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4700e-03, 1.6211e-03, 0.0000e+00,  ..., 8.5397e-06, 0.0000e+00,
         0.0000e+00],
        [7.4683e-03, 1.6207e-03, 0.0000e+00,  ..., 8.5378e-06, 0.0000e+00,
         0.0000e+00],
        [7.4651e-03, 1.6200e-03, 0.0000e+00,  ..., 8.5340e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46795.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0404, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0409, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0527, 0.0522, 0.0512,  ..., 0.0000, 0.0465, 0.0000],
        [0.0527, 0.0522, 0.0512,  ..., 0.0000, 0.0465, 0.0000],
        [0.0526, 0.0521, 0.0512,  ..., 0.0000, 0.0465, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471681.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5206.3198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(364.5433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16262.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-312.5909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(99.0194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.4154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1255],
        [ 0.1050],
        [ 0.0483],
        ...,
        [-1.5162],
        [-1.5122],
        [-1.5110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265254.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0033],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367691.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0072],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367701.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00],
        [1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00],
        [1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00],
        [1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00],
        [1.8055e-03, 4.1155e-04, 0.0000e+00,  ..., 1.4386e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1697.5681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.6239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.3873, device='cuda:0')



h[100].sum tensor(61.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5506, device='cuda:0')



h[200].sum tensor(-3.9763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2291e-03, 1.6479e-03, 0.0000e+00,  ..., 5.7603e-05, 0.0000e+00,
         0.0000e+00],
        [7.2394e-03, 1.6502e-03, 0.0000e+00,  ..., 5.7686e-05, 0.0000e+00,
         0.0000e+00],
        [7.2533e-03, 1.6534e-03, 0.0000e+00,  ..., 5.7796e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4137e-03, 1.6899e-03, 0.0000e+00,  ..., 5.9074e-05, 0.0000e+00,
         0.0000e+00],
        [7.4120e-03, 1.6896e-03, 0.0000e+00,  ..., 5.9061e-05, 0.0000e+00,
         0.0000e+00],
        [7.4088e-03, 1.6888e-03, 0.0000e+00,  ..., 5.9035e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58160.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0506, 0.0496,  ..., 0.0000, 0.0456, 0.0000],
        [0.0522, 0.0507, 0.0497,  ..., 0.0000, 0.0457, 0.0000],
        [0.0523, 0.0508, 0.0499,  ..., 0.0000, 0.0458, 0.0000],
        ...,
        [0.0534, 0.0504, 0.0497,  ..., 0.0000, 0.0461, 0.0000],
        [0.0535, 0.0520, 0.0512,  ..., 0.0000, 0.0469, 0.0000],
        [0.0535, 0.0519, 0.0511,  ..., 0.0000, 0.0468, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532802.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5223.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.3255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18483.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-349.4903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(234.9069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.6583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7425],
        [-1.7334],
        [-1.6995],
        ...,
        [-1.4557],
        [-1.4991],
        [-1.5152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198039.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0072],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367701.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(520.2859, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0073],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367711.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4702e-02,  1.3596e-02, -5.3462e-04,  ...,  1.7780e-02,
         -7.3622e-03, -9.2661e-03],
        [ 1.4965e-02,  1.3865e-02, -5.4550e-04,  ...,  1.8141e-02,
         -7.5120e-03, -9.4545e-03],
        [ 1.9277e-02,  1.8271e-02, -7.2406e-04,  ...,  2.4076e-02,
         -9.9709e-03, -1.2549e-02],
        ...,
        [ 1.7916e-03,  4.0358e-04,  0.0000e+00,  ...,  1.0978e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7916e-03,  4.0358e-04,  0.0000e+00,  ...,  1.0978e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7916e-03,  4.0358e-04,  0.0000e+00,  ...,  1.0978e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1608.2083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.5487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9542, device='cuda:0')



h[100].sum tensor(60.0058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6463, device='cuda:0')



h[200].sum tensor(-3.5492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.5634e-02, 6.1354e-02, 0.0000e+00,  ..., 8.0502e-02, 0.0000e+00,
         0.0000e+00],
        [6.2754e-02, 5.8403e-02, 0.0000e+00,  ..., 7.6525e-02, 0.0000e+00,
         0.0000e+00],
        [6.6783e-02, 6.2509e-02, 0.0000e+00,  ..., 8.2051e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3576e-03, 1.6574e-03, 0.0000e+00,  ..., 4.5084e-05, 0.0000e+00,
         0.0000e+00],
        [7.3559e-03, 1.6570e-03, 0.0000e+00,  ..., 4.5073e-05, 0.0000e+00,
         0.0000e+00],
        [7.3527e-03, 1.6563e-03, 0.0000e+00,  ..., 4.5054e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52705.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0330, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0318, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0540, 0.0519, 0.0514,  ..., 0.0000, 0.0474, 0.0000],
        [0.0540, 0.0519, 0.0513,  ..., 0.0000, 0.0474, 0.0000],
        [0.0540, 0.0518, 0.0513,  ..., 0.0000, 0.0473, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501823.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5382.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.8820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17153.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-326.2601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(226.7448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.5398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1063],
        [ 0.1277],
        [ 0.1248],
        ...,
        [-1.5390],
        [-1.5350],
        [-1.5336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268377.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0073],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367711.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0074],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367721.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3011e-02,  2.2084e-02, -8.7018e-04,  ...,  2.9219e-02,
         -1.2085e-02, -1.5214e-02],
        [ 1.7761e-03,  3.9606e-04,  0.0000e+00,  ...,  8.5715e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7761e-03,  3.9606e-04,  0.0000e+00,  ...,  8.5715e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.0792e-02,  9.6042e-03, -3.6946e-04,  ...,  1.2411e-02,
         -5.1309e-03, -6.4597e-03],
        [ 2.1712e-02,  2.0756e-02, -8.1692e-04,  ...,  2.7432e-02,
         -1.1345e-02, -1.4283e-02],
        [ 1.5916e-02,  1.4837e-02, -5.7943e-04,  ...,  1.9459e-02,
         -8.0468e-03, -1.0131e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1709.5850, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.5584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.5596, device='cuda:0')



h[100].sum tensor(60.8696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6147, device='cuda:0')



h[200].sum tensor(-3.9266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.3228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0857, 0.0818, 0.0000,  ..., 0.1081, 0.0000, 0.0000],
        [0.0580, 0.0535, 0.0000,  ..., 0.0700, 0.0000, 0.0000],
        [0.0302, 0.0251, 0.0000,  ..., 0.0317, 0.0000, 0.0000],
        ...,
        [0.0448, 0.0399, 0.0000,  ..., 0.0516, 0.0000, 0.0000],
        [0.0732, 0.0690, 0.0000,  ..., 0.0907, 0.0000, 0.0000],
        [0.0848, 0.0808, 0.0000,  ..., 0.1066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55338.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0192, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0270, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0320, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0410, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0355, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0351, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516876.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5444.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.0088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17583.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.2622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(327.3686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.2958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0937],
        [-0.0832],
        [-0.0772],
        ...,
        [-0.1212],
        [ 0.0597],
        [ 0.0374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277441.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0074],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367721.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0075],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367731.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0151e-02,  2.9367e-02, -1.1503e-03,  ...,  3.9028e-02,
         -1.6111e-02, -2.0289e-02],
        [ 3.2425e-02,  3.1688e-02, -1.2425e-03,  ...,  4.2154e-02,
         -1.7402e-02, -2.1915e-02],
        [ 2.9934e-02,  2.9145e-02, -1.1415e-03,  ...,  3.8729e-02,
         -1.5988e-02, -2.0134e-02],
        ...,
        [ 1.7815e-03,  4.0913e-04,  0.0000e+00,  ...,  2.3644e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7815e-03,  4.0913e-04,  0.0000e+00,  ...,  2.3644e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7815e-03,  4.0913e-04,  0.0000e+00,  ...,  2.3644e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1569.4949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.4541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8453, device='cuda:0')



h[100].sum tensor(59.4238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8625, device='cuda:0')



h[200].sum tensor(-3.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0747e-01, 1.0406e-01, 0.0000e+00,  ..., 1.3805e-01, 0.0000e+00,
         0.0000e+00],
        [1.1026e-01, 1.0689e-01, 0.0000e+00,  ..., 1.4186e-01, 0.0000e+00,
         0.0000e+00],
        [1.0238e-01, 9.8845e-02, 0.0000e+00,  ..., 1.3102e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3184e-03, 1.6807e-03, 0.0000e+00,  ..., 9.7128e-05, 0.0000e+00,
         0.0000e+00],
        [7.3167e-03, 1.6803e-03, 0.0000e+00,  ..., 9.7107e-05, 0.0000e+00,
         0.0000e+00],
        [7.3136e-03, 1.6796e-03, 0.0000e+00,  ..., 9.7065e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51247.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0242, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0277, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0548, 0.0512, 0.0515,  ..., 0.0000, 0.0479, 0.0000],
        [0.0548, 0.0512, 0.0514,  ..., 0.0000, 0.0479, 0.0000],
        [0.0548, 0.0512, 0.0514,  ..., 0.0000, 0.0479, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493503.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5498.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(378.0905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17373.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-333.6620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(134.5222, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.0239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0544],
        [ 0.0507],
        [ 0.0023],
        ...,
        [-1.5543],
        [-1.5502],
        [-1.5489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242212.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0075],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367731.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0076],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367741.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1113e-03,  7.8951e-03, -2.9421e-04,  ...,  1.0108e-02,
         -4.1556e-03, -5.2349e-03],
        [ 1.3235e-02,  1.2102e-02, -4.5966e-04,  ...,  1.5774e-02,
         -6.4925e-03, -8.1788e-03],
        [ 2.6076e-02,  2.5204e-02, -9.7488e-04,  ...,  3.3421e-02,
         -1.3770e-02, -1.7346e-02],
        ...,
        [ 1.7788e-03,  4.1383e-04,  0.0000e+00,  ...,  3.1172e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7788e-03,  4.1383e-04,  0.0000e+00,  ...,  3.1172e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7788e-03,  4.1383e-04,  0.0000e+00,  ...,  3.1172e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1670.5476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.6826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.4733, device='cuda:0')



h[100].sum tensor(60.4174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8393, device='cuda:0')



h[200].sum tensor(-3.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.8886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0335, 0.0286, 0.0000,  ..., 0.0364, 0.0000, 0.0000],
        [0.0677, 0.0634, 0.0000,  ..., 0.0833, 0.0000, 0.0000],
        [0.0839, 0.0800, 0.0000,  ..., 0.1057, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54484.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0410, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0332, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0278, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0551, 0.0509, 0.0516,  ..., 0.0000, 0.0482, 0.0000],
        [0.0551, 0.0509, 0.0515,  ..., 0.0000, 0.0481, 0.0000],
        [0.0551, 0.0509, 0.0515,  ..., 0.0000, 0.0481, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512401.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5510.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.6920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17856.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.4273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(250.9455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.9601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0940],
        [ 0.0995],
        [ 0.0828],
        ...,
        [-1.5601],
        [-1.5559],
        [-1.5542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242310.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0076],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367741.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0077],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367751.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2828e-02,  1.1680e-02, -4.3872e-04,  ...,  1.5212e-02,
         -6.2497e-03, -7.8752e-03],
        [ 2.2369e-02,  2.1411e-02, -8.1755e-04,  ...,  2.8319e-02,
         -1.1646e-02, -1.4675e-02],
        [ 6.9085e-03,  5.6430e-03, -2.0370e-04,  ...,  7.0808e-03,
         -2.9017e-03, -3.6565e-03],
        ...,
        [ 1.7780e-03,  4.1064e-04,  0.0000e+00,  ...,  3.3283e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7780e-03,  4.1064e-04,  0.0000e+00,  ...,  3.3283e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7780e-03,  4.1064e-04,  0.0000e+00,  ...,  3.3283e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1599.9849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.4749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8282, device='cuda:0')



h[100].sum tensor(59.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8562, device='cuda:0')



h[200].sum tensor(-3.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0912, 0.0874, 0.0000,  ..., 0.1156, 0.0000, 0.0000],
        [0.0529, 0.0484, 0.0000,  ..., 0.0631, 0.0000, 0.0000],
        [0.0502, 0.0456, 0.0000,  ..., 0.0593, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52008.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0356, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0385, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0553, 0.0508, 0.0518,  ..., 0.0000, 0.0484, 0.0000],
        [0.0553, 0.0508, 0.0518,  ..., 0.0000, 0.0484, 0.0000],
        [0.0553, 0.0508, 0.0517,  ..., 0.0000, 0.0484, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502969.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5546.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(380.6587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17547.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-338.2067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(238.3368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.2285, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1012],
        [ 0.1038],
        [ 0.1114],
        ...,
        [-1.5698],
        [-1.5657],
        [-1.5644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252585.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0077],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367751.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0078],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367761., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8982e-02,  2.8142e-02, -1.0687e-03,  ...,  3.7390e-02,
         -1.5355e-02, -1.9354e-02],
        [ 2.9966e-02,  2.9145e-02, -1.1074e-03,  ...,  3.8741e-02,
         -1.5910e-02, -2.0054e-02],
        [ 1.4454e-02,  1.3331e-02, -4.9796e-04,  ...,  1.7441e-02,
         -7.1544e-03, -9.0179e-03],
        ...,
        [ 1.7793e-03,  4.0935e-04,  0.0000e+00,  ...,  3.6897e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7793e-03,  4.0935e-04,  0.0000e+00,  ...,  3.6897e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7793e-03,  4.0935e-04,  0.0000e+00,  ...,  3.6897e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1677.3145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.0322, device='cuda:0')



h[100].sum tensor(60.4212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6753, device='cuda:0')



h[200].sum tensor(-3.4761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.3739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0906, 0.0867, 0.0000,  ..., 0.1148, 0.0000, 0.0000],
        [0.0909, 0.0871, 0.0000,  ..., 0.1152, 0.0000, 0.0000],
        [0.0873, 0.0833, 0.0000,  ..., 0.1102, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54225.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0320, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0350, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0555, 0.0507, 0.0520,  ..., 0.0000, 0.0487, 0.0000],
        [0.0555, 0.0507, 0.0520,  ..., 0.0000, 0.0486, 0.0000],
        [0.0554, 0.0507, 0.0520,  ..., 0.0000, 0.0486, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511932.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5516.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(382.2973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18060.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-349.5263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(207.5862, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.6073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0994],
        [ 0.1123],
        [ 0.1251],
        ...,
        [-1.5765],
        [-1.5673],
        [-1.5456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217978.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0078],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367761., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0037],
        [1.0079],
        ...,
        [1.0013],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367770.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0210e-03,  6.7304e-03, -2.4223e-04,  ...,  8.5735e-03,
         -3.5102e-03, -4.4258e-03],
        [ 7.6156e-03,  6.3174e-03, -2.2648e-04,  ...,  8.0173e-03,
         -3.2819e-03, -4.1379e-03],
        [ 1.3847e-02,  1.2667e-02, -4.6871e-04,  ...,  1.6570e-02,
         -6.7920e-03, -8.5637e-03],
        ...,
        [ 1.7893e-03,  3.8069e-04,  0.0000e+00,  ...,  2.1089e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7893e-03,  3.8069e-04,  0.0000e+00,  ...,  2.1089e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7893e-03,  3.8069e-04,  0.0000e+00,  ...,  2.1089e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1443.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.1682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7263, device='cuda:0')



h[100].sum tensor(58.0730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9600, device='cuda:0')



h[200].sum tensor(-2.4874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.8495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7108e-02, 2.1845e-02, 0.0000e+00,  ..., 2.7454e-02, 0.0000e+00,
         0.0000e+00],
        [4.9011e-02, 4.4154e-02, 0.0000e+00,  ..., 5.7499e-02, 0.0000e+00,
         0.0000e+00],
        [2.7197e-02, 2.1914e-02, 0.0000e+00,  ..., 2.7540e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3544e-03, 1.5647e-03, 0.0000e+00,  ..., 8.6680e-05, 0.0000e+00,
         0.0000e+00],
        [7.3527e-03, 1.5643e-03, 0.0000e+00,  ..., 8.6661e-05, 0.0000e+00,
         0.0000e+00],
        [7.3496e-03, 1.5637e-03, 0.0000e+00,  ..., 8.6624e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46915.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0460, 0.0010, 0.0016,  ..., 0.0000, 0.0116, 0.0000],
        [0.0429, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0454, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0555, 0.0511, 0.0526,  ..., 0.0000, 0.0491, 0.0000],
        [0.0555, 0.0511, 0.0526,  ..., 0.0000, 0.0491, 0.0000],
        [0.0554, 0.0511, 0.0525,  ..., 0.0000, 0.0491, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478440.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5525.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.9872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16788.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-327.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(127.4291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-371.0400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0469],
        [ 0.0842],
        [ 0.0069],
        ...,
        [-1.5931],
        [-1.5891],
        [-1.5873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278333.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0037],
        [1.0079],
        ...,
        [1.0013],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367770.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0080],
        ...,
        [1.0013],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367780.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00],
        [1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00],
        [1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00],
        [1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00],
        [1.8052e-03, 3.6070e-04, 0.0000e+00,  ..., 7.7855e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1804.7312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.1368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.3149, device='cuda:0')



h[100].sum tensor(62.4496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8954, device='cuda:0')



h[200].sum tensor(-3.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.2040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2298e-03, 1.4446e-03, 0.0000e+00,  ..., 3.1180e-05, 0.0000e+00,
         0.0000e+00],
        [7.2408e-03, 1.4468e-03, 0.0000e+00,  ..., 3.1227e-05, 0.0000e+00,
         0.0000e+00],
        [7.2562e-03, 1.4498e-03, 0.0000e+00,  ..., 3.1294e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4209e-03, 1.4828e-03, 0.0000e+00,  ..., 3.2004e-05, 0.0000e+00,
         0.0000e+00],
        [7.4192e-03, 1.4824e-03, 0.0000e+00,  ..., 3.1997e-05, 0.0000e+00,
         0.0000e+00],
        [7.4160e-03, 1.4818e-03, 0.0000e+00,  ..., 3.1983e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57539.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0535, 0.0443, 0.0460,  ..., 0.0000, 0.0455, 0.0000],
        [0.0534, 0.0426, 0.0445,  ..., 0.0000, 0.0448, 0.0000],
        [0.0534, 0.0413, 0.0434,  ..., 0.0000, 0.0443, 0.0000],
        ...,
        [0.0544, 0.0373, 0.0401,  ..., 0.0000, 0.0442, 0.0000],
        [0.0552, 0.0482, 0.0501,  ..., 0.0000, 0.0483, 0.0000],
        [0.0554, 0.0515, 0.0531,  ..., 0.0000, 0.0495, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532539.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5388.2412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.8927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18372.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-358.5367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.2888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.5955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0681],
        [-0.9461],
        [-0.8320],
        ...,
        [-1.3093],
        [-1.4728],
        [-1.5595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241644.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0080],
        ...,
        [1.0013],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367780.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0038],
        [1.0081],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367790.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4368e-02,  1.3139e-02, -4.7771e-04,  ...,  1.7221e-02,
         -7.0425e-03, -8.8848e-03],
        [ 1.8137e-03,  3.5653e-04,  0.0000e+00,  ...,  5.8200e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8137e-03,  3.5653e-04,  0.0000e+00,  ...,  5.8200e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8137e-03,  3.5653e-04,  0.0000e+00,  ...,  5.8200e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8137e-03,  3.5653e-04,  0.0000e+00,  ...,  5.8200e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.8137e-03,  3.5653e-04,  0.0000e+00,  ...,  5.8200e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1537.9670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.2026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3651, device='cuda:0')



h[100].sum tensor(59.4431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9407, device='cuda:0')



h[200].sum tensor(-2.7944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.6762e-02, 2.1280e-02, 0.0000e+00,  ..., 2.6760e-02, 0.0000e+00,
         0.0000e+00],
        [1.9860e-02, 1.4244e-02, 0.0000e+00,  ..., 1.7280e-02, 0.0000e+00,
         0.0000e+00],
        [7.2908e-03, 1.4332e-03, 0.0000e+00,  ..., 2.3395e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4569e-03, 1.4658e-03, 0.0000e+00,  ..., 2.3928e-05, 0.0000e+00,
         0.0000e+00],
        [7.4550e-03, 1.4655e-03, 0.0000e+00,  ..., 2.3922e-05, 0.0000e+00,
         0.0000e+00],
        [7.4519e-03, 1.4649e-03, 0.0000e+00,  ..., 2.3912e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50403.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0423, 0.0015, 0.0022,  ..., 0.0000, 0.0079, 0.0000],
        [0.0459, 0.0089, 0.0090,  ..., 0.0000, 0.0107, 0.0000],
        [0.0488, 0.0109, 0.0116,  ..., 0.0000, 0.0205, 0.0000],
        ...,
        [0.0556, 0.0518, 0.0536,  ..., 0.0000, 0.0498, 0.0000],
        [0.0556, 0.0518, 0.0536,  ..., 0.0000, 0.0498, 0.0000],
        [0.0555, 0.0518, 0.0536,  ..., 0.0000, 0.0497, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501636.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5420.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.8364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17295.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-337.9834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(233.3455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.7054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0441],
        [-0.0414],
        [-0.0343],
        ...,
        [-1.6188],
        [-1.6146],
        [-1.6130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285447.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0038],
        [1.0081],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367790.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0083],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367800.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0890e-03,  7.7652e-03, -2.7374e-04,  ...,  9.9777e-03,
         -4.0705e-03, -5.1368e-03],
        [ 1.8182e-03,  3.6367e-04,  0.0000e+00,  ...,  1.0541e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8182e-03,  3.6367e-04,  0.0000e+00,  ...,  1.0541e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8182e-03,  3.6367e-04,  0.0000e+00,  ...,  1.0541e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8182e-03,  3.6367e-04,  0.0000e+00,  ...,  1.0541e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8182e-03,  3.6367e-04,  0.0000e+00,  ...,  1.0541e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1706.6722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.7166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.1557, device='cuda:0')



h[100].sum tensor(61.0992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7212, device='cuda:0')



h[200].sum tensor(-3.4338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8924e-02, 1.3308e-02, 0.0000e+00,  ..., 1.6002e-02, 0.0000e+00,
         0.0000e+00],
        [1.4582e-02, 8.8787e-03, 0.0000e+00,  ..., 1.0034e-02, 0.0000e+00,
         0.0000e+00],
        [7.3092e-03, 1.4620e-03, 0.0000e+00,  ..., 4.2377e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4762e-03, 1.4954e-03, 0.0000e+00,  ..., 4.3346e-05, 0.0000e+00,
         0.0000e+00],
        [7.4743e-03, 1.4950e-03, 0.0000e+00,  ..., 4.3335e-05, 0.0000e+00,
         0.0000e+00],
        [7.4712e-03, 1.4944e-03, 0.0000e+00,  ..., 4.3317e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57255.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0097, 0.0117,  ..., 0.0000, 0.0249, 0.0000],
        [0.0516, 0.0196, 0.0229,  ..., 0.0000, 0.0333, 0.0000],
        [0.0532, 0.0326, 0.0357,  ..., 0.0000, 0.0408, 0.0000],
        ...,
        [0.0558, 0.0520, 0.0540,  ..., 0.0000, 0.0499, 0.0000],
        [0.0557, 0.0520, 0.0539,  ..., 0.0000, 0.0499, 0.0000],
        [0.0557, 0.0520, 0.0539,  ..., 0.0000, 0.0499, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551320.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5366.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.3415, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18758.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-360.0086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(421.7990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.9998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9021],
        [-0.9284],
        [-0.8632],
        ...,
        [-1.6285],
        [-1.6242],
        [-1.6228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245617.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0083],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367800.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(491.6500, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0039],
        [1.0084],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367809.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3755e-02,  1.2542e-02, -4.4501e-04,  ...,  1.6396e-02,
         -6.6747e-03, -8.4259e-03],
        [ 2.2552e-02,  2.1497e-02, -7.7274e-04,  ...,  2.8454e-02,
         -1.1590e-02, -1.4631e-02],
        [ 8.5091e-03,  7.2025e-03, -2.4959e-04,  ...,  9.2065e-03,
         -3.7436e-03, -4.7258e-03],
        ...,
        [ 1.8094e-03,  3.8253e-04,  0.0000e+00,  ...,  2.3692e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8094e-03,  3.8253e-04,  0.0000e+00,  ...,  2.3692e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8094e-03,  3.8253e-04,  0.0000e+00,  ...,  2.3692e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1709.8844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.4295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9263, device='cuda:0')



h[100].sum tensor(60.5478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6359, device='cuda:0')



h[200].sum tensor(-3.4188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0168e-01, 9.7662e-02, 0.0000e+00,  ..., 1.2953e-01, 0.0000e+00,
         0.0000e+00],
        [7.2705e-02, 6.8155e-02, 0.0000e+00,  ..., 8.9797e-02, 0.0000e+00,
         0.0000e+00],
        [4.2747e-02, 3.7647e-02, 0.0000e+00,  ..., 4.8714e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4413e-03, 1.5732e-03, 0.0000e+00,  ..., 9.7435e-05, 0.0000e+00,
         0.0000e+00],
        [7.4393e-03, 1.5728e-03, 0.0000e+00,  ..., 9.7410e-05, 0.0000e+00,
         0.0000e+00],
        [7.4363e-03, 1.5721e-03, 0.0000e+00,  ..., 9.7369e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56186.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0354, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0420, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0561, 0.0520, 0.0542,  ..., 0.0000, 0.0500, 0.0000],
        [0.0561, 0.0520, 0.0541,  ..., 0.0000, 0.0500, 0.0000],
        [0.0561, 0.0520, 0.0541,  ..., 0.0000, 0.0500, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532622.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5420.7378, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.9513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18653.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-362.6772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(201.3874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.0151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0809],
        [ 0.0848],
        [ 0.0607],
        ...,
        [-1.6372],
        [-1.6331],
        [-1.6315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218845.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0039],
        [1.0084],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367809.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0040],
        [1.0085],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367819.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8182e-03,  3.7073e-04,  0.0000e+00,  ...,  1.0854e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3147e-02,  1.1900e-02, -4.1754e-04,  ...,  1.5533e-02,
         -6.3174e-03, -7.9772e-03],
        [ 2.3886e-02,  2.2830e-02, -8.1334e-04,  ...,  3.0247e-02,
         -1.2306e-02, -1.5539e-02],
        ...,
        [ 1.8182e-03,  3.7073e-04,  0.0000e+00,  ...,  1.0854e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8182e-03,  3.7073e-04,  0.0000e+00,  ...,  1.0854e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8182e-03,  3.7073e-04,  0.0000e+00,  ...,  1.0854e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1835.3599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.8120, device='cuda:0')



h[100].sum tensor(61.8604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0801, device='cuda:0')



h[200].sum tensor(-3.8910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7841, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7889e-02, 2.2456e-02, 0.0000e+00,  ..., 2.8277e-02, 0.0000e+00,
         0.0000e+00],
        [4.7513e-02, 4.2418e-02, 0.0000e+00,  ..., 5.5149e-02, 0.0000e+00,
         0.0000e+00],
        [6.7762e-02, 6.3014e-02, 0.0000e+00,  ..., 8.2872e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4786e-03, 1.5249e-03, 0.0000e+00,  ..., 4.4642e-05, 0.0000e+00,
         0.0000e+00],
        [7.4766e-03, 1.5245e-03, 0.0000e+00,  ..., 4.4630e-05, 0.0000e+00,
         0.0000e+00],
        [7.4736e-03, 1.5238e-03, 0.0000e+00,  ..., 4.4612e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59397.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.0024, 0.0044,  ..., 0.0000, 0.0153, 0.0000],
        [0.0418, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0365, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0555, 0.0413, 0.0443,  ..., 0.0000, 0.0458, 0.0000],
        [0.0563, 0.0527, 0.0547,  ..., 0.0000, 0.0504, 0.0000],
        [0.0562, 0.0527, 0.0547,  ..., 0.0000, 0.0504, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549840.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5387.2612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18900.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-367.1150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(324.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.8154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1010],
        [ 0.1082],
        [ 0.0996],
        ...,
        [-1.2438],
        [-1.4666],
        [-1.5886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231532.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0040],
        [1.0085],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367819.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0040],
        [1.0086],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367829.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2906e-03,  6.9248e-03, -2.3570e-04,  ...,  8.8414e-03,
         -3.5974e-03, -4.5439e-03],
        [ 1.8460e-02,  1.7272e-02, -6.0651e-04,  ...,  2.2770e-02,
         -9.2567e-03, -1.1692e-02],
        [ 2.2153e-02,  2.1029e-02, -7.4117e-04,  ...,  2.7828e-02,
         -1.1312e-02, -1.4288e-02],
        ...,
        [ 1.8264e-03,  3.4767e-04,  0.0000e+00,  ..., -1.2301e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8264e-03,  3.4767e-04,  0.0000e+00,  ..., -1.2301e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8264e-03,  3.4767e-04,  0.0000e+00,  ..., -1.2301e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1903.9475, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.2452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.8971, device='cuda:0')



h[100].sum tensor(62.4670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8551, device='cuda:0')



h[200].sum tensor(-4.1575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.2170, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0412, 0.0358, 0.0000,  ..., 0.0463, 0.0000, 0.0000],
        [0.0698, 0.0649, 0.0000,  ..., 0.0855, 0.0000, 0.0000],
        [0.0987, 0.0944, 0.0000,  ..., 0.1251, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60803.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0378, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0355, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0565, 0.0537, 0.0554,  ..., 0.0000, 0.0511, 0.0000],
        [0.0565, 0.0536, 0.0554,  ..., 0.0000, 0.0511, 0.0000],
        [0.0564, 0.0536, 0.0553,  ..., 0.0000, 0.0511, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555990.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5383.9658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.1292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18830.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.6380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(367.1635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.4749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0578],
        [-0.0452],
        [-0.0314],
        ...,
        [-1.6778],
        [-1.6735],
        [-1.6720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269419.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0040],
        [1.0086],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367829.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0041],
        [1.0087],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367838.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8369e-03,  3.3228e-04,  0.0000e+00,  ..., -2.9907e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1526.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.6396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8629, device='cuda:0')



h[100].sum tensor(58.5134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7541, device='cuda:0')



h[200].sum tensor(-2.6746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49066.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0528, 0.0540,  ..., 0.0000, 0.0502, 0.0000],
        [0.0551, 0.0529, 0.0541,  ..., 0.0000, 0.0503, 0.0000],
        [0.0552, 0.0530, 0.0543,  ..., 0.0000, 0.0504, 0.0000],
        ...,
        [0.0565, 0.0543, 0.0558,  ..., 0.0000, 0.0516, 0.0000],
        [0.0565, 0.0543, 0.0558,  ..., 0.0000, 0.0516, 0.0000],
        [0.0565, 0.0542, 0.0558,  ..., 0.0000, 0.0516, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495027.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5475.0415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.4767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17406.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-339.2931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(91.3597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.9358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9131],
        [-1.9269],
        [-1.9317],
        ...,
        [-1.6848],
        [-1.6832],
        [-1.6821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270524.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0041],
        [1.0087],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367838.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0042],
        [1.0088],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367848.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8610e-03,  3.5311e-04,  0.0000e+00,  ..., -3.4324e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1525.5958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.9561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0052, device='cuda:0')



h[100].sum tensor(58.9879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4353, device='cuda:0')



h[200].sum tensor(-2.5879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48357.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0549, 0.0529, 0.0541,  ..., 0.0000, 0.0501, 0.0000],
        [0.0550, 0.0530, 0.0542,  ..., 0.0000, 0.0502, 0.0000],
        [0.0551, 0.0531, 0.0545,  ..., 0.0000, 0.0503, 0.0000],
        ...,
        [0.0564, 0.0543, 0.0560,  ..., 0.0000, 0.0515, 0.0000],
        [0.0564, 0.0543, 0.0560,  ..., 0.0000, 0.0515, 0.0000],
        [0.0564, 0.0543, 0.0560,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489976.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5403.4917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.1148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17096.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-339.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(92.7403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.7240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6956],
        [-1.7976],
        [-1.8769],
        ...,
        [-1.6929],
        [-1.6887],
        [-1.6872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283234.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0042],
        [1.0088],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367848.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0042],
        [1.0089],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367858.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8791e-03,  3.8945e-04,  0.0000e+00,  ..., -3.1881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8791e-03,  3.8945e-04,  0.0000e+00,  ..., -3.1881e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.7476e-03,  7.3731e-03, -2.4246e-04,  ...,  9.3662e-03,
         -3.7990e-03, -4.8029e-03],
        ...,
        [ 1.8791e-03,  3.8945e-04,  0.0000e+00,  ..., -3.1881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8791e-03,  3.8945e-04,  0.0000e+00,  ..., -3.1881e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8791e-03,  3.8945e-04,  0.0000e+00,  ..., -3.1881e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1672.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.8669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.7552, device='cuda:0')



h[100].sum tensor(60.8732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8290, device='cuda:0')



h[200].sum tensor(-3.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.7171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0119, 0.0000,  ..., 0.0139, 0.0000, 0.0000],
        [0.0144, 0.0086, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0132, 0.0073, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52281.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0504, 0.0059, 0.0086,  ..., 0.0000, 0.0225, 0.0000],
        [0.0518, 0.0098, 0.0144,  ..., 0.0000, 0.0306, 0.0000],
        [0.0524, 0.0164, 0.0207,  ..., 0.0000, 0.0337, 0.0000],
        ...,
        [0.0565, 0.0541, 0.0560,  ..., 0.0000, 0.0513, 0.0000],
        [0.0565, 0.0541, 0.0560,  ..., 0.0000, 0.0513, 0.0000],
        [0.0565, 0.0540, 0.0560,  ..., 0.0000, 0.0513, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509445.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5361.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(381.8496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17619.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-354.0658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(182.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.4364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1048],
        [-0.3390],
        [-0.6120],
        ...,
        [-1.6868],
        [-1.6828],
        [-1.6813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264532.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0042],
        [1.0089],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367858.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0090],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367869., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1442e-03,  5.7713e-03, -1.8346e-04,  ...,  7.1602e-03,
         -2.9000e-03, -3.6674e-03],
        [ 7.1442e-03,  5.7713e-03, -1.8346e-04,  ...,  7.1602e-03,
         -2.9000e-03, -3.6674e-03],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1948.1674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.5402, device='cuda:0')



h[100].sum tensor(64.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7225, device='cuda:0')



h[200].sum tensor(-3.9762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0211, 0.0000,  ..., 0.0260, 0.0000, 0.0000],
        [0.0219, 0.0163, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        [0.0172, 0.0115, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62646.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0002, 0.0012,  ..., 0.0000, 0.0099, 0.0000],
        [0.0490, 0.0035, 0.0043,  ..., 0.0000, 0.0138, 0.0000],
        [0.0510, 0.0123, 0.0147,  ..., 0.0000, 0.0238, 0.0000],
        ...,
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000],
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000],
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583129.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5263.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(347.5402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19245.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-380.2399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(603.6193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-456.0753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0269],
        [-0.0282],
        [-0.2450],
        ...,
        [-1.6743],
        [-1.6705],
        [-1.6691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243698.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0090],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367869., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0090],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367869., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8908e-03,  4.3017e-04,  0.0000e+00,  ..., -2.6603e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1554.8611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.4336, device='cuda:0')



h[100].sum tensor(59.7724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2228, device='cuda:0')



h[200].sum tensor(-2.4976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.6748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49801.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0551, 0.0511, 0.0530,  ..., 0.0000, 0.0484, 0.0000],
        [0.0552, 0.0518, 0.0538,  ..., 0.0000, 0.0493, 0.0000],
        [0.0553, 0.0523, 0.0544,  ..., 0.0000, 0.0498, 0.0000],
        ...,
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000],
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000],
        [0.0566, 0.0535, 0.0559,  ..., 0.0000, 0.0510, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498407.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5357.1567, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(395.7250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17472.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-354.3834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(116.3280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.3328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1919],
        [-1.4367],
        [-1.6218],
        ...,
        [-1.6660],
        [-1.6618],
        [-1.6600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238086.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0090],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367869., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0091],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367879., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8986e-03,  4.6244e-04,  0.0000e+00,  ..., -2.2872e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1685.7207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.7072, device='cuda:0')



h[100].sum tensor(61.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4395, device='cuda:0')



h[200].sum tensor(-2.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.4943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52525.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0509, 0.0531,  ..., 0.0000, 0.0486, 0.0000],
        [0.0549, 0.0484, 0.0508,  ..., 0.0000, 0.0470, 0.0000],
        [0.0544, 0.0402, 0.0434,  ..., 0.0000, 0.0431, 0.0000],
        ...,
        [0.0566, 0.0531, 0.0558,  ..., 0.0000, 0.0508, 0.0000],
        [0.0565, 0.0531, 0.0558,  ..., 0.0000, 0.0508, 0.0000],
        [0.0565, 0.0531, 0.0557,  ..., 0.0000, 0.0508, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509205.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5272.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.7272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17855.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-367.7968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(133.7809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.6637, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2169],
        [-0.9350],
        [-0.5980],
        ...,
        [-1.6631],
        [-1.6594],
        [-1.6580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210209.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0091],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367879., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0092],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367888.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8947e-03,  4.7202e-04,  0.0000e+00,  ..., -2.3696e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2479.2532, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(104.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-44.7197, device='cuda:0')



h[100].sum tensor(70.3585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6209, device='cuda:0')



h[200].sum tensor(-5.7611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-52.1782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79784.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0542, 0.0409, 0.0444,  ..., 0.0000, 0.0455, 0.0000],
        [0.0549, 0.0497, 0.0523,  ..., 0.0000, 0.0489, 0.0000],
        [0.0546, 0.0449, 0.0481,  ..., 0.0000, 0.0469, 0.0000],
        ...,
        [0.0565, 0.0529, 0.0558,  ..., 0.0000, 0.0510, 0.0000],
        [0.0565, 0.0529, 0.0558,  ..., 0.0000, 0.0510, 0.0000],
        [0.0564, 0.0529, 0.0558,  ..., 0.0000, 0.0509, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685800.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5045.8198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.0877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21790.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-433.1303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1168.5077, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.1497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6300],
        [-0.9439],
        [-1.1083],
        ...,
        [-1.6648],
        [-1.6610],
        [-1.6597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195211.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0092],
        ...,
        [1.0013],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367888.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(478.3588, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0093],
        ...,
        [1.0012],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367897.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0619e-02,  9.3314e-03, -2.9531e-04,  ...,  1.1911e-02,
         -4.7935e-03, -6.0675e-03],
        [ 1.8875e-03,  4.5480e-04,  0.0000e+00,  ..., -3.0157e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8875e-03,  4.5480e-04,  0.0000e+00,  ..., -3.0157e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8875e-03,  4.5480e-04,  0.0000e+00,  ..., -3.0157e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8875e-03,  4.5480e-04,  0.0000e+00,  ..., -3.0157e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8875e-03,  4.5480e-04,  0.0000e+00,  ..., -3.0157e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1967.6251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.2890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.4311, device='cuda:0')



h[100].sum tensor(64.6517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3102, device='cuda:0')



h[200].sum tensor(-3.8696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.5064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0297, 0.0243, 0.0000,  ..., 0.0302, 0.0000, 0.0000],
        [0.0163, 0.0107, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        [0.0076, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0078, 0.0019, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62250.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.0013, 0.0024,  ..., 0.0000, 0.0129, 0.0000],
        [0.0506, 0.0173, 0.0193,  ..., 0.0000, 0.0266, 0.0000],
        [0.0540, 0.0343, 0.0384,  ..., 0.0000, 0.0437, 0.0000],
        ...,
        [0.0566, 0.0537, 0.0562,  ..., 0.0000, 0.0517, 0.0000],
        [0.0566, 0.0537, 0.0562,  ..., 0.0000, 0.0517, 0.0000],
        [0.0565, 0.0537, 0.0562,  ..., 0.0000, 0.0517, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577830.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5101.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.5309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19024.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-387.1561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(558.1924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-458.0868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0770],
        [-0.3686],
        [-0.6691],
        ...,
        [-1.6893],
        [-1.6855],
        [-1.6842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250017.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0093],
        ...,
        [1.0012],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367897.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0045],
        [1.0093],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367906.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8762e-03,  4.3035e-04,  0.0000e+00,  ..., -3.7425e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8762e-03,  4.3035e-04,  0.0000e+00,  ..., -3.7425e-05,
          0.0000e+00,  0.0000e+00],
        [ 9.6649e-03,  8.3498e-03, -2.6066e-04,  ...,  1.0615e-02,
         -4.2689e-03, -5.4052e-03],
        ...,
        [ 1.8762e-03,  4.3035e-04,  0.0000e+00,  ..., -3.7425e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8762e-03,  4.3035e-04,  0.0000e+00,  ..., -3.7425e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8762e-03,  4.3035e-04,  0.0000e+00,  ..., -3.7425e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2178.2124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.5484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.2607, device='cuda:0')



h[100].sum tensor(66.7881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8486, device='cuda:0')



h[200].sum tensor(-4.6522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.4751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0154, 0.0097, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0139, 0.0082, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67201.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0437, 0.0463,  ..., 0.0000, 0.0472, 0.0000],
        [0.0528, 0.0260, 0.0292,  ..., 0.0000, 0.0384, 0.0000],
        [0.0517, 0.0148, 0.0179,  ..., 0.0000, 0.0313, 0.0000],
        ...,
        [0.0567, 0.0545, 0.0566,  ..., 0.0000, 0.0526, 0.0000],
        [0.0567, 0.0545, 0.0566,  ..., 0.0000, 0.0526, 0.0000],
        [0.0566, 0.0545, 0.0566,  ..., 0.0000, 0.0525, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600415.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4986.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.4906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19947.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-402.6217, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(485.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.5261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2687],
        [-0.8830],
        [-0.4777],
        ...,
        [-1.7089],
        [-1.7050],
        [-1.7035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221501.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0045],
        [1.0093],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367906.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0094],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367915.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2863e-02,  1.1595e-02, -3.6432e-04,  ...,  1.5006e-02,
         -6.0202e-03, -7.6249e-03],
        [ 2.1334e-02,  2.0210e-02, -6.4483e-04,  ...,  2.6592e-02,
         -1.0655e-02, -1.3496e-02],
        [ 3.7017e-02,  3.6159e-02, -1.1642e-03,  ...,  4.8044e-02,
         -1.9237e-02, -2.4365e-02],
        ...,
        [ 1.8615e-03,  4.0712e-04,  0.0000e+00,  ..., -4.2752e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8615e-03,  4.0712e-04,  0.0000e+00,  ..., -4.2752e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8615e-03,  4.0712e-04,  0.0000e+00,  ..., -4.2752e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1770.0686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.6579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.4292, device='cuda:0')



h[100].sum tensor(62.2301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4512, device='cuda:0')



h[200].sum tensor(-3.1634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.6703, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0434, 0.0382, 0.0000,  ..., 0.0490, 0.0000, 0.0000],
        [0.0919, 0.0875, 0.0000,  ..., 0.1153, 0.0000, 0.0000],
        [0.1115, 0.1074, 0.0000,  ..., 0.1421, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0077, 0.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54936.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0307, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0236, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0554, 0.0363, 0.0401,  ..., 0.0000, 0.0462, 0.0000],
        [0.0567, 0.0550, 0.0568,  ..., 0.0000, 0.0533, 0.0000],
        [0.0567, 0.0550, 0.0568,  ..., 0.0000, 0.0533, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527499.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5075.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.4850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18053.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-370.8423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(198.1201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.7776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1321],
        [ 0.1350],
        [ 0.1256],
        ...,
        [-0.9220],
        [-1.3351],
        [-1.5818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260419.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0094],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367915.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0095],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367924.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1570e-02,  1.0285e-02, -3.1886e-04,  ...,  1.3267e-02,
         -5.3163e-03, -6.7355e-03],
        [ 1.8391e-03,  3.8631e-04,  0.0000e+00,  ..., -4.5353e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.1570e-02,  1.0285e-02, -3.1886e-04,  ...,  1.3267e-02,
         -5.3163e-03, -6.7355e-03],
        ...,
        [ 1.8391e-03,  3.8631e-04,  0.0000e+00,  ..., -4.5353e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8391e-03,  3.8631e-04,  0.0000e+00,  ..., -4.5353e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8391e-03,  3.8631e-04,  0.0000e+00,  ..., -4.5353e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1891.5500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.1918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.3467, device='cuda:0')



h[100].sum tensor(63.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9072, device='cuda:0')



h[200].sum tensor(-3.6124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.2412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0096, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0429, 0.0376, 0.0000,  ..., 0.0484, 0.0000, 0.0000],
        [0.0154, 0.0097, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59869.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0129, 0.0160,  ..., 0.0000, 0.0333, 0.0000],
        [0.0483, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.0000],
        [0.0494, 0.0009, 0.0019,  ..., 0.0000, 0.0210, 0.0000],
        ...,
        [0.0569, 0.0556, 0.0571,  ..., 0.0000, 0.0541, 0.0000],
        [0.0569, 0.0555, 0.0571,  ..., 0.0000, 0.0541, 0.0000],
        [0.0568, 0.0555, 0.0570,  ..., 0.0000, 0.0540, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564719.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5041.1528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(373.6073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19063.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-381.2277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(374.2462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.5208, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5369],
        [-0.1848],
        [ 0.0404],
        ...,
        [-1.7619],
        [-1.7576],
        [-1.7561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239112.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0095],
        ...,
        [1.0012],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367924.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0047],
        [1.0095],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367933.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8148e-03,  3.8469e-04,  0.0000e+00,  ..., -3.9315e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.8026e-03,  6.4786e-03, -1.9418e-04,  ...,  8.1555e-03,
         -3.2667e-03, -4.1400e-03],
        [ 7.5015e-03,  6.1721e-03, -1.8441e-04,  ...,  7.7433e-03,
         -3.1024e-03, -3.9318e-03],
        ...,
        [ 1.8148e-03,  3.8469e-04,  0.0000e+00,  ..., -3.9315e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8148e-03,  3.8469e-04,  0.0000e+00,  ..., -3.9315e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8148e-03,  3.8469e-04,  0.0000e+00,  ..., -3.9315e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1655.9993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.2596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1449, device='cuda:0')



h[100].sum tensor(60.2547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2305, device='cuda:0')



h[200].sum tensor(-2.7415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0076, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0269, 0.0215, 0.0000,  ..., 0.0267, 0.0000, 0.0000],
        [0.0482, 0.0432, 0.0000,  ..., 0.0558, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51874.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0512, 0.0181, 0.0196,  ..., 0.0000, 0.0305, 0.0000],
        [0.0475, 0.0024, 0.0033,  ..., 0.0000, 0.0154, 0.0000],
        [0.0444, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        ...,
        [0.0569, 0.0556, 0.0572,  ..., 0.0000, 0.0545, 0.0000],
        [0.0569, 0.0556, 0.0571,  ..., 0.0000, 0.0545, 0.0000],
        [0.0569, 0.0556, 0.0571,  ..., 0.0000, 0.0545, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512443.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5069.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.0627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17789.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-362.4778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(117.8429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.0567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2590],
        [-0.0066],
        [ 0.0990],
        ...,
        [-1.7732],
        [-1.7688],
        [-1.7673]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262189.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0047],
        [1.0095],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367933.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0096],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367942.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7840e-03,  3.8975e-04,  0.0000e+00,  ..., -2.9012e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1937.7029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.1712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.1854, device='cuda:0')



h[100].sum tensor(63.0097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2189, device='cuda:0')



h[200].sum tensor(-3.6986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.2197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58098.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0554, 0.0537, 0.0553,  ..., 0.0000, 0.0532, 0.0000],
        [0.0555, 0.0538, 0.0554,  ..., 0.0000, 0.0533, 0.0000],
        [0.0556, 0.0540, 0.0556,  ..., 0.0000, 0.0535, 0.0000],
        ...,
        [0.0570, 0.0552, 0.0572,  ..., 0.0000, 0.0548, 0.0000],
        [0.0570, 0.0552, 0.0572,  ..., 0.0000, 0.0548, 0.0000],
        [0.0569, 0.0552, 0.0572,  ..., 0.0000, 0.0548, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538484.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4991.0698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.7870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18441.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-377.4702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-437.5851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5753],
        [-1.7721],
        [-1.9093],
        ...,
        [-1.7782],
        [-1.7737],
        [-1.7721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249283.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0096],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367942.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0048],
        [1.0096],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367952.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7504e-03,  3.9926e-04,  0.0000e+00,  ..., -1.6733e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2179.4285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.3802, device='cuda:0')



h[100].sum tensor(65.4537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5213, device='cuda:0')



h[200].sum tensor(-4.4837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.4477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68189.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0535, 0.0290, 0.0339,  ..., 0.0000, 0.0438, 0.0000],
        [0.0540, 0.0333, 0.0378,  ..., 0.0000, 0.0458, 0.0000],
        [0.0508, 0.0217, 0.0233,  ..., 0.0000, 0.0291, 0.0000],
        ...,
        [0.0570, 0.0546, 0.0573,  ..., 0.0000, 0.0551, 0.0000],
        [0.0570, 0.0546, 0.0573,  ..., 0.0000, 0.0550, 0.0000],
        [0.0570, 0.0545, 0.0573,  ..., 0.0000, 0.0550, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615805.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4836.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(358.3463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20450.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-406.8678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(543.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-492.7076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0826],
        [-0.0329],
        [ 0.0409],
        ...,
        [-1.7788],
        [-1.7744],
        [-1.7731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221761.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0048],
        [1.0096],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367952.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0097],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367961.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6193e-02,  1.5100e-02, -4.5373e-04,  ...,  1.9767e-02,
         -7.8426e-03, -9.9483e-03],
        [ 2.2913e-02,  2.1946e-02, -6.6489e-04,  ...,  2.8972e-02,
         -1.1493e-02, -1.4578e-02],
        [ 1.6193e-02,  1.5100e-02, -4.5373e-04,  ...,  1.9767e-02,
         -7.8426e-03, -9.9483e-03],
        ...,
        [ 1.7516e-03,  3.9088e-04,  0.0000e+00,  ..., -1.1653e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7516e-03,  3.9088e-04,  0.0000e+00,  ..., -1.1653e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7516e-03,  3.9088e-04,  0.0000e+00,  ..., -1.1653e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2144.7849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.0177, device='cuda:0')



h[100].sum tensor(65.6747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0149, device='cuda:0')



h[200].sum tensor(-4.2656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.8580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0846, 0.0805, 0.0000,  ..., 0.1061, 0.0000, 0.0000],
        [0.0901, 0.0862, 0.0000,  ..., 0.1137, 0.0000, 0.0000],
        [0.1494, 0.1466, 0.0000,  ..., 0.1950, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65016.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0222, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0132, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0567, 0.0542, 0.0576,  ..., 0.0000, 0.0554, 0.0000],
        [0.0566, 0.0542, 0.0576,  ..., 0.0000, 0.0554, 0.0000],
        [0.0561, 0.0486, 0.0526,  ..., 0.0000, 0.0530, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579998.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4722.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.7087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19537.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-401.2675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(392.8284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.4687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1778],
        [ 0.1681],
        [ 0.1565],
        ...,
        [-1.7311],
        [-1.6375],
        [-1.4761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239941.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0097],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367961.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0050],
        [1.0097],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367970.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7614e-03,  3.7667e-04,  0.0000e+00,  ..., -1.1097e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1970.5635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.6205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.9111, device='cuda:0')



h[100].sum tensor(64.7989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7453, device='cuda:0')



h[200].sum tensor(-3.5398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.7329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59839.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0431, 0.0479,  ..., 0.0000, 0.0504, 0.0000],
        [0.0547, 0.0509, 0.0547,  ..., 0.0000, 0.0535, 0.0000],
        [0.0550, 0.0525, 0.0562,  ..., 0.0000, 0.0544, 0.0000],
        ...,
        [0.0563, 0.0537, 0.0578,  ..., 0.0000, 0.0557, 0.0000],
        [0.0563, 0.0537, 0.0578,  ..., 0.0000, 0.0557, 0.0000],
        [0.0563, 0.0537, 0.0578,  ..., 0.0000, 0.0557, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552141., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4604.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.1233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19032.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-394.8505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(227.8493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.6316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8764],
        [-1.2447],
        [-1.5202],
        ...,
        [-1.7687],
        [-1.7641],
        [-1.7626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225393.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0050],
        [1.0097],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367970.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0098],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367979.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3337e-02,  3.2530e-02, -9.7137e-04,  ...,  4.3246e-02,
         -1.7098e-02, -2.1702e-02],
        [ 3.6475e-02,  3.5725e-02, -1.0679e-03,  ...,  4.7543e-02,
         -1.8796e-02, -2.3858e-02],
        [ 2.3859e-02,  2.2881e-02, -6.7995e-04,  ...,  3.0269e-02,
         -1.1968e-02, -1.5191e-02],
        ...,
        [ 1.7452e-03,  3.6813e-04,  0.0000e+00,  ..., -8.1693e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7452e-03,  3.6813e-04,  0.0000e+00,  ..., -8.1693e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7452e-03,  3.6813e-04,  0.0000e+00,  ..., -8.1693e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1666.4072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.7409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.8420, device='cuda:0')



h[100].sum tensor(62.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3746, device='cuda:0')



h[200].sum tensor(-2.3843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1537, 0.1509, 0.0000,  ..., 0.2009, 0.0000, 0.0000],
        [0.1298, 0.1265, 0.0000,  ..., 0.1680, 0.0000, 0.0000],
        [0.1005, 0.0967, 0.0000,  ..., 0.1280, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49634.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0164, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0562, 0.0531, 0.0580,  ..., 0.0000, 0.0561, 0.0000],
        [0.0562, 0.0530, 0.0579,  ..., 0.0000, 0.0561, 0.0000],
        [0.0562, 0.0530, 0.0579,  ..., 0.0000, 0.0561, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495501.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4669.3306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.8909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17532.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-370.8680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56.6482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.6855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1065],
        [ 0.1075],
        [ 0.1134],
        ...,
        [-1.7758],
        [-1.7715],
        [-1.7699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248709.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0098],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367979.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(510.8289, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0099],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367988.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4482e-03,  5.1805e-03, -1.4411e-04,  ...,  6.4814e-03,
         -2.5599e-03, -3.2502e-03],
        [ 6.4482e-03,  5.1805e-03, -1.4411e-04,  ...,  6.4814e-03,
         -2.5599e-03, -3.2502e-03],
        [ 1.7110e-03,  3.5791e-04,  0.0000e+00,  ..., -4.8014e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7110e-03,  3.5791e-04,  0.0000e+00,  ..., -4.8014e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7110e-03,  3.5791e-04,  0.0000e+00,  ..., -4.8014e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7110e-03,  3.5791e-04,  0.0000e+00,  ..., -4.8014e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1798.6537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3617, device='cuda:0')



h[100].sum tensor(63.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6828, device='cuda:0')



h[200].sum tensor(-2.7830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0102, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0155, 0.0102, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0155, 0.0102, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52432.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0498, 0.0121, 0.0220,  ..., 0.0000, 0.0346, 0.0000],
        [0.0494, 0.0100, 0.0191,  ..., 0.0000, 0.0328, 0.0000],
        [0.0499, 0.0134, 0.0222,  ..., 0.0000, 0.0346, 0.0000],
        ...,
        [0.0566, 0.0526, 0.0581,  ..., 0.0000, 0.0567, 0.0000],
        [0.0566, 0.0526, 0.0581,  ..., 0.0000, 0.0567, 0.0000],
        [0.0566, 0.0526, 0.0581,  ..., 0.0000, 0.0567, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506877.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4681.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.4167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18169.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-381.8383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(64.7737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.4360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4057],
        [-0.3066],
        [-0.3266],
        ...,
        [-1.7851],
        [-1.7808],
        [-1.7794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212994.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0099],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367988.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0099],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367997.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6699e-03,  3.3314e-04,  0.0000e+00,  ..., -5.1160e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.2853e-02,  1.1718e-02, -3.3659e-04,  ...,  1.5307e-02,
         -6.0339e-03, -7.6634e-03],
        [ 2.3055e-02,  2.2104e-02, -6.4365e-04,  ...,  2.9276e-02,
         -1.1538e-02, -1.4654e-02],
        ...,
        [ 1.6699e-03,  3.3314e-04,  0.0000e+00,  ..., -5.1160e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6699e-03,  3.3314e-04,  0.0000e+00,  ..., -5.1160e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6699e-03,  3.3314e-04,  0.0000e+00,  ..., -5.1160e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1644.6827, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.3446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.0111, device='cuda:0')



h[100].sum tensor(61.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0658, device='cuda:0')



h[200].sum tensor(-2.2489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0127, 0.0000,  ..., 0.0153, 0.0000, 0.0000],
        [0.0459, 0.0412, 0.0000,  ..., 0.0536, 0.0000, 0.0000],
        [0.1035, 0.0999, 0.0000,  ..., 0.1325, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48727.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0471, 0.0139, 0.0181,  ..., 0.0000, 0.0279, 0.0000],
        [0.0361, 0.0000, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0206, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0573, 0.0526, 0.0585,  ..., 0.0000, 0.0577, 0.0000],
        [0.0573, 0.0526, 0.0584,  ..., 0.0000, 0.0577, 0.0000],
        [0.0572, 0.0526, 0.0584,  ..., 0.0000, 0.0577, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495173.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4858.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.7487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17616.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.8083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(68.9042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.5746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5062],
        [-0.1685],
        [ 0.0265],
        ...,
        [-1.8083],
        [-1.8040],
        [-1.8025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260102.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0099],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367997.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0100],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368006.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2216e-03,  8.0536e-03, -2.2642e-04,  ...,  1.0409e-02,
         -4.0964e-03, -5.2042e-03],
        [ 1.6184e-03,  3.1214e-04,  0.0000e+00,  ..., -2.8961e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.4037e-02,  1.2957e-02, -3.6983e-04,  ...,  1.7004e-02,
         -6.6908e-03, -8.5003e-03],
        ...,
        [ 1.6184e-03,  3.1214e-04,  0.0000e+00,  ..., -2.8961e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6184e-03,  3.1214e-04,  0.0000e+00,  ..., -2.8961e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6184e-03,  3.1214e-04,  0.0000e+00,  ..., -2.8961e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1912.2468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.4798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.7839, device='cuda:0')



h[100].sum tensor(63.8051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9547, device='cuda:0')



h[200].sum tensor(-3.1670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.2509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0457, 0.0412, 0.0000,  ..., 0.0537, 0.0000, 0.0000],
        [0.0430, 0.0384, 0.0000,  ..., 0.0500, 0.0000, 0.0000],
        [0.0320, 0.0272, 0.0000,  ..., 0.0349, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0067, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0231, 0.0181, 0.0000,  ..., 0.0226, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55427.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0381, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0400, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0577, 0.0475, 0.0545,  ..., 0.0000, 0.0569, 0.0000],
        [0.0557, 0.0290, 0.0359,  ..., 0.0000, 0.0496, 0.0000],
        [0.0501, 0.0109, 0.0161,  ..., 0.0000, 0.0302, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530486.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4935.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.5185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18625.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-379.3649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(160.8050, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.4485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1050],
        [ 0.1130],
        [ 0.1144],
        ...,
        [-1.5999],
        [-1.2830],
        [-0.8591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262344.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0100],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368006.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0101],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368015.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4973e-02,  1.3962e-02, -3.9490e-04,  ...,  1.8366e-02,
         -7.2105e-03, -9.1633e-03],
        [ 1.3220e-02,  1.2176e-02, -3.4324e-04,  ...,  1.5965e-02,
         -6.2672e-03, -7.9646e-03],
        [ 1.2995e-02,  1.1947e-02, -3.3661e-04,  ...,  1.5656e-02,
         -6.1462e-03, -7.8108e-03],
        ...,
        [ 1.5710e-03,  3.1232e-04,  0.0000e+00,  ...,  7.8406e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5710e-03,  3.1232e-04,  0.0000e+00,  ...,  7.8406e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.5710e-03,  3.1232e-04,  0.0000e+00,  ...,  7.8406e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1898.5645, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.4924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9519, device='cuda:0')



h[100].sum tensor(62.9424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6455, device='cuda:0')



h[200].sum tensor(-3.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.4728e-02, 5.0576e-02, 0.0000e+00,  ..., 6.6370e-02, 0.0000e+00,
         0.0000e+00],
        [7.0229e-02, 6.6354e-02, 0.0000e+00,  ..., 8.7587e-02, 0.0000e+00,
         0.0000e+00],
        [6.7441e-02, 6.3501e-02, 0.0000e+00,  ..., 8.3747e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4794e-03, 1.2881e-03, 0.0000e+00,  ..., 3.2337e-05, 0.0000e+00,
         0.0000e+00],
        [6.4771e-03, 1.2877e-03, 0.0000e+00,  ..., 3.2326e-05, 0.0000e+00,
         0.0000e+00],
        [6.4747e-03, 1.2872e-03, 0.0000e+00,  ..., 3.2314e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56142.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0328, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0271, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0265, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0587, 0.0526, 0.0592,  ..., 0.0000, 0.0589, 0.0000],
        [0.0587, 0.0526, 0.0591,  ..., 0.0000, 0.0589, 0.0000],
        [0.0587, 0.0526, 0.0591,  ..., 0.0000, 0.0589, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537923.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5038.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.2061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18904.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-383.7682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(213.4456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.7717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0896],
        [ 0.1131],
        [ 0.1135],
        ...,
        [-1.7683],
        [-1.7025],
        [-1.6580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255589.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0101],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368015.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0102],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368024.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3633e-03,  6.2283e-03, -1.6929e-04,  ...,  7.9747e-03,
         -3.1196e-03, -3.9658e-03],
        [ 7.4802e-03,  6.3474e-03, -1.7269e-04,  ...,  8.1349e-03,
         -3.1824e-03, -4.0456e-03],
        [ 1.5550e-03,  3.1287e-04,  0.0000e+00,  ...,  1.8768e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5550e-03,  3.1287e-04,  0.0000e+00,  ...,  1.8768e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5550e-03,  3.1287e-04,  0.0000e+00,  ...,  1.8768e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.5550e-03,  3.1287e-04,  0.0000e+00,  ...,  1.8768e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1754.1742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.0906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8906, device='cuda:0')



h[100].sum tensor(61.1907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1360, device='cuda:0')



h[200].sum tensor(-2.5723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.6798e-02, 4.2566e-02, 0.0000e+00,  ..., 5.5638e-02, 0.0000e+00,
         0.0000e+00],
        [2.5900e-02, 2.1273e-02, 0.0000e+00,  ..., 2.6997e-02, 0.0000e+00,
         0.0000e+00],
        [1.2218e-02, 7.3265e-03, 0.0000e+00,  ..., 8.2350e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4141e-03, 1.2906e-03, 0.0000e+00,  ..., 7.7418e-05, 0.0000e+00,
         0.0000e+00],
        [6.4118e-03, 1.2901e-03, 0.0000e+00,  ..., 7.7390e-05, 0.0000e+00,
         0.0000e+00],
        [6.4093e-03, 1.2896e-03, 0.0000e+00,  ..., 7.7361e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50915.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0426, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0468, 0.0000, 0.0023,  ..., 0.0000, 0.0183, 0.0000],
        [0.0514, 0.0100, 0.0153,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0590, 0.0525, 0.0595,  ..., 0.0000, 0.0590, 0.0000],
        [0.0590, 0.0525, 0.0594,  ..., 0.0000, 0.0590, 0.0000],
        [0.0590, 0.0525, 0.0594,  ..., 0.0000, 0.0590, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506524.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5130.5103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.2125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17982.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-370.0221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(93.9118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.0625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0976],
        [ 0.0870],
        [ 0.0497],
        ...,
        [-1.8600],
        [-1.8550],
        [-1.8527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285361.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0102],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368024.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0103],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368033.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00],
        [1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00],
        [1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00],
        [1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00],
        [1.5479e-03, 3.1831e-04, 0.0000e+00,  ..., 3.2048e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2042.7955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.5535, device='cuda:0')



h[100].sum tensor(64.1126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9840, device='cuda:0')



h[200].sum tensor(-3.4789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.4824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0142, 0.0094, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0102, 0.0053, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0062, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0064, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0064, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0064, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59668.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0208, 0.0315,  ..., 0.0000, 0.0416, 0.0000],
        [0.0547, 0.0297, 0.0392,  ..., 0.0000, 0.0463, 0.0000],
        [0.0563, 0.0402, 0.0484,  ..., 0.0000, 0.0519, 0.0000],
        ...,
        [0.0592, 0.0525, 0.0597,  ..., 0.0000, 0.0589, 0.0000],
        [0.0592, 0.0524, 0.0597,  ..., 0.0000, 0.0589, 0.0000],
        [0.0592, 0.0524, 0.0597,  ..., 0.0000, 0.0589, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559372.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5037.2173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.8507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19444.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-395.1361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(313.1767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.4820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2261],
        [-1.3944],
        [-1.5864],
        ...,
        [-1.8679],
        [-1.8632],
        [-1.8615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257582.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0103],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368033.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0104],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368043.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00],
        [1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00],
        [1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00],
        [1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00],
        [1.5709e-03, 3.0987e-04, 0.0000e+00,  ..., 3.3509e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1793.3760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.0749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1418, device='cuda:0')



h[100].sum tensor(61.8683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2294, device='cuda:0')



h[200].sum tensor(-2.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0063, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0063, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0065, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0065, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0065, 0.0013, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53840.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0512, 0.0580,  ..., 0.0000, 0.0572, 0.0000],
        [0.0576, 0.0513, 0.0582,  ..., 0.0000, 0.0573, 0.0000],
        [0.0577, 0.0514, 0.0584,  ..., 0.0000, 0.0575, 0.0000],
        ...,
        [0.0592, 0.0526, 0.0602,  ..., 0.0000, 0.0589, 0.0000],
        [0.0591, 0.0526, 0.0601,  ..., 0.0000, 0.0589, 0.0000],
        [0.0591, 0.0526, 0.0601,  ..., 0.0000, 0.0589, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528959.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5020.1045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.5088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18837.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-385.3573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(137.2907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.1159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9872],
        [-2.0551],
        [-2.1084],
        ...,
        [-1.8670],
        [-1.8646],
        [-1.8641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257113.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0104],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368043.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0056],
        [1.0104],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368051.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00],
        [1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00],
        [1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00],
        [1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00],
        [1.6117e-03, 2.9761e-04, 0.0000e+00,  ..., 3.2725e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1750.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.9942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.6699, device='cuda:0')



h[100].sum tensor(62.0899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6823, device='cuda:0')



h[200].sum tensor(-2.3988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.1173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0074, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0065, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0065, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0066, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0066, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50991.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0511, 0.0188, 0.0233,  ..., 0.0000, 0.0330, 0.0000],
        [0.0551, 0.0292, 0.0395,  ..., 0.0000, 0.0477, 0.0000],
        [0.0559, 0.0361, 0.0455,  ..., 0.0000, 0.0504, 0.0000],
        ...,
        [0.0589, 0.0528, 0.0606,  ..., 0.0000, 0.0588, 0.0000],
        [0.0589, 0.0527, 0.0606,  ..., 0.0000, 0.0588, 0.0000],
        [0.0588, 0.0527, 0.0605,  ..., 0.0000, 0.0587, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511374.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4979.2651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.5683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18143.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-374.1770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(109.1531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.8927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3012],
        [-0.5769],
        [-0.7496],
        ...,
        [-1.8844],
        [-1.8800],
        [-1.8780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286568.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0056],
        [1.0104],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368051.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0106],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368060.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00],
        [1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00],
        [1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00],
        [1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00],
        [1.6360e-03, 2.8405e-04, 0.0000e+00,  ..., 2.9088e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1961.5040, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.4972, device='cuda:0')



h[100].sum tensor(64.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8481, device='cuda:0')



h[200].sum tensor(-3.0374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0066, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0066, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55108.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0572, 0.0517, 0.0588,  ..., 0.0000, 0.0570, 0.0000],
        [0.0573, 0.0518, 0.0590,  ..., 0.0000, 0.0571, 0.0000],
        [0.0574, 0.0519, 0.0592,  ..., 0.0000, 0.0573, 0.0000],
        ...,
        [0.0588, 0.0531, 0.0610,  ..., 0.0000, 0.0588, 0.0000],
        [0.0588, 0.0531, 0.0610,  ..., 0.0000, 0.0587, 0.0000],
        [0.0588, 0.0530, 0.0609,  ..., 0.0000, 0.0587, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523998.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4892.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.7913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18744.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-387.3114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(118.3484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.2238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0670],
        [-2.1036],
        [-2.1275],
        ...,
        [-1.8945],
        [-1.8891],
        [-1.8869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252692.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0106],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368060.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0106],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368060.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6360e-03,  2.8405e-04,  0.0000e+00,  ...,  2.9088e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6360e-03,  2.8405e-04,  0.0000e+00,  ...,  2.9088e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0410e-02,  1.9384e-02, -5.2311e-04,  ...,  2.5716e-02,
         -1.0005e-02, -1.2734e-02],
        ...,
        [ 1.6360e-03,  2.8405e-04,  0.0000e+00,  ...,  2.9088e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6360e-03,  2.8405e-04,  0.0000e+00,  ...,  2.9088e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6360e-03,  2.8405e-04,  0.0000e+00,  ...,  2.9088e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1880.0804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.9613, device='cuda:0')



h[100].sum tensor(63.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9056, device='cuda:0')



h[200].sum tensor(-2.7735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.9576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0408, 0.0360, 0.0000,  ..., 0.0470, 0.0000, 0.0000],
        [0.0754, 0.0711, 0.0000,  ..., 0.0943, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54096.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0149, 0.0201,  ..., 0.0000, 0.0295, 0.0000],
        [0.0394, 0.0000, 0.0020,  ..., 0.0000, 0.0125, 0.0000],
        [0.0287, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0588, 0.0531, 0.0610,  ..., 0.0000, 0.0588, 0.0000],
        [0.0588, 0.0531, 0.0610,  ..., 0.0000, 0.0587, 0.0000],
        [0.0588, 0.0530, 0.0609,  ..., 0.0000, 0.0587, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525453.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4906.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.5569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18604.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-382.9469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(172.7123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.6202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [ 0.0303],
        [ 0.0782],
        ...,
        [-1.8984],
        [-1.8936],
        [-1.8918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271398.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0106],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368060.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(439.7474, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0057],
        [1.0107],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368069.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9726e-03,  5.6838e-03, -1.4642e-04,  ...,  7.3016e-03,
         -2.8267e-03, -3.5989e-03],
        [ 8.1440e-03,  6.8753e-03, -1.7869e-04,  ...,  8.9040e-03,
         -3.4497e-03, -4.3922e-03],
        [ 1.6578e-03,  2.7814e-04,  0.0000e+00,  ...,  3.1884e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6578e-03,  2.7814e-04,  0.0000e+00,  ...,  3.1884e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6578e-03,  2.7814e-04,  0.0000e+00,  ...,  3.1884e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6578e-03,  2.7814e-04,  0.0000e+00,  ...,  3.1884e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2160.6179, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.0720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.5231, device='cuda:0')



h[100].sum tensor(66.7537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7161, device='cuda:0')



h[200].sum tensor(-3.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.7805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0493, 0.0445, 0.0000,  ..., 0.0585, 0.0000, 0.0000],
        [0.0264, 0.0212, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0196, 0.0143, 0.0000,  ..., 0.0178, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0068, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0068, 0.0011, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64695.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0454, 0.0000, 0.0010,  ..., 0.0000, 0.0148, 0.0000],
        [0.0483, 0.0000, 0.0047,  ..., 0.0000, 0.0223, 0.0000],
        ...,
        [0.0590, 0.0532, 0.0617,  ..., 0.0000, 0.0588, 0.0000],
        [0.0590, 0.0532, 0.0617,  ..., 0.0000, 0.0588, 0.0000],
        [0.0590, 0.0532, 0.0617,  ..., 0.0000, 0.0588, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609108.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4826.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.4357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20437.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.1346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(614.1398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-490.3637, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1027],
        [ 0.0804],
        [ 0.0396],
        ...,
        [-1.9081],
        [-1.9033],
        [-1.9015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272858.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0057],
        [1.0107],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368069.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0108],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368078.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00],
        [1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00],
        [1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00],
        [1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00],
        [1.6696e-03, 2.8337e-04, 0.0000e+00,  ..., 4.0837e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2245.7935, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.2931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.0415, device='cuda:0')



h[100].sum tensor(67.5989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2804, device='cuda:0')



h[200].sum tensor(-3.8311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.5522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65429.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0519, 0.0598,  ..., 0.0000, 0.0568, 0.0000],
        [0.0575, 0.0508, 0.0590,  ..., 0.0000, 0.0563, 0.0000],
        [0.0572, 0.0477, 0.0565,  ..., 0.0000, 0.0548, 0.0000],
        ...,
        [0.0592, 0.0533, 0.0621,  ..., 0.0000, 0.0586, 0.0000],
        [0.0588, 0.0487, 0.0582,  ..., 0.0000, 0.0569, 0.0000],
        [0.0573, 0.0335, 0.0454,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597773., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4845.2622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(358.5804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20139.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-401.5818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(577.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-494.7536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6042],
        [-1.5016],
        [-1.3407],
        ...,
        [-1.8532],
        [-1.7163],
        [-1.4523]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280489.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0108],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368078.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0060],
        [1.0109],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368087.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6691e-03,  2.9227e-04,  0.0000e+00,  ...,  5.2759e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6691e-03,  2.9227e-04,  0.0000e+00,  ...,  5.2759e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.4973e-03,  7.2367e-03, -1.8393e-04,  ...,  9.3906e-03,
         -3.6181e-03, -4.6094e-03],
        ...,
        [ 1.6691e-03,  2.9227e-04,  0.0000e+00,  ...,  5.2759e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6691e-03,  2.9227e-04,  0.0000e+00,  ...,  5.2759e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6691e-03,  2.9227e-04,  0.0000e+00,  ...,  5.2759e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1872.9238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0024, device='cuda:0')



h[100].sum tensor(63.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5492, device='cuda:0')



h[200].sum tensor(-2.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0136, 0.0082, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0123, 0.0069, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55262.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0565, 0.0425, 0.0517,  ..., 0.0000, 0.0522, 0.0000],
        [0.0548, 0.0273, 0.0379,  ..., 0.0000, 0.0452, 0.0000],
        [0.0541, 0.0205, 0.0322,  ..., 0.0000, 0.0421, 0.0000],
        ...,
        [0.0593, 0.0537, 0.0621,  ..., 0.0000, 0.0582, 0.0000],
        [0.0593, 0.0537, 0.0620,  ..., 0.0000, 0.0582, 0.0000],
        [0.0593, 0.0537, 0.0620,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539085.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4882.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.5541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19049.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-388.9756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(195.1616, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-437.3745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1288],
        [-1.3238],
        [-1.4322],
        ...,
        [-1.9278],
        [-1.9230],
        [-1.9211]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252427.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0060],
        [1.0109],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368087.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0110],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368096.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00],
        [1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00],
        [1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00],
        [1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00],
        [1.6693e-03, 2.9507e-04, 0.0000e+00,  ..., 5.9162e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2281.4263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.0896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.3747, device='cuda:0')



h[100].sum tensor(67.3107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7759, device='cuda:0')



h[200].sum tensor(-3.8967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.1077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65417.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0577, 0.0528, 0.0599,  ..., 0.0000, 0.0562, 0.0000],
        [0.0579, 0.0529, 0.0601,  ..., 0.0000, 0.0564, 0.0000],
        [0.0580, 0.0530, 0.0604,  ..., 0.0000, 0.0565, 0.0000],
        ...,
        [0.0559, 0.0167, 0.0291,  ..., 0.0000, 0.0429, 0.0000],
        [0.0560, 0.0167, 0.0307,  ..., 0.0000, 0.0437, 0.0000],
        [0.0569, 0.0261, 0.0386,  ..., 0.0000, 0.0472, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598271.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4805.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.7625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20404.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-415.2978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(483.0663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-488.9056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9947],
        [-2.0570],
        [-2.0875],
        ...,
        [-0.5956],
        [-0.6084],
        [-0.6658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241066.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0110],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368096.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0111],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368105.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6625e-03,  2.9403e-04,  0.0000e+00,  ...,  6.1184e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6625e-03,  2.9403e-04,  0.0000e+00,  ...,  6.1184e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6625e-03,  2.9403e-04,  0.0000e+00,  ...,  6.1184e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6625e-03,  2.9403e-04,  0.0000e+00,  ...,  6.1184e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6625e-03,  2.9403e-04,  0.0000e+00,  ...,  6.1184e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.0237e-02,  9.0144e-03, -2.2581e-04,  ...,  1.1785e-02,
         -4.5265e-03, -5.7704e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1962.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.6373, device='cuda:0')



h[100].sum tensor(63.4712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5285, device='cuda:0')



h[200].sum tensor(-2.8935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.9131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0157, 0.0102, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0141, 0.0086, 0.0000,  ..., 0.0101, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56797.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0568, 0.0394, 0.0487,  ..., 0.0000, 0.0506, 0.0000],
        [0.0576, 0.0469, 0.0550,  ..., 0.0000, 0.0535, 0.0000],
        [0.0570, 0.0405, 0.0498,  ..., 0.0000, 0.0507, 0.0000],
        ...,
        [0.0588, 0.0435, 0.0531,  ..., 0.0000, 0.0534, 0.0000],
        [0.0564, 0.0246, 0.0339,  ..., 0.0000, 0.0432, 0.0000],
        [0.0554, 0.0152, 0.0243,  ..., 0.0000, 0.0388, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542802.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4953.0278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.2661, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19012.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-394.7252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(211.2323, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.0116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4896],
        [-0.6455],
        [-0.7106],
        ...,
        [-1.6885],
        [-1.4777],
        [-1.3006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270753.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0111],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368105.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0112],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368114.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00],
        [1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00],
        [1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00],
        [1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00],
        [1.6522e-03, 2.9588e-04, 0.0000e+00,  ..., 6.3893e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1872.5076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.5317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.9794, device='cuda:0')



h[100].sum tensor(61.8950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5407, device='cuda:0')



h[200].sum tensor(-2.5907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0066, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54096.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0346, 0.0446,  ..., 0.0000, 0.0479, 0.0000],
        [0.0581, 0.0458, 0.0541,  ..., 0.0000, 0.0528, 0.0000],
        [0.0582, 0.0454, 0.0539,  ..., 0.0000, 0.0527, 0.0000],
        ...,
        [0.0606, 0.0551, 0.0628,  ..., 0.0000, 0.0583, 0.0000],
        [0.0606, 0.0551, 0.0628,  ..., 0.0000, 0.0583, 0.0000],
        [0.0606, 0.0551, 0.0627,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527409.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5120.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.6648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18861.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-385.5898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(83.3385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-427.6154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3346],
        [-0.5138],
        [-0.6169],
        ...,
        [-1.9740],
        [-1.9693],
        [-1.9674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253609.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0112],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368114.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0113],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368123.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00],
        [1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00],
        [1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00],
        [1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00],
        [1.6615e-03, 2.8232e-04, 0.0000e+00,  ..., 5.4492e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1731.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.0501, device='cuda:0')



h[100].sum tensor(60.0851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0803, device='cuda:0')



h[200].sum tensor(-2.1408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.2273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50288.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0544, 0.0611,  ..., 0.0000, 0.0569, 0.0000],
        [0.0596, 0.0545, 0.0613,  ..., 0.0000, 0.0571, 0.0000],
        [0.0598, 0.0546, 0.0616,  ..., 0.0000, 0.0572, 0.0000],
        ...,
        [0.0613, 0.0559, 0.0635,  ..., 0.0000, 0.0587, 0.0000],
        [0.0613, 0.0559, 0.0634,  ..., 0.0000, 0.0587, 0.0000],
        [0.0612, 0.0558, 0.0634,  ..., 0.0000, 0.0586, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511870.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5310.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.4298, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18366.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.9752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42.9828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.4765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9917],
        [-1.8719],
        [-1.6755],
        ...,
        [-1.9936],
        [-1.9888],
        [-1.9870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279596., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0113],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368123.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0114],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6362e-02,  3.5536e-02, -8.8266e-04,  ...,  4.7444e-02,
         -1.8205e-02, -2.3230e-02],
        [ 2.8976e-02,  2.8027e-02, -6.9470e-04,  ...,  3.7351e-02,
         -1.4328e-02, -1.8283e-02],
        [ 2.4903e-02,  2.3886e-02, -5.9106e-04,  ...,  3.1786e-02,
         -1.2191e-02, -1.5555e-02],
        ...,
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2107.2925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.9464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.5166, device='cuda:0')



h[100].sum tensor(63.8840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9704, device='cuda:0')



h[200].sum tensor(-3.2609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.4395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1092, 0.1053, 0.0000,  ..., 0.1402, 0.0000, 0.0000],
        [0.1353, 0.1318, 0.0000,  ..., 0.1758, 0.0000, 0.0000],
        [0.1426, 0.1392, 0.0000,  ..., 0.1858, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57992.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0123, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0618, 0.0566, 0.0642,  ..., 0.0000, 0.0591, 0.0000],
        [0.0618, 0.0566, 0.0641,  ..., 0.0000, 0.0591, 0.0000],
        [0.0618, 0.0565, 0.0641,  ..., 0.0000, 0.0590, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544866.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5359.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(366.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18886.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-372.3813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(242.7047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.0506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1064],
        [ 0.0904],
        [ 0.0829],
        ...,
        [-2.0099],
        [-2.0050],
        [-2.0031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316718.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0114],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0114],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9724e-02,  2.8787e-02, -7.1374e-04,  ...,  3.8374e-02,
         -1.4721e-02, -1.8784e-02],
        [ 2.0539e-02,  1.9448e-02, -4.7999e-04,  ...,  2.5821e-02,
         -9.9000e-03, -1.2632e-02],
        [ 3.9861e-02,  3.9094e-02, -9.7171e-04,  ...,  5.2226e-02,
         -2.0042e-02, -2.5573e-02],
        ...,
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6779e-03,  2.7087e-04,  0.0000e+00,  ...,  4.6227e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2272.1604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.9629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.0785, device='cuda:0')



h[100].sum tensor(65.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6659, device='cuda:0')



h[200].sum tensor(-3.7660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.7622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0876, 0.0833, 0.0000,  ..., 0.1106, 0.0000, 0.0000],
        [0.1267, 0.1231, 0.0000,  ..., 0.1641, 0.0000, 0.0000],
        [0.0725, 0.0679, 0.0000,  ..., 0.0900, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64423.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0310, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0257, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0325, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0618, 0.0566, 0.0642,  ..., 0.0000, 0.0591, 0.0000],
        [0.0618, 0.0566, 0.0641,  ..., 0.0000, 0.0591, 0.0000],
        [0.0618, 0.0565, 0.0641,  ..., 0.0000, 0.0590, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584166.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5270.2915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20257.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-393.6117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.4615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.3615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1786],
        [ 0.1814],
        [ 0.1661],
        ...,
        [-1.9978],
        [-1.9923],
        [-1.9899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253105.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0114],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0115],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368141.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0872e-02,  1.9777e-02, -4.8266e-04,  ...,  2.6262e-02,
         -1.0050e-02, -1.2828e-02],
        [ 3.1778e-02,  3.0865e-02, -7.5705e-04,  ...,  4.1164e-02,
         -1.5764e-02, -2.0121e-02],
        [ 3.5888e-02,  3.5043e-02, -8.6043e-04,  ...,  4.6779e-02,
         -1.7917e-02, -2.2869e-02],
        ...,
        [ 1.6859e-03,  2.7242e-04,  0.0000e+00,  ...,  4.9484e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6859e-03,  2.7242e-04,  0.0000e+00,  ...,  4.9484e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6859e-03,  2.7242e-04,  0.0000e+00,  ...,  4.9484e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1940.0388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.9515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6674, device='cuda:0')



h[100].sum tensor(62.1032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1681, device='cuda:0')



h[200].sum tensor(-2.7103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.7815, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0985, 0.0944, 0.0000,  ..., 0.1256, 0.0000, 0.0000],
        [0.1019, 0.0978, 0.0000,  ..., 0.1301, 0.0000, 0.0000],
        [0.1145, 0.1105, 0.0000,  ..., 0.1473, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53989.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0246, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0254, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0273, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0621, 0.0569, 0.0646,  ..., 0.0000, 0.0590, 0.0000],
        [0.0621, 0.0569, 0.0645,  ..., 0.0000, 0.0590, 0.0000],
        [0.0620, 0.0569, 0.0645,  ..., 0.0000, 0.0590, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526155.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5424.4136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.0302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18607.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-363.5105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(125.7780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.9554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1495],
        [ 0.1520],
        [ 0.1423],
        ...,
        [-2.0125],
        [-2.0077],
        [-2.0065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294230.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0115],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368141.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(473.3086, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0116],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368151., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00],
        [1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00],
        [1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00],
        [1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00],
        [1.6999e-03, 2.8850e-04, 0.0000e+00,  ..., 6.3246e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1899.3801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.0029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8100, device='cuda:0')



h[100].sum tensor(61.8725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4777, device='cuda:0')



h[200].sum tensor(-2.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6143, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53739.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0601, 0.0541, 0.0613,  ..., 0.0000, 0.0558, 0.0000],
        [0.0586, 0.0349, 0.0455,  ..., 0.0000, 0.0480, 0.0000],
        [0.0565, 0.0175, 0.0234,  ..., 0.0000, 0.0372, 0.0000],
        ...,
        [0.0620, 0.0570, 0.0648,  ..., 0.0000, 0.0586, 0.0000],
        [0.0620, 0.0569, 0.0648,  ..., 0.0000, 0.0585, 0.0000],
        [0.0620, 0.0569, 0.0647,  ..., 0.0000, 0.0585, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526300.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5357.8735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(381.6099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18482.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-364.4648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(131.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.6371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1067],
        [-0.6913],
        [-0.2859],
        ...,
        [-2.0099],
        [-2.0046],
        [-1.9992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299414.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0116],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368151., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0067],
        [1.0117],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368160.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00],
        [1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00],
        [1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00],
        [1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00],
        [1.7122e-03, 3.0243e-04, 0.0000e+00,  ..., 7.4823e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1800.7380, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5262, device='cuda:0')



h[100].sum tensor(61.1646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2572, device='cuda:0')



h[200].sum tensor(-2.1546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0075, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0069, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0135, 0.0079, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50982.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0576, 0.0306, 0.0423,  ..., 0.0000, 0.0447, 0.0000],
        [0.0587, 0.0406, 0.0506,  ..., 0.0000, 0.0496, 0.0000],
        [0.0577, 0.0287, 0.0410,  ..., 0.0000, 0.0443, 0.0000],
        ...,
        [0.0619, 0.0569, 0.0650,  ..., 0.0000, 0.0582, 0.0000],
        [0.0619, 0.0569, 0.0650,  ..., 0.0000, 0.0582, 0.0000],
        [0.0619, 0.0569, 0.0650,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516102.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5332.5493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(390.4305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18160.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-358.6939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(97.1491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.2505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5328],
        [-1.6751],
        [-1.7015],
        ...,
        [-2.0160],
        [-2.0114],
        [-2.0094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295353.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0067],
        [1.0117],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368160.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0068],
        [1.0119],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368169.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00],
        [1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00],
        [1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00],
        [1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00],
        [1.7069e-03, 3.1338e-04, 0.0000e+00,  ..., 8.4087e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2147.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.7085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.3329, device='cuda:0')



h[100].sum tensor(64.4423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9021, device='cuda:0')



h[200].sum tensor(-3.1668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.2251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59455.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0601, 0.0545, 0.0621,  ..., 0.0000, 0.0553, 0.0000],
        [0.0602, 0.0538, 0.0615,  ..., 0.0000, 0.0545, 0.0000],
        [0.0598, 0.0483, 0.0572,  ..., 0.0000, 0.0516, 0.0000],
        ...,
        [0.0620, 0.0570, 0.0652,  ..., 0.0000, 0.0580, 0.0000],
        [0.0620, 0.0570, 0.0652,  ..., 0.0000, 0.0579, 0.0000],
        [0.0620, 0.0569, 0.0652,  ..., 0.0000, 0.0579, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553198.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5216.1982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19267.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.9110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(184.6988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.5898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5945],
        [-1.3899],
        [-1.1327],
        ...,
        [-2.0189],
        [-2.0138],
        [-2.0117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257996.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0068],
        [1.0119],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368169.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0120],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368179.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5296e-03,  7.2493e-03, -1.6387e-04,  ...,  9.4014e-03,
         -3.5461e-03, -4.5320e-03],
        [ 1.7066e-03,  3.1737e-04,  0.0000e+00,  ...,  8.7308e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.5296e-03,  7.2493e-03, -1.6387e-04,  ...,  9.4014e-03,
         -3.5461e-03, -4.5320e-03],
        ...,
        [ 1.7066e-03,  3.1737e-04,  0.0000e+00,  ...,  8.7308e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7066e-03,  3.1737e-04,  0.0000e+00,  ...,  8.7308e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7066e-03,  3.1737e-04,  0.0000e+00,  ...,  8.7308e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.2876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.7380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-38.4082, device='cuda:0')



h[100].sum tensor(67.7708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2751, device='cuda:0')



h[200].sum tensor(-4.1357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-44.8140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0124, 0.0069, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0318, 0.0266, 0.0000,  ..., 0.0344, 0.0000, 0.0000],
        [0.0125, 0.0070, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0013, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0071, 0.0013, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70132.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0555, 0.0077, 0.0231,  ..., 0.0000, 0.0337, 0.0000],
        [0.0538, 0.0000, 0.0098,  ..., 0.0000, 0.0251, 0.0000],
        [0.0556, 0.0105, 0.0244,  ..., 0.0000, 0.0323, 0.0000],
        ...,
        [0.0621, 0.0571, 0.0655,  ..., 0.0000, 0.0579, 0.0000],
        [0.0621, 0.0571, 0.0654,  ..., 0.0000, 0.0579, 0.0000],
        [0.0621, 0.0571, 0.0654,  ..., 0.0000, 0.0579, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619064.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5043.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.0836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20872.9082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-412.7631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(415.7527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-499.0001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2331],
        [-0.2459],
        [-0.1944],
        ...,
        [-2.0312],
        [-2.0264],
        [-2.0245]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243646.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0120],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368179.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0121],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368188.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7196e-03,  3.0173e-04,  0.0000e+00,  ...,  7.5630e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.9027e-03,  7.5970e-03, -1.7051e-04,  ...,  9.8778e-03,
         -3.7257e-03, -4.7630e-03],
        [ 1.7196e-03,  3.0173e-04,  0.0000e+00,  ...,  7.5630e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7196e-03,  3.0173e-04,  0.0000e+00,  ...,  7.5630e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7196e-03,  3.0173e-04,  0.0000e+00,  ...,  7.5630e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7196e-03,  3.0173e-04,  0.0000e+00,  ...,  7.5630e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1931.9170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.6961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8683, device='cuda:0')



h[100].sum tensor(62.5141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4994, device='cuda:0')



h[200].sum tensor(-2.4703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0283, 0.0230, 0.0000,  ..., 0.0296, 0.0000, 0.0000],
        [0.0258, 0.0204, 0.0000,  ..., 0.0260, 0.0000, 0.0000],
        [0.0371, 0.0319, 0.0000,  ..., 0.0415, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54219.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0507, 0.0021, 0.0052,  ..., 0.0000, 0.0128, 0.0000],
        [0.0505, 0.0000, 0.0008,  ..., 0.0000, 0.0119, 0.0000],
        [0.0505, 0.0000, 0.0003,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0622, 0.0576, 0.0659,  ..., 0.0000, 0.0583, 0.0000],
        [0.0621, 0.0576, 0.0659,  ..., 0.0000, 0.0582, 0.0000],
        [0.0621, 0.0576, 0.0659,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529423.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5208.5420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(387.2323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18602.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-372.8273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(91.2301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.2733, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1667],
        [-0.0533],
        [-0.1063],
        ...,
        [-2.0445],
        [-2.0396],
        [-2.0378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269270.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0121],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368188.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0122],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368196.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3881e-02,  2.2775e-02, -5.1985e-04,  ...,  3.0283e-02,
         -1.1469e-02, -1.4667e-02],
        [ 1.6335e-02,  1.5114e-02, -3.4281e-04,  ...,  1.9990e-02,
         -7.5634e-03, -9.6722e-03],
        [ 1.8813e-02,  1.7630e-02, -4.0095e-04,  ...,  2.3370e-02,
         -8.8461e-03, -1.1312e-02],
        ...,
        [ 1.7240e-03,  2.7975e-04,  0.0000e+00,  ...,  5.7648e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7240e-03,  2.7975e-04,  0.0000e+00,  ...,  5.7648e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7240e-03,  2.7975e-04,  0.0000e+00,  ...,  5.7648e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1966.2102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.7248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.9602, device='cuda:0')



h[100].sum tensor(62.9574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9052, device='cuda:0')



h[200].sum tensor(-2.5642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.9563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0688, 0.0639, 0.0000,  ..., 0.0846, 0.0000, 0.0000],
        [0.0780, 0.0733, 0.0000,  ..., 0.0972, 0.0000, 0.0000],
        [0.0716, 0.0667, 0.0000,  ..., 0.0884, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57085.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0347, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0367, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0624, 0.0581, 0.0664,  ..., 0.0000, 0.0589, 0.0000],
        [0.0623, 0.0581, 0.0664,  ..., 0.0000, 0.0588, 0.0000],
        [0.0623, 0.0581, 0.0663,  ..., 0.0000, 0.0588, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556532.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5181.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.9448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19196.7227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-376.3553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(190.7664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-430.9566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2142],
        [ 0.2128],
        [ 0.2117],
        ...,
        [-2.0672],
        [-2.0623],
        [-2.0603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278927.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0122],
        ...,
        [1.0012],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368196.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0123],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368205.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00],
        [1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00],
        [1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00],
        [1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00],
        [1.7247e-03, 2.7270e-04, 0.0000e+00,  ..., 4.9316e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.5911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.1111, device='cuda:0')



h[100].sum tensor(61.9895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8463, device='cuda:0')



h[200].sum tensor(-2.2651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.6321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0119, 0.0061, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0069, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52492.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0549, 0.0107, 0.0193,  ..., 0.0000, 0.0316, 0.0000],
        [0.0578, 0.0272, 0.0402,  ..., 0.0000, 0.0441, 0.0000],
        [0.0600, 0.0465, 0.0562,  ..., 0.0000, 0.0530, 0.0000],
        ...,
        [0.0626, 0.0585, 0.0668,  ..., 0.0000, 0.0592, 0.0000],
        [0.0625, 0.0585, 0.0668,  ..., 0.0000, 0.0592, 0.0000],
        [0.0625, 0.0585, 0.0667,  ..., 0.0000, 0.0592, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524442.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5275.6118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.9819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18347.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-364.0465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(102.0932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.5487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1484],
        [-0.5056],
        [-0.8969],
        ...,
        [-2.0826],
        [-2.0776],
        [-2.0754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298196.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0123],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368205.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0123],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368215., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00],
        [1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00],
        [1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00],
        [1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00],
        [1.7321e-03, 2.7497e-04, 0.0000e+00,  ..., 4.7188e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2681.8184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(96.8065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-43.6015, device='cuda:0')



h[100].sum tensor(70.3388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2052, device='cuda:0')



h[200].sum tensor(-4.5545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-50.8734, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0382, 0.0328, 0.0000,  ..., 0.0427, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0011, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73318.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0435, 0.0032, 0.0062,  ..., 0.0000, 0.0131, 0.0000],
        [0.0556, 0.0270, 0.0311,  ..., 0.0000, 0.0326, 0.0000],
        [0.0597, 0.0396, 0.0507,  ..., 0.0000, 0.0511, 0.0000],
        ...,
        [0.0627, 0.0587, 0.0671,  ..., 0.0000, 0.0594, 0.0000],
        [0.0627, 0.0587, 0.0671,  ..., 0.0000, 0.0594, 0.0000],
        [0.0627, 0.0586, 0.0670,  ..., 0.0000, 0.0594, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633628.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5111.8740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.3014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20896.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-412.5995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(626.7255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-514.1353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0367],
        [-0.0895],
        [-0.2599],
        ...,
        [-2.0913],
        [-2.0865],
        [-2.0845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276553.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0123],
        ...,
        [1.0011],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368215., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0124],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368224.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7252e-03,  7.3895e-03, -1.5854e-04,  ...,  9.5935e-03,
         -3.6017e-03, -4.6103e-03],
        [ 1.7875e-02,  1.6672e-02, -3.6580e-04,  ...,  2.2064e-02,
         -8.3101e-03, -1.0637e-02],
        [ 1.7263e-03,  2.8862e-04,  0.0000e+00,  ...,  5.3947e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7263e-03,  2.8862e-04,  0.0000e+00,  ...,  5.3947e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7263e-03,  2.8862e-04,  0.0000e+00,  ...,  5.3947e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7263e-03,  2.8862e-04,  0.0000e+00,  ...,  5.3947e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2066.8010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.1747, device='cuda:0')



h[100].sum tensor(64.1604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7283, device='cuda:0')



h[200].sum tensor(-2.7361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5401, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0833, 0.0787, 0.0000,  ..., 0.1043, 0.0000, 0.0000],
        [0.0465, 0.0413, 0.0000,  ..., 0.0541, 0.0000, 0.0000],
        [0.0482, 0.0430, 0.0000,  ..., 0.0565, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0012, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56089.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0398, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0443, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0629, 0.0585, 0.0672,  ..., 0.0000, 0.0595, 0.0000],
        [0.0629, 0.0585, 0.0671,  ..., 0.0000, 0.0595, 0.0000],
        [0.0629, 0.0585, 0.0671,  ..., 0.0000, 0.0594, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536665.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5268.5435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.0108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18762.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-378.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(134.4831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.6924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2041],
        [ 0.1875],
        [ 0.0485],
        ...,
        [-2.0958],
        [-2.0906],
        [-2.0888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275336.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0124],
        ...,
        [1.0011],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368224.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0125],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368233.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00],
        [1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00],
        [1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00],
        [1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00],
        [1.6953e-03, 3.0649e-04, 0.0000e+00,  ..., 6.4615e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1851.6794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.3052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.9988, device='cuda:0')



h[100].sum tensor(61.5093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4329, device='cuda:0')



h[200].sum tensor(-2.1028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0012, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0253, 0.0199, 0.0000,  ..., 0.0254, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52294.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0608, 0.0493, 0.0585,  ..., 0.0000, 0.0546, 0.0000],
        [0.0574, 0.0274, 0.0334,  ..., 0.0000, 0.0380, 0.0000],
        [0.0499, 0.0080, 0.0121,  ..., 0.0000, 0.0187, 0.0000],
        ...,
        [0.0634, 0.0582, 0.0670,  ..., 0.0000, 0.0596, 0.0000],
        [0.0634, 0.0582, 0.0670,  ..., 0.0000, 0.0596, 0.0000],
        [0.0634, 0.0582, 0.0669,  ..., 0.0000, 0.0596, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525940.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5355.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.3337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18541.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-375.7326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(73.7641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.6075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3949],
        [-0.1826],
        [ 0.0205],
        ...,
        [-2.1043],
        [-2.0992],
        [-2.0974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281908.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0125],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368233.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(500.7934, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0126],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368242.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00],
        [1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00],
        [1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00],
        [1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00],
        [1.6831e-03, 3.2855e-04, 0.0000e+00,  ..., 7.7241e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1995.3365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3689, device='cuda:0')



h[100].sum tensor(62.8096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6855, device='cuda:0')



h[200].sum tensor(-2.4593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.2664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0013, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0250, 0.0198, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55830.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0600, 0.0376, 0.0490,  ..., 0.0000, 0.0501, 0.0000],
        [0.0560, 0.0198, 0.0253,  ..., 0.0000, 0.0312, 0.0000],
        [0.0494, 0.0040, 0.0070,  ..., 0.0000, 0.0124, 0.0000],
        ...,
        [0.0636, 0.0579, 0.0668,  ..., 0.0000, 0.0595, 0.0000],
        [0.0636, 0.0579, 0.0668,  ..., 0.0000, 0.0595, 0.0000],
        [0.0636, 0.0579, 0.0667,  ..., 0.0000, 0.0595, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544894., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5337.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.8717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18835.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-386.0732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(206.0280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.7556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3436],
        [-0.0586],
        [ 0.0977],
        ...,
        [-2.0532],
        [-2.0684],
        [-2.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292185.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0126],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368242.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0127],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368251.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00],
        [1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00],
        [1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00],
        [1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00],
        [1.6703e-03, 3.5115e-04, 0.0000e+00,  ..., 8.9300e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2074.3088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.0997, device='cuda:0')



h[100].sum tensor(63.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3287, device='cuda:0')



h[200].sum tensor(-2.6289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0014, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0067, 0.0014, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0067, 0.0014, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0069, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0069, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57206.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0576, 0.0126, 0.0241,  ..., 0.0000, 0.0380, 0.0000],
        [0.0571, 0.0104, 0.0185,  ..., 0.0000, 0.0351, 0.0000],
        [0.0573, 0.0103, 0.0189,  ..., 0.0000, 0.0354, 0.0000],
        ...,
        [0.0639, 0.0576, 0.0666,  ..., 0.0000, 0.0595, 0.0000],
        [0.0638, 0.0576, 0.0666,  ..., 0.0000, 0.0595, 0.0000],
        [0.0638, 0.0576, 0.0666,  ..., 0.0000, 0.0594, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549041.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5348.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.8591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18999.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-395.0247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(223.7418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0699],
        [ 0.0413],
        [ 0.1020],
        ...,
        [-2.1072],
        [-2.1022],
        [-2.1004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284258.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0127],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368251.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0129],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368260.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5140e-03,  7.3212e-03, -1.4832e-04,  ...,  9.4377e-03,
         -3.5046e-03, -4.4918e-03],
        [ 1.6534e-03,  3.6439e-04,  0.0000e+00,  ...,  9.3541e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.5140e-03,  7.3212e-03, -1.4832e-04,  ...,  9.4377e-03,
         -3.5046e-03, -4.4918e-03],
        ...,
        [ 1.6534e-03,  3.6439e-04,  0.0000e+00,  ...,  9.3541e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6534e-03,  3.6439e-04,  0.0000e+00,  ...,  9.3541e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6534e-03,  3.6439e-04,  0.0000e+00,  ...,  9.3541e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2055.7603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.8871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.4290, device='cuda:0')



h[100].sum tensor(62.9939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0794, device='cuda:0')



h[200].sum tensor(-2.5394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0122, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0417, 0.0370, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        [0.0173, 0.0122, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0068, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0068, 0.0015, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56662.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0553, 0.0067, 0.0153,  ..., 0.0000, 0.0263, 0.0000],
        [0.0516, 0.0000, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0553, 0.0055, 0.0134,  ..., 0.0000, 0.0249, 0.0000],
        ...,
        [0.0643, 0.0575, 0.0665,  ..., 0.0000, 0.0597, 0.0000],
        [0.0642, 0.0574, 0.0665,  ..., 0.0000, 0.0597, 0.0000],
        [0.0642, 0.0574, 0.0664,  ..., 0.0000, 0.0596, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545518.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5402.1284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.3716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19205.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.8707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(180.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.5631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3494],
        [-0.1404],
        [-0.1694],
        ...,
        [-2.1100],
        [-2.1049],
        [-2.0992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251267.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0129],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368260.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0130],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368268.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6932e-02,  1.5841e-02, -3.2656e-04,  ...,  2.0882e-02,
         -7.7932e-03, -9.9915e-03],
        [ 8.6706e-03,  7.4663e-03, -1.5007e-04,  ...,  9.6342e-03,
         -3.5812e-03, -4.5914e-03],
        [ 1.6464e-03,  3.4589e-04,  0.0000e+00,  ...,  7.0616e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6464e-03,  3.4589e-04,  0.0000e+00,  ...,  7.0616e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6464e-03,  3.4589e-04,  0.0000e+00,  ...,  7.0616e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6464e-03,  3.4589e-04,  0.0000e+00,  ...,  7.0616e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1902.8435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.1905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.5024, device='cuda:0')



h[100].sum tensor(61.2446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6201, device='cuda:0')



h[200].sum tensor(-2.1234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.9218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0212, 0.0000,  ..., 0.0269, 0.0000, 0.0000],
        [0.0278, 0.0228, 0.0000,  ..., 0.0291, 0.0000, 0.0000],
        [0.0415, 0.0367, 0.0000,  ..., 0.0477, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52037.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0504, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0508, 0.0000, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0488, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0648, 0.0579, 0.0668,  ..., 0.0000, 0.0605, 0.0000],
        [0.0648, 0.0579, 0.0668,  ..., 0.0000, 0.0605, 0.0000],
        [0.0648, 0.0579, 0.0668,  ..., 0.0000, 0.0605, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523196.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5554.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.2943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18560.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-388.0637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(72.0184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.9276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2166],
        [ 0.2209],
        [ 0.2224],
        ...,
        [-2.1434],
        [-2.1384],
        [-2.1364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284545.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0130],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368268.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0131],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368276.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1545e-02,  1.0369e-02, -2.0913e-04,  ...,  1.3532e-02,
         -5.0406e-03, -6.4645e-03],
        [ 1.5021e-02,  1.3892e-02, -2.8250e-04,  ...,  1.8263e-02,
         -6.8090e-03, -8.7325e-03],
        [ 1.4789e-02,  1.3657e-02, -2.7762e-04,  ...,  1.7948e-02,
         -6.6913e-03, -8.5815e-03],
        ...,
        [ 1.6398e-03,  3.3060e-04,  0.0000e+00,  ...,  5.0110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6398e-03,  3.3060e-04,  0.0000e+00,  ...,  5.0110e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6398e-03,  3.3060e-04,  0.0000e+00,  ...,  5.0110e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2531.4597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.9865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.5138, device='cuda:0')



h[100].sum tensor(67.1678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9426, device='cuda:0')



h[200].sum tensor(-3.8514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.7704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0281, 0.0232, 0.0000,  ..., 0.0295, 0.0000, 0.0000],
        [0.0408, 0.0360, 0.0000,  ..., 0.0467, 0.0000, 0.0000],
        [0.0815, 0.0773, 0.0000,  ..., 0.1022, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70125.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0488, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0439, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0364, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0654, 0.0583, 0.0672,  ..., 0.0000, 0.0613, 0.0000],
        [0.0654, 0.0583, 0.0672,  ..., 0.0000, 0.0613, 0.0000],
        [0.0654, 0.0583, 0.0671,  ..., 0.0000, 0.0613, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630161.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5518.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.1191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20843.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-425.2737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(674.0657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-511.5802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1587],
        [ 0.1688],
        [ 0.1495],
        ...,
        [-2.1692],
        [-2.1639],
        [-2.1617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308433.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0131],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368276.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0078],
        [1.0132],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368285.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6392e-03,  3.2812e-04,  0.0000e+00,  ...,  4.2219e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6392e-03,  3.2812e-04,  0.0000e+00,  ...,  4.2219e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3328e-02,  1.2173e-02, -2.4388e-04,  ...,  1.5948e-02,
         -5.9368e-03, -7.6163e-03],
        ...,
        [ 1.6392e-03,  3.2812e-04,  0.0000e+00,  ...,  4.2219e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6392e-03,  3.2812e-04,  0.0000e+00,  ...,  4.2219e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6392e-03,  3.2812e-04,  0.0000e+00,  ...,  4.2219e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1915.7939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.4343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2692, device='cuda:0')



h[100].sum tensor(60.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9051, device='cuda:0')



h[200].sum tensor(-2.1581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0013, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0184, 0.0133, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0162, 0.0111, 0.0000,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52438.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0621, 0.0382, 0.0496,  ..., 0.0000, 0.0524, 0.0000],
        [0.0590, 0.0191, 0.0304,  ..., 0.0000, 0.0373, 0.0000],
        [0.0552, 0.0077, 0.0155,  ..., 0.0000, 0.0218, 0.0000],
        ...,
        [0.0657, 0.0585, 0.0674,  ..., 0.0000, 0.0617, 0.0000],
        [0.0657, 0.0584, 0.0674,  ..., 0.0000, 0.0617, 0.0000],
        [0.0657, 0.0584, 0.0673,  ..., 0.0000, 0.0616, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530947.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5699.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(429.6216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18589.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-386.0031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(122.7110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.9093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1591],
        [-1.0106],
        [-0.6774],
        ...,
        [-2.1811],
        [-2.1761],
        [-2.1679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324939.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0078],
        [1.0132],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368285.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0079],
        [1.0133],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368293.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9270e-02,  1.8192e-02, -3.6344e-04,  ...,  2.4024e-02,
         -8.9361e-03, -1.1468e-02],
        [ 4.2414e-02,  4.1642e-02, -8.4064e-04,  ...,  5.5513e-02,
         -2.0669e-02, -2.6525e-02],
        [ 1.8297e-02,  1.7206e-02, -3.4338e-04,  ...,  2.2701e-02,
         -8.4428e-03, -1.0835e-02],
        ...,
        [ 1.6430e-03,  3.3290e-04,  0.0000e+00,  ...,  4.2439e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6430e-03,  3.3290e-04,  0.0000e+00,  ...,  4.2439e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6430e-03,  3.3290e-04,  0.0000e+00,  ...,  4.2439e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2118.4424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.1835, device='cuda:0')



h[100].sum tensor(62.8037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7315, device='cuda:0')



h[200].sum tensor(-2.6776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.5504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0928, 0.0886, 0.0000,  ..., 0.1174, 0.0000, 0.0000],
        [0.0746, 0.0702, 0.0000,  ..., 0.0926, 0.0000, 0.0000],
        [0.0760, 0.0716, 0.0000,  ..., 0.0945, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57662.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0371, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0404, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0659, 0.0584, 0.0675,  ..., 0.0000, 0.0618, 0.0000],
        [0.0659, 0.0584, 0.0675,  ..., 0.0000, 0.0618, 0.0000],
        [0.0659, 0.0583, 0.0674,  ..., 0.0000, 0.0618, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553374., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5634.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.6830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19373.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-404.3962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(161.8927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.8508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1497],
        [ 0.1125],
        [-0.0630],
        ...,
        [-2.1932],
        [-2.1879],
        [-2.1858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297216.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0079],
        [1.0133],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368293.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0079],
        [1.0133],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368302.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4502e-02,  1.3357e-02, -2.6187e-04,  ...,  1.7526e-02,
         -6.5035e-03, -8.3487e-03],
        [ 9.0670e-03,  7.8511e-03, -1.5113e-04,  ...,  1.0133e-02,
         -3.7532e-03, -4.8181e-03],
        [ 1.6496e-03,  3.3714e-04,  0.0000e+00,  ...,  4.3131e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6496e-03,  3.3714e-04,  0.0000e+00,  ...,  4.3131e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6496e-03,  3.3714e-04,  0.0000e+00,  ...,  4.3131e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6496e-03,  3.3714e-04,  0.0000e+00,  ...,  4.3131e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1988.0122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.1472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.7048, device='cuda:0')



h[100].sum tensor(61.5586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4386, device='cuda:0')



h[200].sum tensor(-2.2909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.4915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0497, 0.0450, 0.0000,  ..., 0.0587, 0.0000, 0.0000],
        [0.0301, 0.0251, 0.0000,  ..., 0.0321, 0.0000, 0.0000],
        [0.0141, 0.0089, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54277.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0539, 0.0034, 0.0074,  ..., 0.0000, 0.0171, 0.0000],
        [0.0591, 0.0183, 0.0254,  ..., 0.0000, 0.0358, 0.0000],
        ...,
        [0.0661, 0.0583, 0.0677,  ..., 0.0000, 0.0620, 0.0000],
        [0.0661, 0.0583, 0.0677,  ..., 0.0000, 0.0620, 0.0000],
        [0.0661, 0.0583, 0.0676,  ..., 0.0000, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539035.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5706.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(429.4016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18916.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-394.8681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(154.8042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.4306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1082],
        [-0.0330],
        [-0.2921],
        ...,
        [-2.2011],
        [-2.1959],
        [-2.1938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305159.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0079],
        [1.0133],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368302.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0080],
        [1.0134],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368310.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00],
        [1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00],
        [1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00],
        [1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00],
        [1.6611e-03, 3.4968e-04, 0.0000e+00,  ..., 5.2243e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1934.6028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.9809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.7527, device='cuda:0')



h[100].sum tensor(61.1648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7131, device='cuda:0')



h[200].sum tensor(-2.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.2139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0014, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52149.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0641, 0.0565, 0.0652,  ..., 0.0000, 0.0600, 0.0000],
        [0.0643, 0.0566, 0.0654,  ..., 0.0000, 0.0601, 0.0000],
        [0.0645, 0.0568, 0.0658,  ..., 0.0000, 0.0603, 0.0000],
        ...,
        [0.0661, 0.0581, 0.0678,  ..., 0.0000, 0.0619, 0.0000],
        [0.0661, 0.0581, 0.0678,  ..., 0.0000, 0.0619, 0.0000],
        [0.0661, 0.0580, 0.0678,  ..., 0.0000, 0.0618, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524864.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5701.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.6396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18643.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-393.6927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(69.6013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.7889, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2160],
        [-1.6247],
        [-1.9217],
        ...,
        [-2.2030],
        [-2.1978],
        [-2.1957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298815.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0080],
        [1.0134],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368310.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0080],
        [1.0135],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368319.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00],
        [1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00],
        [1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00],
        [1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00],
        [1.6725e-03, 3.6828e-04, 0.0000e+00,  ..., 6.6280e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2111.7517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.4392, device='cuda:0')



h[100].sum tensor(62.9465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4549, device='cuda:0')



h[200].sum tensor(-2.5214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.6820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57866.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0642, 0.0563, 0.0653,  ..., 0.0000, 0.0598, 0.0000],
        [0.0642, 0.0558, 0.0650,  ..., 0.0000, 0.0595, 0.0000],
        [0.0643, 0.0546, 0.0643,  ..., 0.0000, 0.0585, 0.0000],
        ...,
        [0.0657, 0.0541, 0.0649,  ..., 0.0000, 0.0598, 0.0000],
        [0.0648, 0.0467, 0.0589,  ..., 0.0000, 0.0560, 0.0000],
        [0.0643, 0.0429, 0.0558,  ..., 0.0000, 0.0541, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557903.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5601.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(426.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19636.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-413.6075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(219.8921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.4405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0915],
        [-1.9083],
        [-1.6376],
        ...,
        [-1.9602],
        [-1.7972],
        [-1.6903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259738.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0080],
        [1.0135],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368319.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(491.8271, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0080],
        [1.0135],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368328.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6791e-03,  3.8005e-04,  0.0000e+00,  ...,  7.3937e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6791e-03,  3.8005e-04,  0.0000e+00,  ...,  7.3937e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.0879e-02,  9.6965e-03, -1.8083e-04,  ...,  1.2581e-02,
         -4.6284e-03, -5.9473e-03],
        ...,
        [ 1.6791e-03,  3.8005e-04,  0.0000e+00,  ...,  7.3937e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6791e-03,  3.8005e-04,  0.0000e+00,  ...,  7.3937e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6791e-03,  3.8005e-04,  0.0000e+00,  ...,  7.3937e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1816.9714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.3581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1605, device='cuda:0')



h[100].sum tensor(59.8623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3780, device='cuda:0')



h[200].sum tensor(-1.7173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0015, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0160, 0.0109, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0378, 0.0329, 0.0000,  ..., 0.0424, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50127.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0610, 0.0203, 0.0359,  ..., 0.0000, 0.0437, 0.0000],
        [0.0575, 0.0159, 0.0227,  ..., 0.0000, 0.0291, 0.0000],
        [0.0491, 0.0000, 0.0038,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0663, 0.0577, 0.0680,  ..., 0.0000, 0.0617, 0.0000],
        [0.0663, 0.0577, 0.0679,  ..., 0.0000, 0.0617, 0.0000],
        [0.0663, 0.0577, 0.0679,  ..., 0.0000, 0.0616, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521917.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5679.3530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(446.9817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18540.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-394.3638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(80.6477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.8606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2241],
        [-0.1168],
        [ 0.0224],
        ...,
        [-2.2096],
        [-2.2049],
        [-2.2027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300748.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0080],
        [1.0135],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368328.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368336.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9578e-02,  2.8610e-02, -5.4141e-04,  ...,  3.7960e-02,
         -1.3998e-02, -1.7993e-02],
        [ 2.7195e-02,  2.6197e-02, -4.9514e-04,  ...,  3.4722e-02,
         -1.2802e-02, -1.6455e-02],
        [ 2.5607e-02,  2.4589e-02, -4.6428e-04,  ...,  3.2563e-02,
         -1.2004e-02, -1.5430e-02],
        ...,
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2203.3198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.2580, device='cuda:0')



h[100].sum tensor(63.5728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1309, device='cuda:0')



h[200].sum tensor(-2.7031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.8041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0652, 0.0606, 0.0000,  ..., 0.0796, 0.0000, 0.0000],
        [0.0896, 0.0853, 0.0000,  ..., 0.1128, 0.0000, 0.0000],
        [0.0967, 0.0925, 0.0000,  ..., 0.1224, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59306.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0314, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0304, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000],
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000],
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568284., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5579.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.6576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19793.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.6937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(253.5818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-455.2964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1005],
        [ 0.1019],
        [ 0.1049],
        ...,
        [-2.2228],
        [-2.2177],
        [-2.2155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282132.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368336.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368336.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5688e-02,  1.4545e-02, -2.7166e-04,  ...,  1.9081e-02,
         -7.0237e-03, -9.0281e-03],
        [ 7.7977e-03,  6.5553e-03, -1.1843e-04,  ...,  8.3567e-03,
         -3.0619e-03, -3.9357e-03],
        [ 7.6153e-03,  6.3706e-03, -1.1488e-04,  ...,  8.1088e-03,
         -2.9703e-03, -3.8180e-03],
        ...,
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6996e-03,  3.8036e-04,  0.0000e+00,  ...,  6.8238e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1995.7618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.1276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.7918, device='cuda:0')



h[100].sum tensor(61.5328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0993, device='cuda:0')



h[200].sum tensor(-2.1629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.4263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0343, 0.0294, 0.0000,  ..., 0.0377, 0.0000, 0.0000],
        [0.0448, 0.0400, 0.0000,  ..., 0.0519, 0.0000, 0.0000],
        [0.0382, 0.0332, 0.0000,  ..., 0.0428, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54139.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0480, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0473, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0492, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000],
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000],
        [0.0665, 0.0579, 0.0682,  ..., 0.0000, 0.0618, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539431.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5652.6196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.9212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19052.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-404.6725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(141.9237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.5092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2068],
        [ 0.2081],
        [ 0.2100],
        ...,
        [-2.2228],
        [-2.2177],
        [-2.2155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290655.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0081],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368336.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0081],
        [1.0136],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368344.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1513e-02,  2.0409e-02, -3.7967e-04,  ...,  2.6947e-02,
         -9.9162e-03, -1.2750e-02],
        [ 1.0651e-02,  9.4121e-03, -1.7127e-04,  ...,  1.2187e-02,
         -4.4733e-03, -5.7518e-03],
        [ 1.1573e-02,  1.0345e-02, -1.8894e-04,  ...,  1.3439e-02,
         -4.9349e-03, -6.3452e-03],
        ...,
        [ 1.7243e-03,  3.7418e-04,  0.0000e+00,  ...,  5.7247e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7243e-03,  3.7418e-04,  0.0000e+00,  ...,  5.7247e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7243e-03,  3.7418e-04,  0.0000e+00,  ...,  5.7247e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2047.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.0920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0414, device='cuda:0')



h[100].sum tensor(62.0258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5637, device='cuda:0')



h[200].sum tensor(-2.2765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0557, 0.0509, 0.0000,  ..., 0.0665, 0.0000, 0.0000],
        [0.0603, 0.0555, 0.0000,  ..., 0.0727, 0.0000, 0.0000],
        [0.0386, 0.0335, 0.0000,  ..., 0.0432, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55097.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0484, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0486, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0515, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        ...,
        [0.0667, 0.0582, 0.0686,  ..., 0.0000, 0.0621, 0.0000],
        [0.0667, 0.0582, 0.0685,  ..., 0.0000, 0.0621, 0.0000],
        [0.0666, 0.0582, 0.0685,  ..., 0.0000, 0.0620, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542571.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5640.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.5569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19277.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.3757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(137.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-430.8792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1825],
        [ 0.0896],
        [-0.0731],
        ...,
        [-2.2380],
        [-2.2329],
        [-2.2310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277704., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0081],
        [1.0136],
        ...,
        [1.0011],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368344.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368353.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2151e-02,  1.0921e-02, -1.9748e-04,  ...,  1.4205e-02,
         -5.2105e-03, -6.7018e-03],
        [ 8.4828e-03,  7.2077e-03, -1.2796e-04,  ...,  9.2223e-03,
         -3.3761e-03, -4.3424e-03],
        [ 6.9696e-03,  5.6758e-03, -9.9275e-05,  ...,  7.1666e-03,
         -2.6193e-03, -3.3690e-03],
        ...,
        [ 1.7322e-03,  3.7370e-04,  0.0000e+00,  ...,  5.1329e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7322e-03,  3.7370e-04,  0.0000e+00,  ...,  5.1329e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7322e-03,  3.7370e-04,  0.0000e+00,  ...,  5.1329e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2357.0698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.5557, device='cuda:0')



h[100].sum tensor(64.7219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7282, device='cuda:0')



h[200].sum tensor(-3.0646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.8186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0427, 0.0377, 0.0000,  ..., 0.0487, 0.0000, 0.0000],
        [0.0375, 0.0324, 0.0000,  ..., 0.0416, 0.0000, 0.0000],
        [0.0609, 0.0561, 0.0000,  ..., 0.0735, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62923.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0456, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0440, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0671, 0.0585, 0.0689,  ..., 0.0000, 0.0624, 0.0000],
        [0.0671, 0.0585, 0.0689,  ..., 0.0000, 0.0624, 0.0000],
        [0.0670, 0.0584, 0.0688,  ..., 0.0000, 0.0623, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585230.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5620.4019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.5704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20066.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-422.3267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(373.4470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-474.1403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0615],
        [ 0.0901],
        [ 0.1072],
        ...,
        [-2.2557],
        [-2.2504],
        [-2.2482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309289.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0082],
        [1.0136],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368353.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0137],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368361.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7389e-03, 3.8425e-04, 0.0000e+00,  ..., 5.4210e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1885.6169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.7875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7073, device='cuda:0')



h[100].sum tensor(59.8268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9529, device='cuda:0')



h[200].sum tensor(-1.8349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.8273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0439, 0.0389, 0.0000,  ..., 0.0504, 0.0000, 0.0000],
        [0.0070, 0.0015, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51126.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0002, 0.0042,  ..., 0.0000, 0.0137, 0.0000],
        [0.0604, 0.0217, 0.0278,  ..., 0.0000, 0.0356, 0.0000],
        [0.0636, 0.0326, 0.0470,  ..., 0.0000, 0.0504, 0.0000],
        ...,
        [0.0674, 0.0586, 0.0690,  ..., 0.0000, 0.0624, 0.0000],
        [0.0674, 0.0586, 0.0690,  ..., 0.0000, 0.0624, 0.0000],
        [0.0674, 0.0585, 0.0690,  ..., 0.0000, 0.0624, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524267.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5788.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.5999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18714.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-395.8369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(54.8749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.1036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0246],
        [-0.2798],
        [-0.7224],
        ...,
        [-2.2630],
        [-2.2564],
        [-2.2519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318054.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0137],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368361.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0138],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368369.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00],
        [1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00],
        [1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00],
        [1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00],
        [1.7501e-03, 4.0295e-04, 0.0000e+00,  ..., 6.6498e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2349.6958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.5206, device='cuda:0')



h[100].sum tensor(64.2501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3435, device='cuda:0')



h[200].sum tensor(-2.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.6109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0016, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0258, 0.0206, 0.0000,  ..., 0.0257, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63882.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0383, 0.0514,  ..., 0.0000, 0.0526, 0.0000],
        [0.0626, 0.0275, 0.0378,  ..., 0.0000, 0.0452, 0.0000],
        [0.0570, 0.0080, 0.0160,  ..., 0.0000, 0.0210, 0.0000],
        ...,
        [0.0676, 0.0584, 0.0691,  ..., 0.0000, 0.0623, 0.0000],
        [0.0676, 0.0584, 0.0690,  ..., 0.0000, 0.0623, 0.0000],
        [0.0676, 0.0584, 0.0690,  ..., 0.0000, 0.0622, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592817.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5626.8389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.0986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20765.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-431.1870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(285.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-474.2817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1245],
        [-0.8951],
        [-0.4682],
        ...,
        [-2.2669],
        [-2.2620],
        [-2.2600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243342.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0083],
        [1.0138],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368369.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0139],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368378., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7388e-03,  6.4464e-03, -1.0895e-04,  ...,  8.1628e-03,
         -2.9639e-03, -3.8159e-03],
        [ 8.0180e-03,  6.7289e-03, -1.1405e-04,  ...,  8.5419e-03,
         -3.1027e-03, -3.9946e-03],
        [ 1.3980e-02,  1.2763e-02, -2.2300e-04,  ...,  1.6636e-02,
         -6.0666e-03, -7.8105e-03],
        ...,
        [ 1.7773e-03,  4.1282e-04,  0.0000e+00,  ...,  6.8828e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7773e-03,  4.1282e-04,  0.0000e+00,  ...,  6.8828e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7773e-03,  4.1282e-04,  0.0000e+00,  ...,  6.8828e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2487.7329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.3999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.6792, device='cuda:0')



h[100].sum tensor(65.7321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5174, device='cuda:0')



h[200].sum tensor(-3.2620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.2962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0275, 0.0222, 0.0000,  ..., 0.0279, 0.0000, 0.0000],
        [0.0485, 0.0435, 0.0000,  ..., 0.0564, 0.0000, 0.0000],
        [0.0407, 0.0356, 0.0000,  ..., 0.0458, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67692.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0517, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0505, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0502, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0677, 0.0585, 0.0692,  ..., 0.0000, 0.0622, 0.0000],
        [0.0677, 0.0584, 0.0692,  ..., 0.0000, 0.0622, 0.0000],
        [0.0676, 0.0584, 0.0692,  ..., 0.0000, 0.0622, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623254.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5585.5210, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(385.0589, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21190.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-437.0970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(527.5351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.8356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2253],
        [ 0.2403],
        [ 0.2434],
        ...,
        [-2.2659],
        [-2.2595],
        [-2.2562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277078.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0139],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368378., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0139],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368386.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8158e-03,  4.2982e-04,  0.0000e+00,  ...,  7.7445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3535e-02,  1.2288e-02, -2.1153e-04,  ...,  1.5983e-02,
         -5.8141e-03, -7.4878e-03],
        [ 2.7133e-02,  2.6047e-02, -4.5700e-04,  ...,  3.4440e-02,
         -1.2561e-02, -1.6177e-02],
        ...,
        [ 1.8158e-03,  4.2982e-04,  0.0000e+00,  ...,  7.7445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8158e-03,  4.2982e-04,  0.0000e+00,  ...,  7.7445e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8158e-03,  4.2982e-04,  0.0000e+00,  ...,  7.7445e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2190.7708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.3250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.5064, device='cuda:0')



h[100].sum tensor(63.3059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4799, device='cuda:0')



h[200].sum tensor(-2.4458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.7604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0370, 0.0317, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        [0.0591, 0.0541, 0.0000,  ..., 0.0706, 0.0000, 0.0000],
        [0.0484, 0.0433, 0.0000,  ..., 0.0560, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57877.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0495, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0454, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0467, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0675, 0.0583, 0.0693,  ..., 0.0000, 0.0619, 0.0000],
        [0.0674, 0.0583, 0.0693,  ..., 0.0000, 0.0619, 0.0000],
        [0.0674, 0.0583, 0.0693,  ..., 0.0000, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555192.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5602.5864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.9284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19755.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.8354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(147.4898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.7993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1997],
        [ 0.2022],
        [ 0.2002],
        ...,
        [-2.2627],
        [-2.2579],
        [-2.2559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260361.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0084],
        [1.0139],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368386.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0085],
        [1.0140],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368394.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8341e-03,  4.3941e-04,  0.0000e+00,  ...,  8.2226e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3330e-02,  1.2070e-02, -2.0498e-04,  ...,  1.5681e-02,
         -5.6920e-03, -7.3331e-03],
        [ 1.8341e-03,  4.3941e-04,  0.0000e+00,  ...,  8.2226e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8341e-03,  4.3941e-04,  0.0000e+00,  ...,  8.2226e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8341e-03,  4.3941e-04,  0.0000e+00,  ...,  8.2226e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8341e-03,  4.3941e-04,  0.0000e+00,  ...,  8.2226e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2486.5696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.4538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.8738, device='cuda:0')



h[100].sum tensor(66.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2181, device='cuda:0')



h[200].sum tensor(-3.1328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.3565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0493, 0.0442, 0.0000,  ..., 0.0572, 0.0000, 0.0000],
        [0.0168, 0.0113, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0190, 0.0135, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0076, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64676.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0543, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0586, 0.0000, 0.0113,  ..., 0.0000, 0.0268, 0.0000],
        [0.0570, 0.0000, 0.0043,  ..., 0.0000, 0.0175, 0.0000],
        ...,
        [0.0675, 0.0583, 0.0694,  ..., 0.0000, 0.0619, 0.0000],
        [0.0674, 0.0583, 0.0694,  ..., 0.0000, 0.0619, 0.0000],
        [0.0674, 0.0582, 0.0694,  ..., 0.0000, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590794., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5502.8569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.7932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20525.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-434.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(358.0836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-476.4267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1565],
        [ 0.1676],
        [ 0.1762],
        ...,
        [-2.2641],
        [-2.2594],
        [-2.2574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266925.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0085],
        [1.0140],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368394.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(451.4619, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0085],
        [1.0141],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368402.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00],
        [1.8321e-03, 4.4404e-04, 0.0000e+00,  ..., 8.5409e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2277.7527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.3612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.1282, device='cuda:0')



h[100].sum tensor(64.3341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0827, device='cuda:0')



h[200].sum tensor(-2.5790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.6527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0018, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0076, 0.0018, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0076, 0.0018, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60341.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0655, 0.0566, 0.0668,  ..., 0.0000, 0.0602, 0.0000],
        [0.0657, 0.0567, 0.0670,  ..., 0.0000, 0.0604, 0.0000],
        [0.0659, 0.0569, 0.0674,  ..., 0.0000, 0.0606, 0.0000],
        ...,
        [0.0676, 0.0582, 0.0695,  ..., 0.0000, 0.0621, 0.0000],
        [0.0674, 0.0565, 0.0682,  ..., 0.0000, 0.0613, 0.0000],
        [0.0667, 0.0511, 0.0638,  ..., 0.0000, 0.0585, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570400.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5520.9067, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.2203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20116.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-426.9162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.6749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.8799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0348],
        [-2.2034],
        [-2.3361],
        ...,
        [-2.2374],
        [-2.1703],
        [-2.0495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261336.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0085],
        [1.0141],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368402.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0085],
        [1.0141],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368410.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2518e-02,  2.1385e-02, -3.6032e-04,  ...,  2.8171e-02,
         -1.0214e-02, -1.3168e-02],
        [ 1.4204e-02,  1.2973e-02, -2.1559e-04,  ...,  1.6889e-02,
         -6.1116e-03, -7.8788e-03],
        [ 7.9020e-03,  6.5968e-03, -1.0590e-04,  ...,  8.3379e-03,
         -3.0020e-03, -3.8701e-03],
        ...,
        [ 1.8181e-03,  4.4107e-04,  0.0000e+00,  ...,  8.2347e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8181e-03,  4.4107e-04,  0.0000e+00,  ...,  8.2347e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8181e-03,  4.4107e-04,  0.0000e+00,  ...,  8.2347e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2011.3397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.7207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1218, device='cuda:0')



h[100].sum tensor(61.5526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4786, device='cuda:0')



h[200].sum tensor(-1.9097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0672, 0.0624, 0.0000,  ..., 0.0816, 0.0000, 0.0000],
        [0.0656, 0.0608, 0.0000,  ..., 0.0794, 0.0000, 0.0000],
        [0.0589, 0.0539, 0.0000,  ..., 0.0703, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52706.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0389, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0409, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0678, 0.0582, 0.0696,  ..., 0.0000, 0.0626, 0.0000],
        [0.0678, 0.0582, 0.0696,  ..., 0.0000, 0.0626, 0.0000],
        [0.0678, 0.0582, 0.0695,  ..., 0.0000, 0.0625, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529396.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5664.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.0230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19084.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.8350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(105.3626, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.2751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1499],
        [ 0.1723],
        [ 0.1875],
        ...,
        [-2.2876],
        [-2.2827],
        [-2.2807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278775.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0085],
        [1.0141],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368410.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0086],
        [1.0142],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368418.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8494e-02,  1.7335e-02, -2.8716e-04,  ...,  2.2736e-02,
         -8.2251e-03, -1.0607e-02],
        [ 1.9347e-02,  1.8198e-02, -3.0183e-04,  ...,  2.3893e-02,
         -8.6453e-03, -1.1149e-02],
        [ 2.5678e-02,  2.4605e-02, -4.1071e-04,  ...,  3.2485e-02,
         -1.1764e-02, -1.5170e-02],
        ...,
        [ 1.7975e-03,  4.3911e-04,  0.0000e+00,  ...,  7.7685e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7975e-03,  4.3911e-04,  0.0000e+00,  ...,  7.7685e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7975e-03,  4.3911e-04,  0.0000e+00,  ...,  7.7685e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2445.5498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.2335, device='cuda:0')



h[100].sum tensor(65.2691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6085, device='cuda:0')



h[200].sum tensor(-2.9600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.4427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0997, 0.0953, 0.0000,  ..., 0.1258, 0.0000, 0.0000],
        [0.0762, 0.0716, 0.0000,  ..., 0.0939, 0.0000, 0.0000],
        [0.0447, 0.0396, 0.0000,  ..., 0.0511, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0075, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0152, 0.0096, 0.0000,  ..., 0.0108, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64380.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0371, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0467, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0681, 0.0561, 0.0679,  ..., 0.0000, 0.0621, 0.0000],
        [0.0670, 0.0440, 0.0582,  ..., 0.0000, 0.0569, 0.0000],
        [0.0633, 0.0216, 0.0316,  ..., 0.0000, 0.0394, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592164.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5606.9771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20877.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-439.7577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(346.5677, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.6985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1363],
        [ 0.1662],
        [ 0.1952],
        ...,
        [-2.0257],
        [-1.6158],
        [-1.0128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242192.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0086],
        [1.0142],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368418.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0143],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368426.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9403e-02,  1.8268e-02, -2.9955e-04,  ...,  2.3986e-02,
         -8.6695e-03, -1.1184e-02],
        [ 1.2010e-02,  1.0785e-02, -1.7393e-04,  ...,  1.3954e-02,
         -5.0339e-03, -6.4937e-03],
        [ 7.8612e-03,  6.5863e-03, -1.0343e-04,  ...,  8.3231e-03,
         -2.9935e-03, -3.8616e-03],
        ...,
        [ 1.7744e-03,  4.2555e-04,  0.0000e+00,  ...,  6.2190e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7744e-03,  4.2555e-04,  0.0000e+00,  ...,  6.2190e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7744e-03,  4.2555e-04,  0.0000e+00,  ...,  6.2190e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2104.4177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.7156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8840, device='cuda:0')



h[100].sum tensor(61.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5052, device='cuda:0')



h[200].sum tensor(-2.1520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.7006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0589, 0.0541, 0.0000,  ..., 0.0705, 0.0000, 0.0000],
        [0.0494, 0.0445, 0.0000,  ..., 0.0576, 0.0000, 0.0000],
        [0.0317, 0.0266, 0.0000,  ..., 0.0336, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55467.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0505, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0517, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0538, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0691, 0.0587, 0.0699,  ..., 0.0000, 0.0640, 0.0000],
        [0.0690, 0.0587, 0.0699,  ..., 0.0000, 0.0639, 0.0000],
        [0.0690, 0.0587, 0.0698,  ..., 0.0000, 0.0639, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546536.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5838.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(429.5905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19536.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-412.7935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(149.0075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.8287, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2410],
        [ 0.2518],
        [ 0.2363],
        ...,
        [-2.3382],
        [-2.3329],
        [-2.3310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298930.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0143],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368426.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0143],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368433.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7482e-03,  4.1402e-04,  0.0000e+00,  ...,  4.9254e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7482e-03,  4.1402e-04,  0.0000e+00,  ...,  4.9254e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8420e-02,  1.7291e-02, -2.7990e-04,  ...,  2.2677e-02,
         -8.1856e-03, -1.0563e-02],
        ...,
        [ 1.7482e-03,  4.1402e-04,  0.0000e+00,  ...,  4.9254e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7482e-03,  4.1402e-04,  0.0000e+00,  ...,  4.9254e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7482e-03,  4.1402e-04,  0.0000e+00,  ...,  4.9254e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2179.2468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.7720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.2599, device='cuda:0')



h[100].sum tensor(61.3732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3883, device='cuda:0')



h[200].sum tensor(-2.3449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.4728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0304, 0.0253, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0528, 0.0480, 0.0000,  ..., 0.0623, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56924.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0643, 0.0275, 0.0384,  ..., 0.0000, 0.0468, 0.0000],
        [0.0571, 0.0064, 0.0151,  ..., 0.0000, 0.0219, 0.0000],
        [0.0490, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        ...,
        [0.0698, 0.0590, 0.0701,  ..., 0.0000, 0.0647, 0.0000],
        [0.0697, 0.0590, 0.0701,  ..., 0.0000, 0.0647, 0.0000],
        [0.0697, 0.0589, 0.0700,  ..., 0.0000, 0.0647, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555645.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5951.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.0922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19919.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-416.3541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(133.0009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2731],
        [-0.7689],
        [-0.3075],
        ...,
        [-2.3643],
        [-2.3587],
        [-2.3562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295221.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0143],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368433.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0144],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368442.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5783e-02,  1.4663e-02, -2.3328e-04,  ...,  1.9148e-02,
         -6.8937e-03, -8.8986e-03],
        [ 1.4289e-02,  1.3151e-02, -2.0850e-04,  ...,  1.7120e-02,
         -6.1614e-03, -7.9533e-03],
        [ 1.4449e-02,  1.3312e-02, -2.1115e-04,  ...,  1.7337e-02,
         -6.2396e-03, -8.0543e-03],
        ...,
        [ 1.7194e-03,  4.2428e-04,  0.0000e+00,  ...,  5.7368e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7194e-03,  4.2428e-04,  0.0000e+00,  ...,  5.7368e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7194e-03,  4.2428e-04,  0.0000e+00,  ...,  5.7368e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2354.4395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.6949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.5911, device='cuda:0')



h[100].sum tensor(62.6222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9980, device='cuda:0')



h[200].sum tensor(-2.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.5263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0624, 0.0579, 0.0000,  ..., 0.0756, 0.0000, 0.0000],
        [0.0442, 0.0395, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0456, 0.0409, 0.0000,  ..., 0.0527, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63690.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0472, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0520, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0543, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        ...,
        [0.0701, 0.0588, 0.0702,  ..., 0.0000, 0.0650, 0.0000],
        [0.0701, 0.0588, 0.0702,  ..., 0.0000, 0.0650, 0.0000],
        [0.0701, 0.0588, 0.0701,  ..., 0.0000, 0.0650, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603033.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5941.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.3460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20910.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-431.4934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(456.0904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-482.7164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0623],
        [-0.1513],
        [-0.2698],
        ...,
        [-2.3754],
        [-2.3701],
        [-2.3678]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297113.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0086],
        [1.0144],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368442.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0145],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368450.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0601e-03,  7.8838e-03, -1.2046e-04,  ...,  1.0051e-02,
         -3.5971e-03, -4.6448e-03],
        [ 6.5694e-03,  5.3618e-03, -7.9645e-05,  ...,  6.6701e-03,
         -2.3783e-03, -3.0711e-03],
        [ 1.3921e-02,  1.2806e-02, -2.0010e-04,  ...,  1.6650e-02,
         -5.9754e-03, -7.7158e-03],
        ...,
        [ 1.7088e-03,  4.4005e-04,  0.0000e+00,  ...,  7.1742e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7088e-03,  4.4005e-04,  0.0000e+00,  ...,  7.1742e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7088e-03,  4.4005e-04,  0.0000e+00,  ...,  7.1742e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2001.6771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(75.9792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1331, device='cuda:0')



h[100].sum tensor(59.2471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4828, device='cuda:0')



h[200].sum tensor(-1.8590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.4910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.0223, 0.0000,  ..., 0.0279, 0.0000, 0.0000],
        [0.0585, 0.0541, 0.0000,  ..., 0.0704, 0.0000, 0.0000],
        [0.0373, 0.0326, 0.0000,  ..., 0.0416, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52858.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0566, 0.0023, 0.0063,  ..., 0.0000, 0.0164, 0.0000],
        [0.0513, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0535, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        ...,
        [0.0701, 0.0585, 0.0702,  ..., 0.0000, 0.0649, 0.0000],
        [0.0701, 0.0585, 0.0702,  ..., 0.0000, 0.0649, 0.0000],
        [0.0700, 0.0585, 0.0702,  ..., 0.0000, 0.0649, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539539.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5988.5771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.6399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19300.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.6181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(129.2486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.7228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0359],
        [ 0.1736],
        [ 0.1958],
        ...,
        [-2.3743],
        [-2.3688],
        [-2.3668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330794.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0145],
        ...,
        [1.0010],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368450.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0145],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368458.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6285e-02,  1.5179e-02, -2.3555e-04,  ...,  1.9833e-02,
         -7.1081e-03, -9.1814e-03],
        [ 1.7287e-03,  4.4364e-04,  0.0000e+00,  ...,  7.8033e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7287e-03,  4.4364e-04,  0.0000e+00,  ...,  7.8033e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7287e-03,  4.4364e-04,  0.0000e+00,  ...,  7.8033e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7287e-03,  4.4364e-04,  0.0000e+00,  ...,  7.8033e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7287e-03,  4.4364e-04,  0.0000e+00,  ...,  7.8033e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2540.5820, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.3128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.9769, device='cuda:0')



h[100].sum tensor(65.1552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2564, device='cuda:0')



h[200].sum tensor(-3.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.4769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0189, 0.0139, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0216, 0.0166, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0072, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0072, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66759.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0570, 0.0047, 0.0108,  ..., 0.0000, 0.0196, 0.0000],
        [0.0606, 0.0082, 0.0161,  ..., 0.0000, 0.0299, 0.0000],
        [0.0632, 0.0131, 0.0265,  ..., 0.0000, 0.0415, 0.0000],
        ...,
        [0.0696, 0.0583, 0.0705,  ..., 0.0000, 0.0648, 0.0000],
        [0.0696, 0.0583, 0.0705,  ..., 0.0000, 0.0648, 0.0000],
        [0.0696, 0.0583, 0.0705,  ..., 0.0000, 0.0648, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617319.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5695.0024, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(399.9875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21252.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-444.2025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(516.2653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.4406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1725],
        [ 0.1390],
        [ 0.1077],
        ...,
        [-2.3690],
        [-2.3634],
        [-2.3614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288563.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0145],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368458.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0087],
        [1.0146],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368467.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00],
        [1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00],
        [1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00],
        [1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00],
        [1.7496e-03, 4.4247e-04, 0.0000e+00,  ..., 8.0771e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2266.8267, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.3619, device='cuda:0')



h[100].sum tensor(63.3708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4262, device='cuda:0')



h[200].sum tensor(-2.3403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.5918, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0130, 0.0078, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59270.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0550, 0.0667,  ..., 0.0000, 0.0621, 0.0000],
        [0.0660, 0.0448, 0.0588,  ..., 0.0000, 0.0577, 0.0000],
        [0.0635, 0.0255, 0.0389,  ..., 0.0000, 0.0462, 0.0000],
        ...,
        [0.0693, 0.0581, 0.0708,  ..., 0.0000, 0.0649, 0.0000],
        [0.0692, 0.0581, 0.0708,  ..., 0.0000, 0.0649, 0.0000],
        [0.0692, 0.0581, 0.0707,  ..., 0.0000, 0.0648, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567114.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5608.4448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(423.0514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20221.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-432.6241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(190.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.0187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1320],
        [-1.7503],
        [-1.2127],
        ...,
        [-2.3644],
        [-2.3595],
        [-2.3575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272417.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0087],
        [1.0146],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368467.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0146],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368475.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7679e-03,  4.3211e-04,  0.0000e+00,  ...,  7.5924e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.4033e-02,  1.2839e-02, -1.9356e-04,  ...,  1.6711e-02,
         -5.9652e-03, -7.7102e-03],
        [ 8.9402e-03,  7.6870e-03, -1.1319e-04,  ...,  9.8034e-03,
         -3.4882e-03, -4.5086e-03],
        ...,
        [ 1.7679e-03,  4.3211e-04,  0.0000e+00,  ...,  7.5924e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7679e-03,  4.3211e-04,  0.0000e+00,  ...,  7.5924e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7679e-03,  4.3211e-04,  0.0000e+00,  ...,  7.5924e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2443.0811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.3773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.7775, device='cuda:0')



h[100].sum tensor(65.8615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0673, device='cuda:0')



h[200].sum tensor(-2.6852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.7438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0361, 0.0310, 0.0000,  ..., 0.0396, 0.0000, 0.0000],
        [0.0244, 0.0193, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0527, 0.0478, 0.0000,  ..., 0.0621, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64165.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0000, 0.0009,  ..., 0.0000, 0.0148, 0.0000],
        [0.0541, 0.0000, 0.0044,  ..., 0.0000, 0.0127, 0.0000],
        [0.0466, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0689, 0.0580, 0.0710,  ..., 0.0000, 0.0651, 0.0000],
        [0.0689, 0.0580, 0.0710,  ..., 0.0000, 0.0651, 0.0000],
        [0.0689, 0.0580, 0.0710,  ..., 0.0000, 0.0650, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597386.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5429.3638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.1394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20955.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-446.7271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(357.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-481.5789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2268],
        [ 0.2257],
        [ 0.2226],
        ...,
        [-2.3656],
        [-2.3604],
        [-2.3584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258107.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0146],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368475.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(512.9041, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0147],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368483.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00],
        [1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00],
        [1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00],
        [1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00],
        [1.7681e-03, 4.2445e-04, 0.0000e+00,  ..., 7.2545e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2078.9399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.8690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.9288, device='cuda:0')



h[100].sum tensor(62.7663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4069, device='cuda:0')



h[200].sum tensor(-1.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.2526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53126.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0540, 0.0666,  ..., 0.0000, 0.0616, 0.0000],
        [0.0669, 0.0563, 0.0684,  ..., 0.0000, 0.0633, 0.0000],
        [0.0671, 0.0564, 0.0688,  ..., 0.0000, 0.0635, 0.0000],
        ...,
        [0.0689, 0.0579, 0.0712,  ..., 0.0000, 0.0654, 0.0000],
        [0.0689, 0.0579, 0.0712,  ..., 0.0000, 0.0654, 0.0000],
        [0.0689, 0.0579, 0.0712,  ..., 0.0000, 0.0653, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534609., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5541.2739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.0359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19288.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-420.2917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(70.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-427.0883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3902],
        [-1.3587],
        [-1.2230],
        ...,
        [-2.3722],
        [-2.3672],
        [-2.3649]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291581.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0147],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368483.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0088],
        [1.0148],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368491.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00],
        [1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00],
        [1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00],
        [1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00],
        [1.7531e-03, 4.1565e-04, 0.0000e+00,  ..., 6.6356e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3048.2441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-45.1070, device='cuda:0')



h[100].sum tensor(71.8873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.7648, device='cuda:0')



h[200].sum tensor(-3.9951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-52.6301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0073, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76973.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0671, 0.0563, 0.0686,  ..., 0.0000, 0.0638, 0.0000],
        [0.0673, 0.0564, 0.0689,  ..., 0.0000, 0.0640, 0.0000],
        [0.0675, 0.0566, 0.0693,  ..., 0.0000, 0.0642, 0.0000],
        ...,
        [0.0693, 0.0578, 0.0715,  ..., 0.0000, 0.0659, 0.0000],
        [0.0693, 0.0578, 0.0715,  ..., 0.0000, 0.0659, 0.0000],
        [0.0692, 0.0578, 0.0714,  ..., 0.0000, 0.0659, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669245.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5408.2974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.9952, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22167.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-469.2143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(912.6550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.4490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2992],
        [-2.3580],
        [-2.3510],
        ...,
        [-2.3895],
        [-2.3841],
        [-2.3820]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289705.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0088],
        [1.0148],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368491.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0088],
        [1.0149],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368498.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7306e-03,  4.0525e-04,  0.0000e+00,  ...,  5.7253e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7306e-03,  4.0525e-04,  0.0000e+00,  ...,  5.7253e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.9749e-03,  6.7207e-03, -9.4952e-05,  ...,  8.5246e-03,
         -3.0206e-03, -3.9081e-03],
        ...,
        [ 1.7306e-03,  4.0525e-04,  0.0000e+00,  ...,  5.7253e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7306e-03,  4.0525e-04,  0.0000e+00,  ...,  5.7253e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7306e-03,  4.0525e-04,  0.0000e+00,  ...,  5.7253e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1965.3723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.1329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3538, device='cuda:0')



h[100].sum tensor(61.2082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4498, device='cuda:0')



h[200].sum tensor(-1.5403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.2481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0133, 0.0080, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0266, 0.0214, 0.0000,  ..., 0.0268, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50522.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0339, 0.0514,  ..., 0.0000, 0.0552, 0.0000],
        [0.0607, 0.0167, 0.0255,  ..., 0.0000, 0.0364, 0.0000],
        [0.0546, 0.0018, 0.0066,  ..., 0.0000, 0.0172, 0.0000],
        ...,
        [0.0699, 0.0579, 0.0718,  ..., 0.0000, 0.0666, 0.0000],
        [0.0699, 0.0579, 0.0717,  ..., 0.0000, 0.0666, 0.0000],
        [0.0698, 0.0578, 0.0717,  ..., 0.0000, 0.0666, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526033., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5758.7573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(451.6284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18962.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-408.4788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(72.5368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.2000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3881],
        [-0.1070],
        [ 0.0637],
        ...,
        [-2.4121],
        [-2.4068],
        [-2.4045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318097.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0088],
        [1.0149],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368498.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368506.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8468e-02,  2.7477e-02, -4.0192e-04,  ...,  3.6348e-02,
         -1.2922e-02, -1.6725e-02],
        [ 2.4450e-02,  2.3412e-02, -3.4155e-04,  ...,  3.0898e-02,
         -1.0981e-02, -1.4213e-02],
        [ 2.3484e-02,  2.2435e-02, -3.2705e-04,  ...,  2.9589e-02,
         -1.0515e-02, -1.3609e-02],
        ...,
        [ 1.7103e-03,  4.0833e-04,  0.0000e+00,  ...,  5.8888e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7103e-03,  4.0833e-04,  0.0000e+00,  ...,  5.8888e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7103e-03,  4.0833e-04,  0.0000e+00,  ...,  5.8888e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2200.0737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1351, device='cuda:0')



h[100].sum tensor(63.1317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5985, device='cuda:0')



h[200].sum tensor(-2.0632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0779, 0.0735, 0.0000,  ..., 0.0966, 0.0000, 0.0000],
        [0.1054, 0.1013, 0.0000,  ..., 0.1338, 0.0000, 0.0000],
        [0.0758, 0.0713, 0.0000,  ..., 0.0936, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56353.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0397, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0354, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0401, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0704, 0.0578, 0.0719,  ..., 0.0000, 0.0670, 0.0000],
        [0.0703, 0.0578, 0.0719,  ..., 0.0000, 0.0669, 0.0000],
        [0.0703, 0.0578, 0.0718,  ..., 0.0000, 0.0669, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554012.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5750.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.1510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19756.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-423.1961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(156.6646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.1325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2117],
        [ 0.1976],
        [ 0.1841],
        ...,
        [-2.4264],
        [-2.4210],
        [-2.4188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305917.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368506.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368506.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00],
        [1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00],
        [1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00],
        [1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00],
        [1.7103e-03, 4.0833e-04, 0.0000e+00,  ..., 5.8888e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2514.2986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.7997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.8263, device='cuda:0')



h[100].sum tensor(66.0982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4571, device='cuda:0')



h[200].sum tensor(-2.7677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.9676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0016, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0219, 0.0168, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0017, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65717.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.0393, 0.0559,  ..., 0.0000, 0.0567, 0.0000],
        [0.0632, 0.0207, 0.0332,  ..., 0.0000, 0.0449, 0.0000],
        [0.0577, 0.0079, 0.0138,  ..., 0.0000, 0.0233, 0.0000],
        ...,
        [0.0704, 0.0578, 0.0719,  ..., 0.0000, 0.0670, 0.0000],
        [0.0703, 0.0578, 0.0719,  ..., 0.0000, 0.0669, 0.0000],
        [0.0703, 0.0578, 0.0718,  ..., 0.0000, 0.0669, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607134.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5617.3896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.3630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21361.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-449.5528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(347.5095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-491.0170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7009],
        [-0.2993],
        [-0.0050],
        ...,
        [-2.4248],
        [-2.4192],
        [-2.4165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246451.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368506.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368514.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1387e-02,  2.0356e-02, -2.9235e-04,  ...,  2.6795e-02,
         -9.5002e-03, -1.2300e-02],
        [ 1.2931e-02,  1.1800e-02, -1.6689e-04,  ...,  1.5325e-02,
         -5.4231e-03, -7.0213e-03],
        [ 1.7736e-02,  1.6661e-02, -2.3817e-04,  ...,  2.1842e-02,
         -7.7396e-03, -1.0020e-02],
        ...,
        [ 1.6829e-03,  4.1837e-04,  0.0000e+00,  ...,  6.7477e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.1433e-02,  1.0284e-02, -1.4465e-04,  ...,  1.3292e-02,
         -4.7007e-03, -6.0859e-03],
        [ 1.1433e-02,  1.0284e-02, -1.4465e-04,  ...,  1.3292e-02,
         -4.7007e-03, -6.0859e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2226.8777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.3864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.6561, device='cuda:0')



h[100].sum tensor(62.9659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7922, device='cuda:0')



h[200].sum tensor(-2.1087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.6015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0342, 0.0295, 0.0000,  ..., 0.0375, 0.0000, 0.0000],
        [0.0660, 0.0616, 0.0000,  ..., 0.0806, 0.0000, 0.0000],
        [0.0407, 0.0360, 0.0000,  ..., 0.0462, 0.0000, 0.0000],
        ...,
        [0.0254, 0.0203, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        [0.0254, 0.0203, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        [0.0253, 0.0203, 0.0000,  ..., 0.0252, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56214.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0519, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0489, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0525, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        ...,
        [0.0631, 0.0103, 0.0214,  ..., 0.0000, 0.0362, 0.0000],
        [0.0615, 0.0031, 0.0092,  ..., 0.0000, 0.0296, 0.0000],
        [0.0615, 0.0031, 0.0092,  ..., 0.0000, 0.0296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552207.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5860.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.5699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19641.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-421.6999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(187.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.9464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1503],
        [ 0.1737],
        [ 0.1710],
        ...,
        [-1.3278],
        [-1.0686],
        [-1.0671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311642.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0089],
        [1.0150],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368514.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0090],
        [1.0151],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368521.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00],
        [1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00],
        [1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00],
        [1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00],
        [1.6621e-03, 4.2874e-04, 0.0000e+00,  ..., 7.4845e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2083.6680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.3782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0304, device='cuda:0')



h[100].sum tensor(61.3377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4447, device='cuda:0')



h[200].sum tensor(-1.7712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52962.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0563, 0.0690,  ..., 0.0000, 0.0652, 0.0000],
        [0.0691, 0.0560, 0.0690,  ..., 0.0000, 0.0650, 0.0000],
        [0.0685, 0.0491, 0.0639,  ..., 0.0000, 0.0614, 0.0000],
        ...,
        [0.0712, 0.0578, 0.0719,  ..., 0.0000, 0.0674, 0.0000],
        [0.0712, 0.0578, 0.0719,  ..., 0.0000, 0.0674, 0.0000],
        [0.0712, 0.0577, 0.0719,  ..., 0.0000, 0.0674, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538451.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5949.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.2367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19273.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-414.7197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(123.2579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.1526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2113],
        [-1.9335],
        [-1.5233],
        ...,
        [-2.4461],
        [-2.4405],
        [-2.4382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323617.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0090],
        [1.0151],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368521.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0090],
        [1.0151],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368521.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6621e-03,  4.2874e-04,  0.0000e+00,  ...,  7.4845e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6621e-03,  4.2874e-04,  0.0000e+00,  ...,  7.4845e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6621e-03,  4.2874e-04,  0.0000e+00,  ...,  7.4845e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.5983e-03,  5.4244e-03, -7.2338e-05,  ...,  6.7712e-03,
         -2.3759e-03, -3.0771e-03],
        [ 9.8217e-03,  8.6865e-03, -1.1957e-04,  ...,  1.1144e-02,
         -3.9274e-03, -5.0865e-03],
        [ 1.6621e-03,  4.2874e-04,  0.0000e+00,  ...,  7.4845e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2229.7310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.6414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.7610, device='cuda:0')



h[100].sum tensor(62.7109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8312, device='cuda:0')



h[200].sum tensor(-2.0941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.7239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0531, 0.0485, 0.0000,  ..., 0.0629, 0.0000, 0.0000],
        [0.0231, 0.0182, 0.0000,  ..., 0.0223, 0.0000, 0.0000],
        [0.0154, 0.0103, 0.0000,  ..., 0.0118, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58700.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0303, 0.0465,  ..., 0.0000, 0.0531, 0.0000],
        [0.0685, 0.0511, 0.0651,  ..., 0.0000, 0.0629, 0.0000],
        [0.0694, 0.0565, 0.0697,  ..., 0.0000, 0.0656, 0.0000],
        ...,
        [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0592, 0.0054, 0.0117,  ..., 0.0000, 0.0218, 0.0000],
        [0.0651, 0.0205, 0.0312,  ..., 0.0000, 0.0424, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576830.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5857.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(441.2765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20302.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-429.5894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(292.6142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.1626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7616],
        [-1.3435],
        [-1.8605],
        ...,
        [ 0.0193],
        [-0.3610],
        [-0.9870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296757.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0090],
        [1.0151],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368521.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0090],
        [1.0152],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368529.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00],
        [1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00],
        [1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00],
        [1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00],
        [1.6461e-03, 4.3472e-04, 0.0000e+00,  ..., 7.8208e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2004.5105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(71.9642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.0030, device='cuda:0')



h[100].sum tensor(60.4876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6911, device='cuda:0')



h[200].sum tensor(-1.5781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.0056, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0066, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51471.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0563, 0.0690,  ..., 0.0000, 0.0654, 0.0000],
        [0.0694, 0.0564, 0.0692,  ..., 0.0000, 0.0656, 0.0000],
        [0.0697, 0.0565, 0.0696,  ..., 0.0000, 0.0658, 0.0000],
        ...,
        [0.0695, 0.0397, 0.0577,  ..., 0.0000, 0.0596, 0.0000],
        [0.0708, 0.0517, 0.0671,  ..., 0.0000, 0.0649, 0.0000],
        [0.0714, 0.0578, 0.0718,  ..., 0.0000, 0.0675, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530537.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5992.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.6686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19219.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-413.4698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56.4640, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-426.9289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1039],
        [-1.8753],
        [-1.5069],
        ...,
        [-1.7317],
        [-2.0320],
        [-2.2617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306848.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0090],
        [1.0152],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368529.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0091],
        [1.0152],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368537.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6456e-03,  4.2603e-04,  0.0000e+00,  ...,  7.0430e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6456e-03,  4.2603e-04,  0.0000e+00,  ...,  7.0430e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6456e-03,  4.2603e-04,  0.0000e+00,  ...,  7.0430e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6456e-03,  4.2603e-04,  0.0000e+00,  ...,  7.0430e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6456e-03,  4.2603e-04,  0.0000e+00,  ...,  7.0430e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6097e-02,  1.5051e-02, -2.0654e-04,  ...,  1.9673e-02,
         -6.9307e-03, -8.9820e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2961.0210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.1444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-41.8210, device='cuda:0')



h[100].sum tensor(69.5040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5435, device='cuda:0')



h[200].sum tensor(-3.6450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-48.7959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0066, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0218, 0.0169, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0341, 0.0293, 0.0000,  ..., 0.0373, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76828.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0694, 0.0565, 0.0691,  ..., 0.0000, 0.0657, 0.0000],
        [0.0683, 0.0446, 0.0600,  ..., 0.0000, 0.0606, 0.0000],
        [0.0646, 0.0245, 0.0360,  ..., 0.0000, 0.0444, 0.0000],
        ...,
        [0.0695, 0.0338, 0.0530,  ..., 0.0000, 0.0585, 0.0000],
        [0.0638, 0.0151, 0.0252,  ..., 0.0000, 0.0364, 0.0000],
        [0.0568, 0.0000, 0.0055,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675872.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5670.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(385.9488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22765.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-471.6457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(745.9084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-554.2006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7837],
        [-1.2668],
        [-0.6364],
        ...,
        [-1.7676],
        [-1.1788],
        [-0.5855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270313.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0091],
        [1.0152],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368537.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(401.5698, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0092],
        [1.0153],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368544.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00],
        [1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00],
        [1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00],
        [1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00],
        [1.6336e-03, 4.2208e-04, 0.0000e+00,  ..., 6.6683e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1982.9347, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(70.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2672, device='cuda:0')



h[100].sum tensor(60.2662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4176, device='cuda:0')



h[200].sum tensor(-1.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.1470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0091, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0102, 0.0054, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0392, 0.0347, 0.0000,  ..., 0.0444, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50260.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0641, 0.0175, 0.0374,  ..., 0.0000, 0.0446, 0.0000],
        [0.0623, 0.0128, 0.0228,  ..., 0.0000, 0.0362, 0.0000],
        [0.0544, 0.0000, 0.0034,  ..., 0.0000, 0.0157, 0.0000],
        ...,
        [0.0720, 0.0582, 0.0721,  ..., 0.0000, 0.0682, 0.0000],
        [0.0720, 0.0581, 0.0721,  ..., 0.0000, 0.0682, 0.0000],
        [0.0720, 0.0581, 0.0721,  ..., 0.0000, 0.0681, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527094.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6084.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.7485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18879.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-404.7510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(86.6391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.5275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.8807e-01],
        [-3.8116e-01],
        [-1.9741e-03],
        ...,
        [-2.4828e+00],
        [-2.4769e+00],
        [-2.4746e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350230.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0092],
        [1.0153],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368544.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0093],
        [1.0154],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368552.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00],
        [1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00],
        [1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00],
        [1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00],
        [1.6390e-03, 4.1704e-04, 0.0000e+00,  ..., 6.1022e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2840.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-38.5812, device='cuda:0')



h[100].sum tensor(68.4476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3394, device='cuda:0')



h[200].sum tensor(-3.3261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-45.0158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0217, 0.0000,  ..., 0.0270, 0.0000, 0.0000],
        [0.0271, 0.0224, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0137, 0.0088, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71270.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.9252e-02, 5.6701e-05, 6.6555e-03,  ..., 0.0000e+00, 2.4778e-02,
         0.0000e+00],
        [5.9460e-02, 0.0000e+00, 6.3510e-03,  ..., 0.0000e+00, 2.4855e-02,
         0.0000e+00],
        [6.3434e-02, 1.2772e-02, 2.4276e-02,  ..., 0.0000e+00, 3.9950e-02,
         0.0000e+00],
        ...,
        [7.2093e-02, 5.8396e-02, 7.2327e-02,  ..., 0.0000e+00, 6.8319e-02,
         0.0000e+00],
        [7.2075e-02, 5.8383e-02, 7.2300e-02,  ..., 0.0000e+00, 6.8300e-02,
         0.0000e+00],
        [7.2039e-02, 5.8358e-02, 7.2254e-02,  ..., 0.0000e+00, 6.8263e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640968.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5773.9399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.8298, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22122.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.2603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(514.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-521.7126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6579],
        [-0.7456],
        [-1.0839],
        ...,
        [-2.4937],
        [-2.4880],
        [-2.4854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252543.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0093],
        [1.0154],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368552.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0155],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368559.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9954e-03,  6.8257e-03, -8.7069e-05,  ...,  8.6436e-03,
         -3.0178e-03, -3.9150e-03],
        [ 1.6670e-03,  4.2361e-04,  0.0000e+00,  ...,  6.3499e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6670e-03,  4.2361e-04,  0.0000e+00,  ...,  6.3499e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6670e-03,  4.2361e-04,  0.0000e+00,  ...,  6.3499e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6670e-03,  4.2361e-04,  0.0000e+00,  ...,  6.3499e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6670e-03,  4.2361e-04,  0.0000e+00,  ...,  6.3499e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2796.1167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.5728, device='cuda:0')



h[100].sum tensor(68.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5929, device='cuda:0')



h[200].sum tensor(-3.1686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-42.6724, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.0228, 0.0000,  ..., 0.0285, 0.0000, 0.0000],
        [0.0131, 0.0082, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0067, 0.0017, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73888.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0572, 0.0022, 0.0092,  ..., 0.0000, 0.0208, 0.0000],
        [0.0617, 0.0052, 0.0190,  ..., 0.0000, 0.0347, 0.0000],
        [0.0638, 0.0047, 0.0246,  ..., 0.0000, 0.0420, 0.0000],
        ...,
        [0.0717, 0.0586, 0.0725,  ..., 0.0000, 0.0680, 0.0000],
        [0.0717, 0.0586, 0.0725,  ..., 0.0000, 0.0680, 0.0000],
        [0.0717, 0.0586, 0.0724,  ..., 0.0000, 0.0680, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674344.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5691.5654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(381.7127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22339.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.2404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(880.1595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-537.6609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0606],
        [-0.0902],
        [-0.2317],
        ...,
        [-2.4834],
        [-2.4775],
        [-2.4749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298963.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0155],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368559.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0094],
        [1.0156],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368567.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3332e-02,  1.2227e-02, -1.5830e-04,  ...,  1.5869e-02,
         -5.5466e-03, -7.1979e-03],
        [ 1.6794e-03,  4.3849e-04,  0.0000e+00,  ...,  7.1737e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6794e-03,  4.3849e-04,  0.0000e+00,  ...,  7.1737e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6794e-03,  4.3849e-04,  0.0000e+00,  ...,  7.1737e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6794e-03,  4.3849e-04,  0.0000e+00,  ...,  7.1737e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6794e-03,  4.3849e-04,  0.0000e+00,  ...,  7.1737e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2307.8091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.1437, device='cuda:0')



h[100].sum tensor(64.3323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9734, device='cuda:0')



h[200].sum tensor(-2.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.1704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0605, 0.0561, 0.0000,  ..., 0.0731, 0.0000, 0.0000],
        [0.0265, 0.0218, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59361.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0483, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0548, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        ...,
        [0.0716, 0.0588, 0.0726,  ..., 0.0000, 0.0677, 0.0000],
        [0.0716, 0.0588, 0.0725,  ..., 0.0000, 0.0676, 0.0000],
        [0.0715, 0.0588, 0.0725,  ..., 0.0000, 0.0676, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571673.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5704.1924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.7217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20496.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-435.4722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(164.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.6179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1592],
        [ 0.1603],
        [ 0.1618],
        ...,
        [-2.4910],
        [-2.4854],
        [-2.4828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258482.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0094],
        [1.0156],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368567.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0157],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368574.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00],
        [1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00],
        [1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00],
        [1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00],
        [1.6829e-03, 4.4118e-04, 0.0000e+00,  ..., 6.9113e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2838.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.5097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-37.1851, device='cuda:0')



h[100].sum tensor(69.2750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8205, device='cuda:0')



h[200].sum tensor(-3.1761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-43.3868, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0018, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75499.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0564, 0.0689,  ..., 0.0000, 0.0642, 0.0000],
        [0.0696, 0.0573, 0.0697,  ..., 0.0000, 0.0653, 0.0000],
        [0.0699, 0.0578, 0.0704,  ..., 0.0000, 0.0660, 0.0000],
        ...,
        [0.0718, 0.0591, 0.0727,  ..., 0.0000, 0.0677, 0.0000],
        [0.0717, 0.0591, 0.0727,  ..., 0.0000, 0.0677, 0.0000],
        [0.0717, 0.0591, 0.0726,  ..., 0.0000, 0.0677, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693667.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5582.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.9984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22836.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-465.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(985.0118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-541.3610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3548],
        [-1.7265],
        [-2.0575],
        ...,
        [-2.5015],
        [-2.4959],
        [-2.4934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297031.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0157],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368574.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0157],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368582.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8132e-03,  5.6337e-03, -6.7889e-05,  ...,  7.0172e-03,
         -2.4310e-03, -3.1568e-03],
        [ 1.5919e-02,  1.4843e-02, -1.8847e-04,  ...,  1.9357e-02,
         -6.7486e-03, -8.7636e-03],
        [ 1.6865e-03,  4.4844e-04,  0.0000e+00,  ...,  6.9441e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6865e-03,  4.4844e-04,  0.0000e+00,  ...,  6.9441e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6865e-03,  4.4844e-04,  0.0000e+00,  ...,  6.9441e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6865e-03,  4.4844e-04,  0.0000e+00,  ...,  6.9441e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2324.0356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6477, device='cuda:0')



h[100].sum tensor(64.4684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1607, device='cuda:0')



h[200].sum tensor(-2.0711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.7585, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0831, 0.0790, 0.0000,  ..., 0.1038, 0.0000, 0.0000],
        [0.0279, 0.0231, 0.0000,  ..., 0.0288, 0.0000, 0.0000],
        [0.0212, 0.0163, 0.0000,  ..., 0.0198, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57969.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0447, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0552, 0.0015, 0.0064,  ..., 0.0000, 0.0172, 0.0000],
        [0.0619, 0.0141, 0.0238,  ..., 0.0000, 0.0325, 0.0000],
        ...,
        [0.0720, 0.0594, 0.0728,  ..., 0.0000, 0.0677, 0.0000],
        [0.0719, 0.0593, 0.0728,  ..., 0.0000, 0.0677, 0.0000],
        [0.0719, 0.0593, 0.0727,  ..., 0.0000, 0.0676, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561663.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5781.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.6034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20192.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-430.9479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(134.2316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.9991, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1788],
        [-0.0369],
        [-0.4475],
        ...,
        [-2.4959],
        [-2.5015],
        [-2.5011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280532.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0157],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368582.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0096],
        [1.0158],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368590.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6983e-03,  4.4253e-04,  0.0000e+00,  ...,  5.8415e-05,
          0.0000e+00,  0.0000e+00],
        [ 9.5839e-03,  8.4163e-03, -1.0308e-04,  ...,  1.0742e-02,
         -3.7317e-03, -4.8475e-03],
        [ 1.6983e-03,  4.4253e-04,  0.0000e+00,  ...,  5.8415e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6983e-03,  4.4253e-04,  0.0000e+00,  ...,  5.8415e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6983e-03,  4.4253e-04,  0.0000e+00,  ...,  5.8415e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6983e-03,  4.4253e-04,  0.0000e+00,  ...,  5.8415e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2596.3633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.5859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.6743, device='cuda:0')



h[100].sum tensor(67.0959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7723, device='cuda:0')



h[200].sum tensor(-2.6279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.9570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0356, 0.0309, 0.0000,  ..., 0.0393, 0.0000, 0.0000],
        [0.0133, 0.0083, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0337, 0.0289, 0.0000,  ..., 0.0366, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62961.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0617, 0.0000, 0.0147,  ..., 0.0000, 0.0336, 0.0000],
        [0.0616, 0.0050, 0.0162,  ..., 0.0000, 0.0320, 0.0000],
        [0.0549, 0.0000, 0.0017,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0722, 0.0598, 0.0730,  ..., 0.0000, 0.0680, 0.0000],
        [0.0722, 0.0598, 0.0730,  ..., 0.0000, 0.0680, 0.0000],
        [0.0721, 0.0598, 0.0729,  ..., 0.0000, 0.0679, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582239.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5774.4790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20579.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-439.8856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(262.8351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-469.6290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1466],
        [-0.7174],
        [-0.2431],
        ...,
        [-2.5265],
        [-2.5207],
        [-2.5181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291496.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0096],
        [1.0158],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368590.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0097],
        [1.0159],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368597.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7034e-03,  7.5354e-03, -9.0375e-05,  ...,  9.5470e-03,
         -3.3076e-03, -4.2981e-03],
        [ 1.7005e-03,  4.5491e-04,  0.0000e+00,  ...,  6.1315e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7005e-03,  4.5491e-04,  0.0000e+00,  ...,  6.1315e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7005e-03,  4.5491e-04,  0.0000e+00,  ...,  6.1315e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7005e-03,  4.5491e-04,  0.0000e+00,  ...,  6.1315e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7005e-03,  4.5491e-04,  0.0000e+00,  ...,  6.1315e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2250.0525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.5839, device='cuda:0')



h[100].sum tensor(63.7637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3937, device='cuda:0')



h[200].sum tensor(-1.8890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3505, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0237, 0.0000,  ..., 0.0296, 0.0000, 0.0000],
        [0.0139, 0.0090, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0069, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57397.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0009, 0.0058,  ..., 0.0000, 0.0111, 0.0000],
        [0.0599, 0.0132, 0.0154,  ..., 0.0000, 0.0234, 0.0000],
        [0.0648, 0.0161, 0.0253,  ..., 0.0000, 0.0423, 0.0000],
        ...,
        [0.0725, 0.0600, 0.0730,  ..., 0.0000, 0.0679, 0.0000],
        [0.0724, 0.0600, 0.0730,  ..., 0.0000, 0.0679, 0.0000],
        [0.0724, 0.0600, 0.0730,  ..., 0.0000, 0.0679, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563135.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5858.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.3947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20184.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-429.5189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(183.5852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-438.9475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0755],
        [ 0.0632],
        [ 0.0487],
        ...,
        [-2.5234],
        [-2.5174],
        [-2.5147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292923.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0097],
        [1.0159],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368597.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0160],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368605.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00],
        [1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00],
        [1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00],
        [1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00],
        [1.7024e-03, 4.6114e-04, 0.0000e+00,  ..., 5.9583e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2444.6951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.2235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.1818, device='cuda:0')



h[100].sum tensor(65.3198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1026, device='cuda:0')



h[200].sum tensor(-2.2728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.7152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0143, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0106, 0.0056, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0136, 0.0087, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62893.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0619, 0.0082, 0.0194,  ..., 0.0000, 0.0320, 0.0000],
        [0.0633, 0.0081, 0.0246,  ..., 0.0000, 0.0372, 0.0000],
        [0.0629, 0.0049, 0.0193,  ..., 0.0000, 0.0347, 0.0000],
        ...,
        [0.0728, 0.0603, 0.0731,  ..., 0.0000, 0.0680, 0.0000],
        [0.0727, 0.0603, 0.0731,  ..., 0.0000, 0.0680, 0.0000],
        [0.0727, 0.0603, 0.0730,  ..., 0.0000, 0.0680, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597550.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5813.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(421.3494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21079.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-443.8992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(334.4264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-465.3918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2062],
        [-0.0627],
        [ 0.0265],
        ...,
        [-2.5460],
        [-2.5399],
        [-2.5375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283678.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0160],
        ...,
        [1.0010],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368605.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0098],
        [1.0161],
        ...,
        [1.0010],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368612.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7124e-03,  4.5934e-04,  0.0000e+00,  ...,  5.1203e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7124e-03,  4.5934e-04,  0.0000e+00,  ...,  5.1203e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.0806e-03,  5.8866e-03, -6.7516e-05,  ...,  7.3204e-03,
         -2.5258e-03, -3.2844e-03],
        ...,
        [ 1.7124e-03,  4.5934e-04,  0.0000e+00,  ...,  5.1203e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7124e-03,  4.5934e-04,  0.0000e+00,  ...,  5.1203e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7124e-03,  4.5934e-04,  0.0000e+00,  ...,  5.1203e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2544.3135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.7725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.5465, device='cuda:0')



h[100].sum tensor(65.9953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9815, device='cuda:0')



h[200].sum tensor(-2.4651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.4743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0018, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0215, 0.0166, 0.0000,  ..., 0.0200, 0.0000, 0.0000],
        [0.0264, 0.0216, 0.0000,  ..., 0.0266, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0019, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63868.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0622, 0.0013, 0.0126,  ..., 0.0000, 0.0307, 0.0000],
        [0.0606, 0.0004, 0.0090,  ..., 0.0000, 0.0239, 0.0000],
        [0.0580, 0.0013, 0.0062,  ..., 0.0000, 0.0127, 0.0000],
        ...,
        [0.0732, 0.0608, 0.0735,  ..., 0.0000, 0.0683, 0.0000],
        [0.0731, 0.0608, 0.0734,  ..., 0.0000, 0.0682, 0.0000],
        [0.0731, 0.0608, 0.0734,  ..., 0.0000, 0.0682, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598327.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5876.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.9009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21020.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-442.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(321.4110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-470.1013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2153],
        [ 0.2184],
        [ 0.2221],
        ...,
        [-2.5639],
        [-2.5578],
        [-2.5553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305488.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0098],
        [1.0161],
        ...,
        [1.0010],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368612.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(505.6480, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0098],
        [1.0161],
        ...,
        [1.0010],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368620.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9167e-02,  2.8222e-02, -3.4080e-04,  ...,  3.7219e-02,
         -1.2891e-02, -1.6768e-02],
        [ 2.0200e-02,  1.9156e-02, -2.2946e-04,  ...,  2.5077e-02,
         -8.6793e-03, -1.1290e-02],
        [ 1.1518e-02,  1.0379e-02, -1.2167e-04,  ...,  1.3322e-02,
         -4.6021e-03, -5.9863e-03],
        ...,
        [ 1.7184e-03,  4.7176e-04,  0.0000e+00,  ...,  5.4420e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7184e-03,  4.7176e-04,  0.0000e+00,  ...,  5.4420e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7184e-03,  4.7176e-04,  0.0000e+00,  ...,  5.4420e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2413.2720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.9477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.2520, device='cuda:0')



h[100].sum tensor(64.6760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7570, device='cuda:0')



h[200].sum tensor(-2.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.6303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0906, 0.0865, 0.0000,  ..., 0.1135, 0.0000, 0.0000],
        [0.0894, 0.0852, 0.0000,  ..., 0.1118, 0.0000, 0.0000],
        [0.0476, 0.0430, 0.0000,  ..., 0.0553, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60993.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0394, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0485, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0733, 0.0609, 0.0736,  ..., 0.0000, 0.0682, 0.0000],
        [0.0733, 0.0609, 0.0736,  ..., 0.0000, 0.0681, 0.0000],
        [0.0732, 0.0609, 0.0735,  ..., 0.0000, 0.0681, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584053.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5941.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(426.5855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20761.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-438.5637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(277.1451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.1127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1773],
        [ 0.1765],
        [ 0.1675],
        ...,
        [-2.5698],
        [-2.5637],
        [-2.5611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301154.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0098],
        [1.0161],
        ...,
        [1.0010],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368620.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0099],
        [1.0162],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368628.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00],
        [1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00],
        [1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00],
        [1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00],
        [1.7135e-03, 4.9163e-04, 0.0000e+00,  ..., 6.3879e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2540.4875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.3125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.4514, device='cuda:0')



h[100].sum tensor(65.7158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9461, device='cuda:0')



h[200].sum tensor(-2.3927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.3634, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63302.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0592, 0.0706,  ..., 0.0000, 0.0658, 0.0000],
        [0.0713, 0.0593, 0.0709,  ..., 0.0000, 0.0660, 0.0000],
        [0.0715, 0.0594, 0.0713,  ..., 0.0000, 0.0662, 0.0000],
        ...,
        [0.0735, 0.0608, 0.0736,  ..., 0.0000, 0.0680, 0.0000],
        [0.0734, 0.0608, 0.0736,  ..., 0.0000, 0.0679, 0.0000],
        [0.0734, 0.0607, 0.0735,  ..., 0.0000, 0.0679, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595630.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5930.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.4365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21071.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-445.6524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(358.4560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-468.0680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6606],
        [-2.5582],
        [-2.3717],
        ...,
        [-2.5718],
        [-2.5658],
        [-2.5631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294390.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0099],
        [1.0162],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368628.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0100],
        [1.0163],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368636.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00],
        [1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00],
        [1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00],
        [1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00],
        [1.7077e-03, 5.0754e-04, 0.0000e+00,  ..., 7.2275e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2270.2224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0712, device='cuda:0')



h[100].sum tensor(63.1866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2031, device='cuda:0')



h[200].sum tensor(-1.8177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57737.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0402, 0.0561,  ..., 0.0000, 0.0574, 0.0000],
        [0.0711, 0.0576, 0.0697,  ..., 0.0000, 0.0650, 0.0000],
        [0.0716, 0.0593, 0.0713,  ..., 0.0000, 0.0661, 0.0000],
        ...,
        [0.0735, 0.0606, 0.0737,  ..., 0.0000, 0.0678, 0.0000],
        [0.0735, 0.0606, 0.0737,  ..., 0.0000, 0.0678, 0.0000],
        [0.0734, 0.0606, 0.0736,  ..., 0.0000, 0.0678, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566859., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5966.3413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.9763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20444.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-436.9818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(215.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.2370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3097],
        [-1.7543],
        [-2.0076],
        ...,
        [-2.5710],
        [-2.5648],
        [-2.5619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302874.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0100],
        [1.0163],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368636.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0100],
        [1.0164],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368643.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1184e-02,  1.0105e-02, -1.1331e-04,  ...,  1.2915e-02,
         -4.4304e-03, -5.7689e-03],
        [ 1.2763e-02,  1.1701e-02, -1.3217e-04,  ...,  1.5052e-02,
         -5.1679e-03, -6.7291e-03],
        [ 1.6973e-03,  5.1547e-04,  0.0000e+00,  ...,  7.4823e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6973e-03,  5.1547e-04,  0.0000e+00,  ...,  7.4823e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6973e-03,  5.1547e-04,  0.0000e+00,  ...,  7.4823e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6973e-03,  5.1547e-04,  0.0000e+00,  ...,  7.4823e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2632.6492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.3225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.7557, device='cuda:0')



h[100].sum tensor(66.4195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4309, device='cuda:0')



h[200].sum tensor(-2.5085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.8852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1023, 0.0986, 0.0000,  ..., 0.1296, 0.0000, 0.0000],
        [0.0402, 0.0358, 0.0000,  ..., 0.0454, 0.0000, 0.0000],
        [0.0180, 0.0134, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65495.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0444, 0.0000, 0.0006,  ..., 0.0000, 0.0082, 0.0000],
        [0.0577, 0.0086, 0.0119,  ..., 0.0000, 0.0192, 0.0000],
        ...,
        [0.0736, 0.0606, 0.0738,  ..., 0.0000, 0.0679, 0.0000],
        [0.0736, 0.0606, 0.0737,  ..., 0.0000, 0.0679, 0.0000],
        [0.0736, 0.0605, 0.0737,  ..., 0.0000, 0.0678, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605045.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5874.2939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.2797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21530.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.2948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(392.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-481.4849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0033],
        [ 0.0133],
        [ 0.0327],
        ...,
        [-2.5617],
        [-2.5563],
        [-2.5543]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279152.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0100],
        [1.0164],
        ...,
        [1.0010],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368643.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0101],
        [1.0165],
        ...,
        [1.0010],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368651.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00],
        [1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00],
        [1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00],
        [1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00],
        [1.6872e-03, 5.1451e-04, 0.0000e+00,  ..., 7.2249e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2409.9229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.9994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.2195, device='cuda:0')



h[100].sum tensor(64.4350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3733, device='cuda:0')



h[200].sum tensor(-2.0454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.4257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0068, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0166, 0.0119, 0.0000,  ..., 0.0134, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61799.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0633, 0.0131, 0.0206,  ..., 0.0000, 0.0314, 0.0000],
        [0.0604, 0.0079, 0.0113,  ..., 0.0000, 0.0209, 0.0000],
        [0.0565, 0.0020, 0.0068,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0737, 0.0605, 0.0739,  ..., 0.0000, 0.0682, 0.0000],
        [0.0736, 0.0605, 0.0738,  ..., 0.0000, 0.0681, 0.0000],
        [0.0736, 0.0605, 0.0738,  ..., 0.0000, 0.0681, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588682., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5868.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.8472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21285.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-455.4288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(287.0718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.4987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0394],
        [ 0.0579],
        [ 0.0669],
        ...,
        [-2.5886],
        [-2.5825],
        [-2.5795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269832.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0101],
        [1.0165],
        ...,
        [1.0010],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368651.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0101],
        [1.0166],
        ...,
        [1.0010],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368659.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6833e-03,  5.0722e-04,  0.0000e+00,  ...,  6.3877e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6833e-03,  5.0722e-04,  0.0000e+00,  ...,  6.3877e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.0639e-02,  1.9665e-02, -2.2062e-04,  ...,  2.5715e-02,
         -8.8205e-03, -1.1493e-02],
        ...,
        [ 1.6833e-03,  5.0722e-04,  0.0000e+00,  ...,  6.3877e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6833e-03,  5.0722e-04,  0.0000e+00,  ...,  6.3877e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6833e-03,  5.0722e-04,  0.0000e+00,  ...,  6.3877e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2279.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.9370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1619, device='cuda:0')



h[100].sum tensor(63.2213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2369, device='cuda:0')



h[200].sum tensor(-1.7715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0259, 0.0214, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0225, 0.0179, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55809.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0347, 0.0521,  ..., 0.0000, 0.0568, 0.0000],
        [0.0641, 0.0124, 0.0268,  ..., 0.0000, 0.0361, 0.0000],
        [0.0613, 0.0023, 0.0115,  ..., 0.0000, 0.0239, 0.0000],
        ...,
        [0.0738, 0.0607, 0.0740,  ..., 0.0000, 0.0686, 0.0000],
        [0.0738, 0.0607, 0.0740,  ..., 0.0000, 0.0686, 0.0000],
        [0.0737, 0.0606, 0.0739,  ..., 0.0000, 0.0685, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550097.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5992.3706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.4971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20255.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-441.4610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(134.2927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.3207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0741],
        [-1.6018],
        [-1.1356],
        ...,
        [-2.6028],
        [-2.5968],
        [-2.5943]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296819.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0101],
        [1.0166],
        ...,
        [1.0010],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368659.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0102],
        [1.0167],
        ...,
        [1.0009],
        [0.9999],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368666.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7439e-02,  1.6429e-02, -1.8100e-04,  ...,  2.1377e-02,
         -7.3177e-03, -9.5381e-03],
        [ 1.4053e-02,  1.3007e-02, -1.4211e-04,  ...,  1.6796e-02,
         -5.7452e-03, -7.4885e-03],
        [ 3.5888e-02,  3.5072e-02, -3.9294e-04,  ...,  4.6339e-02,
         -1.5886e-02, -2.0707e-02],
        ...,
        [ 1.6827e-03,  5.0643e-04,  0.0000e+00,  ...,  5.9351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6827e-03,  5.0643e-04,  0.0000e+00,  ...,  5.9351e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6827e-03,  5.0643e-04,  0.0000e+00,  ...,  5.9351e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2535.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.2608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7841, device='cuda:0')



h[100].sum tensor(65.4686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3264, device='cuda:0')



h[200].sum tensor(-2.2387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.4180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0423, 0.0379, 0.0000,  ..., 0.0483, 0.0000, 0.0000],
        [0.1029, 0.0992, 0.0000,  ..., 0.1303, 0.0000, 0.0000],
        [0.0972, 0.0934, 0.0000,  ..., 0.1226, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62454.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0510, 0.0000, 0.0017,  ..., 0.0000, 0.0125, 0.0000],
        [0.0366, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0310, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0739, 0.0609, 0.0742,  ..., 0.0000, 0.0689, 0.0000],
        [0.0739, 0.0609, 0.0741,  ..., 0.0000, 0.0688, 0.0000],
        [0.0738, 0.0609, 0.0741,  ..., 0.0000, 0.0688, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591482.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5909.4819, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.5869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21077.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-455.3003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.1609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-472.8300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1040],
        [ 0.1232],
        [ 0.1457],
        ...,
        [-2.6149],
        [-2.6087],
        [-2.6060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313308., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0102],
        [1.0167],
        ...,
        [1.0009],
        [0.9999],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368666.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0103],
        [1.0168],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368674.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1014e-02,  9.9321e-03, -1.0572e-04,  ...,  1.2673e-02,
         -4.3223e-03, -5.6358e-03],
        [ 1.6903e-03,  5.1018e-04,  0.0000e+00,  ...,  5.8752e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.5733e-03,  5.4445e-03, -5.5368e-05,  ...,  6.6648e-03,
         -2.2636e-03, -2.9515e-03],
        ...,
        [ 1.7902e-02,  1.6893e-02, -1.8382e-04,  ...,  2.1991e-02,
         -7.5154e-03, -9.7992e-03],
        [ 1.6903e-03,  5.1018e-04,  0.0000e+00,  ...,  5.8752e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6903e-03,  5.1018e-04,  0.0000e+00,  ...,  5.8752e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2884.0581, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.7952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.7028, device='cuda:0')



h[100].sum tensor(68.7013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2696, device='cuda:0')



h[200].sum tensor(-2.8725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.6574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0409, 0.0365, 0.0000,  ..., 0.0464, 0.0000, 0.0000],
        [0.0328, 0.0284, 0.0000,  ..., 0.0354, 0.0000, 0.0000],
        [0.0207, 0.0160, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        ...,
        [0.0208, 0.0160, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0239, 0.0192, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69667.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0495, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0542, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0587, 0.0000, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        ...,
        [0.0654, 0.0041, 0.0154,  ..., 0.0000, 0.0326, 0.0000],
        [0.0673, 0.0151, 0.0317,  ..., 0.0000, 0.0407, 0.0000],
        [0.0718, 0.0390, 0.0571,  ..., 0.0000, 0.0600, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628856.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5814.8726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.8951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21987.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-473.9633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(627.9667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-507.9669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1827],
        [ 0.2078],
        [ 0.2249],
        ...,
        [-1.2496],
        [-1.6017],
        [-2.0292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298153.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0103],
        [1.0168],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368674.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0104],
        [1.0169],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368681.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00],
        [1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00],
        [1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00],
        [1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00],
        [1.6963e-03, 5.0726e-04, 0.0000e+00,  ..., 5.2115e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2571.4961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.1742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.5791, device='cuda:0')



h[100].sum tensor(65.7712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6219, device='cuda:0')



h[200].sum tensor(-2.2544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.3456, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65086.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0716, 0.0597, 0.0714,  ..., 0.0000, 0.0669, 0.0000],
        [0.0718, 0.0599, 0.0716,  ..., 0.0000, 0.0671, 0.0000],
        [0.0721, 0.0600, 0.0720,  ..., 0.0000, 0.0673, 0.0000],
        ...,
        [0.0740, 0.0613, 0.0744,  ..., 0.0000, 0.0691, 0.0000],
        [0.0740, 0.0613, 0.0744,  ..., 0.0000, 0.0691, 0.0000],
        [0.0740, 0.0613, 0.0743,  ..., 0.0000, 0.0691, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614215.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5809.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.5928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21757.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-465.1083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(459.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-481.5906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7863],
        [-2.7993],
        [-2.7933],
        ...,
        [-2.6329],
        [-2.6269],
        [-2.6243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299905.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0104],
        [1.0169],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368681.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0170],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368688.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00],
        [1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00],
        [1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00],
        [1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00],
        [1.7098e-03, 4.9947e-04, 0.0000e+00,  ..., 4.0714e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2884.7534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.4140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.8373, device='cuda:0')



h[100].sum tensor(68.3038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3195, device='cuda:0')



h[200].sum tensor(-2.8379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.8143, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71183.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0719, 0.0602, 0.0717,  ..., 0.0000, 0.0672, 0.0000],
        [0.0720, 0.0601, 0.0717,  ..., 0.0000, 0.0672, 0.0000],
        [0.0722, 0.0596, 0.0716,  ..., 0.0000, 0.0667, 0.0000],
        ...,
        [0.0743, 0.0619, 0.0747,  ..., 0.0000, 0.0695, 0.0000],
        [0.0742, 0.0618, 0.0747,  ..., 0.0000, 0.0694, 0.0000],
        [0.0742, 0.0618, 0.0746,  ..., 0.0000, 0.0694, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642393.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5824.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.0584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22351.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-474.7846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(644.7337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-510.6777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6130],
        [-2.4583],
        [-2.2259],
        ...,
        [-2.6508],
        [-2.6448],
        [-2.6419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290992.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0170],
        ...,
        [1.0009],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368688.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(505.6306, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0105],
        [1.0171],
        ...,
        [1.0008],
        [0.9998],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368694.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7071e-03,  4.9341e-04,  0.0000e+00,  ...,  3.0882e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7071e-03,  4.9341e-04,  0.0000e+00,  ...,  3.0882e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7071e-03,  4.9341e-04,  0.0000e+00,  ...,  3.0882e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.0120e-02,  9.0035e-03, -9.1763e-05,  ...,  1.1419e-02,
         -3.8802e-03, -5.0646e-03],
        [ 1.7071e-03,  4.9341e-04,  0.0000e+00,  ...,  3.0882e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7071e-03,  4.9341e-04,  0.0000e+00,  ...,  3.0882e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2719.9292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.3157, device='cuda:0')



h[100].sum tensor(66.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0107, device='cuda:0')



h[200].sum tensor(-2.5268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-37.7054, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0020, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0337, 0.0290, 0.0000,  ..., 0.0361, 0.0000, 0.0000],
        [0.0159, 0.0109, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68362.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0714, 0.0509, 0.0643,  ..., 0.0000, 0.0636, 0.0000],
        [0.0691, 0.0259, 0.0451,  ..., 0.0000, 0.0526, 0.0000],
        [0.0662, 0.0123, 0.0220,  ..., 0.0000, 0.0386, 0.0000],
        ...,
        [0.0579, 0.0000, 0.0044,  ..., 0.0000, 0.0138, 0.0000],
        [0.0660, 0.0167, 0.0263,  ..., 0.0000, 0.0323, 0.0000],
        [0.0720, 0.0343, 0.0520,  ..., 0.0000, 0.0576, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637926.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5883.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.5959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22349.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-466.3640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(520.5470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-493.7019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8494],
        [-0.3953],
        [-0.0473],
        ...,
        [-0.1183],
        [-0.7296],
        [-1.5020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290435.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0105],
        [1.0171],
        ...,
        [1.0008],
        [0.9998],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368694.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0106],
        [1.0172],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368701.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00],
        [1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00],
        [1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00],
        [1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00],
        [1.6961e-03, 5.0557e-04, 0.0000e+00,  ..., 3.4665e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2283.5449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.2021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1181, device='cuda:0')



h[100].sum tensor(61.5980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2206, device='cuda:0')



h[200].sum tensor(-1.7007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.8070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0087, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0068, 0.0020, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0020, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55541.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0685, 0.0244, 0.0385,  ..., 0.0000, 0.0471, 0.0000],
        [0.0717, 0.0485, 0.0625,  ..., 0.0000, 0.0619, 0.0000],
        [0.0728, 0.0572, 0.0695,  ..., 0.0000, 0.0663, 0.0000],
        ...,
        [0.0752, 0.0626, 0.0749,  ..., 0.0000, 0.0700, 0.0000],
        [0.0751, 0.0625, 0.0749,  ..., 0.0000, 0.0700, 0.0000],
        [0.0751, 0.0625, 0.0748,  ..., 0.0000, 0.0699, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553597.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6143.1450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.2336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20199.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-433.6681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(115.6089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.3236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8639],
        [-1.3451],
        [-1.5980],
        ...,
        [-2.6842],
        [-2.6775],
        [-2.6746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331207.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0106],
        [1.0172],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368701.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0107],
        [1.0173],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368708.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0519e-02,  9.4684e-03, -9.3913e-05,  ...,  1.2011e-02,
         -4.0619e-03, -5.3053e-03],
        [ 1.6849e-03,  5.2299e-04,  0.0000e+00,  ...,  4.4319e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.6849e-02,  2.6004e-02, -2.6751e-04,  ...,  3.4132e-02,
         -1.1570e-02, -1.5112e-02],
        ...,
        [ 1.6849e-03,  5.2299e-04,  0.0000e+00,  ...,  4.4319e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6849e-03,  5.2299e-04,  0.0000e+00,  ...,  4.4319e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6849e-03,  5.2299e-04,  0.0000e+00,  ...,  4.4319e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2567.7041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.7406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.5608, device='cuda:0')



h[100].sum tensor(63.7019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6151, device='cuda:0')



h[200].sum tensor(-2.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.3242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0094, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0557, 0.0516, 0.0000,  ..., 0.0664, 0.0000, 0.0000],
        [0.0312, 0.0268, 0.0000,  ..., 0.0332, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64149.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.0000, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        [0.0552, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0558, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        ...,
        [0.0755, 0.0625, 0.0749,  ..., 0.0000, 0.0699, 0.0000],
        [0.0754, 0.0625, 0.0749,  ..., 0.0000, 0.0699, 0.0000],
        [0.0754, 0.0624, 0.0748,  ..., 0.0000, 0.0698, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600687.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6074.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.0690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21626.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-457.8707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(320.2083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-474.5646, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2477],
        [ 0.2548],
        [ 0.2589],
        ...,
        [-2.6878],
        [-2.6816],
        [-2.6789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279185.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0107],
        [1.0173],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368708.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0108],
        [1.0174],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368716.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3585e-02,  1.2569e-02, -1.2472e-04,  ...,  1.6155e-02,
         -5.4558e-03, -7.1283e-03],
        [ 9.1936e-03,  8.1231e-03, -7.8646e-05,  ...,  1.0207e-02,
         -3.4404e-03, -4.4951e-03],
        [ 7.5493e-03,  6.4584e-03, -6.1396e-05,  ...,  7.9798e-03,
         -2.6858e-03, -3.5092e-03],
        ...,
        [ 1.6969e-03,  5.3309e-04,  0.0000e+00,  ...,  5.3154e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6969e-03,  5.3309e-04,  0.0000e+00,  ...,  5.3154e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6969e-03,  5.3309e-04,  0.0000e+00,  ...,  5.3154e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.3649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8932, device='cuda:0')



h[100].sum tensor(61.5415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7653, device='cuda:0')



h[200].sum tensor(-1.6081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0440, 0.0398, 0.0000,  ..., 0.0505, 0.0000, 0.0000],
        [0.0406, 0.0363, 0.0000,  ..., 0.0459, 0.0000, 0.0000],
        [0.0193, 0.0147, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55538.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0593, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0617, 0.0000, 0.0038,  ..., 0.0000, 0.0139, 0.0000],
        [0.0666, 0.0149, 0.0253,  ..., 0.0000, 0.0360, 0.0000],
        ...,
        [0.0751, 0.0622, 0.0750,  ..., 0.0000, 0.0696, 0.0000],
        [0.0751, 0.0622, 0.0749,  ..., 0.0000, 0.0696, 0.0000],
        [0.0751, 0.0622, 0.0749,  ..., 0.0000, 0.0696, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554337.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6063.7432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.0068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20307.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-438.9520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(123.9078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.3917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1400],
        [-0.2083],
        [-0.7893],
        ...,
        [-2.6752],
        [-2.6696],
        [-2.6680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313255.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0108],
        [1.0174],
        ...,
        [1.0008],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368716.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0109],
        [1.0175],
        ...,
        [1.0008],
        [0.9997],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368723.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00],
        [1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00],
        [1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00],
        [1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00],
        [1.7092e-03, 5.3899e-04, 0.0000e+00,  ..., 5.9539e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2583.2180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.7542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.4437, device='cuda:0')



h[100].sum tensor(64.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1999, device='cuda:0')



h[200].sum tensor(-2.1097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0328, 0.0284, 0.0000,  ..., 0.0353, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63421.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0712, 0.0480, 0.0624,  ..., 0.0000, 0.0620, 0.0000],
        [0.0683, 0.0280, 0.0384,  ..., 0.0000, 0.0478, 0.0000],
        [0.0591, 0.0055, 0.0136,  ..., 0.0000, 0.0214, 0.0000],
        ...,
        [0.0748, 0.0620, 0.0750,  ..., 0.0000, 0.0694, 0.0000],
        [0.0747, 0.0619, 0.0749,  ..., 0.0000, 0.0694, 0.0000],
        [0.0747, 0.0619, 0.0749,  ..., 0.0000, 0.0693, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594088.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5854.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.2280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21343.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-461.4737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(324.9119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-471.6734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2132],
        [-0.5656],
        [-0.0688],
        ...,
        [-2.6717],
        [-2.6658],
        [-2.6633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282200.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0109],
        [1.0175],
        ...,
        [1.0008],
        [0.9997],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368723.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0110],
        [1.0175],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368730.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9578e-03,  6.8489e-03, -6.3762e-05,  ...,  8.5074e-03,
         -2.8535e-03, -3.7309e-03],
        [ 1.7159e-03,  5.3281e-04,  0.0000e+00,  ...,  5.7358e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7159e-03,  5.3281e-04,  0.0000e+00,  ...,  5.7358e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7159e-03,  5.3281e-04,  0.0000e+00,  ...,  5.7358e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7159e-03,  5.3281e-04,  0.0000e+00,  ...,  5.7358e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7159e-03,  5.3281e-04,  0.0000e+00,  ...,  5.7358e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2343.7119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.4119, device='cuda:0')



h[100].sum tensor(63.2060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9581, device='cuda:0')



h[200].sum tensor(-1.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9830, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0229, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0229, 0.0183, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58345.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0612, 0.0021, 0.0070,  ..., 0.0000, 0.0203, 0.0000],
        [0.0639, 0.0093, 0.0201,  ..., 0.0000, 0.0308, 0.0000],
        [0.0691, 0.0295, 0.0489,  ..., 0.0000, 0.0520, 0.0000],
        ...,
        [0.0745, 0.0619, 0.0750,  ..., 0.0000, 0.0695, 0.0000],
        [0.0745, 0.0618, 0.0750,  ..., 0.0000, 0.0695, 0.0000],
        [0.0745, 0.0618, 0.0749,  ..., 0.0000, 0.0695, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576900.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5787.3003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20746.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-450.5987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(228.0778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.2841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0427],
        [-0.2534],
        [-0.6085],
        ...,
        [-2.6728],
        [-2.6671],
        [-2.6644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311212.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0110],
        [1.0175],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368730.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0111],
        [1.0176],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368737.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00],
        [1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00],
        [1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00],
        [1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00],
        [1.7269e-03, 5.2183e-04, 0.0000e+00,  ..., 5.1582e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2655.2056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.9645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.3097, device='cuda:0')



h[100].sum tensor(66.6723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5218, device='cuda:0')



h[200].sum tensor(-2.1374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0098, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0192, 0.0145, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0238, 0.0191, 0.0000,  ..., 0.0230, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64294.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0064, 0.0233,  ..., 0.0000, 0.0352, 0.0000],
        [0.0638, 0.0053, 0.0188,  ..., 0.0000, 0.0331, 0.0000],
        [0.0633, 0.0037, 0.0123,  ..., 0.0000, 0.0303, 0.0000],
        ...,
        [0.0743, 0.0619, 0.0751,  ..., 0.0000, 0.0697, 0.0000],
        [0.0743, 0.0619, 0.0750,  ..., 0.0000, 0.0697, 0.0000],
        [0.0743, 0.0619, 0.0750,  ..., 0.0000, 0.0696, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603329.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5670.2397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.5028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21368.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-467.6820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(434.4211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-476.2465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0648],
        [-0.0304],
        [-0.1872],
        ...,
        [-2.6753],
        [-2.6695],
        [-2.6669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292256.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0111],
        [1.0176],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368737.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0111],
        [1.0176],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368744.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0273e-03,  7.8888e-03, -7.2525e-05,  ...,  9.9137e-03,
         -3.3207e-03, -4.3447e-03],
        [ 1.7343e-03,  5.1533e-04,  0.0000e+00,  ...,  4.8037e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.4207e-02,  2.3236e-02, -2.2348e-04,  ...,  3.0449e-02,
         -1.0233e-02, -1.3388e-02],
        ...,
        [ 1.7343e-03,  5.1533e-04,  0.0000e+00,  ...,  4.8037e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7343e-03,  5.1533e-04,  0.0000e+00,  ...,  4.8037e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7343e-03,  5.1533e-04,  0.0000e+00,  ...,  4.8037e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2463.7090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.7410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1502, device='cuda:0')



h[100].sum tensor(65.5129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6042, device='cuda:0')



h[200].sum tensor(-1.7492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.0112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0081, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0371, 0.0325, 0.0000,  ..., 0.0409, 0.0000, 0.0000],
        [0.0256, 0.0209, 0.0000,  ..., 0.0253, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58328.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.2531e-02, 2.2057e-06, 9.6711e-03,  ..., 0.0000e+00, 2.9876e-02,
         0.0000e+00],
        [5.8938e-02, 0.0000e+00, 4.5519e-04,  ..., 0.0000e+00, 1.7886e-02,
         0.0000e+00],
        [5.9671e-02, 0.0000e+00, 2.1240e-03,  ..., 0.0000e+00, 1.9785e-02,
         0.0000e+00],
        ...,
        [7.4181e-02, 6.1988e-02, 7.4972e-02,  ..., 0.0000e+00, 6.9789e-02,
         0.0000e+00],
        [7.4149e-02, 6.1964e-02, 7.4932e-02,  ..., 0.0000e+00, 6.9758e-02,
         0.0000e+00],
        [7.4104e-02, 6.1933e-02, 7.4871e-02,  ..., 0.0000e+00, 6.9714e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563292.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5654.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.7117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20505.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.2606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(183.7675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.2188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0229],
        [ 0.0239],
        [-0.1026],
        ...,
        [-2.6765],
        [-2.6710],
        [-2.6684]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291207.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0111],
        [1.0176],
        ...,
        [1.0007],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368744.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0112],
        [1.0177],
        ...,
        [1.0007],
        [0.9996],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368751.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7158e-03,  7.5806e-03, -6.8599e-05,  ...,  9.5015e-03,
         -3.1771e-03, -4.1583e-03],
        [ 1.7249e-03,  5.1424e-04,  0.0000e+00,  ...,  4.6651e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7249e-03,  5.1424e-04,  0.0000e+00,  ...,  4.6651e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7249e-03,  5.1424e-04,  0.0000e+00,  ...,  4.6651e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7249e-03,  5.1424e-04,  0.0000e+00,  ...,  4.6651e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7249e-03,  5.1424e-04,  0.0000e+00,  ...,  4.6651e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2475.8967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.4382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.2296, device='cuda:0')



h[100].sum tensor(65.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6337, device='cuda:0')



h[200].sum tensor(-1.7410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0218, 0.0171, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0140, 0.0092, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59466.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0588, 0.0078, 0.0179,  ..., 0.0000, 0.0226, 0.0000],
        [0.0661, 0.0224, 0.0368,  ..., 0.0000, 0.0433, 0.0000],
        [0.0705, 0.0453, 0.0607,  ..., 0.0000, 0.0598, 0.0000],
        ...,
        [0.0744, 0.0621, 0.0749,  ..., 0.0000, 0.0700, 0.0000],
        [0.0743, 0.0621, 0.0748,  ..., 0.0000, 0.0700, 0.0000],
        [0.0743, 0.0620, 0.0748,  ..., 0.0000, 0.0699, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572109.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5647.6470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20799.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-464.8829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(210.3858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.7075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1095],
        [-0.5297],
        [-1.0380],
        ...,
        [-2.6844],
        [-2.6787],
        [-2.6760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279998.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0112],
        [1.0177],
        ...,
        [1.0007],
        [0.9996],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368751.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0113],
        [1.0178],
        ...,
        [1.0007],
        [0.9996],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368758.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1461e-03,  7.0353e-03, -6.2449e-05,  ...,  8.7673e-03,
         -2.9257e-03, -3.8306e-03],
        [ 1.6970e-03,  5.1668e-04,  0.0000e+00,  ...,  4.5787e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03,  5.1668e-04,  0.0000e+00,  ...,  4.5787e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6970e-03,  5.1668e-04,  0.0000e+00,  ...,  4.5787e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03,  5.1668e-04,  0.0000e+00,  ...,  4.5787e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6970e-03,  5.1668e-04,  0.0000e+00,  ...,  4.5787e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2385.3914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.0737, device='cuda:0')



h[100].sum tensor(64.4567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8324, device='cuda:0')



h[200].sum tensor(-1.5739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.5884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0180, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0134, 0.0087, 0.0000,  ..., 0.0090, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57632.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0620, 0.0077, 0.0166,  ..., 0.0000, 0.0261, 0.0000],
        [0.0664, 0.0158, 0.0304,  ..., 0.0000, 0.0429, 0.0000],
        [0.0692, 0.0271, 0.0465,  ..., 0.0000, 0.0531, 0.0000],
        ...,
        [0.0750, 0.0623, 0.0747,  ..., 0.0000, 0.0704, 0.0000],
        [0.0750, 0.0623, 0.0747,  ..., 0.0000, 0.0704, 0.0000],
        [0.0749, 0.0622, 0.0746,  ..., 0.0000, 0.0703, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564607.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5766.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(455.4495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20774.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-463.9926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(131.3947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.5162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0129],
        [-0.1713],
        [-0.2734],
        ...,
        [-2.7016],
        [-2.6964],
        [-2.6939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276673.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0113],
        [1.0178],
        ...,
        [1.0007],
        [0.9996],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368758.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(504.3152, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0114],
        [1.0178],
        ...,
        [1.0007],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368764.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7650e-02,  2.6788e-02, -2.4838e-04,  ...,  3.5189e-02,
         -1.1771e-02, -1.5417e-02],
        [ 1.6585e-03,  5.1413e-04,  0.0000e+00,  ...,  3.9160e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7795e-02,  1.6826e-02, -1.5420e-04,  ...,  2.1861e-02,
         -7.3079e-03, -9.5714e-03],
        ...,
        [ 1.6585e-03,  5.1413e-04,  0.0000e+00,  ...,  3.9160e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6585e-03,  5.1413e-04,  0.0000e+00,  ...,  3.9160e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6585e-03,  5.1413e-04,  0.0000e+00,  ...,  3.9160e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2798.8406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.2182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.2302, device='cuda:0')



h[100].sum tensor(67.3198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6072, device='cuda:0')



h[200].sum tensor(-2.3039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.4388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0782, 0.0744, 0.0000,  ..., 0.0969, 0.0000, 0.0000],
        [0.0917, 0.0880, 0.0000,  ..., 0.1151, 0.0000, 0.0000],
        [0.0265, 0.0221, 0.0000,  ..., 0.0269, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66519.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0288, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0375, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0526, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0760, 0.0626, 0.0748,  ..., 0.0000, 0.0711, 0.0000],
        [0.0759, 0.0626, 0.0748,  ..., 0.0000, 0.0711, 0.0000],
        [0.0759, 0.0625, 0.0747,  ..., 0.0000, 0.0710, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612919.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5881.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.8679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21700.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-477.9123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(450.7385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.1908, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0133],
        [ 0.0490],
        [ 0.0992],
        ...,
        [-2.7314],
        [-2.7256],
        [-2.7226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305173.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0114],
        [1.0178],
        ...,
        [1.0007],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368764.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368771.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6285e-03,  5.3008e-04,  0.0000e+00,  ...,  4.9159e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6529e-02,  1.5594e-02, -1.4051e-04,  ...,  2.0201e-02,
         -6.7366e-03, -8.8262e-03],
        [ 1.6506e-02,  1.5571e-02, -1.4029e-04,  ...,  2.0170e-02,
         -6.7261e-03, -8.8124e-03],
        ...,
        [ 1.6285e-03,  5.3008e-04,  0.0000e+00,  ...,  4.9159e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6285e-03,  5.3008e-04,  0.0000e+00,  ...,  4.9159e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6285e-03,  5.3008e-04,  0.0000e+00,  ...,  4.9159e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2562.2964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.2159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.7271, device='cuda:0')



h[100].sum tensor(64.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5619, device='cuda:0')



h[200].sum tensor(-1.8816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.0179, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0216, 0.0173, 0.0000,  ..., 0.0205, 0.0000, 0.0000],
        [0.0339, 0.0298, 0.0000,  ..., 0.0372, 0.0000, 0.0000],
        [0.0911, 0.0876, 0.0000,  ..., 0.1145, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60033.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0657, 0.0153, 0.0249,  ..., 0.0000, 0.0361, 0.0000],
        [0.0587, 0.0016, 0.0071,  ..., 0.0000, 0.0195, 0.0000],
        [0.0466, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0765, 0.0627, 0.0747,  ..., 0.0000, 0.0712, 0.0000],
        [0.0765, 0.0626, 0.0747,  ..., 0.0000, 0.0712, 0.0000],
        [0.0764, 0.0626, 0.0746,  ..., 0.0000, 0.0711, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577246.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6074.4429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.7886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21074.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-466.2812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(211.8275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.9907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7774],
        [-0.3789],
        [-0.0365],
        ...,
        [-2.7435],
        [-2.7374],
        [-2.7343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285419.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368771.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368778.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1187e-02,  1.0225e-02, -8.9165e-05,  ...,  1.3012e-02,
         -4.3245e-03, -5.6679e-03],
        [ 3.3311e-02,  3.2593e-02, -2.9502e-04,  ...,  4.2932e-02,
         -1.4309e-02, -1.8753e-02],
        [ 1.6400e-02,  1.5496e-02, -1.3767e-04,  ...,  2.0062e-02,
         -6.6770e-03, -8.7512e-03],
        ...,
        [ 1.6041e-03,  5.3652e-04,  0.0000e+00,  ...,  5.2089e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6041e-03,  5.3652e-04,  0.0000e+00,  ...,  5.2089e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6041e-03,  5.3652e-04,  0.0000e+00,  ...,  5.2089e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2320.7114, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(72.4311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1436, device='cuda:0')



h[100].sum tensor(61.8261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4867, device='cuda:0')



h[200].sum tensor(-1.4658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.5032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1015, 0.0983, 0.0000,  ..., 0.1287, 0.0000, 0.0000],
        [0.0774, 0.0739, 0.0000,  ..., 0.0961, 0.0000, 0.0000],
        [0.0789, 0.0754, 0.0000,  ..., 0.0982, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56144.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0445, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0433, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0438, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0772, 0.0629, 0.0748,  ..., 0.0000, 0.0715, 0.0000],
        [0.0772, 0.0629, 0.0748,  ..., 0.0000, 0.0715, 0.0000],
        [0.0771, 0.0628, 0.0747,  ..., 0.0000, 0.0715, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563490.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6257.3916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.6647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20582.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-453.3748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(183.8146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.0929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1215],
        [ 0.1118],
        [ 0.0955],
        ...,
        [-2.7616],
        [-2.7555],
        [-2.7524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317498.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9996],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368778.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9995],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368785.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00],
        [1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00],
        [1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00],
        [1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00],
        [1.6032e-03, 5.4626e-04, 0.0000e+00,  ..., 5.7158e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2933.1860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.2124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.4913, device='cuda:0')



h[100].sum tensor(67.0618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8193, device='cuda:0')



h[200].sum tensor(-2.4993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.2438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0091, 0.0000,  ..., 0.0094, 0.0000, 0.0000],
        [0.0065, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0329, 0.0289, 0.0000,  ..., 0.0359, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70377.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0000, 0.0120,  ..., 0.0000, 0.0314, 0.0000],
        [0.0646, 0.0036, 0.0130,  ..., 0.0000, 0.0237, 0.0000],
        [0.0525, 0.0000, 0.0049,  ..., 0.0000, 0.0127, 0.0000],
        ...,
        [0.0775, 0.0631, 0.0749,  ..., 0.0000, 0.0716, 0.0000],
        [0.0775, 0.0631, 0.0748,  ..., 0.0000, 0.0715, 0.0000],
        [0.0775, 0.0631, 0.0748,  ..., 0.0000, 0.0715, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647126.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6149.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.6800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22511.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-485.3296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(665.8570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-523.4361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1499],
        [ 0.0984],
        [ 0.0399],
        ...,
        [-2.7707],
        [-2.7648],
        [-2.7615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306672.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0115],
        [1.0179],
        ...,
        [1.0006],
        [0.9995],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368785.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0116],
        [1.0180],
        ...,
        [1.0006],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368792., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2901e-02,  1.1959e-02, -1.0220e-04,  ...,  1.5316e-02,
         -5.0730e-03, -6.6535e-03],
        [ 7.0960e-03,  6.0915e-03, -4.9637e-05,  ...,  7.4694e-03,
         -2.4639e-03, -3.2316e-03],
        [ 1.9469e-02,  1.8599e-02, -1.6168e-04,  ...,  2.4196e-02,
         -8.0256e-03, -1.0526e-02],
        ...,
        [ 1.6143e-03,  5.5050e-04,  0.0000e+00,  ...,  5.8860e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6143e-03,  5.5050e-04,  0.0000e+00,  ...,  5.8860e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6143e-03,  5.5050e-04,  0.0000e+00,  ...,  5.8860e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2636.0574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.2709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.4831, device='cuda:0')



h[100].sum tensor(64.4155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2146, device='cuda:0')



h[200].sum tensor(-1.9741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0436, 0.0397, 0.0000,  ..., 0.0503, 0.0000, 0.0000],
        [0.0545, 0.0507, 0.0000,  ..., 0.0650, 0.0000, 0.0000],
        [0.0414, 0.0374, 0.0000,  ..., 0.0473, 0.0000, 0.0000],
        ...,
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0067, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61927.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0543, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0535, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0544, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0778, 0.0634, 0.0751,  ..., 0.0000, 0.0717, 0.0000],
        [0.0778, 0.0634, 0.0750,  ..., 0.0000, 0.0716, 0.0000],
        [0.0777, 0.0633, 0.0750,  ..., 0.0000, 0.0716, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585608.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6301.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.4529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21119.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-466.6983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(299.5689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-478.6125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2133],
        [ 0.1772],
        [ 0.1298],
        ...,
        [-2.7791],
        [-2.7738],
        [-2.7708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315968.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0116],
        [1.0180],
        ...,
        [1.0006],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368792., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0180],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368798.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7558e-02,  1.6642e-02, -1.4219e-04,  ...,  2.1575e-02,
         -7.1408e-03, -9.3688e-03],
        [ 2.4309e-02,  2.3464e-02, -2.0248e-04,  ...,  3.0699e-02,
         -1.0169e-02, -1.3342e-02],
        [ 2.1529e-02,  2.0655e-02, -1.7766e-04,  ...,  2.6942e-02,
         -8.9219e-03, -1.1706e-02],
        ...,
        [ 1.6379e-03,  5.5268e-04,  0.0000e+00,  ...,  5.9498e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6379e-03,  5.5268e-04,  0.0000e+00,  ...,  5.9498e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6379e-03,  5.5268e-04,  0.0000e+00,  ...,  5.9498e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2737.0010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.7329, device='cuda:0')



h[100].sum tensor(65.4868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0507, device='cuda:0')



h[200].sum tensor(-2.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-34.6918, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0700, 0.0663, 0.0000,  ..., 0.0859, 0.0000, 0.0000],
        [0.0832, 0.0796, 0.0000,  ..., 0.1037, 0.0000, 0.0000],
        [0.0869, 0.0833, 0.0000,  ..., 0.1087, 0.0000, 0.0000],
        ...,
        [0.0068, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0068, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62988.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0448, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0436, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0774, 0.0595, 0.0720,  ..., 0.0000, 0.0695, 0.0000],
        [0.0772, 0.0574, 0.0703,  ..., 0.0000, 0.0684, 0.0000],
        [0.0774, 0.0595, 0.0719,  ..., 0.0000, 0.0695, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588961.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6298.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.2566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21160.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-468.1836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(304.4720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-482.2252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1684],
        [ 0.1637],
        [ 0.1493],
        ...,
        [-2.4581],
        [-2.3961],
        [-2.3956]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321535.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0180],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368798.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0180],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368805., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4277e-02,  3.3526e-02, -2.8743e-04,  ...,  4.4146e-02,
         -1.4604e-02, -1.9168e-02],
        [ 1.4282e-02,  1.3321e-02, -1.1127e-04,  ...,  1.7129e-02,
         -5.6537e-03, -7.4204e-03],
        [ 1.9110e-02,  1.8200e-02, -1.5381e-04,  ...,  2.3653e-02,
         -7.8150e-03, -1.0257e-02],
        ...,
        [ 1.6517e-03,  5.5787e-04,  0.0000e+00,  ...,  6.2909e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6517e-03,  5.5787e-04,  0.0000e+00,  ...,  6.2909e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6517e-03,  5.5787e-04,  0.0000e+00,  ...,  6.2909e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2559.9714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.2689, device='cuda:0')



h[100].sum tensor(63.9339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3916, device='cuda:0')



h[200].sum tensor(-1.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.4833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0697, 0.0659, 0.0000,  ..., 0.0854, 0.0000, 0.0000],
        [0.0843, 0.0807, 0.0000,  ..., 0.1051, 0.0000, 0.0000],
        [0.0773, 0.0736, 0.0000,  ..., 0.0956, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0023, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60425.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0459, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0464, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0489, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0782, 0.0641, 0.0755,  ..., 0.0000, 0.0717, 0.0000],
        [0.0781, 0.0640, 0.0754,  ..., 0.0000, 0.0717, 0.0000],
        [0.0781, 0.0640, 0.0754,  ..., 0.0000, 0.0717, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580958.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6362.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.3118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20993.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-462.7073, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(273.7213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-467.8196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1912],
        [ 0.2060],
        [ 0.2200],
        ...,
        [-2.7916],
        [-2.7854],
        [-2.7825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315762.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0180],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368805., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0181],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368811.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9501e-02,  1.8574e-02, -1.5499e-04,  ...,  2.4152e-02,
         -7.9678e-03, -1.0461e-02],
        [ 1.6375e-02,  1.5415e-02, -1.2783e-04,  ...,  1.9929e-02,
         -6.5712e-03, -8.6276e-03],
        [ 1.9274e-02,  1.8344e-02, -1.5302e-04,  ...,  2.3845e-02,
         -7.8662e-03, -1.0328e-02],
        ...,
        [ 1.6645e-03,  5.5310e-04,  0.0000e+00,  ...,  5.7280e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6645e-03,  5.5310e-04,  0.0000e+00,  ...,  5.7280e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6645e-03,  5.5310e-04,  0.0000e+00,  ...,  5.7280e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2410.5610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.1638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9976, device='cuda:0')



h[100].sum tensor(62.5486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1758, device='cuda:0')



h[200].sum tensor(-1.5576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0600, 0.0561, 0.0000,  ..., 0.0723, 0.0000, 0.0000],
        [0.0762, 0.0724, 0.0000,  ..., 0.0941, 0.0000, 0.0000],
        [0.0849, 0.0812, 0.0000,  ..., 0.1058, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56702.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0375, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0366, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0785, 0.0645, 0.0757,  ..., 0.0000, 0.0721, 0.0000],
        [0.0785, 0.0645, 0.0757,  ..., 0.0000, 0.0720, 0.0000],
        [0.0785, 0.0645, 0.0756,  ..., 0.0000, 0.0720, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560047.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6466.5347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(471.9912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20610.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.3220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(124.7911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-445.1841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0175],
        [ 0.0064],
        [ 0.0051],
        ...,
        [-2.8104],
        [-2.8042],
        [-2.8008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309332.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0181],
        ...,
        [1.0005],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368811.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0117],
        [1.0181],
        ...,
        [1.0005],
        [0.9994],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368817.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1821e-03,  7.1180e-03, -5.5806e-05,  ...,  8.8364e-03,
         -2.9026e-03, -3.8123e-03],
        [ 1.6713e-03,  5.4035e-04,  0.0000e+00,  ...,  4.2644e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6713e-03,  5.4035e-04,  0.0000e+00,  ...,  4.2644e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6713e-03,  5.4035e-04,  0.0000e+00,  ...,  4.2644e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6713e-03,  5.4035e-04,  0.0000e+00,  ...,  4.2644e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6713e-03,  5.4035e-04,  0.0000e+00,  ...,  4.2644e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3151.0747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(92.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-39.9343, device='cuda:0')



h[100].sum tensor(68.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8423, device='cuda:0')



h[200].sum tensor(-2.7890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-46.5946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0212, 0.0168, 0.0000,  ..., 0.0197, 0.0000, 0.0000],
        [0.0133, 0.0088, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0068, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78844.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.0113, 0.0207,  ..., 0.0000, 0.0307, 0.0000],
        [0.0719, 0.0256, 0.0398,  ..., 0.0000, 0.0488, 0.0000],
        [0.0751, 0.0475, 0.0611,  ..., 0.0000, 0.0620, 0.0000],
        ...,
        [0.0791, 0.0652, 0.0761,  ..., 0.0000, 0.0727, 0.0000],
        [0.0786, 0.0596, 0.0718,  ..., 0.0000, 0.0704, 0.0000],
        [0.0769, 0.0413, 0.0575,  ..., 0.0000, 0.0627, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709745.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6373.6416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.0408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23867.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.6907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1045.4497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-560.2231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2628],
        [-0.7800],
        [-1.3422],
        ...,
        [-2.7331],
        [-2.5357],
        [-2.1751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304175.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0117],
        [1.0181],
        ...,
        [1.0005],
        [0.9994],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368817.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0118],
        [1.0181],
        ...,
        [1.0004],
        [0.9994],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368823.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00],
        [1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00],
        [1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00],
        [1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00],
        [1.6923e-03, 5.2702e-04, 0.0000e+00,  ..., 2.7793e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2407.9033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.4404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3119, device='cuda:0')



h[100].sum tensor(62.2243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2926, device='cuda:0')



h[200].sum tensor(-1.5655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0068, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0069, 0.0021, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0071, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56941.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0768, 0.0640, 0.0734,  ..., 0.0000, 0.0707, 0.0000],
        [0.0770, 0.0641, 0.0737,  ..., 0.0000, 0.0709, 0.0000],
        [0.0773, 0.0643, 0.0741,  ..., 0.0000, 0.0712, 0.0000],
        ...,
        [0.0794, 0.0657, 0.0766,  ..., 0.0000, 0.0732, 0.0000],
        [0.0794, 0.0657, 0.0766,  ..., 0.0000, 0.0731, 0.0000],
        [0.0794, 0.0657, 0.0765,  ..., 0.0000, 0.0731, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565419.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6637.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.3656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20645.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.4114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(125.7509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.0532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9903],
        [-3.0232],
        [-3.0395],
        ...,
        [-2.8533],
        [-2.8466],
        [-2.8437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338940.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0118],
        [1.0181],
        ...,
        [1.0004],
        [0.9994],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368823.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(482.5134, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0118],
        [1.0182],
        ...,
        [1.0004],
        [0.9993],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368829.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7203e-03,  5.2268e-04,  0.0000e+00,  ...,  2.2247e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.8398e-03,  5.6926e-03, -4.2679e-05,  ...,  6.9327e-03,
         -2.2726e-03, -2.9870e-03],
        [ 6.8398e-03,  5.6926e-03, -4.2679e-05,  ...,  6.9327e-03,
         -2.2726e-03, -2.9870e-03],
        ...,
        [ 1.7203e-03,  5.2268e-04,  0.0000e+00,  ...,  2.2247e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7203e-03,  5.2268e-04,  0.0000e+00,  ...,  2.2247e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7203e-03,  5.2268e-04,  0.0000e+00,  ...,  2.2247e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2399.6973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.1238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0922, device='cuda:0')



h[100].sum tensor(62.3984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2110, device='cuda:0')



h[200].sum tensor(-1.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7768, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6285e-02, 1.1553e-02, 0.0000e+00,  ..., 1.2718e-02, 0.0000e+00,
         0.0000e+00],
        [1.6344e-02, 1.1598e-02, 0.0000e+00,  ..., 1.2770e-02, 0.0000e+00,
         0.0000e+00],
        [1.6393e-02, 1.1629e-02, 0.0000e+00,  ..., 1.2801e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.1729e-03, 2.1794e-03, 0.0000e+00,  ..., 9.2762e-05, 0.0000e+00,
         0.0000e+00],
        [7.1699e-03, 2.1784e-03, 0.0000e+00,  ..., 9.2723e-05, 0.0000e+00,
         0.0000e+00],
        [7.1648e-03, 2.1769e-03, 0.0000e+00,  ..., 9.2658e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57453.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0152, 0.0363,  ..., 0.0000, 0.0451, 0.0000],
        [0.0719, 0.0243, 0.0436,  ..., 0.0000, 0.0484, 0.0000],
        [0.0724, 0.0267, 0.0458,  ..., 0.0000, 0.0497, 0.0000],
        ...,
        [0.0794, 0.0661, 0.0771,  ..., 0.0000, 0.0733, 0.0000],
        [0.0794, 0.0661, 0.0770,  ..., 0.0000, 0.0732, 0.0000],
        [0.0793, 0.0660, 0.0770,  ..., 0.0000, 0.0732, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575037.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6597.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.8679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20723.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.2701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(197.5489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.2769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0526],
        [-1.5187],
        [-1.9251],
        ...,
        [-2.8647],
        [-2.8580],
        [-2.8544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363333.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0118],
        [1.0182],
        ...,
        [1.0004],
        [0.9993],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368829.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0118],
        [1.0183],
        ...,
        [1.0003],
        [0.9993],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368836.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00],
        [1.7528e-03, 5.5711e-04, 0.0000e+00,  ..., 4.7210e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2709.3142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.1113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.3985, device='cuda:0')



h[100].sum tensor(65.5853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5548, device='cuda:0')



h[200].sum tensor(-1.9830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.1349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0022, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0073, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65427.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0714, 0.0274, 0.0338,  ..., 0.0000, 0.0471, 0.0000],
        [0.0721, 0.0289, 0.0377,  ..., 0.0000, 0.0496, 0.0000],
        [0.0732, 0.0319, 0.0444,  ..., 0.0000, 0.0537, 0.0000],
        ...,
        [0.0788, 0.0658, 0.0773,  ..., 0.0000, 0.0724, 0.0000],
        [0.0788, 0.0658, 0.0772,  ..., 0.0000, 0.0723, 0.0000],
        [0.0787, 0.0657, 0.0771,  ..., 0.0000, 0.0723, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619492.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6321.5425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.3947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21876.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-472.4790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(426.8551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.2647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0844],
        [-0.1207],
        [-0.1957],
        ...,
        [-2.5306],
        [-2.5053],
        [-2.4015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323301.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0118],
        [1.0183],
        ...,
        [1.0003],
        [0.9993],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368836.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0119],
        [1.0183],
        ...,
        [1.0003],
        [0.9992],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368843.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6528e-03,  5.5157e-03, -3.9523e-05,  ...,  6.6523e-03,
         -2.1549e-03, -2.8342e-03],
        [ 9.0042e-03,  7.8897e-03, -5.8589e-05,  ...,  9.8251e-03,
         -3.1944e-03, -4.2014e-03],
        [ 1.7785e-03,  5.9438e-04,  0.0000e+00,  ...,  7.5123e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7785e-03,  5.9438e-04,  0.0000e+00,  ...,  7.5123e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7785e-03,  5.9438e-04,  0.0000e+00,  ...,  7.5123e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7785e-03,  5.9438e-04,  0.0000e+00,  ...,  7.5123e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2967.2651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.3912, device='cuda:0')



h[100].sum tensor(68.3258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4104, device='cuda:0')



h[200].sum tensor(-2.3391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.9602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0474, 0.0430, 0.0000,  ..., 0.0546, 0.0000, 0.0000],
        [0.0221, 0.0174, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        [0.0145, 0.0098, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0074, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71377.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0659, 0.0087, 0.0178,  ..., 0.0000, 0.0241, 0.0000],
        [0.0711, 0.0253, 0.0383,  ..., 0.0000, 0.0459, 0.0000],
        ...,
        [0.0776, 0.0612, 0.0741,  ..., 0.0000, 0.0691, 0.0000],
        [0.0774, 0.0591, 0.0725,  ..., 0.0000, 0.0680, 0.0000],
        [0.0776, 0.0611, 0.0740,  ..., 0.0000, 0.0691, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646567., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6076.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.4260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22706.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-496.7434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(585.5255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-511.9029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1623],
        [-0.1279],
        [-0.6546],
        ...,
        [-2.4734],
        [-2.4188],
        [-2.4643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285350.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0119],
        [1.0183],
        ...,
        [1.0003],
        [0.9992],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368843.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0119],
        [1.0184],
        ...,
        [1.0002],
        [0.9992],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368849.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8084e-03,  6.2878e-04,  0.0000e+00,  ...,  9.9232e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.0753e-02,  9.6590e-03, -7.1528e-05,  ...,  1.2167e-02,
         -3.9464e-03, -5.1922e-03],
        [ 1.7583e-02,  1.6554e-02, -1.2615e-04,  ...,  2.1382e-02,
         -6.9597e-03, -9.1570e-03],
        ...,
        [ 1.8084e-03,  6.2878e-04,  0.0000e+00,  ...,  9.9232e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8084e-03,  6.2878e-04,  0.0000e+00,  ...,  9.9232e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8084e-03,  6.2878e-04,  0.0000e+00,  ...,  9.9232e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2614.1113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.8578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.0240, device='cuda:0')



h[100].sum tensor(65.8364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3006, device='cuda:0')



h[200].sum tensor(-1.7190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.1976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0190, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0362, 0.0317, 0.0000,  ..., 0.0393, 0.0000, 0.0000],
        [0.0573, 0.0530, 0.0000,  ..., 0.0678, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0026, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0075, 0.0026, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0075, 0.0026, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60620.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0139, 0.0234,  ..., 0.0000, 0.0254, 0.0000],
        [0.0580, 0.0009, 0.0060,  ..., 0.0000, 0.0098, 0.0000],
        [0.0506, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0774, 0.0652, 0.0776,  ..., 0.0000, 0.0705, 0.0000],
        [0.0773, 0.0652, 0.0775,  ..., 0.0000, 0.0705, 0.0000],
        [0.0773, 0.0652, 0.0775,  ..., 0.0000, 0.0705, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577319., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6032.2651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.2654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20930.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-477.5035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(299.2220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.5802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4082],
        [ 0.0246],
        [ 0.2433],
        ...,
        [-2.7899],
        [-2.7844],
        [-2.7813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306061.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0119],
        [1.0184],
        ...,
        [1.0002],
        [0.9992],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368849.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0120],
        [1.0185],
        ...,
        [1.0001],
        [0.9991],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368855.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4643e-02,  1.3579e-02, -1.0103e-04,  ...,  1.7392e-02,
         -5.6405e-03, -7.4239e-03],
        [ 1.8333e-03,  6.4726e-04,  0.0000e+00,  ...,  1.1107e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.1824e-02,  1.0734e-02, -7.8799e-05,  ...,  1.3589e-02,
         -4.3994e-03, -5.7903e-03],
        ...,
        [ 1.8333e-03,  6.4726e-04,  0.0000e+00,  ...,  1.1107e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8333e-03,  6.4726e-04,  0.0000e+00,  ...,  1.1107e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8333e-03,  6.4726e-04,  0.0000e+00,  ...,  1.1107e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2347.5842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.7979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.2585, device='cuda:0')



h[100].sum tensor(63.9125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7861, device='cuda:0')



h[200].sum tensor(-1.2587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.3037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0231, 0.0000,  ..., 0.0278, 0.0000, 0.0000],
        [0.0516, 0.0472, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0697, 0.0655, 0.0000,  ..., 0.0845, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0027, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0076, 0.0027, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0076, 0.0027, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55401.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0554, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0462, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0356, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0768, 0.0653, 0.0779,  ..., 0.0000, 0.0701, 0.0000],
        [0.0768, 0.0653, 0.0779,  ..., 0.0000, 0.0701, 0.0000],
        [0.0767, 0.0653, 0.0778,  ..., 0.0000, 0.0700, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551763.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5907.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(445.8364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20448.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-473.0238, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(107.8902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.8979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2171],
        [ 0.1722],
        [ 0.1187],
        ...,
        [-2.7782],
        [-2.7727],
        [-2.7696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288908.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0120],
        [1.0185],
        ...,
        [1.0001],
        [0.9991],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368855.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0120],
        [1.0186],
        ...,
        [1.0001],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368861.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8712e-03,  6.3866e-04,  0.0000e+00,  ...,  1.0378e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.1089e-02,  9.9423e-03, -7.1693e-05,  ...,  1.2536e-02,
         -4.0505e-03, -5.3331e-03],
        [ 8.1024e-03,  6.9276e-03, -4.8462e-05,  ...,  8.5074e-03,
         -2.7380e-03, -3.6050e-03],
        ...,
        [ 1.8712e-03,  6.3866e-04,  0.0000e+00,  ...,  1.0378e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8712e-03,  6.3866e-04,  0.0000e+00,  ...,  1.0378e-04,
          0.0000e+00,  0.0000e+00],
        [ 1.8712e-03,  6.3866e-04,  0.0000e+00,  ...,  1.0378e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2483.0095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(82.6398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2983, device='cuda:0')



h[100].sum tensor(65.8735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9159, device='cuda:0')



h[200].sum tensor(-1.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8505, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0119, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0215, 0.0166, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        [0.0624, 0.0579, 0.0000,  ..., 0.0743, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0027, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0078, 0.0027, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0078, 0.0027, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56403.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0198, 0.0309,  ..., 0.0000, 0.0361, 0.0000],
        [0.0617, 0.0093, 0.0152,  ..., 0.0000, 0.0193, 0.0000],
        [0.0528, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0762, 0.0658, 0.0785,  ..., 0.0000, 0.0701, 0.0000],
        [0.0761, 0.0658, 0.0784,  ..., 0.0000, 0.0700, 0.0000],
        [0.0761, 0.0658, 0.0784,  ..., 0.0000, 0.0700, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551030.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5759.2744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(435.6985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20416.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-479.6984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(143.3113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.5312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0533],
        [ 0.1463],
        [ 0.2045],
        ...,
        [-2.7785],
        [-2.7728],
        [-2.7658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276268.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0120],
        [1.0186],
        ...,
        [1.0001],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368861.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0121],
        [1.0186],
        ...,
        [1.0000],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368867.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00],
        [1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00],
        [1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00],
        [1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00],
        [1.8960e-03, 6.1308e-04, 0.0000e+00,  ..., 8.2613e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2461.1438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.1020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.5638, device='cuda:0')



h[100].sum tensor(66.1288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6429, device='cuda:0')



h[200].sum tensor(-1.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.9934, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0077, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0077, 0.0025, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0026, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0079, 0.0026, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0079, 0.0026, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59595.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0734, 0.0648, 0.0758,  ..., 0.0000, 0.0683, 0.0000],
        [0.0733, 0.0625, 0.0742,  ..., 0.0000, 0.0673, 0.0000],
        [0.0718, 0.0442, 0.0607,  ..., 0.0000, 0.0593, 0.0000],
        ...,
        [0.0759, 0.0666, 0.0791,  ..., 0.0000, 0.0706, 0.0000],
        [0.0758, 0.0666, 0.0791,  ..., 0.0000, 0.0705, 0.0000],
        [0.0758, 0.0665, 0.0790,  ..., 0.0000, 0.0705, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580436.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5568.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.6526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21284.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-491.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(199.8798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.7892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6946],
        [-1.3805],
        [-0.8902],
        ...,
        [-2.7984],
        [-2.7937],
        [-2.7910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260100.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0121],
        [1.0186],
        ...,
        [1.0000],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368867.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0121],
        [1.0187],
        ...,
        [1.0000],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368872.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00],
        [1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00],
        [1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00],
        [1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00],
        [1.8975e-03, 5.8018e-04, 0.0000e+00,  ..., 5.4710e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2612.4512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.5691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.9289, device='cuda:0')



h[100].sum tensor(67.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2653, device='cuda:0')



h[200].sum tensor(-1.6556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.0866, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0077, 0.0023, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0077, 0.0024, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0024, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0079, 0.0024, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0079, 0.0024, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61745.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0734, 0.0656, 0.0765,  ..., 0.0000, 0.0691, 0.0000],
        [0.0736, 0.0657, 0.0767,  ..., 0.0000, 0.0693, 0.0000],
        [0.0739, 0.0659, 0.0772,  ..., 0.0000, 0.0696, 0.0000],
        ...,
        [0.0759, 0.0674, 0.0798,  ..., 0.0000, 0.0715, 0.0000],
        [0.0759, 0.0674, 0.0798,  ..., 0.0000, 0.0714, 0.0000],
        [0.0758, 0.0673, 0.0797,  ..., 0.0000, 0.0714, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589759.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5548.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.7991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21339.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.7735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(269.8971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.7380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8085],
        [-2.7750],
        [-2.6968],
        ...,
        [-2.8363],
        [-2.8301],
        [-2.8269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286409.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0121],
        [1.0187],
        ...,
        [1.0000],
        [0.9990],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368872.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0121],
        [1.0187],
        ...,
        [1.0000],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368878.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00],
        [1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00],
        [1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00],
        [1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00],
        [1.8938e-03, 5.4619e-04, 0.0000e+00,  ..., 2.4946e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2456.9067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.6147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3605, device='cuda:0')



h[100].sum tensor(65.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9390, device='cuda:0')



h[200].sum tensor(-1.4354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0077, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0077, 0.0022, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0079, 0.0023, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0079, 0.0023, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0079, 0.0023, 0.0000,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58243.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0737, 0.0663, 0.0770,  ..., 0.0000, 0.0700, 0.0000],
        [0.0739, 0.0665, 0.0773,  ..., 0.0000, 0.0702, 0.0000],
        [0.0741, 0.0666, 0.0777,  ..., 0.0000, 0.0705, 0.0000],
        ...,
        [0.0761, 0.0682, 0.0804,  ..., 0.0000, 0.0724, 0.0000],
        [0.0761, 0.0682, 0.0804,  ..., 0.0000, 0.0724, 0.0000],
        [0.0760, 0.0681, 0.0803,  ..., 0.0000, 0.0723, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570043.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5603.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.3056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21013.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-484.3042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(94.3382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.0225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6760],
        [-2.8153],
        [-2.9366],
        ...,
        [-2.8689],
        [-2.8625],
        [-2.8588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287576.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0121],
        [1.0187],
        ...,
        [1.0000],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368878.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0122],
        [1.0188],
        ...,
        [0.9999],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368883.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00],
        [1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00],
        [1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00],
        [1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00],
        [1.8755e-03, 5.2591e-04, 0.0000e+00,  ..., 4.9527e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2502.5728, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.2931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8763, device='cuda:0')



h[100].sum tensor(65.8628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5024, device='cuda:0')



h[200].sum tensor(-1.5238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.6916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.5586e-03, 2.1195e-03, 0.0000e+00,  ..., 1.9961e-05, 0.0000e+00,
         0.0000e+00],
        [7.5803e-03, 2.1256e-03, 0.0000e+00,  ..., 2.0018e-05, 0.0000e+00,
         0.0000e+00],
        [7.6088e-03, 2.1336e-03, 0.0000e+00,  ..., 2.0093e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.8253e-03, 2.1943e-03, 0.0000e+00,  ..., 2.0665e-05, 0.0000e+00,
         0.0000e+00],
        [7.8218e-03, 2.1934e-03, 0.0000e+00,  ..., 2.0656e-05, 0.0000e+00,
         0.0000e+00],
        [7.8166e-03, 2.1919e-03, 0.0000e+00,  ..., 2.0642e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60261.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0742, 0.0669, 0.0774,  ..., 0.0000, 0.0708, 0.0000],
        [0.0744, 0.0670, 0.0777,  ..., 0.0000, 0.0710, 0.0000],
        [0.0746, 0.0672, 0.0781,  ..., 0.0000, 0.0713, 0.0000],
        ...,
        [0.0767, 0.0688, 0.0808,  ..., 0.0000, 0.0732, 0.0000],
        [0.0766, 0.0687, 0.0808,  ..., 0.0000, 0.0732, 0.0000],
        [0.0766, 0.0687, 0.0807,  ..., 0.0000, 0.0732, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591171.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5717.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(420.7185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21275.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-483.9891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(288.2908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.3144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7522],
        [-2.9164],
        [-3.0225],
        ...,
        [-2.9060],
        [-2.8994],
        [-2.8963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317366.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0122],
        [1.0188],
        ...,
        [0.9999],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368883.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(465.1436, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0122],
        [1.0188],
        ...,
        [0.9999],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368889.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8281e-03,  5.2573e-04,  0.0000e+00,  ..., -1.0984e-07,
          0.0000e+00,  0.0000e+00],
        [ 8.6862e-03,  7.4465e-03, -4.9779e-05,  ...,  9.2456e-03,
         -2.9859e-03, -3.9383e-03],
        [ 6.9360e-03,  5.6803e-03, -3.7075e-05,  ...,  6.8861e-03,
         -2.2239e-03, -2.9333e-03],
        ...,
        [ 1.8281e-03,  5.2573e-04,  0.0000e+00,  ..., -1.0984e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.8281e-03,  5.2573e-04,  0.0000e+00,  ..., -1.0984e-07,
          0.0000e+00,  0.0000e+00],
        [ 1.8281e-03,  5.2573e-04,  0.0000e+00,  ..., -1.0984e-07,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2306.2510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6683, device='cuda:0')



h[100].sum tensor(62.9554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9384, device='cuda:0')



h[200].sum tensor(-1.2525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.7819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0091, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0224, 0.0173, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0593, 0.0545, 0.0000,  ..., 0.0699, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0076, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56533.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0252, 0.0397,  ..., 0.0000, 0.0449, 0.0000],
        [0.0636, 0.0098, 0.0202,  ..., 0.0000, 0.0232, 0.0000],
        [0.0542, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        ...,
        [0.0776, 0.0691, 0.0810,  ..., 0.0000, 0.0740, 0.0000],
        [0.0776, 0.0691, 0.0809,  ..., 0.0000, 0.0740, 0.0000],
        [0.0775, 0.0691, 0.0809,  ..., 0.0000, 0.0740, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574556.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5935.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.2259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20695.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-472.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(198.0947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.7695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1289],
        [ 0.0524],
        [ 0.1896],
        ...,
        [-2.9377],
        [-2.9307],
        [-2.9276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373744.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0122],
        [1.0188],
        ...,
        [0.9999],
        [0.9989],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368889.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0123],
        [1.0189],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368895., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.4449e-03,  7.2477e-03, -4.7629e-05,  ...,  8.9676e-03,
         -2.8916e-03, -3.8153e-03],
        [ 1.7921e-03,  5.3271e-04,  0.0000e+00,  ..., -2.5352e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7921e-03,  5.3271e-04,  0.0000e+00,  ..., -2.5352e-06,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7921e-03,  5.3271e-04,  0.0000e+00,  ..., -2.5352e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7921e-03,  5.3271e-04,  0.0000e+00,  ..., -2.5352e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7921e-03,  5.3271e-04,  0.0000e+00,  ..., -2.5352e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2780.1289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.8501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.2340, device='cuda:0')



h[100].sum tensor(66.1774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2370, device='cuda:0')



h[200].sum tensor(-1.9754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.2765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0273, 0.0225, 0.0000,  ..., 0.0271, 0.0000, 0.0000],
        [0.0434, 0.0386, 0.0000,  ..., 0.0487, 0.0000, 0.0000],
        [0.0374, 0.0325, 0.0000,  ..., 0.0406, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65504.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.0000, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0563, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0560, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        ...,
        [0.0782, 0.0692, 0.0811,  ..., 0.0000, 0.0746, 0.0000],
        [0.0782, 0.0692, 0.0810,  ..., 0.0000, 0.0745, 0.0000],
        [0.0782, 0.0691, 0.0810,  ..., 0.0000, 0.0745, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621289.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5979.5112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.1928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22002.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-495.6591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(432.3282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-480.6920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2204],
        [ 0.2040],
        [ 0.1881],
        ...,
        [-2.9581],
        [-2.9511],
        [-2.9479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332481.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0123],
        [1.0189],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368895., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0124],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368900.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1093e-02,  2.0083e-02, -1.3657e-04,  ...,  2.6084e-02,
         -8.3917e-03, -1.1076e-02],
        [ 2.9334e-02,  2.8403e-02, -1.9476e-04,  ...,  3.7197e-02,
         -1.1967e-02, -1.5796e-02],
        [ 2.1590e-02,  2.0584e-02, -1.4007e-04,  ...,  2.6753e-02,
         -8.6071e-03, -1.1361e-02],
        ...,
        [ 1.7534e-03,  5.5774e-04,  0.0000e+00,  ...,  2.9210e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7534e-03,  5.5774e-04,  0.0000e+00,  ...,  2.9210e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.7534e-03,  5.5774e-04,  0.0000e+00,  ...,  2.9210e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2808.6921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.5328, device='cuda:0')



h[100].sum tensor(65.6754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3480, device='cuda:0')



h[200].sum tensor(-2.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-35.6251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1026e-01, 1.0643e-01, 0.0000e+00,  ..., 1.3917e-01, 0.0000e+00,
         0.0000e+00],
        [9.7181e-02, 9.3210e-02, 0.0000e+00,  ..., 1.2151e-01, 0.0000e+00,
         0.0000e+00],
        [8.2077e-02, 7.7942e-02, 0.0000e+00,  ..., 1.0110e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.3178e-03, 2.3277e-03, 0.0000e+00,  ..., 1.2191e-05, 0.0000e+00,
         0.0000e+00],
        [7.3146e-03, 2.3266e-03, 0.0000e+00,  ..., 1.2185e-05, 0.0000e+00,
         0.0000e+00],
        [7.3098e-03, 2.3251e-03, 0.0000e+00,  ..., 1.2177e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67672.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0323, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0352, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0393, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0786, 0.0688, 0.0811,  ..., 0.0000, 0.0747, 0.0000],
        [0.0786, 0.0687, 0.0810,  ..., 0.0000, 0.0747, 0.0000],
        [0.0785, 0.0687, 0.0810,  ..., 0.0000, 0.0746, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627747.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5992.5435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.4120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22300.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.6764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(423.1245, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.9167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0352],
        [ 0.0351],
        [ 0.0402],
        ...,
        [-2.7237],
        [-2.6517],
        [-2.6480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321260.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0124],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368900.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0125],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368907.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00],
        [1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00],
        [1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00],
        [1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00],
        [1.7226e-03, 5.9054e-04, 0.0000e+00,  ..., 1.8175e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2753.7966, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(84.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.0055, device='cuda:0')



h[100].sum tensor(64.8020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7804, device='cuda:0')



h[200].sum tensor(-1.9035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.8431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9441e-03, 2.3806e-03, 0.0000e+00,  ..., 7.3268e-05, 0.0000e+00,
         0.0000e+00],
        [6.9641e-03, 2.3875e-03, 0.0000e+00,  ..., 7.3479e-05, 0.0000e+00,
         0.0000e+00],
        [6.9909e-03, 2.3967e-03, 0.0000e+00,  ..., 7.3762e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.1895e-03, 2.4647e-03, 0.0000e+00,  ..., 7.5857e-05, 0.0000e+00,
         0.0000e+00],
        [7.1863e-03, 2.4636e-03, 0.0000e+00,  ..., 7.5823e-05, 0.0000e+00,
         0.0000e+00],
        [7.1816e-03, 2.4620e-03, 0.0000e+00,  ..., 7.5774e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65412.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0201, 0.0303,  ..., 0.0000, 0.0425, 0.0000],
        [0.0699, 0.0189, 0.0278,  ..., 0.0000, 0.0409, 0.0000],
        [0.0694, 0.0146, 0.0226,  ..., 0.0000, 0.0376, 0.0000],
        ...,
        [0.0787, 0.0682, 0.0809,  ..., 0.0000, 0.0745, 0.0000],
        [0.0787, 0.0681, 0.0809,  ..., 0.0000, 0.0745, 0.0000],
        [0.0786, 0.0681, 0.0808,  ..., 0.0000, 0.0745, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608094.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6002.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(445.1758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22214.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-513.4277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(257.8518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-486.3736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0601],
        [ 0.0655],
        [ 0.0741],
        ...,
        [-2.9588],
        [-2.9520],
        [-2.9487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270517.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0125],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368907.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0126],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368913.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00],
        [1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00],
        [1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00],
        [1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00],
        [1.7007e-03, 5.9351e-04, 0.0000e+00,  ..., 1.1277e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2259.1846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(73.9131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9136, device='cuda:0')



h[100].sum tensor(60.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6579, device='cuda:0')



h[200].sum tensor(-1.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.9012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.8563e-03, 2.3927e-03, 0.0000e+00,  ..., 4.5462e-05, 0.0000e+00,
         0.0000e+00],
        [6.8760e-03, 2.3996e-03, 0.0000e+00,  ..., 4.5593e-05, 0.0000e+00,
         0.0000e+00],
        [1.4820e-02, 1.0402e-02, 0.0000e+00,  ..., 1.0722e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.0987e-03, 2.4773e-03, 0.0000e+00,  ..., 4.7069e-05, 0.0000e+00,
         0.0000e+00],
        [7.0955e-03, 2.4762e-03, 0.0000e+00,  ..., 4.7048e-05, 0.0000e+00,
         0.0000e+00],
        [7.0909e-03, 2.4746e-03, 0.0000e+00,  ..., 4.7018e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53281.3711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0761, 0.0639, 0.0761,  ..., 0.0000, 0.0714, 0.0000],
        [0.0750, 0.0516, 0.0669,  ..., 0.0000, 0.0654, 0.0000],
        [0.0715, 0.0256, 0.0404,  ..., 0.0000, 0.0478, 0.0000],
        ...,
        [0.0789, 0.0678, 0.0811,  ..., 0.0000, 0.0749, 0.0000],
        [0.0788, 0.0678, 0.0810,  ..., 0.0000, 0.0749, 0.0000],
        [0.0788, 0.0677, 0.0810,  ..., 0.0000, 0.0748, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549382.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6189.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(476.3456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20242.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-481.4978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(107.6815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.4885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2803],
        [-1.7278],
        [-1.0469],
        ...,
        [-2.9658],
        [-2.9592],
        [-2.9557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364500.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0126],
        [1.0190],
        ...,
        [0.9998],
        [0.9988],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368913.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0126],
        [1.0190],
        ...,
        [0.9997],
        [0.9987],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368919.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6767e-03,  5.9579e-04,  0.0000e+00,  ...,  4.2623e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6767e-03,  5.9579e-04,  0.0000e+00,  ...,  4.2623e-06,
          0.0000e+00,  0.0000e+00],
        [ 2.0303e-02,  1.9395e-02, -1.2612e-04,  ...,  2.5116e-02,
         -8.0371e-03, -1.0620e-02],
        ...,
        [ 1.6767e-03,  5.9579e-04,  0.0000e+00,  ...,  4.2623e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6767e-03,  5.9579e-04,  0.0000e+00,  ...,  4.2623e-06,
          0.0000e+00,  0.0000e+00],
        [ 1.6767e-03,  5.9579e-04,  0.0000e+00,  ...,  4.2623e-06,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2524.2952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.4426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.0522, device='cuda:0')



h[100].sum tensor(62.4263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9394, device='cuda:0')



h[200].sum tensor(-1.5379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.0636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.7600e-03, 2.4020e-03, 0.0000e+00,  ..., 1.7184e-05, 0.0000e+00,
         0.0000e+00],
        [2.5630e-02, 2.1434e-02, 0.0000e+00,  ..., 2.5431e-02, 0.0000e+00,
         0.0000e+00],
        [2.2236e-02, 1.7991e-02, 0.0000e+00,  ..., 2.0820e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9992e-03, 2.4870e-03, 0.0000e+00,  ..., 1.7792e-05, 0.0000e+00,
         0.0000e+00],
        [6.9960e-03, 2.4859e-03, 0.0000e+00,  ..., 1.7784e-05, 0.0000e+00,
         0.0000e+00],
        [6.9915e-03, 2.4843e-03, 0.0000e+00,  ..., 1.7773e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59544.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0744, 0.0429, 0.0605,  ..., 0.0000, 0.0629, 0.0000],
        [0.0698, 0.0169, 0.0340,  ..., 0.0000, 0.0409, 0.0000],
        [0.0681, 0.0049, 0.0171,  ..., 0.0000, 0.0312, 0.0000],
        ...,
        [0.0791, 0.0675, 0.0811,  ..., 0.0000, 0.0752, 0.0000],
        [0.0791, 0.0675, 0.0811,  ..., 0.0000, 0.0752, 0.0000],
        [0.0791, 0.0674, 0.0810,  ..., 0.0000, 0.0752, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581712.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6115.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.7020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21289.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-503.3762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(198.4774, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-467.1936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1276],
        [-1.4590],
        [-0.8272],
        ...,
        [-2.9679],
        [-2.9613],
        [-2.9589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327551.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0126],
        [1.0190],
        ...,
        [0.9997],
        [0.9987],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368919.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0127],
        [1.0190],
        ...,
        [0.9997],
        [0.9987],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368926.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6918e-03,  5.7708e-04,  0.0000e+00,  ..., -1.8181e-05,
          0.0000e+00,  0.0000e+00],
        [ 8.0730e-03,  7.0130e-03, -4.2590e-05,  ...,  8.5799e-03,
         -2.7472e-03, -3.6313e-03],
        [ 7.4355e-03,  6.3700e-03, -3.8335e-05,  ...,  7.7209e-03,
         -2.4727e-03, -3.2685e-03],
        ...,
        [ 1.6918e-03,  5.7708e-04,  0.0000e+00,  ..., -1.8181e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6918e-03,  5.7708e-04,  0.0000e+00,  ..., -1.8181e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6918e-03,  5.7708e-04,  0.0000e+00,  ..., -1.8181e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2411.8477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.4622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2418, device='cuda:0')



h[100].sum tensor(61.8763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8949, device='cuda:0')



h[200].sum tensor(-1.3508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0088, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0179, 0.0135, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0352, 0.0309, 0.0000,  ..., 0.0380, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57941.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0688, 0.0156, 0.0287,  ..., 0.0000, 0.0397, 0.0000],
        [0.0651, 0.0073, 0.0133,  ..., 0.0000, 0.0242, 0.0000],
        [0.0615, 0.0000, 0.0013,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0790, 0.0675, 0.0814,  ..., 0.0000, 0.0757, 0.0000],
        [0.0790, 0.0674, 0.0814,  ..., 0.0000, 0.0756, 0.0000],
        [0.0790, 0.0674, 0.0813,  ..., 0.0000, 0.0756, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577995.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6080.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(467.9567, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21084.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-501.1449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(187.1576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.6763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0603],
        [ 0.1009],
        [ 0.1922],
        ...,
        [-2.9831],
        [-2.9764],
        [-2.9729]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340061.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0127],
        [1.0190],
        ...,
        [0.9997],
        [0.9987],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368926.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0128],
        [1.0190],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368932.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7669e-02,  2.6723e-02, -1.7079e-04,  ...,  3.4920e-02,
         -1.1152e-02, -1.4746e-02],
        [ 3.6753e-02,  3.5878e-02, -2.3055e-04,  ...,  4.7152e-02,
         -1.5054e-02, -1.9905e-02],
        [ 2.8695e-02,  2.7757e-02, -1.7754e-04,  ...,  3.6301e-02,
         -1.1592e-02, -1.5328e-02],
        ...,
        [ 1.7071e-03,  5.5564e-04,  0.0000e+00,  ..., -4.0365e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7071e-03,  5.5564e-04,  0.0000e+00,  ..., -4.0365e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7071e-03,  5.5564e-04,  0.0000e+00,  ..., -4.0365e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2716.1238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.0019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7660, device='cuda:0')



h[100].sum tensor(64.7643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3197, device='cuda:0')



h[200].sum tensor(-1.7685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.3969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1254, 0.1217, 0.0000,  ..., 0.1594, 0.0000, 0.0000],
        [0.1404, 0.1368, 0.0000,  ..., 0.1796, 0.0000, 0.0000],
        [0.1384, 0.1347, 0.0000,  ..., 0.1768, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65111.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0217, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0169, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0791, 0.0676, 0.0817,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0675, 0.0817,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0675, 0.0816,  ..., 0.0000, 0.0761, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620457.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5963.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(442.2887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22099.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.7560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(424.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.1704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0698],
        [-0.0797],
        [-0.0902],
        ...,
        [-2.9956],
        [-2.9888],
        [-2.9851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321160.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0128],
        [1.0190],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368932.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0129],
        [1.0190],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368938.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7204e-03,  5.3930e-04,  0.0000e+00,  ..., -5.7731e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2476.3752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.4578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.4467, device='cuda:0')



h[100].sum tensor(62.9416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3427, device='cuda:0')



h[200].sum tensor(-1.4063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.1904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0113, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0115, 0.0067, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0136, 0.0088, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58690.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0238, 0.0484,  ..., 0.0000, 0.0492, 0.0000],
        [0.0712, 0.0303, 0.0532,  ..., 0.0000, 0.0529, 0.0000],
        [0.0706, 0.0230, 0.0482,  ..., 0.0000, 0.0491, 0.0000],
        ...,
        [0.0791, 0.0677, 0.0821,  ..., 0.0000, 0.0766, 0.0000],
        [0.0791, 0.0677, 0.0820,  ..., 0.0000, 0.0765, 0.0000],
        [0.0790, 0.0677, 0.0820,  ..., 0.0000, 0.0765, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580899.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6014.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(463.8265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21169.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-508.1613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(177.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.0421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9880],
        [-0.8260],
        [-0.6445],
        ...,
        [-3.0044],
        [-2.9981],
        [-2.9950]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337352.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0129],
        [1.0190],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368938.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0130],
        [1.0191],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368944.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2980e-02,  1.1873e-02, -7.1936e-05,  ...,  1.5077e-02,
         -4.8135e-03, -6.3694e-03],
        [ 9.6189e-03,  8.4879e-03, -5.0453e-05,  ...,  1.0554e-02,
         -3.3759e-03, -4.4672e-03],
        [ 1.7258e-03,  5.3851e-04,  0.0000e+00,  ..., -6.6954e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7258e-03,  5.3851e-04,  0.0000e+00,  ..., -6.6954e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7258e-03,  5.3851e-04,  0.0000e+00,  ..., -6.6954e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7258e-03,  5.3851e-04,  0.0000e+00,  ..., -6.6954e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2588.1001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.6783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.4582, device='cuda:0')



h[100].sum tensor(64.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0903, device='cuda:0')



h[200].sum tensor(-1.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.5373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0516, 0.0472, 0.0000,  ..., 0.0599, 0.0000, 0.0000],
        [0.0249, 0.0202, 0.0000,  ..., 0.0239, 0.0000, 0.0000],
        [0.0150, 0.0102, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60181.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0629, 0.0029, 0.0127,  ..., 0.0000, 0.0199, 0.0000],
        [0.0678, 0.0135, 0.0298,  ..., 0.0000, 0.0389, 0.0000],
        ...,
        [0.0790, 0.0678, 0.0823,  ..., 0.0000, 0.0766, 0.0000],
        [0.0790, 0.0678, 0.0822,  ..., 0.0000, 0.0766, 0.0000],
        [0.0790, 0.0677, 0.0821,  ..., 0.0000, 0.0765, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584370., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5956.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(455.9511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21272.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.6885, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(179.2058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-471.2696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1223],
        [ 0.0423],
        [-0.1334],
        ...,
        [-3.0114],
        [-3.0047],
        [-3.0010]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324370.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0130],
        [1.0191],
        ...,
        [0.9996],
        [0.9986],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368944.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(483.7685, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0192],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368950., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7269e-03,  5.4640e-04,  0.0000e+00,  ..., -7.1536e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2467.0220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(78.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3430, device='cuda:0')



h[100].sum tensor(63.2097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9325, device='cuda:0')



h[200].sum tensor(-1.3463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.9027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57695.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0764, 0.0660, 0.0790,  ..., 0.0000, 0.0739, 0.0000],
        [0.0766, 0.0661, 0.0792,  ..., 0.0000, 0.0741, 0.0000],
        [0.0766, 0.0640, 0.0781,  ..., 0.0000, 0.0732, 0.0000],
        ...,
        [0.0790, 0.0678, 0.0824,  ..., 0.0000, 0.0765, 0.0000],
        [0.0790, 0.0677, 0.0824,  ..., 0.0000, 0.0765, 0.0000],
        [0.0790, 0.0677, 0.0823,  ..., 0.0000, 0.0764, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574173.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5967.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.6398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20904., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-507.2249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(166.4692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.0599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9372],
        [-2.7512],
        [-2.4431],
        ...,
        [-2.7217],
        [-2.9130],
        [-2.9726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340985.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0192],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368950., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0193],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368955.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2887.3354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(86.0931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.8579, device='cuda:0')



h[100].sum tensor(66.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4689, device='cuda:0')



h[200].sum tensor(-1.9071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-36.0045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67449.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0715, 0.0300, 0.0529,  ..., 0.0000, 0.0540, 0.0000],
        [0.0739, 0.0482, 0.0665,  ..., 0.0000, 0.0626, 0.0000],
        [0.0751, 0.0545, 0.0714,  ..., 0.0000, 0.0660, 0.0000],
        ...,
        [0.0791, 0.0677, 0.0825,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0677, 0.0825,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0676, 0.0824,  ..., 0.0000, 0.0760, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625242., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5788.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.7746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22409.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.1844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(377.0963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.7018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3651],
        [-0.7584],
        [-1.0900],
        ...,
        [-3.0097],
        [-2.9938],
        [-2.9454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285450.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0193],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368955.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0193],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368955.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4705e-02,  1.3641e-02, -8.0658e-05,  ...,  1.7397e-02,
         -5.5313e-03, -7.3245e-03],
        [ 9.4929e-03,  8.3920e-03, -4.8279e-05,  ...,  1.0385e-02,
         -3.3108e-03, -4.3841e-03],
        [ 1.9023e-02,  1.7988e-02, -1.0748e-04,  ...,  2.3206e-02,
         -7.3707e-03, -9.7602e-03],
        ...,
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7212e-03,  5.6619e-04,  0.0000e+00,  ..., -7.0657e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3021.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(88.7940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.2581, device='cuda:0')



h[100].sum tensor(67.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3609, device='cuda:0')



h[200].sum tensor(-2.0937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.8049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0385, 0.0341, 0.0000,  ..., 0.0422, 0.0000, 0.0000],
        [0.0495, 0.0451, 0.0000,  ..., 0.0570, 0.0000, 0.0000],
        [0.0292, 0.0247, 0.0000,  ..., 0.0297, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69426.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0572, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0559, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0594, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0791, 0.0677, 0.0825,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0677, 0.0825,  ..., 0.0000, 0.0761, 0.0000],
        [0.0790, 0.0676, 0.0824,  ..., 0.0000, 0.0760, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633080.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5792.2397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(423.4568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22427.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-537.8784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(473.7143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-517.3744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2241],
        [ 0.2520],
        [ 0.2481],
        ...,
        [-2.9767],
        [-2.9565],
        [-2.9469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312488.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0131],
        [1.0193],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368955.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0132],
        [1.0194],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368961.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8485e-03,  6.7723e-03, -3.7634e-05,  ...,  8.1997e-03,
         -2.6128e-03, -3.4611e-03],
        [ 1.7044e-03,  5.8375e-04,  0.0000e+00,  ..., -6.7659e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7044e-03,  5.8375e-04,  0.0000e+00,  ..., -6.7659e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7044e-03,  5.8375e-04,  0.0000e+00,  ..., -6.7659e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7044e-03,  5.8375e-04,  0.0000e+00,  ..., -6.7659e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7044e-03,  5.8375e-04,  0.0000e+00,  ..., -6.7659e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2423.6768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.1062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1707, device='cuda:0')



h[100].sum tensor(62.5457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4968, device='cuda:0')



h[200].sum tensor(-1.2496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.5347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0278, 0.0234, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0131, 0.0086, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0069, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57401.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0053, 0.0104,  ..., 0.0000, 0.0177, 0.0000],
        [0.0680, 0.0196, 0.0300,  ..., 0.0000, 0.0379, 0.0000],
        [0.0736, 0.0381, 0.0592,  ..., 0.0000, 0.0595, 0.0000],
        ...,
        [0.0794, 0.0677, 0.0826,  ..., 0.0000, 0.0760, 0.0000],
        [0.0794, 0.0677, 0.0825,  ..., 0.0000, 0.0760, 0.0000],
        [0.0793, 0.0676, 0.0825,  ..., 0.0000, 0.0759, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570338.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5983.4209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(471.9055, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21179.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-513.5322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(97.7291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.6647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0841],
        [-0.1030],
        [-0.4751],
        ...,
        [-3.0160],
        [-3.0093],
        [-3.0061]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281241.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0132],
        [1.0194],
        ...,
        [0.9995],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368961.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0133],
        [1.0195],
        ...,
        [0.9994],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368966.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6855e-03,  5.8850e-04,  0.0000e+00,  ..., -6.9273e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6855e-03,  5.8850e-04,  0.0000e+00,  ..., -6.9273e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.3652e-02,  1.2645e-02, -7.2271e-05,  ...,  1.6035e-02,
         -5.0799e-03, -6.7317e-03],
        ...,
        [ 1.6855e-03,  5.8850e-04,  0.0000e+00,  ..., -6.9273e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6855e-03,  5.8850e-04,  0.0000e+00,  ..., -6.9273e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6855e-03,  5.8850e-04,  0.0000e+00,  ..., -6.9273e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2342.6946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6205, device='cuda:0')



h[100].sum tensor(61.0811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9206, device='cuda:0')



h[200].sum tensor(-1.1444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.7260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0239, 0.0196, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0229, 0.0185, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0025, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55456.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0722, 0.0216, 0.0463,  ..., 0.0000, 0.0519, 0.0000],
        [0.0686, 0.0128, 0.0295,  ..., 0.0000, 0.0372, 0.0000],
        [0.0658, 0.0077, 0.0170,  ..., 0.0000, 0.0253, 0.0000],
        ...,
        [0.0801, 0.0679, 0.0828,  ..., 0.0000, 0.0764, 0.0000],
        [0.0801, 0.0679, 0.0827,  ..., 0.0000, 0.0764, 0.0000],
        [0.0800, 0.0679, 0.0827,  ..., 0.0000, 0.0763, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566230.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6165.7373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(472.4742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20704.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.5110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(147.3971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-452.4011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2308],
        [-0.1546],
        [ 0.0257],
        ...,
        [-3.0106],
        [-3.0186],
        [-3.0174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349949.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0133],
        [1.0195],
        ...,
        [0.9994],
        [0.9985],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368966.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0134],
        [1.0196],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368971.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6790e-03,  5.8341e-04,  0.0000e+00,  ..., -7.4633e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2541.2598, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.9680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.4418, device='cuda:0')



h[100].sum tensor(62.0727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7126, device='cuda:0')



h[200].sum tensor(-1.4202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.3515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59038.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0774, 0.0615, 0.0759,  ..., 0.0000, 0.0715, 0.0000],
        [0.0775, 0.0617, 0.0763,  ..., 0.0000, 0.0714, 0.0000],
        [0.0774, 0.0589, 0.0745,  ..., 0.0000, 0.0701, 0.0000],
        ...,
        [0.0803, 0.0646, 0.0803,  ..., 0.0000, 0.0752, 0.0000],
        [0.0807, 0.0683, 0.0830,  ..., 0.0000, 0.0769, 0.0000],
        [0.0807, 0.0682, 0.0829,  ..., 0.0000, 0.0768, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585229.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6250.9155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.3199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21081.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-507.8892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(242.6317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-472.5995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6885],
        [-1.6255],
        [-1.3555],
        ...,
        [-2.8211],
        [-2.9697],
        [-3.0301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-360412.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0134],
        [1.0196],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368971.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0135],
        [1.0197],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368976.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6663e-03,  5.8151e-04,  0.0000e+00,  ..., -7.5543e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2646.2419, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.9854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9582, device='cuda:0')



h[100].sum tensor(61.8533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6478, device='cuda:0')



h[200].sum tensor(-1.5781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.2875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0067, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62178.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0783, 0.0645, 0.0780,  ..., 0.0000, 0.0730, 0.0000],
        [0.0788, 0.0663, 0.0794,  ..., 0.0000, 0.0744, 0.0000],
        [0.0793, 0.0673, 0.0805,  ..., 0.0000, 0.0754, 0.0000],
        ...,
        [0.0815, 0.0688, 0.0833,  ..., 0.0000, 0.0775, 0.0000],
        [0.0815, 0.0688, 0.0833,  ..., 0.0000, 0.0774, 0.0000],
        [0.0814, 0.0688, 0.0832,  ..., 0.0000, 0.0774, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598893.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6350.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.7034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21488.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-513.7468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(260.0004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-486.3780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8291],
        [-2.2990],
        [-2.6232],
        ...,
        [-3.0852],
        [-3.0784],
        [-3.0748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349342.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0135],
        [1.0197],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368976.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0137],
        [1.0199],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368980.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6651e-03,  5.7829e-04,  0.0000e+00,  ..., -7.6060e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2457.0771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(76.3372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0554, device='cuda:0')



h[100].sum tensor(59.5892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1973, device='cuda:0')



h[200].sum tensor(-1.3269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0067, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0068, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57715.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0791, 0.0675, 0.0800,  ..., 0.0000, 0.0752, 0.0000],
        [0.0794, 0.0676, 0.0803,  ..., 0.0000, 0.0754, 0.0000],
        [0.0797, 0.0678, 0.0807,  ..., 0.0000, 0.0757, 0.0000],
        ...,
        [0.0819, 0.0693, 0.0835,  ..., 0.0000, 0.0778, 0.0000],
        [0.0819, 0.0693, 0.0835,  ..., 0.0000, 0.0778, 0.0000],
        [0.0818, 0.0693, 0.0834,  ..., 0.0000, 0.0777, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577524.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6460.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(477.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20994.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-501.8096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(86.9830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-461.2007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1819],
        [-3.0823],
        [-2.9035],
        ...,
        [-3.1042],
        [-3.0968],
        [-3.0933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-353262.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0137],
        [1.0199],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368980.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0138],
        [1.0200],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368985.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6709e-03,  5.7010e-04,  0.0000e+00,  ..., -7.6201e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.4256e-02,  1.3273e-02, -7.1839e-05,  ...,  1.6882e-02,
         -5.3066e-03, -7.0423e-03],
        [ 1.5040e-02,  1.4065e-02, -7.6316e-05,  ...,  1.7939e-02,
         -5.6374e-03, -7.4812e-03],
        ...,
        [ 1.6709e-03,  5.7010e-04,  0.0000e+00,  ..., -7.6201e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6709e-03,  5.7010e-04,  0.0000e+00,  ..., -7.6201e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.6709e-03,  5.7010e-04,  0.0000e+00,  ..., -7.6201e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2650.4568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(80.4814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.4794, device='cuda:0')



h[100].sum tensor(60.9646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8415, device='cuda:0')



h[200].sum tensor(-1.5828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.8957, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0151, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        [0.0307, 0.0265, 0.0000,  ..., 0.0321, 0.0000, 0.0000],
        [0.0783, 0.0745, 0.0000,  ..., 0.0960, 0.0000, 0.0000],
        ...,
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62399.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0722, 0.0236, 0.0393,  ..., 0.0000, 0.0440, 0.0000],
        [0.0661, 0.0061, 0.0177,  ..., 0.0000, 0.0235, 0.0000],
        [0.0555, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        ...,
        [0.0820, 0.0697, 0.0838,  ..., 0.0000, 0.0781, 0.0000],
        [0.0820, 0.0697, 0.0837,  ..., 0.0000, 0.0780, 0.0000],
        [0.0819, 0.0697, 0.0837,  ..., 0.0000, 0.0780, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608358.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6405.0483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.1221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21664.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-510.6331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(235.8541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.3286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1975],
        [-0.7512],
        [-0.2496],
        ...,
        [-3.1180],
        [-3.1106],
        [-3.1070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344643.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0138],
        [1.0200],
        ...,
        [0.9994],
        [0.9984],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368985.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0139],
        [1.0201],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368991.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7002e-03,  5.5781e-04,  0.0000e+00,  ..., -7.6149e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2946.3857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(87.2194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.8595, device='cuda:0')



h[100].sum tensor(63.9557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2128, device='cuda:0')



h[200].sum tensor(-1.9546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-38.3398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0069, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70641.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0788, 0.0681, 0.0805,  ..., 0.0000, 0.0754, 0.0000],
        [0.0788, 0.0664, 0.0794,  ..., 0.0000, 0.0745, 0.0000],
        [0.0780, 0.0589, 0.0744,  ..., 0.0000, 0.0701, 0.0000],
        ...,
        [0.0816, 0.0700, 0.0840,  ..., 0.0000, 0.0780, 0.0000],
        [0.0816, 0.0700, 0.0840,  ..., 0.0000, 0.0780, 0.0000],
        [0.0815, 0.0699, 0.0839,  ..., 0.0000, 0.0779, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649755.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6172.1694, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.3697, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22739.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-533.3954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(434.1716, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-517.6224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9268],
        [-1.7541],
        [-1.3384],
        ...,
        [-3.1065],
        [-3.1052],
        [-3.1033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307673.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0139],
        [1.0201],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368991.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(438.8774, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0140],
        [1.0201],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368997.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7371e-03,  5.2706e-04,  0.0000e+00,  ..., -8.3043e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2604.2900, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.5168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.0956, device='cuda:0')



h[100].sum tensor(62.2493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3272, device='cuda:0')



h[200].sum tensor(-1.4837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63748.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0781, 0.0681, 0.0808,  ..., 0.0000, 0.0755, 0.0000],
        [0.0783, 0.0683, 0.0810,  ..., 0.0000, 0.0758, 0.0000],
        [0.0787, 0.0685, 0.0816,  ..., 0.0000, 0.0761, 0.0000],
        ...,
        [0.0808, 0.0700, 0.0844,  ..., 0.0000, 0.0782, 0.0000],
        [0.0808, 0.0700, 0.0843,  ..., 0.0000, 0.0782, 0.0000],
        [0.0807, 0.0700, 0.0842,  ..., 0.0000, 0.0781, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628425.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6099.4087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(437.8977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21662.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.3771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(466.2078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.6327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5454],
        [-2.7198],
        [-2.8275],
        ...,
        [-3.1222],
        [-3.1150],
        [-3.1112]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-381747.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0140],
        [1.0201],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368997.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0141],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369003.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7767e-03,  5.0673e-04,  0.0000e+00,  ..., -8.6694e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2651.3872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(83.5187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.5338, device='cuda:0')



h[100].sum tensor(64.0357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4901, device='cuda:0')



h[200].sum tensor(-1.5175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.7924, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0020, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62031.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0771, 0.0671, 0.0805,  ..., 0.0000, 0.0744, 0.0000],
        [0.0764, 0.0614, 0.0767,  ..., 0.0000, 0.0708, 0.0000],
        [0.0744, 0.0450, 0.0652,  ..., 0.0000, 0.0623, 0.0000],
        ...,
        [0.0799, 0.0699, 0.0847,  ..., 0.0000, 0.0780, 0.0000],
        [0.0799, 0.0698, 0.0846,  ..., 0.0000, 0.0780, 0.0000],
        [0.0798, 0.0698, 0.0845,  ..., 0.0000, 0.0779, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594760.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5807.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(448.7876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21488.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-521.8723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(113.9533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.5371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2021],
        [-1.7585],
        [-1.2278],
        ...,
        [-3.1148],
        [-3.1080],
        [-3.1041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308412.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0141],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369003.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0142],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369009.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5288e-02,  1.4099e-02, -7.2629e-05,  ...,  1.8074e-02,
         -5.6404e-03, -7.4961e-03],
        [ 2.6576e-02,  2.5476e-02, -1.3340e-04,  ...,  3.3265e-02,
         -1.0360e-02, -1.3768e-02],
        [ 3.1768e-02,  3.0709e-02, -1.6135e-04,  ...,  4.0253e-02,
         -1.2530e-02, -1.6653e-02],
        ...,
        [ 1.7964e-03,  5.0237e-04,  0.0000e+00,  ..., -8.2920e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7964e-03,  5.0237e-04,  0.0000e+00,  ..., -8.2920e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7964e-03,  5.0237e-04,  0.0000e+00,  ..., -8.2920e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2522.8184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(81.3997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.6030, device='cuda:0')



h[100].sum tensor(64.0626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4008, device='cuda:0')



h[200].sum tensor(-1.3265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.3728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0947, 0.0902, 0.0000,  ..., 0.1174, 0.0000, 0.0000],
        [0.1234, 0.1191, 0.0000,  ..., 0.1560, 0.0000, 0.0000],
        [0.1234, 0.1190, 0.0000,  ..., 0.1559, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60127.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0224, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0146, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0793, 0.0696, 0.0848,  ..., 0.0000, 0.0778, 0.0000],
        [0.0793, 0.0696, 0.0848,  ..., 0.0000, 0.0778, 0.0000],
        [0.0792, 0.0696, 0.0847,  ..., 0.0000, 0.0777, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591195.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5657.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(446.3778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21309.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(130.0055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-457.9587, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1803],
        [ 0.1668],
        [ 0.1642],
        ...,
        [-3.1037],
        [-3.0966],
        [-3.0927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313846., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0142],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369009.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0143],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369015., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3507e-02,  1.2310e-02, -6.2105e-05,  ...,  1.5679e-02,
         -4.8841e-03, -6.4934e-03],
        [ 1.1303e-02,  1.0089e-02, -5.0411e-05,  ...,  1.2713e-02,
         -3.9645e-03, -5.2708e-03],
        [ 2.3396e-02,  2.2275e-02, -1.1457e-04,  ...,  2.8985e-02,
         -9.0100e-03, -1.1979e-02],
        ...,
        [ 1.8011e-03,  5.1480e-04,  0.0000e+00,  ..., -7.2545e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8011e-03,  5.1480e-04,  0.0000e+00,  ..., -7.2545e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.8011e-03,  5.1480e-04,  0.0000e+00,  ..., -7.2545e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2178.8523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.4873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8564, device='cuda:0')



h[100].sum tensor(62.0213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5217, device='cuda:0')



h[200].sum tensor(-0.8604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.3342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.0214, 0.0000,  ..., 0.0257, 0.0000, 0.0000],
        [0.0682, 0.0635, 0.0000,  ..., 0.0817, 0.0000, 0.0000],
        [0.0707, 0.0660, 0.0000,  ..., 0.0851, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51578.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0626, 0.0089, 0.0226,  ..., 0.0000, 0.0254, 0.0000],
        [0.0515, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0474, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0789, 0.0695, 0.0848,  ..., 0.0000, 0.0773, 0.0000],
        [0.0789, 0.0694, 0.0848,  ..., 0.0000, 0.0773, 0.0000],
        [0.0788, 0.0694, 0.0847,  ..., 0.0000, 0.0772, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543939.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5722.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(465.4174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19904.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.6266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(27.5668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-420.2018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9015],
        [-0.2357],
        [ 0.1344],
        ...,
        [-3.0925],
        [-3.0861],
        [-3.0829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-360526., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0143],
        [1.0202],
        ...,
        [0.9993],
        [0.9983],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369015., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0144],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369020.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9049e-03,  6.6903e-03, -3.1971e-05,  ...,  8.1663e-03,
         -2.5462e-03, -3.3864e-03],
        [ 1.7907e-03,  5.2910e-04,  0.0000e+00,  ..., -6.0755e-05,
          0.0000e+00,  0.0000e+00],
        [ 7.9049e-03,  6.6903e-03, -3.1971e-05,  ...,  8.1663e-03,
         -2.5462e-03, -3.3864e-03],
        ...,
        [ 1.7907e-03,  5.2910e-04,  0.0000e+00,  ..., -6.0755e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7907e-03,  5.2910e-04,  0.0000e+00,  ..., -6.0755e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7907e-03,  5.2910e-04,  0.0000e+00,  ..., -6.0755e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2465.5415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.7759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.0898, device='cuda:0')



h[100].sum tensor(64.6437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8384, device='cuda:0')



h[200].sum tensor(-1.2187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.6072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0072, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0297, 0.0248, 0.0000,  ..., 0.0300, 0.0000, 0.0000],
        [0.0123, 0.0073, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0075, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57429.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0720, 0.0409, 0.0623,  ..., 0.0000, 0.0589, 0.0000],
        [0.0690, 0.0214, 0.0487,  ..., 0.0000, 0.0478, 0.0000],
        [0.0725, 0.0409, 0.0629,  ..., 0.0000, 0.0593, 0.0000],
        ...,
        [0.0790, 0.0693, 0.0847,  ..., 0.0000, 0.0771, 0.0000],
        [0.0789, 0.0693, 0.0847,  ..., 0.0000, 0.0771, 0.0000],
        [0.0789, 0.0693, 0.0846,  ..., 0.0000, 0.0770, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568816.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5637.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.6590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20711.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.6372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(128.3621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.8666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0270],
        [-2.1014],
        [-2.3315],
        ...,
        [-3.0897],
        [-3.0835],
        [-3.0801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332387.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0144],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369020.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0145],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369026.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7725e-03,  5.4394e-04,  0.0000e+00,  ..., -4.9077e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.0183e-02,  9.0188e-03, -4.3343e-05,  ...,  1.1267e-02,
         -3.4957e-03, -4.6510e-03],
        [ 3.3103e-02,  3.2115e-02, -1.6147e-04,  ...,  4.2107e-02,
         -1.3023e-02, -1.7326e-02],
        ...,
        [ 1.7725e-03,  5.4394e-04,  0.0000e+00,  ..., -4.9077e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7725e-03,  5.4394e-04,  0.0000e+00,  ..., -4.9077e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7725e-03,  5.4394e-04,  0.0000e+00,  ..., -4.9077e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3859.1667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(106.7315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-52.2777, device='cuda:0')



h[100].sum tensor(76.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.4299, device='cuda:0')



h[200].sum tensor(-2.9763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-60.9967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0296, 0.0248, 0.0000,  ..., 0.0301, 0.0000, 0.0000],
        [0.0732, 0.0688, 0.0000,  ..., 0.0887, 0.0000, 0.0000],
        [0.0959, 0.0916, 0.0000,  ..., 0.1191, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91080.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0060, 0.0112,  ..., 0.0000, 0.0142, 0.0000],
        [0.0276, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0792, 0.0691, 0.0846,  ..., 0.0000, 0.0769, 0.0000],
        [0.0792, 0.0691, 0.0846,  ..., 0.0000, 0.0769, 0.0000],
        [0.0791, 0.0691, 0.0845,  ..., 0.0000, 0.0768, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(770372.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5303.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(336.7901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25344.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-593.2484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1313.2064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-623.7052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0483],
        [ 0.0287],
        [-0.0135],
        ...,
        [-3.0889],
        [-3.0827],
        [-3.0792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295807.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0145],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369026.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0146],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369031.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7429e-03,  5.6064e-04,  0.0000e+00,  ..., -3.5102e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2268.8579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(74.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8680, device='cuda:0')



h[100].sum tensor(62.8531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2693, device='cuda:0')



h[200].sum tensor(-0.9622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6813, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53916.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0771, 0.0672, 0.0808,  ..., 0.0000, 0.0743, 0.0000],
        [0.0773, 0.0674, 0.0811,  ..., 0.0000, 0.0745, 0.0000],
        [0.0772, 0.0639, 0.0789,  ..., 0.0000, 0.0731, 0.0000],
        ...,
        [0.0797, 0.0691, 0.0844,  ..., 0.0000, 0.0769, 0.0000],
        [0.0797, 0.0690, 0.0843,  ..., 0.0000, 0.0769, 0.0000],
        [0.0796, 0.0690, 0.0843,  ..., 0.0000, 0.0768, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556005., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5817.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.4838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20319.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.8821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(128.9162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-440.3087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9279],
        [-2.7026],
        [-2.3147],
        ...,
        [-3.0963],
        [-3.0900],
        [-3.0868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-365223.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0146],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369031.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0147],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369036.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7210e-03,  5.7215e-04,  0.0000e+00,  ..., -2.4053e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2563.4348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(79.5225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.5995, device='cuda:0')



h[100].sum tensor(64.9429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7712, device='cuda:0')



h[200].sum tensor(-1.3312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0072, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59961.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0776, 0.0674, 0.0806,  ..., 0.0000, 0.0744, 0.0000],
        [0.0778, 0.0675, 0.0809,  ..., 0.0000, 0.0746, 0.0000],
        [0.0781, 0.0677, 0.0814,  ..., 0.0000, 0.0749, 0.0000],
        ...,
        [0.0802, 0.0692, 0.0842,  ..., 0.0000, 0.0770, 0.0000],
        [0.0802, 0.0692, 0.0841,  ..., 0.0000, 0.0769, 0.0000],
        [0.0801, 0.0691, 0.0840,  ..., 0.0000, 0.0769, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581798.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5794.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(459.0337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21388.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.7120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(160.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.9076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9520],
        [-3.0077],
        [-3.0025],
        ...,
        [-3.1078],
        [-3.1016],
        [-3.0982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303278.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0147],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369036.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369042., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4413e-02,  3.3548e-02, -1.6141e-04,  ...,  4.4005e-02,
         -1.3523e-02, -1.8012e-02],
        [ 2.3308e-02,  2.2353e-02, -1.0661e-04,  ...,  2.9059e-02,
         -8.9317e-03, -1.1897e-02],
        [ 1.2951e-02,  1.1913e-02, -5.5508e-05,  ...,  1.5122e-02,
         -4.6504e-03, -6.1941e-03],
        ...,
        [ 1.7019e-03,  5.7332e-04,  0.0000e+00,  ..., -1.6132e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7019e-03,  5.7332e-04,  0.0000e+00,  ..., -1.6132e-05,
          0.0000e+00,  0.0000e+00],
        [ 1.7019e-03,  5.7332e-04,  0.0000e+00,  ..., -1.6132e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3105.8623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(89.7116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-35.0608, device='cuda:0')



h[100].sum tensor(68.9638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0309, device='cuda:0')



h[200].sum tensor(-2.0049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-40.9082, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1047, 0.1009, 0.0000,  ..., 0.1316, 0.0000, 0.0000],
        [0.0785, 0.0746, 0.0000,  ..., 0.0964, 0.0000, 0.0000],
        [0.0470, 0.0428, 0.0000,  ..., 0.0539, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73365.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0353, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0444, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0807, 0.0694, 0.0841,  ..., 0.0000, 0.0772, 0.0000],
        [0.0807, 0.0694, 0.0841,  ..., 0.0000, 0.0772, 0.0000],
        [0.0806, 0.0693, 0.0840,  ..., 0.0000, 0.0772, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660989.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5663.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.3321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23469.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-560.0713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(493.4552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-533.8615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2340],
        [ 0.2507],
        [ 0.2709],
        ...,
        [-3.1258],
        [-3.1193],
        [-3.1160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270071.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0202],
        ...,
        [0.9992],
        [0.9982],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369042., device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:15.579533
evaluation loss: 492.8539733886719
epoch: 0 mean loss: 497.510986328125
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 117, in <module>
    writer.add_scalars(f'loss for {modelname}, Batchsize {BatchSize}, {TraEvN} training events, {evalbatchnum * BatchSize} evaluation events', \
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 397, in add_scalars
    fw = FileWriter(fw_tag, self.max_queue, self.flush_secs,
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 60, in __init__
    self.event_writer = EventFileWriter(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py", line 72, in __init__
    tf.io.gfile.makedirs(logdir)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 900, in makedirs
    return get_filesystem(path).makedirs(path)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 201, in makedirs
    os.makedirs(path, exist_ok=True)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/os.py", line 225, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: 'runs/Jul18_17-52-58_cmsgpu001.ihep.ac.cn/loss for DGLpppipiGcnReNewestweight7N2, Batchsize 5, 1998 training events, 120 evaluation events_training (Lr and weight_decay=0.0001,5e-05)'

real	1m7.637s
user	0m30.033s
sys	0m12.937s
