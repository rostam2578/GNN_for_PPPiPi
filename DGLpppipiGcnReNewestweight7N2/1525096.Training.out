0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:22:32 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   34C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aaff26ab910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.039s
user	0m2.321s
sys	0m0.833s
[02:22:38] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.0538],
        [ 0.2689],
        [ 0.9598],
        ...,
        [-0.8192],
        [-1.2730],
        [-0.3091]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-25.9871, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 1.0134e-01, -1.3346e-01,  1.1538e-01, -1.2118e-01,  6.9273e-02,
          2.7971e-02,  1.0145e-01,  1.0442e-01,  1.1087e-01, -8.3964e-02,
         -1.4542e-01,  9.0106e-02, -1.2227e-01,  1.2383e-01, -7.9154e-03,
          5.3948e-03,  1.0761e-01, -4.5528e-02,  1.3102e-01, -2.3727e-02,
         -1.4739e-01, -1.8990e-02,  5.5518e-02,  1.4887e-01,  6.0800e-05,
         -3.1117e-02,  7.4956e-02,  1.4596e-01, -1.4132e-01, -1.3068e-02,
          5.3682e-02, -1.0387e-01,  1.1426e-01,  3.1515e-02, -4.2104e-02,
         -1.4353e-01, -6.3810e-02,  8.8551e-02,  1.0728e-01, -1.1688e-01,
         -2.0807e-02,  1.0742e-01, -5.5394e-02, -1.0939e-01, -4.2361e-02,
          7.6825e-02, -1.3649e-01, -1.2498e-01, -1.6756e-02, -1.4069e-01,
         -1.2825e-01, -7.6698e-02,  1.2301e-01, -3.0992e-03, -1.5266e-01,
          5.0872e-02, -4.6297e-02, -1.0355e-01,  4.5482e-02, -1.0238e-01,
         -5.3988e-02,  1.4502e-01, -3.3871e-02, -5.6687e-02,  1.5239e-01,
         -1.1556e-01,  1.4800e-01, -4.6863e-02, -3.2165e-02,  2.5708e-02,
          5.9543e-02,  1.2904e-01, -1.0658e-01, -3.2175e-02, -1.4783e-01,
         -2.6742e-02,  2.3223e-03,  2.6043e-02,  6.9697e-02,  2.7436e-02,
          5.1971e-02,  7.8666e-02, -1.3616e-01,  8.4803e-02, -1.1559e-01,
          9.1478e-02,  1.3448e-01,  6.0818e-02, -1.1684e-01,  9.5332e-02,
          9.1998e-02,  1.1085e-01,  1.2373e-01,  3.7515e-02,  3.5196e-02,
         -8.9669e-02, -2.7837e-02, -1.1697e-02, -9.6469e-02, -2.5272e-02,
         -4.7954e-02,  3.6220e-02,  8.1333e-02,  1.4797e-01,  2.2277e-02,
         -1.2388e-01,  1.0487e-01, -3.6970e-02, -3.1468e-02,  1.0683e-01,
          1.5188e-02, -1.4229e-01, -4.5978e-02, -7.8735e-02,  9.3283e-02,
          6.4390e-02, -6.0116e-02,  8.3748e-02, -3.1803e-02,  7.3783e-02,
          9.8942e-02, -1.4607e-01,  1.0319e-01, -2.7773e-02, -1.2689e-01,
          3.2343e-03,  7.6813e-02, -1.3222e-01,  1.4914e-01,  1.0355e-01,
         -1.4496e-01,  2.3656e-02, -3.1177e-02, -1.1026e-01,  1.1580e-01,
          7.3229e-03,  1.5188e-02, -1.3747e-01, -1.1874e-01, -6.2173e-02,
         -1.3746e-01,  9.2264e-02, -1.2497e-01, -4.3429e-02, -8.4967e-03,
         -1.0470e-01, -2.5438e-02, -7.6839e-02,  1.0644e-01, -4.9178e-02,
         -1.4278e-01,  3.6118e-02, -9.9556e-02,  1.0090e-01,  4.7805e-02,
         -1.4081e-01,  1.2232e-01,  1.4763e-01,  1.3926e-02,  1.3604e-01,
          2.4772e-02,  1.5244e-01, -6.5324e-03, -1.4981e-01,  6.3468e-02,
          7.7845e-02,  4.1885e-02, -5.1955e-02, -3.1722e-03,  9.6385e-02,
          4.2019e-02, -1.3109e-01, -1.0047e-01, -1.0204e-01,  1.3043e-01,
          2.3613e-03,  6.0644e-02,  1.3514e-01,  1.4490e-01, -1.3225e-01,
         -5.7933e-02,  1.4487e-01,  8.1290e-02,  1.3954e-01, -1.0525e-01,
         -1.4665e-01, -2.5011e-02, -1.2279e-01, -2.4850e-02, -1.3207e-01,
          2.8018e-02,  9.5848e-02,  3.1973e-02, -6.8069e-02,  2.9912e-02,
          9.8928e-02, -6.8647e-02,  6.2801e-02,  8.3249e-02,  4.0401e-02,
         -6.6951e-02, -1.1564e-02,  1.2934e-01,  1.5158e-01,  8.0799e-02,
         -1.7571e-02,  1.3840e-01,  5.1570e-02, -5.3098e-02,  8.8057e-02,
         -1.2496e-01,  3.1391e-04,  1.1552e-01, -1.0335e-01, -1.1405e-01,
         -8.4458e-02, -9.1562e-02,  1.8900e-02,  1.1307e-01,  1.2942e-01,
         -2.3065e-02,  3.6807e-02,  1.3175e-01,  1.1308e-01, -1.3807e-01,
          1.1354e-01, -2.5829e-02,  2.3494e-02, -4.8630e-02, -9.0019e-02,
         -4.9069e-03, -1.3216e-01,  1.4699e-01,  2.6514e-02,  4.3748e-02,
          5.3469e-02,  1.4307e-01, -9.2300e-02, -1.3033e-01, -5.2427e-02,
         -6.0858e-02, -7.0629e-02,  5.2647e-02,  7.6548e-03, -6.5025e-02,
         -1.3142e-01, -2.3296e-02, -3.1235e-02, -1.8658e-02, -6.2570e-02,
          1.4316e-01,  2.3148e-02,  9.2244e-02,  2.3061e-02,  4.0642e-02,
          1.1480e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 1.0134e-01, -1.3346e-01,  1.1538e-01, -1.2118e-01,  6.9273e-02,
          2.7971e-02,  1.0145e-01,  1.0442e-01,  1.1087e-01, -8.3964e-02,
         -1.4542e-01,  9.0106e-02, -1.2227e-01,  1.2383e-01, -7.9154e-03,
          5.3948e-03,  1.0761e-01, -4.5528e-02,  1.3102e-01, -2.3727e-02,
         -1.4739e-01, -1.8990e-02,  5.5518e-02,  1.4887e-01,  6.0800e-05,
         -3.1117e-02,  7.4956e-02,  1.4596e-01, -1.4132e-01, -1.3068e-02,
          5.3682e-02, -1.0387e-01,  1.1426e-01,  3.1515e-02, -4.2104e-02,
         -1.4353e-01, -6.3810e-02,  8.8551e-02,  1.0728e-01, -1.1688e-01,
         -2.0807e-02,  1.0742e-01, -5.5394e-02, -1.0939e-01, -4.2361e-02,
          7.6825e-02, -1.3649e-01, -1.2498e-01, -1.6756e-02, -1.4069e-01,
         -1.2825e-01, -7.6698e-02,  1.2301e-01, -3.0992e-03, -1.5266e-01,
          5.0872e-02, -4.6297e-02, -1.0355e-01,  4.5482e-02, -1.0238e-01,
         -5.3988e-02,  1.4502e-01, -3.3871e-02, -5.6687e-02,  1.5239e-01,
         -1.1556e-01,  1.4800e-01, -4.6863e-02, -3.2165e-02,  2.5708e-02,
          5.9543e-02,  1.2904e-01, -1.0658e-01, -3.2175e-02, -1.4783e-01,
         -2.6742e-02,  2.3223e-03,  2.6043e-02,  6.9697e-02,  2.7436e-02,
          5.1971e-02,  7.8666e-02, -1.3616e-01,  8.4803e-02, -1.1559e-01,
          9.1478e-02,  1.3448e-01,  6.0818e-02, -1.1684e-01,  9.5332e-02,
          9.1998e-02,  1.1085e-01,  1.2373e-01,  3.7515e-02,  3.5196e-02,
         -8.9669e-02, -2.7837e-02, -1.1697e-02, -9.6469e-02, -2.5272e-02,
         -4.7954e-02,  3.6220e-02,  8.1333e-02,  1.4797e-01,  2.2277e-02,
         -1.2388e-01,  1.0487e-01, -3.6970e-02, -3.1468e-02,  1.0683e-01,
          1.5188e-02, -1.4229e-01, -4.5978e-02, -7.8735e-02,  9.3283e-02,
          6.4390e-02, -6.0116e-02,  8.3748e-02, -3.1803e-02,  7.3783e-02,
          9.8942e-02, -1.4607e-01,  1.0319e-01, -2.7773e-02, -1.2689e-01,
          3.2343e-03,  7.6813e-02, -1.3222e-01,  1.4914e-01,  1.0355e-01,
         -1.4496e-01,  2.3656e-02, -3.1177e-02, -1.1026e-01,  1.1580e-01,
          7.3229e-03,  1.5188e-02, -1.3747e-01, -1.1874e-01, -6.2173e-02,
         -1.3746e-01,  9.2264e-02, -1.2497e-01, -4.3429e-02, -8.4967e-03,
         -1.0470e-01, -2.5438e-02, -7.6839e-02,  1.0644e-01, -4.9178e-02,
         -1.4278e-01,  3.6118e-02, -9.9556e-02,  1.0090e-01,  4.7805e-02,
         -1.4081e-01,  1.2232e-01,  1.4763e-01,  1.3926e-02,  1.3604e-01,
          2.4772e-02,  1.5244e-01, -6.5324e-03, -1.4981e-01,  6.3468e-02,
          7.7845e-02,  4.1885e-02, -5.1955e-02, -3.1722e-03,  9.6385e-02,
          4.2019e-02, -1.3109e-01, -1.0047e-01, -1.0204e-01,  1.3043e-01,
          2.3613e-03,  6.0644e-02,  1.3514e-01,  1.4490e-01, -1.3225e-01,
         -5.7933e-02,  1.4487e-01,  8.1290e-02,  1.3954e-01, -1.0525e-01,
         -1.4665e-01, -2.5011e-02, -1.2279e-01, -2.4850e-02, -1.3207e-01,
          2.8018e-02,  9.5848e-02,  3.1973e-02, -6.8069e-02,  2.9912e-02,
          9.8928e-02, -6.8647e-02,  6.2801e-02,  8.3249e-02,  4.0401e-02,
         -6.6951e-02, -1.1564e-02,  1.2934e-01,  1.5158e-01,  8.0799e-02,
         -1.7571e-02,  1.3840e-01,  5.1570e-02, -5.3098e-02,  8.8057e-02,
         -1.2496e-01,  3.1391e-04,  1.1552e-01, -1.0335e-01, -1.1405e-01,
         -8.4458e-02, -9.1562e-02,  1.8900e-02,  1.1307e-01,  1.2942e-01,
         -2.3065e-02,  3.6807e-02,  1.3175e-01,  1.1308e-01, -1.3807e-01,
          1.1354e-01, -2.5829e-02,  2.3494e-02, -4.8630e-02, -9.0019e-02,
         -4.9069e-03, -1.3216e-01,  1.4699e-01,  2.6514e-02,  4.3748e-02,
          5.3469e-02,  1.4307e-01, -9.2300e-02, -1.3033e-01, -5.2427e-02,
         -6.0858e-02, -7.0629e-02,  5.2647e-02,  7.6548e-03, -6.5025e-02,
         -1.3142e-01, -2.3296e-02, -3.1235e-02, -1.8658e-02, -6.2570e-02,
          1.4316e-01,  2.3148e-02,  9.2244e-02,  2.3061e-02,  4.0642e-02,
          1.1480e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0777, -0.1018, -0.1088,  ...,  0.0896,  0.0720, -0.0350],
        [-0.1153, -0.0016, -0.0749,  ...,  0.0469, -0.0052, -0.0568],
        [ 0.0980,  0.0114, -0.0846,  ..., -0.0216, -0.0285, -0.0025],
        ...,
        [ 0.0973,  0.1121, -0.0726,  ...,  0.0638, -0.0857,  0.0530],
        [-0.1054,  0.0180,  0.0098,  ...,  0.0533, -0.0742,  0.1176],
        [-0.0510, -0.1056,  0.1212,  ..., -0.0376,  0.0244, -0.1237]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0777, -0.1018, -0.1088,  ...,  0.0896,  0.0720, -0.0350],
        [-0.1153, -0.0016, -0.0749,  ...,  0.0469, -0.0052, -0.0568],
        [ 0.0980,  0.0114, -0.0846,  ..., -0.0216, -0.0285, -0.0025],
        ...,
        [ 0.0973,  0.1121, -0.0726,  ...,  0.0638, -0.0857,  0.0530],
        [-0.1054,  0.0180,  0.0098,  ...,  0.0533, -0.0742,  0.1176],
        [-0.0510, -0.1056,  0.1212,  ..., -0.0376,  0.0244, -0.1237]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0910, -0.0414, -0.1167,  ..., -0.0804,  0.0356,  0.0042],
        [ 0.1678, -0.1148, -0.1387,  ...,  0.1702, -0.1362,  0.1377],
        [ 0.1430, -0.1193, -0.1397,  ...,  0.0154,  0.1687, -0.0650],
        ...,
        [ 0.1045, -0.1455, -0.0537,  ...,  0.1692, -0.0560, -0.1335],
        [ 0.1532,  0.0496,  0.1168,  ...,  0.1260, -0.0668,  0.0362],
        [ 0.1577,  0.1084, -0.1713,  ...,  0.0228, -0.0043, -0.1763]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0910, -0.0414, -0.1167,  ..., -0.0804,  0.0356,  0.0042],
        [ 0.1678, -0.1148, -0.1387,  ...,  0.1702, -0.1362,  0.1377],
        [ 0.1430, -0.1193, -0.1397,  ...,  0.0154,  0.1687, -0.0650],
        ...,
        [ 0.1045, -0.1455, -0.0537,  ...,  0.1692, -0.0560, -0.1335],
        [ 0.1532,  0.0496,  0.1168,  ...,  0.1260, -0.0668,  0.0362],
        [ 0.1577,  0.1084, -0.1713,  ...,  0.0228, -0.0043, -0.1763]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0683,  0.0573,  0.2068,  ...,  0.1673, -0.0005,  0.2132],
        [ 0.1773,  0.0204,  0.0779,  ..., -0.0495, -0.1547, -0.2316],
        [-0.0504,  0.1559,  0.1786,  ..., -0.0115, -0.0878,  0.1576],
        ...,
        [ 0.2094, -0.1733,  0.1848,  ..., -0.1268,  0.1957,  0.0893],
        [-0.1506,  0.2263, -0.1939,  ..., -0.0536,  0.2162,  0.2086],
        [ 0.2474,  0.2148,  0.1044,  ...,  0.0077,  0.1920,  0.0747]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0683,  0.0573,  0.2068,  ...,  0.1673, -0.0005,  0.2132],
        [ 0.1773,  0.0204,  0.0779,  ..., -0.0495, -0.1547, -0.2316],
        [-0.0504,  0.1559,  0.1786,  ..., -0.0115, -0.0878,  0.1576],
        ...,
        [ 0.2094, -0.1733,  0.1848,  ..., -0.1268,  0.1957,  0.0893],
        [-0.1506,  0.2263, -0.1939,  ..., -0.0536,  0.2162,  0.2086],
        [ 0.2474,  0.2148,  0.1044,  ...,  0.0077,  0.1920,  0.0747]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2086],
        [ 0.0114],
        [-0.1537],
        [-0.2323],
        [ 0.0475],
        [-0.1358],
        [ 0.2278],
        [-0.0198],
        [-0.0313],
        [ 0.2352],
        [-0.2263],
        [-0.2830],
        [-0.0150],
        [-0.0539],
        [ 0.3809],
        [-0.2276],
        [ 0.4224],
        [-0.3338],
        [ 0.2508],
        [ 0.2109],
        [ 0.3174],
        [ 0.0705],
        [ 0.0504],
        [-0.4009],
        [-0.1929],
        [ 0.2764],
        [ 0.2267],
        [ 0.0096],
        [ 0.2357],
        [ 0.2276],
        [ 0.1157],
        [-0.3608]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2086],
        [ 0.0114],
        [-0.1537],
        [-0.2323],
        [ 0.0475],
        [-0.1358],
        [ 0.2278],
        [-0.0198],
        [-0.0313],
        [ 0.2352],
        [-0.2263],
        [-0.2830],
        [-0.0150],
        [-0.0539],
        [ 0.3809],
        [-0.2276],
        [ 0.4224],
        [-0.3338],
        [ 0.2508],
        [ 0.2109],
        [ 0.3174],
        [ 0.0705],
        [ 0.0504],
        [-0.4009],
        [-0.1929],
        [ 0.2764],
        [ 0.2267],
        [ 0.0096],
        [ 0.2357],
        [ 0.2276],
        [ 0.1157],
        [-0.3608]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0905,  0.0165,  0.0494, -0.1356,  0.1133, -0.0818, -0.1161, -0.0761,
          0.0960,  0.1483,  0.0334,  0.0125,  0.0159,  0.0409, -0.1389, -0.1158,
         -0.0373,  0.1056, -0.0973,  0.0611,  0.0799,  0.0926, -0.1309, -0.0569,
          0.0597,  0.0840,  0.1430,  0.0872,  0.0020,  0.0887, -0.1060,  0.0429,
          0.0136, -0.0778, -0.0213,  0.0852, -0.1276, -0.0230,  0.0115,  0.1500,
          0.0342,  0.1187, -0.0539, -0.0361,  0.0267, -0.0985, -0.0033,  0.0638,
         -0.1434,  0.0284,  0.0792, -0.0765,  0.0268, -0.1422,  0.0386, -0.1319,
         -0.0940,  0.1070, -0.0556, -0.1080, -0.1214, -0.0370,  0.1254, -0.0359,
         -0.0164,  0.1408,  0.0314,  0.0905,  0.0461,  0.1241,  0.0427, -0.0587,
          0.0228,  0.1457, -0.0629,  0.0059,  0.0410,  0.0409, -0.0711,  0.1401,
          0.0996,  0.1233,  0.0720, -0.0269,  0.0480,  0.0643,  0.1239, -0.1278,
         -0.0779,  0.0217,  0.0567, -0.0329,  0.1173,  0.0822, -0.0265, -0.0703,
          0.0395, -0.0984,  0.0502, -0.1326,  0.0680,  0.0568,  0.0004,  0.1135,
          0.0469,  0.0275,  0.0345, -0.0407, -0.0812, -0.0860,  0.0904,  0.0098,
         -0.1212,  0.0990, -0.0530, -0.1370,  0.0921, -0.1439, -0.0177,  0.1199,
         -0.0429,  0.1075,  0.1256,  0.0289,  0.1107, -0.0200,  0.0779, -0.0547,
         -0.1487,  0.1173, -0.0814,  0.0219, -0.1278, -0.1071, -0.0872, -0.0339,
          0.0655,  0.0445, -0.0062,  0.0645, -0.0336,  0.0484,  0.0760, -0.0252,
          0.0407, -0.1321, -0.0049, -0.0261,  0.0564, -0.0809, -0.0073,  0.1069,
         -0.0218,  0.0514,  0.0631,  0.0344, -0.1279, -0.0977,  0.0517,  0.0538,
         -0.0344,  0.0762, -0.0528, -0.0180,  0.1047,  0.0420, -0.1092, -0.1089,
         -0.0576, -0.0923,  0.0521,  0.0159,  0.0908, -0.0894, -0.0652,  0.0194,
          0.0501, -0.0474, -0.0340,  0.1158, -0.0659,  0.0251,  0.1377,  0.0835,
          0.1508,  0.0957, -0.0152, -0.1052, -0.1013,  0.0218,  0.0850,  0.0309,
         -0.0322, -0.1110, -0.0019,  0.1013,  0.0183,  0.0168,  0.1049,  0.0316,
         -0.0074,  0.0390,  0.0721,  0.1016,  0.1198, -0.1142, -0.0559,  0.0357,
          0.0545, -0.0051, -0.0626,  0.0420,  0.1039,  0.0655, -0.0410, -0.1473,
          0.1365, -0.0924, -0.1413, -0.0792, -0.0308, -0.1428, -0.0848, -0.0673,
          0.0256, -0.1525, -0.0217,  0.1022,  0.1255, -0.0379,  0.1496,  0.1327,
          0.0592,  0.0130, -0.0979, -0.0717, -0.0360, -0.0505,  0.0812, -0.0323,
         -0.1155, -0.1119, -0.1085, -0.0537,  0.1011, -0.1079,  0.0282,  0.0633,
          0.1507,  0.0875,  0.1132, -0.0101,  0.1518,  0.0787, -0.1211,  0.1481]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0905,  0.0165,  0.0494, -0.1356,  0.1133, -0.0818, -0.1161, -0.0761,
          0.0960,  0.1483,  0.0334,  0.0125,  0.0159,  0.0409, -0.1389, -0.1158,
         -0.0373,  0.1056, -0.0973,  0.0611,  0.0799,  0.0926, -0.1309, -0.0569,
          0.0597,  0.0840,  0.1430,  0.0872,  0.0020,  0.0887, -0.1060,  0.0429,
          0.0136, -0.0778, -0.0213,  0.0852, -0.1276, -0.0230,  0.0115,  0.1500,
          0.0342,  0.1187, -0.0539, -0.0361,  0.0267, -0.0985, -0.0033,  0.0638,
         -0.1434,  0.0284,  0.0792, -0.0765,  0.0268, -0.1422,  0.0386, -0.1319,
         -0.0940,  0.1070, -0.0556, -0.1080, -0.1214, -0.0370,  0.1254, -0.0359,
         -0.0164,  0.1408,  0.0314,  0.0905,  0.0461,  0.1241,  0.0427, -0.0587,
          0.0228,  0.1457, -0.0629,  0.0059,  0.0410,  0.0409, -0.0711,  0.1401,
          0.0996,  0.1233,  0.0720, -0.0269,  0.0480,  0.0643,  0.1239, -0.1278,
         -0.0779,  0.0217,  0.0567, -0.0329,  0.1173,  0.0822, -0.0265, -0.0703,
          0.0395, -0.0984,  0.0502, -0.1326,  0.0680,  0.0568,  0.0004,  0.1135,
          0.0469,  0.0275,  0.0345, -0.0407, -0.0812, -0.0860,  0.0904,  0.0098,
         -0.1212,  0.0990, -0.0530, -0.1370,  0.0921, -0.1439, -0.0177,  0.1199,
         -0.0429,  0.1075,  0.1256,  0.0289,  0.1107, -0.0200,  0.0779, -0.0547,
         -0.1487,  0.1173, -0.0814,  0.0219, -0.1278, -0.1071, -0.0872, -0.0339,
          0.0655,  0.0445, -0.0062,  0.0645, -0.0336,  0.0484,  0.0760, -0.0252,
          0.0407, -0.1321, -0.0049, -0.0261,  0.0564, -0.0809, -0.0073,  0.1069,
         -0.0218,  0.0514,  0.0631,  0.0344, -0.1279, -0.0977,  0.0517,  0.0538,
         -0.0344,  0.0762, -0.0528, -0.0180,  0.1047,  0.0420, -0.1092, -0.1089,
         -0.0576, -0.0923,  0.0521,  0.0159,  0.0908, -0.0894, -0.0652,  0.0194,
          0.0501, -0.0474, -0.0340,  0.1158, -0.0659,  0.0251,  0.1377,  0.0835,
          0.1508,  0.0957, -0.0152, -0.1052, -0.1013,  0.0218,  0.0850,  0.0309,
         -0.0322, -0.1110, -0.0019,  0.1013,  0.0183,  0.0168,  0.1049,  0.0316,
         -0.0074,  0.0390,  0.0721,  0.1016,  0.1198, -0.1142, -0.0559,  0.0357,
          0.0545, -0.0051, -0.0626,  0.0420,  0.1039,  0.0655, -0.0410, -0.1473,
          0.1365, -0.0924, -0.1413, -0.0792, -0.0308, -0.1428, -0.0848, -0.0673,
          0.0256, -0.1525, -0.0217,  0.1022,  0.1255, -0.0379,  0.1496,  0.1327,
          0.0592,  0.0130, -0.0979, -0.0717, -0.0360, -0.0505,  0.0812, -0.0323,
         -0.1155, -0.1119, -0.1085, -0.0537,  0.1011, -0.1079,  0.0282,  0.0633,
          0.1507,  0.0875,  0.1132, -0.0101,  0.1518,  0.0787, -0.1211,  0.1481]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0702,  0.0436, -0.0060,  ...,  0.1173, -0.0254, -0.1029],
        [-0.0874,  0.0831, -0.0259,  ..., -0.0902,  0.0205,  0.0871],
        [-0.0768, -0.0517, -0.0753,  ..., -0.0874, -0.0678,  0.0670],
        ...,
        [-0.0135, -0.0815,  0.0302,  ...,  0.1030,  0.1159,  0.1205],
        [-0.0016,  0.0390, -0.0930,  ...,  0.1060,  0.0594,  0.0364],
        [-0.0178,  0.1175, -0.0738,  ..., -0.1113, -0.0145,  0.0124]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0702,  0.0436, -0.0060,  ...,  0.1173, -0.0254, -0.1029],
        [-0.0874,  0.0831, -0.0259,  ..., -0.0902,  0.0205,  0.0871],
        [-0.0768, -0.0517, -0.0753,  ..., -0.0874, -0.0678,  0.0670],
        ...,
        [-0.0135, -0.0815,  0.0302,  ...,  0.1030,  0.1159,  0.1205],
        [-0.0016,  0.0390, -0.0930,  ...,  0.1060,  0.0594,  0.0364],
        [-0.0178,  0.1175, -0.0738,  ..., -0.1113, -0.0145,  0.0124]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0764, -0.0316,  0.0915,  ..., -0.0060, -0.1505, -0.0294],
        [ 0.0538, -0.1480,  0.0098,  ...,  0.1429, -0.1405,  0.0217],
        [ 0.0832, -0.0494, -0.0845,  ...,  0.0241,  0.0065,  0.0925],
        ...,
        [-0.1389,  0.0622,  0.0166,  ..., -0.0884, -0.0761,  0.0968],
        [-0.0651, -0.1131, -0.0101,  ..., -0.0697, -0.0644,  0.0969],
        [-0.1406, -0.1733, -0.0537,  ...,  0.0720, -0.1020,  0.0445]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0764, -0.0316,  0.0915,  ..., -0.0060, -0.1505, -0.0294],
        [ 0.0538, -0.1480,  0.0098,  ...,  0.1429, -0.1405,  0.0217],
        [ 0.0832, -0.0494, -0.0845,  ...,  0.0241,  0.0065,  0.0925],
        ...,
        [-0.1389,  0.0622,  0.0166,  ..., -0.0884, -0.0761,  0.0968],
        [-0.0651, -0.1131, -0.0101,  ..., -0.0697, -0.0644,  0.0969],
        [-0.1406, -0.1733, -0.0537,  ...,  0.0720, -0.1020,  0.0445]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0469, -0.1721,  0.1900,  ...,  0.0070,  0.2363,  0.1110],
        [ 0.1930,  0.1295, -0.1013,  ...,  0.1345, -0.1945,  0.1126],
        [ 0.1815, -0.1229, -0.0327,  ...,  0.2311,  0.1770,  0.1335],
        ...,
        [ 0.2301, -0.1386,  0.2160,  ...,  0.2355, -0.1387, -0.0255],
        [ 0.0933, -0.1050,  0.1566,  ...,  0.1658,  0.1742, -0.0515],
        [ 0.1830,  0.0670,  0.1557,  ..., -0.1822,  0.2189, -0.1163]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0469, -0.1721,  0.1900,  ...,  0.0070,  0.2363,  0.1110],
        [ 0.1930,  0.1295, -0.1013,  ...,  0.1345, -0.1945,  0.1126],
        [ 0.1815, -0.1229, -0.0327,  ...,  0.2311,  0.1770,  0.1335],
        ...,
        [ 0.2301, -0.1386,  0.2160,  ...,  0.2355, -0.1387, -0.0255],
        [ 0.0933, -0.1050,  0.1566,  ...,  0.1658,  0.1742, -0.0515],
        [ 0.1830,  0.0670,  0.1557,  ..., -0.1822,  0.2189, -0.1163]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.1172],
        [ 0.1249],
        [ 0.2289],
        [ 0.2038],
        [-0.0901],
        [-0.3501],
        [-0.3930],
        [-0.1306],
        [-0.2542],
        [-0.3489],
        [ 0.2622],
        [ 0.1490],
        [ 0.2535],
        [-0.0224],
        [ 0.0823],
        [ 0.3776],
        [-0.2797],
        [-0.0905],
        [ 0.3993],
        [ 0.2434],
        [-0.3983],
        [-0.2950],
        [ 0.0744],
        [-0.1397],
        [-0.0267],
        [-0.0985],
        [ 0.0314],
        [ 0.2146],
        [ 0.1213],
        [ 0.0444],
        [ 0.0750],
        [ 0.1394]], device='cuda:0') 
 Parameter containing:
tensor([[-0.1172],
        [ 0.1249],
        [ 0.2289],
        [ 0.2038],
        [-0.0901],
        [-0.3501],
        [-0.3930],
        [-0.1306],
        [-0.2542],
        [-0.3489],
        [ 0.2622],
        [ 0.1490],
        [ 0.2535],
        [-0.0224],
        [ 0.0823],
        [ 0.3776],
        [-0.2797],
        [-0.0905],
        [ 0.3993],
        [ 0.2434],
        [-0.3983],
        [-0.2950],
        [ 0.0744],
        [-0.1397],
        [-0.0267],
        [-0.0985],
        [ 0.0314],
        [ 0.2146],
        [ 0.1213],
        [ 0.0444],
        [ 0.0750],
        [ 0.1394]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(75.6864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.4304, device='cuda:0')



h[100].sum tensor(-2.8041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.8779, device='cuda:0')



h[200].sum tensor(-1.7316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3208.2795, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0003,  ..., 0.0000, 0.0036, 0.0011],
        [0.0096, 0.0096, 0.0017,  ..., 0.0000, 0.0188, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(17489.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(374.1790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(29.9333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.3974, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(163.7735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.1014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1188],
        [-0.1454],
        [-0.2096],
        ...,
        [-0.0335],
        [-0.0336],
        [-0.0266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2996.4487, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.1188],
        [-0.1454],
        [-0.2096],
        ...,
        [-0.0335],
        [-0.0336],
        [-0.0266]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-73.9884, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3228, device='cuda:0')



h[100].sum tensor(3.0377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0131, device='cuda:0')



h[200].sum tensor(3.6653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(3.6356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(13626.1035, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0085, 0.0000,  ..., 0.0065, 0.0400, 0.0000],
        [0.0063, 0.0018, 0.0000,  ..., 0.0014, 0.0084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(86469.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2440.2253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(171.6812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(914.3248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(64.3270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.6020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(16.9275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1647],
        [-0.1010],
        [-0.0618],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-9671.5820, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1188],
        [-0.1454],
        [-0.2096],
        ...,
        [-0.0335],
        [-0.0336],
        [-0.0266]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 1 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 2.2646e-02,  4.4338e-02, -7.3909e-02, -3.1162e-03, -2.7599e-02,
         -3.6124e-03, -1.3313e-01,  1.3833e-01,  6.4358e-02, -3.5638e-02,
          7.9957e-02,  4.8267e-02,  1.1471e-01,  3.9506e-02,  1.2659e-01,
         -7.7062e-02, -2.1614e-02,  1.3373e-01,  2.7217e-02,  4.6655e-02,
          6.0282e-02, -1.3773e-01,  8.2652e-02,  1.5031e-01,  3.7780e-02,
          7.7525e-02,  6.3788e-02,  1.0284e-01, -7.6078e-02, -4.7381e-02,
         -1.4438e-02,  1.5031e-01, -3.6001e-03,  9.2257e-05, -5.4709e-02,
         -1.3161e-01,  8.7594e-02, -5.2127e-02,  3.7251e-03, -7.0279e-02,
          8.1905e-03,  3.2758e-02, -1.0850e-01,  3.6495e-02, -5.3533e-02,
          9.6546e-02, -1.6099e-02, -1.1599e-02,  5.9008e-02, -3.2071e-02,
          1.7162e-02, -1.0856e-01,  9.3092e-02,  1.1467e-01,  2.6460e-02,
         -1.0846e-01,  1.1406e-01, -1.2783e-01, -5.9598e-02, -1.4662e-01,
         -7.9233e-03, -5.7192e-02, -1.0581e-02,  2.6579e-02, -7.4405e-02,
          1.2520e-01,  6.5444e-02,  2.9855e-02,  1.2059e-01, -1.3544e-01,
         -7.5486e-02,  6.3780e-02, -2.2906e-03, -7.0975e-02, -1.3330e-01,
         -7.2451e-02,  3.9229e-02,  1.1414e-01,  7.3905e-02,  4.9706e-02,
         -8.7193e-02,  4.3964e-02, -9.6160e-02,  1.1795e-01,  1.3808e-02,
          2.3383e-02, -8.5094e-02, -9.6927e-02,  4.0859e-02,  2.4223e-02,
          1.2568e-01, -8.6330e-02, -1.1677e-01,  6.3524e-02, -9.4973e-02,
          7.6239e-02,  5.3280e-02, -1.9039e-02,  1.4791e-02,  3.9286e-03,
          3.2399e-02, -1.7145e-02,  3.4219e-02, -5.6282e-02, -9.3449e-02,
         -1.3462e-01, -4.8286e-02,  4.8155e-02,  1.0808e-02,  1.4511e-01,
          1.4901e-02,  8.2303e-02,  1.4570e-02,  1.3121e-01, -2.4327e-02,
          5.6672e-02,  1.2451e-01,  1.9265e-02,  8.6497e-03,  5.3873e-03,
         -1.0652e-01,  5.6771e-02,  1.2373e-03,  8.0441e-02, -7.6166e-02,
         -7.8153e-02, -2.9695e-03,  2.6398e-02, -1.1761e-01,  6.8748e-02,
         -1.1158e-01,  9.5423e-02, -2.0401e-02, -1.7234e-02,  2.7334e-02,
          4.5853e-04,  4.9106e-02, -5.7169e-02,  1.4199e-01,  2.8409e-02,
          7.0090e-02, -1.4052e-02,  7.5664e-03, -1.1975e-01,  9.2675e-02,
         -6.6504e-02, -4.2249e-02, -1.2904e-01,  1.2277e-01,  1.4092e-01,
         -6.0227e-02,  3.5631e-02, -1.0632e-01,  1.5012e-02,  1.4655e-01,
          9.9469e-02,  2.1591e-02,  2.2828e-02, -4.4971e-02,  5.3098e-02,
          1.2001e-01,  7.8146e-02, -1.1217e-01,  5.1680e-02,  3.8485e-02,
          1.2933e-01, -1.2959e-01, -2.8783e-02, -7.4887e-03, -8.4266e-03,
          6.5872e-02,  5.0864e-02, -2.0535e-03, -1.4106e-02, -1.3423e-01,
         -7.3038e-02, -1.1683e-01,  6.3771e-02, -7.3040e-02, -6.4789e-03,
          7.7009e-02, -1.9712e-02,  1.1683e-01, -6.7334e-02, -3.4811e-02,
         -8.2033e-02, -1.1989e-01,  9.5432e-02,  1.1193e-01, -6.0184e-02,
          1.8572e-02,  6.0772e-02, -1.0937e-01, -5.1276e-02, -9.9542e-02,
          6.1126e-03, -1.2967e-01, -4.5861e-02,  8.5726e-02, -1.1986e-02,
          2.7801e-02,  1.3043e-01,  1.3001e-01,  4.1278e-04, -1.2513e-01,
          1.0867e-02, -1.2410e-01,  2.2596e-02, -1.3285e-01,  1.2462e-01,
         -1.3342e-01,  1.3563e-01, -1.1701e-01, -1.0820e-01,  1.2221e-01,
          8.7730e-02,  7.4792e-02, -9.1421e-02,  1.1801e-01, -9.4878e-02,
         -9.9678e-02,  5.7689e-02, -1.2252e-01,  1.7744e-03,  7.4476e-02,
          4.9881e-02,  1.4943e-01,  8.8014e-02, -1.4008e-01,  1.3298e-01,
          9.2959e-02, -1.4907e-01,  6.2603e-02,  1.2105e-01,  7.9262e-02,
         -1.0078e-02, -6.4594e-02, -1.4917e-01, -6.5808e-02,  2.8324e-02,
          5.9699e-03, -1.2319e-01,  5.9128e-02,  4.9591e-02,  7.7124e-02,
          6.5842e-02, -9.5722e-02,  8.8251e-02, -1.2534e-01,  6.6926e-02,
         -5.5634e-02, -7.3780e-02,  8.5190e-03,  5.8676e-02,  1.4263e-02,
          7.2063e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1114, -0.0974, -0.1147,  ..., -0.0552,  0.0919, -0.0177],
        [ 0.0294, -0.0206,  0.0101,  ..., -0.0559, -0.0600, -0.0727],
        [ 0.1214, -0.0856,  0.0969,  ...,  0.0960,  0.0932,  0.1076],
        ...,
        [-0.0066, -0.1014,  0.0603,  ..., -0.0989, -0.0021,  0.0178],
        [-0.0437, -0.0303,  0.0288,  ...,  0.0550, -0.0808, -0.0296],
        [ 0.0448,  0.0090,  0.0682,  ...,  0.1085,  0.0664, -0.0192]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0008, -0.1649, -0.1211,  ...,  0.0094, -0.1574, -0.0611],
        [ 0.1003, -0.1139, -0.0490,  ...,  0.1012,  0.1614, -0.0600],
        [-0.1239,  0.0283, -0.0968,  ..., -0.0914, -0.0052,  0.0686],
        ...,
        [-0.0046,  0.0003, -0.0255,  ..., -0.1216,  0.0179, -0.0983],
        [ 0.1272, -0.0844, -0.1458,  ..., -0.1322, -0.0297,  0.0684],
        [ 0.1522,  0.0211, -0.0979,  ...,  0.0465, -0.1678, -0.1610]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0550,  0.0261,  0.2389,  ..., -0.2424, -0.1887, -0.1800],
        [ 0.1304, -0.0620,  0.1372,  ...,  0.2218, -0.1177,  0.1943],
        [ 0.1000,  0.1115, -0.1761,  ...,  0.1632, -0.0053,  0.0256],
        ...,
        [-0.1576, -0.0810, -0.2076,  ..., -0.1362,  0.1083,  0.1858],
        [ 0.2018, -0.2187, -0.0208,  ...,  0.2036, -0.0832,  0.0247],
        [-0.2333,  0.0801, -0.0970,  ..., -0.0406,  0.0109,  0.1072]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0591],
        [-0.0639],
        [ 0.4139],
        [-0.2937],
        [-0.3271],
        [ 0.2708],
        [-0.0098],
        [-0.3681],
        [-0.0639],
        [-0.3138],
        [-0.2010],
        [-0.3885],
        [-0.2198],
        [ 0.1751],
        [-0.1029],
        [ 0.0383],
        [-0.2026],
        [ 0.0748],
        [-0.0948],
        [ 0.1588],
        [-0.4067],
        [ 0.1120],
        [-0.0535],
        [ 0.3789],
        [ 0.1055],
        [-0.1323],
        [ 0.1767],
        [-0.2640],
        [-0.1262],
        [ 0.1712],
        [-0.3501],
        [-0.3829]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 2.2646e-02,  4.4338e-02, -7.3909e-02, -3.1162e-03, -2.7599e-02,
         -3.6124e-03, -1.3313e-01,  1.3833e-01,  6.4358e-02, -3.5638e-02,
          7.9957e-02,  4.8267e-02,  1.1471e-01,  3.9506e-02,  1.2659e-01,
         -7.7062e-02, -2.1614e-02,  1.3373e-01,  2.7217e-02,  4.6655e-02,
          6.0282e-02, -1.3773e-01,  8.2652e-02,  1.5031e-01,  3.7780e-02,
          7.7525e-02,  6.3788e-02,  1.0284e-01, -7.6078e-02, -4.7381e-02,
         -1.4438e-02,  1.5031e-01, -3.6001e-03,  9.2257e-05, -5.4709e-02,
         -1.3161e-01,  8.7594e-02, -5.2127e-02,  3.7251e-03, -7.0279e-02,
          8.1905e-03,  3.2758e-02, -1.0850e-01,  3.6495e-02, -5.3533e-02,
          9.6546e-02, -1.6099e-02, -1.1599e-02,  5.9008e-02, -3.2071e-02,
          1.7162e-02, -1.0856e-01,  9.3092e-02,  1.1467e-01,  2.6460e-02,
         -1.0846e-01,  1.1406e-01, -1.2783e-01, -5.9598e-02, -1.4662e-01,
         -7.9233e-03, -5.7192e-02, -1.0581e-02,  2.6579e-02, -7.4405e-02,
          1.2520e-01,  6.5444e-02,  2.9855e-02,  1.2059e-01, -1.3544e-01,
         -7.5486e-02,  6.3780e-02, -2.2906e-03, -7.0975e-02, -1.3330e-01,
         -7.2451e-02,  3.9229e-02,  1.1414e-01,  7.3905e-02,  4.9706e-02,
         -8.7193e-02,  4.3964e-02, -9.6160e-02,  1.1795e-01,  1.3808e-02,
          2.3383e-02, -8.5094e-02, -9.6927e-02,  4.0859e-02,  2.4223e-02,
          1.2568e-01, -8.6330e-02, -1.1677e-01,  6.3524e-02, -9.4973e-02,
          7.6239e-02,  5.3280e-02, -1.9039e-02,  1.4791e-02,  3.9286e-03,
          3.2399e-02, -1.7145e-02,  3.4219e-02, -5.6282e-02, -9.3449e-02,
         -1.3462e-01, -4.8286e-02,  4.8155e-02,  1.0808e-02,  1.4511e-01,
          1.4901e-02,  8.2303e-02,  1.4570e-02,  1.3121e-01, -2.4327e-02,
          5.6672e-02,  1.2451e-01,  1.9265e-02,  8.6497e-03,  5.3873e-03,
         -1.0652e-01,  5.6771e-02,  1.2373e-03,  8.0441e-02, -7.6166e-02,
         -7.8153e-02, -2.9695e-03,  2.6398e-02, -1.1761e-01,  6.8748e-02,
         -1.1158e-01,  9.5423e-02, -2.0401e-02, -1.7234e-02,  2.7334e-02,
          4.5853e-04,  4.9106e-02, -5.7169e-02,  1.4199e-01,  2.8409e-02,
          7.0090e-02, -1.4052e-02,  7.5664e-03, -1.1975e-01,  9.2675e-02,
         -6.6504e-02, -4.2249e-02, -1.2904e-01,  1.2277e-01,  1.4092e-01,
         -6.0227e-02,  3.5631e-02, -1.0632e-01,  1.5012e-02,  1.4655e-01,
          9.9469e-02,  2.1591e-02,  2.2828e-02, -4.4971e-02,  5.3098e-02,
          1.2001e-01,  7.8146e-02, -1.1217e-01,  5.1680e-02,  3.8485e-02,
          1.2933e-01, -1.2959e-01, -2.8783e-02, -7.4887e-03, -8.4266e-03,
          6.5872e-02,  5.0864e-02, -2.0535e-03, -1.4106e-02, -1.3423e-01,
         -7.3038e-02, -1.1683e-01,  6.3771e-02, -7.3040e-02, -6.4789e-03,
          7.7009e-02, -1.9712e-02,  1.1683e-01, -6.7334e-02, -3.4811e-02,
         -8.2033e-02, -1.1989e-01,  9.5432e-02,  1.1193e-01, -6.0184e-02,
          1.8572e-02,  6.0772e-02, -1.0937e-01, -5.1276e-02, -9.9542e-02,
          6.1126e-03, -1.2967e-01, -4.5861e-02,  8.5726e-02, -1.1986e-02,
          2.7801e-02,  1.3043e-01,  1.3001e-01,  4.1278e-04, -1.2513e-01,
          1.0867e-02, -1.2410e-01,  2.2596e-02, -1.3285e-01,  1.2462e-01,
         -1.3342e-01,  1.3563e-01, -1.1701e-01, -1.0820e-01,  1.2221e-01,
          8.7730e-02,  7.4792e-02, -9.1421e-02,  1.1801e-01, -9.4878e-02,
         -9.9678e-02,  5.7689e-02, -1.2252e-01,  1.7744e-03,  7.4476e-02,
          4.9881e-02,  1.4943e-01,  8.8014e-02, -1.4008e-01,  1.3298e-01,
          9.2959e-02, -1.4907e-01,  6.2603e-02,  1.2105e-01,  7.9262e-02,
         -1.0078e-02, -6.4594e-02, -1.4917e-01, -6.5808e-02,  2.8324e-02,
          5.9699e-03, -1.2319e-01,  5.9128e-02,  4.9591e-02,  7.7124e-02,
          6.5842e-02, -9.5722e-02,  8.8251e-02, -1.2534e-01,  6.6926e-02,
         -5.5634e-02, -7.3780e-02,  8.5190e-03,  5.8676e-02,  1.4263e-02,
          7.2063e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1114, -0.0974, -0.1147,  ..., -0.0552,  0.0919, -0.0177],
        [ 0.0294, -0.0206,  0.0101,  ..., -0.0559, -0.0600, -0.0727],
        [ 0.1214, -0.0856,  0.0969,  ...,  0.0960,  0.0932,  0.1076],
        ...,
        [-0.0066, -0.1014,  0.0603,  ..., -0.0989, -0.0021,  0.0178],
        [-0.0437, -0.0303,  0.0288,  ...,  0.0550, -0.0808, -0.0296],
        [ 0.0448,  0.0090,  0.0682,  ...,  0.1085,  0.0664, -0.0192]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0008, -0.1649, -0.1211,  ...,  0.0094, -0.1574, -0.0611],
        [ 0.1003, -0.1139, -0.0490,  ...,  0.1012,  0.1614, -0.0600],
        [-0.1239,  0.0283, -0.0968,  ..., -0.0914, -0.0052,  0.0686],
        ...,
        [-0.0046,  0.0003, -0.0255,  ..., -0.1216,  0.0179, -0.0983],
        [ 0.1272, -0.0844, -0.1458,  ..., -0.1322, -0.0297,  0.0684],
        [ 0.1522,  0.0211, -0.0979,  ...,  0.0465, -0.1678, -0.1610]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0550,  0.0261,  0.2389,  ..., -0.2424, -0.1887, -0.1800],
        [ 0.1304, -0.0620,  0.1372,  ...,  0.2218, -0.1177,  0.1943],
        [ 0.1000,  0.1115, -0.1761,  ...,  0.1632, -0.0053,  0.0256],
        ...,
        [-0.1576, -0.0810, -0.2076,  ..., -0.1362,  0.1083,  0.1858],
        [ 0.2018, -0.2187, -0.0208,  ...,  0.2036, -0.0832,  0.0247],
        [-0.2333,  0.0801, -0.0970,  ..., -0.0406,  0.0109,  0.1072]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0591],
        [-0.0639],
        [ 0.4139],
        [-0.2937],
        [-0.3271],
        [ 0.2708],
        [-0.0098],
        [-0.3681],
        [-0.0639],
        [-0.3138],
        [-0.2010],
        [-0.3885],
        [-0.2198],
        [ 0.1751],
        [-0.1029],
        [ 0.0383],
        [-0.2026],
        [ 0.0748],
        [-0.0948],
        [ 0.1588],
        [-0.4067],
        [ 0.1120],
        [-0.0535],
        [ 0.3789],
        [ 0.1055],
        [-0.1323],
        [ 0.1767],
        [-0.2640],
        [-0.1262],
        [ 0.1712],
        [-0.3501],
        [-0.3829]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0065, -0.0108,  ...,  0.0086,  0.0021,  0.0106],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(310.4415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.6247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1548, device='cuda:0')



h[100].sum tensor(9.4781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(8.1329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.3217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0126, 0.0246, 0.0000,  ..., 0.0325, 0.0079, 0.0400],
        [0.0103, 0.0202, 0.0000,  ..., 0.0268, 0.0065, 0.0329],
        [0.0024, 0.0047, 0.0000,  ..., 0.0063, 0.0015, 0.0077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28705.4316, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1056, 0.0000, 0.0000,  ..., 0.1610, 0.0000, 0.0389],
        [0.0905, 0.0000, 0.0000,  ..., 0.1380, 0.0000, 0.0333],
        [0.0726, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.0268],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(147153.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1470.9412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.8481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2411.0405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(27.2032, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-88.4728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2878e+00],
        [-2.4755e+00],
        [-2.7317e+00],
        ...,
        [-2.7485e-06],
        [-3.6111e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42638.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(91.2672, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        ...,
        [ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
         -1.0000e-04, -1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(313.3462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9003, device='cuda:0')



h[100].sum tensor(12.1674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(4.0767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.7745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29679.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156034.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1184.6062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.7848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2232.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.9457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-93.5589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0093],
        [-0.0199],
        [-0.0391],
        ...,
        [-0.0017],
        [-0.0017],
        [-0.0017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30123.5234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002],
        [ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002],
        [ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002],
        ...,
        [ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002],
        [ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002],
        [ 0.0002,  0.0002,  0.0000,  ..., -0.0002, -0.0001, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(240.4530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.7368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4058, device='cuda:0')



h[100].sum tensor(8.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-1.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.6313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0119, 0.0228, 0.0000,  ..., 0.0283, 0.0066, 0.0349],
        [0.0081, 0.0154, 0.0000,  ..., 0.0188, 0.0044, 0.0232],
        [0.0068, 0.0129, 0.0000,  ..., 0.0155, 0.0036, 0.0191],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24648.4238, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0795, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.0278],
        [0.0633, 0.0000, 0.0000,  ..., 0.0998, 0.0000, 0.0222],
        [0.0476, 0.0000, 0.0000,  ..., 0.0752, 0.0000, 0.0168],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(135569.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(853.1818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(50.8227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1670.7310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(62.4758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-79.8241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6595],
        [-0.5911],
        [-0.4772],
        ...,
        [-0.0091],
        [-0.0091],
        [-0.0091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20575.2539, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365888.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4599e-03,  2.7230e-03, -4.0095e-03,  ...,  2.9199e-03,
          6.8032e-04,  3.6567e-03],
        [ 3.2345e-03,  6.1841e-03, -9.7199e-03,  ...,  7.4500e-03,
          1.7773e-03,  9.2270e-03],
        [ 2.4241e-03,  4.6035e-03, -7.1121e-03,  ...,  5.3812e-03,
          1.2764e-03,  6.6832e-03],
        ...,
        [ 2.1384e-04,  2.9279e-04,  0.0000e+00,  ..., -2.6093e-04,
         -8.9943e-05, -2.5454e-04],
        [ 2.1384e-04,  2.9279e-04,  0.0000e+00,  ..., -2.6093e-04,
         -8.9943e-05, -2.5454e-04],
        [ 2.1384e-04,  2.9279e-04,  0.0000e+00,  ..., -2.6093e-04,
         -8.9943e-05, -2.5454e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(286.6622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2345, device='cuda:0')



h[100].sum tensor(9.4549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-4.3059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8863, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0222, 0.0000,  ..., 0.0265, 0.0063, 0.0328],
        [0.0078, 0.0147, 0.0000,  ..., 0.0167, 0.0039, 0.0208],
        [0.0067, 0.0126, 0.0000,  ..., 0.0141, 0.0033, 0.0176],
        ...,
        [0.0009, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26826.8496, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.9668e-02, 0.0000e+00, 0.0000e+00,  ..., 7.9452e-02, 0.0000e+00,
         1.8423e-02],
        [4.3805e-02, 0.0000e+00, 0.0000e+00,  ..., 7.0301e-02, 0.0000e+00,
         1.6550e-02],
        [3.7304e-02, 0.0000e+00, 0.0000e+00,  ..., 5.9921e-02, 0.0000e+00,
         1.4140e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 4.4650e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4649e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4647e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141013.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(713.5045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.6783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1493.5103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(77.5585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-89.0106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4954],
        [-0.4633],
        [-0.4025],
        ...,
        [-0.0166],
        [-0.0165],
        [-0.0165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18559.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365888.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365879.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04],
        [ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04],
        [ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04],
        ...,
        [ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04],
        [ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04],
        [ 2.6977e-04,  3.7851e-04,  0.0000e+00,  ..., -3.0515e-04,
         -2.6138e-05, -2.8629e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(299.6554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8408, device='cuda:0')



h[100].sum tensor(8.9998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-7.9509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0015, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26405.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.1936e-06],
        [1.1396e-03, 0.0000e+00, 0.0000e+00,  ..., 1.9473e-03, 0.0000e+00,
         7.9392e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(136043.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(528.1018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(0.8407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1104.8718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(80.5741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-89.0942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0308],
        [-0.0357],
        [-0.0552],
        ...,
        [-0.0215],
        [-0.0221],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17041.8965, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365879.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365872.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4245e-03,  4.6072e-03, -6.8061e-03,  ...,  5.0750e-03,
          1.3691e-03,  6.3605e-03],
        [ 2.9804e-04,  4.5876e-04,  0.0000e+00,  ..., -3.2925e-04,
          4.6799e-05, -2.8732e-04],
        [ 2.9804e-04,  4.5876e-04,  0.0000e+00,  ..., -3.2925e-04,
          4.6799e-05, -2.8732e-04],
        ...,
        [ 2.9804e-04,  4.5876e-04,  0.0000e+00,  ..., -3.2925e-04,
          4.6799e-05, -2.8732e-04],
        [ 2.9804e-04,  4.5876e-04,  0.0000e+00,  ..., -3.2925e-04,
          4.6799e-05, -2.8732e-04],
        [ 2.9804e-04,  4.5876e-04,  0.0000e+00,  ..., -3.2925e-04,
          4.6799e-05, -2.8732e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(432.2185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.6423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9985, device='cuda:0')



h[100].sum tensor(12.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-8.4843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.5282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0173, 0.0000,  ..., 0.0195, 0.0051, 0.0242],
        [0.0033, 0.0060, 0.0000,  ..., 0.0051, 0.0015, 0.0064],
        [0.0012, 0.0018, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0012, 0.0018, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0012, 0.0018, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0012, 0.0018, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38348.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0406, 0.0000, 0.0000,  ..., 0.0664, 0.0000, 0.0170],
        [0.0164, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.0075],
        [0.0038, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206413., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(949.2828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(8.7339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1780.9581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.3517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.8328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1311],
        [-0.0738],
        [-0.0440],
        ...,
        [-0.0271],
        [-0.0270],
        [-0.0270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-22497.0742, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365872.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365868.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        ...,
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(431.9501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8273, device='cuda:0')



h[100].sum tensor(13.0379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-12.0283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36179.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0035],
        [0.0003, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190615.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(729.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.3591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1275.8137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.9195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.5976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0475],
        [-0.0273],
        [-0.0308],
        ...,
        [-0.0319],
        [-0.0318],
        [-0.0317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23887.7539, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365868.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365868.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0082, -0.0125,  ...,  0.0096,  0.0026,  0.0120],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        ...,
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003],
        [ 0.0003,  0.0005,  0.0000,  ..., -0.0003,  0.0001, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(369.7346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.5570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9166, device='cuda:0')



h[100].sum tensor(11.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-13.5259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0158, 0.0000,  ..., 0.0172, 0.0049, 0.0214],
        [0.0051, 0.0098, 0.0000,  ..., 0.0096, 0.0030, 0.0120],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0012, 0.0021, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30410.7559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.0241],
        [0.0269, 0.0000, 0.0000,  ..., 0.0440, 0.0000, 0.0122],
        [0.0062, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161346.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(435.5310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.3214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(759.0561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.6584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-101.9429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2351],
        [-0.1280],
        [-0.0625],
        ...,
        [-0.0318],
        [-0.0317],
        [-0.0316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17949.3164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9995],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365868.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365866.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002],
        [ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002],
        [ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002],
        ...,
        [ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002],
        [ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002],
        [ 0.0003,  0.0006,  0.0000,  ..., -0.0003,  0.0002, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(360.8731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.6933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6105, device='cuda:0')



h[100].sum tensor(11.4622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-16.8073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.7129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0131, 0.0000,  ..., 0.0134, 0.0043, 0.0169],
        [0.0038, 0.0077, 0.0000,  ..., 0.0067, 0.0026, 0.0084],
        [0.0059, 0.0117, 0.0000,  ..., 0.0119, 0.0039, 0.0148],
        ...,
        [0.0011, 0.0023, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0011, 0.0023, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0011, 0.0023, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27998.8613, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0225, 0.0000, 0.0000,  ..., 0.0381, 0.0000, 0.0129],
        [0.0203, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0115],
        [0.0260, 0.0000, 0.0000,  ..., 0.0430, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152466.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(330.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.5179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(469.6339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.2484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-91.8348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0085],
        [-0.0211],
        [-0.0415],
        ...,
        [-0.0361],
        [-0.0360],
        [-0.0359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19344.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365866.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365866.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006,  0.0000,  ..., -0.0003,  0.0003, -0.0002],
        [ 0.0018,  0.0036, -0.0050,  ...,  0.0036,  0.0013,  0.0047],
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0003,  0.0003, -0.0002],
        ...,
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0003,  0.0003, -0.0002],
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0003,  0.0003, -0.0002],
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0003,  0.0003, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(371.7514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.8173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0152, device='cuda:0')



h[100].sum tensor(11.5802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0078, device='cuda:0')



h[200].sum tensor(-19.1848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0024, 0.0055, 0.0000,  ..., 0.0036, 0.0021, 0.0047],
        [0.0031, 0.0067, 0.0000,  ..., 0.0053, 0.0025, 0.0067],
        [0.0084, 0.0171, 0.0000,  ..., 0.0178, 0.0058, 0.0227],
        ...,
        [0.0009, 0.0024, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27947.5684, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0075],
        [0.0112, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0103],
        [0.0190, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156472.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(283.1339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.5213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(246.5687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.6568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-88.3916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0178],
        [ 0.0264],
        [ 0.0304],
        ...,
        [-0.0391],
        [-0.0389],
        [-0.0387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19003.5762, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9994],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365866.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9993],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365867.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006,  0.0000,  ..., -0.0004,  0.0003, -0.0002],
        [ 0.0053,  0.0107, -0.0164,  ...,  0.0127,  0.0036,  0.0159],
        [ 0.0054,  0.0108, -0.0166,  ...,  0.0129,  0.0036,  0.0161],
        ...,
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0004,  0.0003, -0.0002],
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0004,  0.0003, -0.0002],
        [ 0.0002,  0.0006,  0.0000,  ..., -0.0004,  0.0003, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(472.6049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.2456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1951, device='cuda:0')



h[100].sum tensor(14.0120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-19.0699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8592, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0156, 0.0000,  ..., 0.0165, 0.0056, 0.0207],
        [0.0100, 0.0208, 0.0000,  ..., 0.0228, 0.0073, 0.0288],
        [0.0203, 0.0409, 0.0000,  ..., 0.0486, 0.0138, 0.0608],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34493.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0358, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0205],
        [0.0583, 0.0000, 0.0000,  ..., 0.0979, 0.0000, 0.0297],
        [0.0907, 0.0000, 0.0000,  ..., 0.1521, 0.0000, 0.0428],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189035.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(582.7426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.7878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(472.4793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.1390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-104.7116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1293],
        [-0.2170],
        [-0.3145],
        ...,
        [-0.0406],
        [-0.0404],
        [-0.0403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17369.2305, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9993],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365867.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(562.5564, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9993],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365869.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9065e-03,  4.1418e-03, -5.7853e-03,  ...,  4.2362e-03,
          1.5322e-03,  5.5001e-03],
        [ 2.2881e-03,  4.8908e-03, -7.0047e-03,  ...,  5.2091e-03,
          1.7771e-03,  6.6975e-03],
        [ 3.7030e-03,  7.6683e-03, -1.1526e-02,  ...,  8.8167e-03,
          2.6852e-03,  1.1137e-02],
        ...,
        [ 9.6162e-05,  5.8790e-04,  0.0000e+00,  ..., -3.7983e-04,
          3.7022e-04, -1.8074e-04],
        [ 9.6162e-05,  5.8790e-04,  0.0000e+00,  ..., -3.7983e-04,
          3.7022e-04, -1.8074e-04],
        [ 9.6162e-05,  5.8790e-04,  0.0000e+00,  ..., -3.7983e-04,
          3.7022e-04, -1.8074e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(599.3891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.0754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3821, device='cuda:0')



h[100].sum tensor(17.1484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0159, device='cuda:0')



h[200].sum tensor(-18.1299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.4777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0098, 0.0209, 0.0000,  ..., 0.0225, 0.0075, 0.0289],
        [0.0105, 0.0223, 0.0000,  ..., 0.0244, 0.0080, 0.0311],
        [0.0074, 0.0162, 0.0000,  ..., 0.0164, 0.0060, 0.0213],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0000, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42560.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0298, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0196],
        [0.0388, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0241],
        [0.0481, 0.0000, 0.0000,  ..., 0.0834, 0.0000, 0.0286],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225407.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1077.0803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.3842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(746.7819, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.4016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0102],
        [-0.0622],
        [-0.1267],
        ...,
        [-0.0406],
        [-0.0404],
        [-0.0403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14403.7910, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9993],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365869.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9992],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365873.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04],
        [ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04],
        [ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04],
        ...,
        [ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04],
        [ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04],
        [ 4.2723e-05,  5.8446e-04,  0.0000e+00,  ..., -4.0976e-04,
          3.9864e-04, -1.7751e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(757.2513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.8089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.5358, device='cuda:0')



h[100].sum tensor(21.1228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0221, device='cuda:0')



h[200].sum tensor(-16.3546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.1320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0023, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0027, 0.0073, 0.0000,  ..., 0.0060, 0.0032, 0.0077],
        ...,
        [0.0002, 0.0023, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53502.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0035],
        [0.0036, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0050],
        [0.0107, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0092],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(283887.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1670.6139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.3281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1280.7556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.9621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.2468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0044],
        [ 0.0234],
        [ 0.0502],
        ...,
        [-0.0402],
        [-0.0400],
        [-0.0399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-13013.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9992],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365873.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9992],
        ...,
        [1.0006],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365877.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.1315e-03,  1.0694e-02, -1.6405e-02,  ...,  1.2671e-02,
          3.7539e-03,  1.5962e-02],
        [ 7.4584e-03,  1.5277e-02, -2.3850e-02,  ...,  1.8622e-02,
          5.2683e-03,  2.3287e-02],
        [ 5.3331e-03,  1.1091e-02, -1.7050e-02,  ...,  1.3187e-02,
          3.8851e-03,  1.6597e-02],
        ...,
        [ 4.8059e-06,  5.9494e-04,  0.0000e+00,  ..., -4.4039e-04,
          4.1730e-04, -1.7783e-04],
        [ 4.8059e-06,  5.9494e-04,  0.0000e+00,  ..., -4.4039e-04,
          4.1730e-04, -1.7783e-04],
        [ 4.8059e-06,  5.9494e-04,  0.0000e+00,  ..., -4.4039e-04,
          4.1730e-04, -1.7783e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.1185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.9862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9177, device='cuda:0')



h[100].sum tensor(17.5849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-20.4881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.2978e-02, 4.7603e-02, 0.0000e+00,  ..., 5.6958e-02, 1.6610e-02,
         7.1568e-02],
        [2.6831e-02, 5.5191e-02, 0.0000e+00,  ..., 6.6811e-02, 1.9118e-02,
         8.3696e-02],
        [2.2242e-02, 4.6154e-02, 0.0000e+00,  ..., 5.5077e-02, 1.6131e-02,
         6.9252e-02],
        ...,
        [1.9232e-05, 2.3808e-03, 0.0000e+00,  ..., 0.0000e+00, 1.6699e-03,
         0.0000e+00],
        [1.9231e-05, 2.3807e-03, 0.0000e+00,  ..., 0.0000e+00, 1.6698e-03,
         0.0000e+00],
        [1.9230e-05, 2.3805e-03, 0.0000e+00,  ..., 0.0000e+00, 1.6697e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45327.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0979, 0.0000, 0.0000,  ..., 0.1633, 0.0000, 0.0451],
        [0.1197, 0.0000, 0.0000,  ..., 0.1998, 0.0000, 0.0537],
        [0.1150, 0.0000, 0.0000,  ..., 0.1927, 0.0000, 0.0524],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0033],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0033],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(250257.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1487.8440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-143.3243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(768.1425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.4789, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.2734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1321],
        [-0.2106],
        [-0.2656],
        ...,
        [-0.0408],
        [-0.0407],
        [-0.0406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9971.0059, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9992],
        ...,
        [1.0006],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365877.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9992],
        [0.9991],
        ...,
        [1.0006],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365883.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04],
        [-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04],
        [-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04],
        ...,
        [-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04],
        [-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04],
        [-2.2975e-05,  6.1456e-04,  0.0000e+00,  ..., -4.7229e-04,
          4.2756e-04, -1.8036e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(653.8770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.7868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4248, device='cuda:0')



h[100].sum tensor(17.1792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-21.8083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.8207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45838.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(262221.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1595.8015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.5374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(765.9097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.4151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.5873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0407],
        [-0.0330],
        [-0.0158],
        ...,
        [-0.0245],
        [-0.0370],
        [-0.0412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9145.1602, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9992],
        [0.9991],
        ...,
        [1.0006],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365883.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9991],
        ...,
        [1.0007],
        [1.0007],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365888.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1971e-03,  5.0849e-03, -7.1877e-03,  ...,  5.2519e-03,
          1.9064e-03,  6.9038e-03],
        [-4.8159e-05,  6.4874e-04,  0.0000e+00,  ..., -5.0334e-04,
          4.2792e-04, -1.8197e-04],
        [ 3.7719e-03,  8.1963e-03, -1.2229e-02,  ...,  9.2884e-03,
          2.9434e-03,  1.1873e-02],
        ...,
        [-4.8159e-05,  6.4874e-04,  0.0000e+00,  ..., -5.0334e-04,
          4.2792e-04, -1.8197e-04],
        [-4.8159e-05,  6.4874e-04,  0.0000e+00,  ..., -5.0334e-04,
          4.2792e-04, -1.8197e-04],
        [-4.8159e-05,  6.4874e-04,  0.0000e+00,  ..., -5.0334e-04,
          4.2792e-04, -1.8197e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(633.4611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.2453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8845, device='cuda:0')



h[100].sum tensor(15.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-23.5500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.0774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0062, 0.0000,  ..., 0.0042, 0.0029, 0.0056],
        [0.0095, 0.0218, 0.0000,  ..., 0.0229, 0.0081, 0.0300],
        [0.0053, 0.0133, 0.0000,  ..., 0.0124, 0.0053, 0.0166],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44093.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0114],
        [0.0263, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0196],
        [0.0257, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0208],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254940.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1449.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-175.2872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(400.5963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.9769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.3916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0733],
        [ 0.1031],
        [ 0.1202],
        ...,
        [-0.0471],
        [-0.0469],
        [-0.0468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-4802.9443, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9991],
        ...,
        [1.0007],
        [1.0007],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365888.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9991],
        ...,
        [1.0008],
        [1.0007],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365893.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9155e-03,  4.6237e-03, -6.3557e-03,  ...,  4.5621e-03,
          1.7333e-03,  6.0918e-03],
        [-7.0448e-05,  6.9534e-04,  0.0000e+00,  ..., -5.3180e-04,
          4.1947e-04, -1.8083e-04],
        [ 1.9155e-03,  4.6237e-03, -6.3557e-03,  ...,  4.5621e-03,
          1.7333e-03,  6.0918e-03],
        ...,
        [-7.0448e-05,  6.9534e-04,  0.0000e+00,  ..., -5.3180e-04,
          4.1947e-04, -1.8083e-04],
        [-7.0448e-05,  6.9534e-04,  0.0000e+00,  ..., -5.3180e-04,
          4.1947e-04, -1.8083e-04],
        [-7.0448e-05,  6.9534e-04,  0.0000e+00,  ..., -5.3180e-04,
          4.1947e-04, -1.8083e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(572.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.1297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1857, device='cuda:0')



h[100].sum tensor(13.4453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-26.0143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0060, 0.0000,  ..., 0.0036, 0.0027, 0.0049],
        [0.0069, 0.0170, 0.0000,  ..., 0.0164, 0.0064, 0.0220],
        [0.0015, 0.0060, 0.0000,  ..., 0.0036, 0.0027, 0.0049],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38075.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0089],
        [0.0144, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0129],
        [0.0088, 0.0000, 0.0000,  ..., 0.0130, 0.0000, 0.0089],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225877.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1252.6990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-204.3506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(267.7121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-99.8735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0138],
        [-0.0041],
        [-0.0067],
        ...,
        [-0.0529],
        [-0.0527],
        [-0.0526]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14889.1934, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9991],
        ...,
        [1.0008],
        [1.0007],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365893.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9990],
        ...,
        [1.0008],
        [1.0008],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2123e-03,  9.2634e-03, -1.3762e-02,  ...,  1.0482e-02,
          3.2633e-03,  1.3431e-02],
        [ 3.8876e-03,  8.6206e-03, -1.2723e-02,  ...,  9.6487e-03,
          3.0477e-03,  1.2404e-02],
        [ 6.0374e-03,  1.2877e-02, -1.9600e-02,  ...,  1.5166e-02,
          4.4753e-03,  1.9200e-02],
        ...,
        [-9.0132e-05,  7.4560e-04,  0.0000e+00,  ..., -5.5852e-04,
          4.0629e-04, -1.6942e-04],
        [-9.0132e-05,  7.4560e-04,  0.0000e+00,  ..., -5.5852e-04,
          4.0629e-04, -1.6942e-04],
        [-9.0132e-05,  7.4560e-04,  0.0000e+00,  ..., -5.5852e-04,
          4.0629e-04, -1.6942e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(631.4736, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0575, device='cuda:0')



h[100].sum tensor(13.8618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-25.7334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.8236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0369, 0.0000,  ..., 0.0417, 0.0130, 0.0535],
        [0.0218, 0.0469, 0.0000,  ..., 0.0548, 0.0164, 0.0695],
        [0.0264, 0.0560, 0.0000,  ..., 0.0665, 0.0194, 0.0840],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41351.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0851, 0.0000, 0.0000,  ..., 0.1494, 0.0000, 0.0435],
        [0.1038, 0.0000, 0.0000,  ..., 0.1802, 0.0000, 0.0503],
        [0.1252, 0.0000, 0.0000,  ..., 0.2155, 0.0000, 0.0582],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239419., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1265.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-199.8144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(291.3991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-109.6596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1122],
        [ 0.0943],
        [ 0.0799],
        ...,
        [-0.0590],
        [-0.0587],
        [-0.0586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16141.8076, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9990],
        ...,
        [1.0008],
        [1.0008],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9990],
        ...,
        [1.0009],
        [1.0008],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365903.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0038, -0.0048,  ...,  0.0033,  0.0014,  0.0046],
        [ 0.0013,  0.0036, -0.0045,  ...,  0.0030,  0.0013,  0.0043],
        [-0.0001,  0.0008,  0.0000,  ..., -0.0006,  0.0004, -0.0002],
        ...,
        [-0.0001,  0.0008,  0.0000,  ..., -0.0006,  0.0004, -0.0002],
        [-0.0001,  0.0008,  0.0000,  ..., -0.0006,  0.0004, -0.0002],
        [-0.0001,  0.0008,  0.0000,  ..., -0.0006,  0.0004, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(815.1379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.7754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.0050, device='cuda:0')



h[100].sum tensor(17.5396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0201, device='cuda:0')



h[200].sum tensor(-22.7402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.7090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0264, 0.0000,  ..., 0.0277, 0.0094, 0.0364],
        [0.0024, 0.0085, 0.0000,  ..., 0.0057, 0.0033, 0.0081],
        [0.0054, 0.0146, 0.0000,  ..., 0.0131, 0.0054, 0.0178],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57394.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0443, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0279],
        [0.0243, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0190],
        [0.0266, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0207],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(329925.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1797.5393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.7979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(806.3835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.8961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.4775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1645],
        [ 0.1677],
        [ 0.1732],
        ...,
        [-0.0547],
        [-0.0544],
        [-0.0561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9315.4121, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9990],
        ...,
        [1.0009],
        [1.0008],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365903.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9991],
        [0.9990],
        ...,
        [1.0010],
        [1.0009],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365906.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001],
        [-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001],
        [-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001],
        ...,
        [-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001],
        [-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001],
        [-0.0001,  0.0009,  0.0000,  ..., -0.0006,  0.0004, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(703.6890, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1055, device='cuda:0')



h[100].sum tensor(12.6670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-26.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46936.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        [0.0005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(275134.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1286.4109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-193.2839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(267.3191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.2263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.0854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0969],
        [-0.0991],
        [-0.0972],
        ...,
        [-0.0709],
        [-0.0686],
        [-0.0641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14335.1289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9991],
        [0.9990],
        ...,
        [1.0010],
        [1.0009],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365906.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0010],
        [1.0009],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7756e-03,  4.6568e-03, -6.0308e-03,  ...,  4.2199e-03,
          1.6062e-03,  5.8921e-03],
        [ 1.8667e-03,  4.8372e-03, -6.3209e-03,  ...,  4.4533e-03,
          1.6669e-03,  6.1802e-03],
        [ 1.7756e-03,  4.6568e-03, -6.0308e-03,  ...,  4.2199e-03,
          1.6062e-03,  5.8921e-03],
        ...,
        [-1.1952e-04,  9.0735e-04,  0.0000e+00,  ..., -6.3179e-04,
          3.4353e-04, -9.6966e-05],
        [-1.1952e-04,  9.0735e-04,  0.0000e+00,  ..., -6.3179e-04,
          3.4353e-04, -9.6966e-05],
        [-1.1952e-04,  9.0735e-04,  0.0000e+00,  ..., -6.3179e-04,
          3.4353e-04, -9.6966e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(771.7981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.4730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9896, device='cuda:0')



h[100].sum tensor(12.3375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0179, device='cuda:0')



h[200].sum tensor(-25.4780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0083, 0.0210, 0.0000,  ..., 0.0199, 0.0072, 0.0273],
        [0.0080, 0.0204, 0.0000,  ..., 0.0192, 0.0070, 0.0265],
        [0.0033, 0.0106, 0.0000,  ..., 0.0078, 0.0037, 0.0110],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53194.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0217, 0.0000, 0.0000,  ..., 0.0423, 0.0000, 0.0198],
        [0.0216, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0201],
        [0.0161, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0168],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310331.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1530.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-173.1742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(697.0403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.8708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.1876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1017],
        [ 0.1181],
        [ 0.1281],
        ...,
        [-0.0743],
        [-0.0735],
        [-0.0738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19439.5742, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0010],
        [1.0009],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(591.9000, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0011],
        [1.0010],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365909.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3168e-03,  3.7992e-03, -4.5563e-03,  ...,  3.0171e-03,
          1.2797e-03,  4.4772e-03],
        [-1.1902e-04,  9.6158e-04,  0.0000e+00,  ..., -6.5193e-04,
          3.2372e-04, -5.6323e-05],
        [-1.1902e-04,  9.6158e-04,  0.0000e+00,  ..., -6.5193e-04,
          3.2372e-04, -5.6323e-05],
        ...,
        [-1.1902e-04,  9.6158e-04,  0.0000e+00,  ..., -6.5193e-04,
          3.2372e-04, -5.6323e-05],
        [-1.1902e-04,  9.6158e-04,  0.0000e+00,  ..., -6.5193e-04,
          3.2372e-04, -5.6323e-05],
        [-1.1902e-04,  9.6158e-04,  0.0000e+00,  ..., -6.5193e-04,
          3.2372e-04, -5.6323e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(646.4315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.1648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2387, device='cuda:0')



h[100].sum tensor(6.7401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-28.9651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0148, 0.0000,  ..., 0.0128, 0.0050, 0.0173],
        [0.0013, 0.0067, 0.0000,  ..., 0.0030, 0.0022, 0.0045],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41956.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0291, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.0220],
        [0.0142, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0134],
        [0.0048, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0071],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0003, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0003, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251009.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1096.3895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.2870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(261.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.4325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-115.0284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1086],
        [ 0.0776],
        [ 0.0332],
        ...,
        [-0.0809],
        [-0.0806],
        [-0.0805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27904.1191, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0011],
        [1.0010],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365909.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05],
        [-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05],
        [-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05],
        ...,
        [-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05],
        [-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05],
        [-1.2151e-04,  1.0011e-03,  0.0000e+00,  ..., -6.7044e-04,
          3.1113e-04, -1.8636e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(614.1824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.3561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9843, device='cuda:0')



h[100].sum tensor(3.8598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-30.4917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.0283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0033, 0.0108, 0.0000,  ..., 0.0081, 0.0035, 0.0109],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38561.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0061],
        [0.0050, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0079],
        [0.0153, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0135],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232515.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1053.8741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-233.3818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(192.2552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.7297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-106.3125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0308],
        [ 0.0239],
        [ 0.0278],
        ...,
        [-0.0825],
        [-0.0822],
        [-0.0821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33314.2383, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0011],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365916.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05],
        [-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05],
        [-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05],
        ...,
        [-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05],
        [-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05],
        [-1.2793e-04,  1.0310e-03,  0.0000e+00,  ..., -6.8734e-04,
          3.0073e-04,  1.2905e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(745.8975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.2230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5934, device='cuda:0')



h[100].sum tensor(5.5274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-28.6519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.9364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.1206e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2020e-03,
         5.1580e-05],
        [0.0000e+00, 4.1204e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2019e-03,
         5.1577e-05],
        [0.0000e+00, 4.1205e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2019e-03,
         5.1579e-05],
        ...,
        [0.0000e+00, 4.1295e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2045e-03,
         5.1691e-05],
        [0.0000e+00, 4.1293e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2045e-03,
         5.1688e-05],
        [0.0000e+00, 4.1290e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2044e-03,
         5.1685e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49103.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0106],
        [0.0024, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0073],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0064],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291093.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1449.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.5742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(466.8801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.8578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.0853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0096],
        [-0.0128],
        [-0.0135],
        ...,
        [-0.0831],
        [-0.0827],
        [-0.0826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-22805.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0011],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365916.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0012],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1048e-03,  3.5052e-03, -3.9047e-03,  ...,  2.4497e-03,
          1.1099e-03,  3.9418e-03],
        [ 1.4432e-03,  4.1716e-03, -4.9702e-03,  ...,  3.3103e-03,
          1.3350e-03,  5.0078e-03],
        [-1.3502e-04,  1.0633e-03,  0.0000e+00,  ..., -7.0431e-04,
          2.8516e-04,  3.5484e-05],
        ...,
        [ 1.9004e-03,  5.0722e-03, -6.4103e-03,  ...,  4.4735e-03,
          1.6391e-03,  6.4484e-03],
        [-1.3502e-04,  1.0633e-03,  0.0000e+00,  ..., -7.0431e-04,
          2.8516e-04,  3.5484e-05],
        [-1.3502e-04,  1.0633e-03,  0.0000e+00,  ..., -7.0431e-04,
          2.8516e-04,  3.5484e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(718.1060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7805, device='cuda:0')



h[100].sum tensor(3.2837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-30.0545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.0060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0274, 0.0000,  ..., 0.0270, 0.0089, 0.0371],
        [0.0044, 0.0135, 0.0000,  ..., 0.0106, 0.0043, 0.0150],
        [0.0014, 0.0074, 0.0000,  ..., 0.0033, 0.0022, 0.0051],
        ...,
        [0.0143, 0.0333, 0.0000,  ..., 0.0354, 0.0110, 0.0466],
        [0.0127, 0.0300, 0.0000,  ..., 0.0311, 0.0098, 0.0413],
        [0.0091, 0.0227, 0.0000,  ..., 0.0224, 0.0074, 0.0297]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44693.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0322],
        [0.0258, 0.0000, 0.0000,  ..., 0.0445, 0.0000, 0.0240],
        [0.0131, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0158],
        ...,
        [0.1031, 0.0000, 0.0000,  ..., 0.1706, 0.0000, 0.0611],
        [0.0818, 0.0000, 0.0000,  ..., 0.1358, 0.0000, 0.0508],
        [0.0564, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(267150.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1402.1879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-223.5805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(408.5720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.5949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.4474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0838],
        [ 0.0749],
        [ 0.0547],
        ...,
        [-0.0942],
        [-0.0445],
        [-0.0026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-28990.4570, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0012],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0012],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365925.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4519e-04,  1.0973e-03,  0.0000e+00,  ..., -7.2353e-04,
          2.6137e-04,  4.0513e-05],
        [ 2.2617e-03,  5.8329e-03, -7.5634e-03,  ...,  5.3916e-03,
          1.8618e-03,  7.6193e-03],
        [ 2.2617e-03,  5.8329e-03, -7.5634e-03,  ...,  5.3916e-03,
          1.8618e-03,  7.6193e-03],
        ...,
        [-1.4519e-04,  1.0973e-03,  0.0000e+00,  ..., -7.2353e-04,
          2.6137e-04,  4.0513e-05],
        [-1.4519e-04,  1.0973e-03,  0.0000e+00,  ..., -7.2353e-04,
          2.6137e-04,  4.0513e-05],
        [-1.4519e-04,  1.0973e-03,  0.0000e+00,  ..., -7.2353e-04,
          2.6137e-04,  4.0513e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(683.5325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.1328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3395, device='cuda:0')



h[100].sum tensor(1.2498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-31.5136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0130, 0.0000,  ..., 0.0097, 0.0039, 0.0139],
        [0.0059, 0.0168, 0.0000,  ..., 0.0139, 0.0052, 0.0201],
        [0.0088, 0.0225, 0.0000,  ..., 0.0212, 0.0072, 0.0291],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0010, 0.0002],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0010, 0.0002],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0010, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42203.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0000, 0.0000,  ..., 0.0533, 0.0000, 0.0274],
        [0.0466, 0.0000, 0.0000,  ..., 0.0784, 0.0000, 0.0354],
        [0.0656, 0.0000, 0.0000,  ..., 0.1094, 0.0000, 0.0449],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(257586.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1413.6633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.3441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(382.9619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.5896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.3666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0027],
        [-0.0442],
        [-0.0850],
        ...,
        [-0.0860],
        [-0.0857],
        [-0.0856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33524.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0012],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365925.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5697e-04,  1.1327e-03,  0.0000e+00,  ..., -7.4473e-04,
          2.3025e-04,  2.9508e-05],
        [-1.5697e-04,  1.1327e-03,  0.0000e+00,  ..., -7.4473e-04,
          2.3025e-04,  2.9508e-05],
        [ 1.3473e-03,  4.0894e-03, -4.7167e-03,  ...,  3.0725e-03,
          1.2301e-03,  4.7632e-03],
        ...,
        [-1.5697e-04,  1.1327e-03,  0.0000e+00,  ..., -7.4473e-04,
          2.3025e-04,  2.9508e-05],
        [-1.5697e-04,  1.1327e-03,  0.0000e+00,  ..., -7.4473e-04,
          2.3025e-04,  2.9508e-05],
        [-1.5697e-04,  1.1327e-03,  0.0000e+00,  ..., -7.4473e-04,
          2.3025e-04,  2.9508e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.0066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4089, device='cuda:0')



h[100].sum tensor(0.8901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-31.7789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.3785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0009, 0.0001],
        [0.0013, 0.0075, 0.0000,  ..., 0.0031, 0.0019, 0.0048],
        [0.0031, 0.0113, 0.0000,  ..., 0.0072, 0.0032, 0.0109],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0009, 0.0001],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0009, 0.0001],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0009, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44466.7852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0112],
        [0.0116, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0168],
        [0.0225, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.0241],
        ...,
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(274723., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1536.3286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-236.2279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(268.6009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.6211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.8569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0531],
        [ 0.0669],
        [ 0.0784],
        ...,
        [-0.0884],
        [-0.0881],
        [-0.0880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-25713.5508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365936.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.0320e-03,  9.4226e-03, -1.3160e-02,  ...,  9.8918e-03,
          2.9867e-03,  1.3233e-02],
        [ 4.1808e-03,  9.7149e-03, -1.3626e-02,  ...,  1.0269e-02,
          3.0856e-03,  1.3701e-02],
        [ 3.5028e-03,  8.3834e-03, -1.1504e-02,  ...,  8.5503e-03,
          2.6351e-03,  1.1568e-02],
        ...,
        [-1.7319e-04,  1.1643e-03,  0.0000e+00,  ..., -7.6850e-04,
          1.9277e-04,  6.1912e-06],
        [-1.7319e-04,  1.1643e-03,  0.0000e+00,  ..., -7.6850e-04,
          1.9277e-04,  6.1912e-06],
        [-1.7319e-04,  1.1643e-03,  0.0000e+00,  ..., -7.6850e-04,
          1.9277e-04,  6.1912e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(676.2835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0909, device='cuda:0')



h[100].sum tensor(-0.5331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-33.0778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.5132e-02, 3.5729e-02, 0.0000e+00,  ..., 3.7043e-02, 1.1284e-02,
         4.9796e-02],
        [1.6338e-02, 3.8097e-02, 0.0000e+00,  ..., 4.0100e-02, 1.2085e-02,
         5.3590e-02],
        [2.0320e-02, 4.5918e-02, 0.0000e+00,  ..., 5.0196e-02, 1.4731e-02,
         6.6115e-02],
        ...,
        [0.0000e+00, 4.6655e-03, 0.0000e+00,  ..., 0.0000e+00, 7.7247e-04,
         2.4810e-05],
        [0.0000e+00, 4.6652e-03, 0.0000e+00,  ..., 0.0000e+00, 7.7242e-04,
         2.4808e-05],
        [0.0000e+00, 4.6649e-03, 0.0000e+00,  ..., 0.0000e+00, 7.7237e-04,
         2.4806e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40151.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0673, 0.0000, 0.0000,  ..., 0.1122, 0.0000, 0.0487],
        [0.0735, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.0522],
        [0.0804, 0.0000, 0.0000,  ..., 0.1341, 0.0000, 0.0559],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0087],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0087],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(252594.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1441.6558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-255.0899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(198.7691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.3376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.2413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0620],
        [ 0.0610],
        [ 0.0637],
        ...,
        [-0.0921],
        [-0.0918],
        [-0.0917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32177.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0011],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365936.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0010],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365943., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9188e-04,  1.1964e-03,  0.0000e+00,  ..., -7.9223e-04,
          1.5001e-04, -1.8619e-05],
        [ 1.4266e-03,  4.3724e-03, -5.0558e-03,  ...,  3.3072e-03,
          1.2250e-03,  5.0701e-03],
        [-1.9188e-04,  1.1964e-03,  0.0000e+00,  ..., -7.9223e-04,
          1.5001e-04, -1.8619e-05],
        ...,
        [-1.9188e-04,  1.1964e-03,  0.0000e+00,  ..., -7.9223e-04,
          1.5001e-04, -1.8619e-05],
        [-1.9188e-04,  1.1964e-03,  0.0000e+00,  ..., -7.9223e-04,
          1.5001e-04, -1.8619e-05],
        [-1.9188e-04,  1.1964e-03,  0.0000e+00,  ..., -7.9223e-04,
          1.5001e-04, -1.8619e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(729.7192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0261, device='cuda:0')



h[100].sum tensor(0.3418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-32.6709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0186, 0.0000,  ..., 0.0146, 0.0053, 0.0220],
        [0.0011, 0.0074, 0.0000,  ..., 0.0026, 0.0015, 0.0041],
        [0.0014, 0.0080, 0.0000,  ..., 0.0033, 0.0017, 0.0051],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43632.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0244, 0.0000, 0.0000,  ..., 0.0429, 0.0000, 0.0267],
        [0.0151, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0198],
        [0.0093, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0155],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0089],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0089],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(272128.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1597.1697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-247.8055, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(261.0236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.8452, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.8423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0675],
        [ 0.0656],
        [ 0.0620],
        ...,
        [-0.0952],
        [-0.0950],
        [-0.0949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30757.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0010],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365943., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365950.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05],
        [-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05],
        [-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05],
        ...,
        [-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05],
        [-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05],
        [-2.1476e-04,  1.2231e-03,  0.0000e+00,  ..., -8.1604e-04,
          1.0240e-04, -4.8178e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(800.5461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1180, device='cuda:0')



h[100].sum tensor(1.8402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-31.9135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.9239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0024, 0.0104, 0.0000,  ..., 0.0055, 0.0023, 0.0088],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0018, 0.0089, 0.0000,  ..., 0.0043, 0.0018, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47919.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0102],
        [0.0089, 0.0000, 0.0000,  ..., 0.0112, 0.0000, 0.0144],
        [0.0282, 0.0000, 0.0000,  ..., 0.0445, 0.0000, 0.0269],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0073, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0129],
        [0.0177, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(295423.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1780.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.5599, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(405.5164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.5878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0136],
        [-0.0109],
        [-0.0630],
        ...,
        [-0.0433],
        [-0.0037],
        [ 0.0274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26453.1113, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0012],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365950.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365957.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05],
        [-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05],
        [-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05],
        ...,
        [-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05],
        [-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05],
        [-2.3956e-04,  1.2493e-03,  0.0000e+00,  ..., -8.3866e-04,
          5.3145e-05, -7.9604e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(694.7853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7754, device='cuda:0')



h[100].sum tensor(-0.9724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-34.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.1986, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39336.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0098],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254718.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1569.1174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-270.4269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(152.3099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.4872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.1121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0199],
        [-0.0669],
        [-0.1032],
        ...,
        [-0.1027],
        [-0.1023],
        [-0.1022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36833.4102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9990],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365957.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(558.4992, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365965.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04],
        [-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04],
        [-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04],
        ...,
        [-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04],
        [-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04],
        [-2.6613e-04,  1.2772e-03,  0.0000e+00,  ..., -8.6041e-04,
          5.9177e-07, -1.1180e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(770.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3560, device='cuda:0')



h[100].sum tensor(0.7817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-33.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.3422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.1043e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3650e-06,
         0.0000e+00],
        [0.0000e+00, 5.1041e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3650e-06,
         0.0000e+00],
        [0.0000e+00, 5.1044e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3651e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.1203e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3725e-06,
         0.0000e+00],
        [0.0000e+00, 5.1200e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3723e-06,
         0.0000e+00],
        [0.0000e+00, 5.1197e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3722e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44421.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0103],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0100],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0098],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0098],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(281651.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1760.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-256.5533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(223.1028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.7251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0246],
        [-0.0019],
        [-0.0333],
        ...,
        [-0.1067],
        [-0.1062],
        [-0.1060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32589.2715, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365965.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365974., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9321e-04,  1.3083e-03,  0.0000e+00,  ..., -8.8031e-04,
         -5.4870e-05, -1.4517e-04],
        [-2.9321e-04,  1.3083e-03,  0.0000e+00,  ..., -8.8031e-04,
         -5.4870e-05, -1.4517e-04],
        [ 1.4757e-03,  4.7721e-03, -5.4946e-03,  ...,  3.5936e-03,
          1.1202e-03,  5.4147e-03],
        ...,
        [-2.9321e-04,  1.3083e-03,  0.0000e+00,  ..., -8.8031e-04,
         -5.4870e-05, -1.4517e-04],
        [-2.9321e-04,  1.3083e-03,  0.0000e+00,  ..., -8.8031e-04,
         -5.4870e-05, -1.4517e-04],
        [-2.9321e-04,  1.3083e-03,  0.0000e+00,  ..., -8.8031e-04,
         -5.4870e-05, -1.4517e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(826.5057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8592, device='cuda:0')



h[100].sum tensor(2.2123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-33.0424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.0600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0025, 0.0114, 0.0000,  ..., 0.0061, 0.0020, 0.0095],
        [0.0069, 0.0205, 0.0000,  ..., 0.0171, 0.0050, 0.0242],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48711.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0168],
        [0.0242, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0268],
        [0.0464, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0405],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0102],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0102],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(308934.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1972.9139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-246.1770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(588.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.3017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-140.1301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0554],
        [ 0.0635],
        [ 0.0590],
        ...,
        [-0.1019],
        [-0.1098],
        [-0.1113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36876.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365974., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365982.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        ...,
        [-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0013,  0.0000,  ..., -0.0009, -0.0001, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(727.1801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6820, device='cuda:0')



h[100].sum tensor(-0.0911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-35.3940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40162.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0108],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(263844.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1635.1635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-281.2934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109.9953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.4893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-112.6489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1600],
        [-0.1690],
        [-0.1761],
        ...,
        [-0.1173],
        [-0.1168],
        [-0.1166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40187.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365982.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365991.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0141, -0.0202,  ...,  0.0156,  0.0042,  0.0203],
        [ 0.0066,  0.0150, -0.0216,  ...,  0.0167,  0.0045,  0.0217],
        [ 0.0052,  0.0123, -0.0173,  ...,  0.0132,  0.0036,  0.0173],
        ...,
        [-0.0003,  0.0014,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0014,  0.0000,  ..., -0.0009, -0.0001, -0.0002],
        [-0.0003,  0.0014,  0.0000,  ..., -0.0009, -0.0001, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(953.3148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4158, device='cuda:0')



h[100].sum tensor(5.2280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0175, device='cuda:0')



h[200].sum tensor(-31.7141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.5596, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0258, 0.0586, 0.0000,  ..., 0.0651, 0.0175, 0.0847],
        [0.0266, 0.0602, 0.0000,  ..., 0.0672, 0.0181, 0.0872],
        [0.0293, 0.0655, 0.0000,  ..., 0.0740, 0.0198, 0.0957],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57122.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1333, 0.0000, 0.0000,  ..., 0.2118, 0.0000, 0.0896],
        [0.1459, 0.0000, 0.0000,  ..., 0.2308, 0.0000, 0.0963],
        [0.1540, 0.0000, 0.0000,  ..., 0.2429, 0.0000, 0.1006],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0112],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0112],
        [0.0024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360797.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2392.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.5420, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1066.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.9972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.9811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0858],
        [-0.1097],
        [-0.1139],
        ...,
        [-0.1219],
        [-0.1214],
        [-0.1213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-41180.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365991.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365999.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0048, -0.0053,  ...,  0.0034,  0.0010,  0.0052],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0002],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0002],
        ...,
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0002],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0002],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(764.4650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2175, device='cuda:0')



h[100].sum tensor(0.5779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-35.7304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.5020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0083, 0.0000,  ..., 0.0026, 0.0008, 0.0042],
        [0.0014, 0.0090, 0.0000,  ..., 0.0034, 0.0010, 0.0052],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41240.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0084, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0176],
        [0.0071, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0163],
        [0.0037, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0133],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271864.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1631.8177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-281.2498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(131.0040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.5517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-112.1860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0955],
        [-0.1014],
        [-0.1007],
        ...,
        [-0.1278],
        [-0.1273],
        [-0.1271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44168.0586, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365999.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0011],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366008.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0076, -0.0098,  ...,  0.0070,  0.0019,  0.0097],
        [ 0.0032,  0.0085, -0.0111,  ...,  0.0082,  0.0022,  0.0111],
        [ 0.0097,  0.0212, -0.0312,  ...,  0.0246,  0.0065,  0.0315],
        ...,
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(930.1628, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1650, device='cuda:0')



h[100].sum tensor(4.1027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0152, device='cuda:0')



h[200].sum tensor(-33.3052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0230, 0.0538, 0.0000,  ..., 0.0585, 0.0156, 0.0763],
        [0.0263, 0.0602, 0.0000,  ..., 0.0668, 0.0178, 0.0866],
        [0.0154, 0.0389, 0.0000,  ..., 0.0392, 0.0105, 0.0523],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53249.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1209, 0.0000, 0.0000,  ..., 0.1943, 0.0000, 0.0856],
        [0.1300, 0.0000, 0.0000,  ..., 0.2083, 0.0000, 0.0907],
        [0.1069, 0.0000, 0.0000,  ..., 0.1726, 0.0000, 0.0778],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339755.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2101.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-236.8271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(795.6866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.9131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.0074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0510],
        [-0.0549],
        [-0.0386],
        ...,
        [-0.1313],
        [-0.1307],
        [-0.1306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42966.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0011],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366008.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0011],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366008.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        ...,
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(909.5718, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4422, device='cuda:0')



h[100].sum tensor(3.6251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0148, device='cuda:0')



h[200].sum tensor(-33.6808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.1464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0140, 0.0000,  ..., 0.0090, 0.0025, 0.0129],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50747.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0142],
        [0.0064, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0163],
        [0.0164, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0246],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(323373.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1955.1753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-246.1432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(442.1943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.5604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.9350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0019],
        [ 0.0297],
        [ 0.0661],
        ...,
        [-0.1313],
        [-0.1307],
        [-0.1306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38345.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0011],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366008.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366017.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0049, -0.0055,  ...,  0.0035,  0.0010,  0.0053],
        [ 0.0007,  0.0037, -0.0035,  ...,  0.0019,  0.0005,  0.0033],
        [ 0.0025,  0.0071, -0.0090,  ...,  0.0064,  0.0017,  0.0089],
        ...,
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(810.9989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1270, device='cuda:0')



h[100].sum tensor(0.7650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-36.0985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.4399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0126, 0.0000,  ..., 0.0070, 0.0019, 0.0105],
        [0.0080, 0.0246, 0.0000,  ..., 0.0207, 0.0056, 0.0292],
        [0.0046, 0.0171, 0.0000,  ..., 0.0119, 0.0032, 0.0175],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42306.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0217, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.0289],
        [0.0319, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0364],
        [0.0286, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0344],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0129],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0129],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0129]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(280538.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1650.3087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-275.2469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(241.3090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.0641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-111.9171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0815],
        [ 0.0840],
        [ 0.0841],
        ...,
        [-0.1233],
        [-0.1157],
        [-0.1096]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50568.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366017.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366026.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        ...,
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003],
        [-0.0004,  0.0014,  0.0000,  ..., -0.0009, -0.0002, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1108.1564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.4108, device='cuda:0')



h[100].sum tensor(6.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0198, device='cuda:0')



h[200].sum tensor(-31.3681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.3013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0076, 0.0000,  ..., 0.0014, 0.0004, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59308.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0134],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0134],
        [0.0028, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0139],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0144],
        [0.0038, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0158],
        [0.0054, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(361587.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2275.6851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-213.8018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1028.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.8867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.3632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1569],
        [-0.1233],
        [-0.0702],
        ...,
        [-0.1021],
        [-0.0757],
        [-0.0520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44301.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366026.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366035.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        ...,
        [-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0004,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(948.9327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2923, device='cuda:0')



h[100].sum tensor(2.6620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-34.7773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.6710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0030, 0.0126, 0.0000,  ..., 0.0078, 0.0021, 0.0105],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48177.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0000, 0.0000,  ..., 0.0318, 0.0000, 0.0275],
        [0.0124, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0212],
        [0.0077, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0182],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0139],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0139],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0139]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309993.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1891.2339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-256.0270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(360.9757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.2318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.2225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0776],
        [ 0.0750],
        [ 0.0723],
        ...,
        [-0.1369],
        [-0.1363],
        [-0.1362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45444.0664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366035.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(527.5331, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366044.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0007,  0.0038, -0.0037,  ...,  0.0021,  0.0005,  0.0034],
        [ 0.0007,  0.0038, -0.0037,  ...,  0.0021,  0.0005,  0.0034],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(935.5289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6799, device='cuda:0')



h[100].sum tensor(1.6245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-35.5274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.8782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0013, 0.0101, 0.0000,  ..., 0.0036, 0.0009, 0.0061],
        [0.0013, 0.0101, 0.0000,  ..., 0.0036, 0.0009, 0.0061],
        [0.0013, 0.0101, 0.0000,  ..., 0.0036, 0.0009, 0.0061],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45913.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0260],
        [0.0092, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0234],
        [0.0074, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0211],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0145],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0145],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297763.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1842.5531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-267.1267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(227.3308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(164.1125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-120.2961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0703],
        [ 0.0282],
        [-0.0320],
        ...,
        [-0.1383],
        [-0.1378],
        [-0.1376]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42288.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0010],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366044.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366054.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [ 0.0050,  0.0121, -0.0167,  ...,  0.0128,  0.0034,  0.0167],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1088.9189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5910, device='cuda:0')



h[100].sum tensor(4.4180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0165, device='cuda:0')



h[200].sum tensor(-33.2918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.3073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0225, 0.0000,  ..., 0.0196, 0.0051, 0.0260],
        [0.0097, 0.0266, 0.0000,  ..., 0.0249, 0.0066, 0.0326],
        [0.0209, 0.0504, 0.0000,  ..., 0.0538, 0.0142, 0.0701],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55597.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0397, 0.0000, 0.0000,  ..., 0.0614, 0.0000, 0.0403],
        [0.0524, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.0486],
        [0.0645, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.0560],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0151],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0151],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(350657.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2287.8860, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-234.9136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(672.3030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(176.2539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.4166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0564],
        [ 0.0608],
        [ 0.0597],
        ...,
        [-0.1397],
        [-0.1392],
        [-0.1390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40300.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366054.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366063.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084,  0.0188, -0.0271,  ...,  0.0214,  0.0057,  0.0275],
        [ 0.0079,  0.0178, -0.0257,  ...,  0.0202,  0.0053,  0.0259],
        [ 0.0080,  0.0180, -0.0260,  ...,  0.0205,  0.0054,  0.0262],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1035.8796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3733, device='cuda:0')



h[100].sum tensor(2.5928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-34.6665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.4129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.0711, 0.0000,  ..., 0.0807, 0.0213, 0.1035],
        [0.0345, 0.0769, 0.0000,  ..., 0.0882, 0.0233, 0.1128],
        [0.0337, 0.0755, 0.0000,  ..., 0.0864, 0.0228, 0.1106],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50504.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1717, 0.0000, 0.0000,  ..., 0.2692, 0.0000, 0.1223],
        [0.1776, 0.0000, 0.0000,  ..., 0.2779, 0.0000, 0.1257],
        [0.1678, 0.0000, 0.0000,  ..., 0.2635, 0.0000, 0.1201],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0158]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324303.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2106.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-255.4894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(282.3259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.9753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.4613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0658],
        [-0.0645],
        [-0.0511],
        ...,
        [-0.1413],
        [-0.1408],
        [-0.1406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38025.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366063.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366072.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1116.4146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2208, device='cuda:0')



h[100].sum tensor(3.4388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0163, device='cuda:0')



h[200].sum tensor(-33.7673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.0532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54083.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0205],
        [0.0035, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0174],
        [0.0031, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0170],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0163],
        [0.0029, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0163],
        [0.0029, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343378., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2360.4224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.2409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(673.1233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.3314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.8225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0431],
        [ 0.0009],
        [-0.0528],
        ...,
        [-0.1409],
        [-0.1404],
        [-0.1402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42936.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366072.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366082.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [ 0.0019,  0.0061, -0.0073,  ...,  0.0050,  0.0013,  0.0070],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.7340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7642, device='cuda:0')



h[100].sum tensor(-0.1324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-36.3654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0141, 0.0000,  ..., 0.0095, 0.0025, 0.0126],
        [0.0019, 0.0106, 0.0000,  ..., 0.0050, 0.0013, 0.0070],
        [0.0060, 0.0196, 0.0000,  ..., 0.0157, 0.0041, 0.0211],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45080.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.0000, 0.0000,  ..., 0.0500, 0.0000, 0.0376],
        [0.0301, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0364],
        [0.0418, 0.0000, 0.0000,  ..., 0.0657, 0.0000, 0.0440],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300070.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2094.3987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.8779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(255.6140, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.9083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.1981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0671],
        [ 0.0673],
        [ 0.0674],
        ...,
        [-0.1417],
        [-0.1412],
        [-0.1411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42924.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366082.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366091.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0072, -0.0090,  ...,  0.0065,  0.0016,  0.0088],
        [ 0.0009,  0.0043, -0.0043,  ...,  0.0026,  0.0006,  0.0040],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1195.5686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0910, device='cuda:0')



h[100].sum tensor(3.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0174, device='cuda:0')



h[200].sum tensor(-33.2229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.3367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0106, 0.0305, 0.0000,  ..., 0.0279, 0.0071, 0.0377],
        [0.0066, 0.0207, 0.0000,  ..., 0.0171, 0.0044, 0.0228],
        [0.0009, 0.0088, 0.0000,  ..., 0.0026, 0.0006, 0.0040],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56649.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0600, 0.0000, 0.0000,  ..., 0.0985, 0.0000, 0.0579],
        [0.0449, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.0473],
        [0.0284, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.0363],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353392.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2517.3081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(583.7169, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.7787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.2986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.2637e-05],
        [ 3.1549e-02],
        [ 5.7040e-02],
        ...,
        [-1.4465e-01],
        [-1.4412e-01],
        [-1.4395e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36399.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366091.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366101.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        ...,
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005],
        [-0.0005,  0.0015,  0.0000,  ..., -0.0010, -0.0003, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1026.2946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0580, device='cuda:0')



h[100].sum tensor(-0.5982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-36.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.7651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44750.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0242, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0314],
        [0.0087, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0211],
        [0.0042, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0183],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0172],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0172],
        [0.0034, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292735.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2077.6438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.8330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(207.5702, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.0505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.8501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0613],
        [ 0.0607],
        [ 0.0641],
        ...,
        [-0.1485],
        [-0.1481],
        [-0.1480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45833.7891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0013],
        [1.0009],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366101.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366111.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [ 0.0012,  0.0048, -0.0051,  ...,  0.0032,  0.0008,  0.0047],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1097.5194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8363, device='cuda:0')



h[100].sum tensor(0.5744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-35.4418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0124, 0.0000,  ..., 0.0072, 0.0018, 0.0096],
        [0.0012, 0.0094, 0.0000,  ..., 0.0032, 0.0008, 0.0047],
        [0.0027, 0.0133, 0.0000,  ..., 0.0073, 0.0018, 0.0105],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48236.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0290],
        [0.0180, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0297],
        [0.0248, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0354],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309939.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2231.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-274.6613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(255.8635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.2200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0733],
        [ 0.0757],
        [ 0.0781],
        ...,
        [-0.1539],
        [-0.1534],
        [-0.1532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-48053.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366111.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366111.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1056.4912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2186, device='cuda:0')



h[100].sum tensor(-0.3008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-36.1327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0020, 0.0111, 0.0000,  ..., 0.0054, 0.0013, 0.0074],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45755.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0186],
        [0.0071, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0201],
        [0.0153, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0262],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299377.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2139.6187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.9598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(171.5183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.3631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-120.2421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0026],
        [-0.0237],
        [-0.0199],
        ...,
        [-0.1542],
        [-0.1536],
        [-0.1535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45510.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366111.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366121.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [ 0.0029,  0.0081, -0.0102,  ...,  0.0075,  0.0019,  0.0100],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1062.0903, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7698, device='cuda:0')



h[100].sum tensor(-0.5016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-36.3158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0155, 0.0000,  ..., 0.0101, 0.0025, 0.0139],
        [0.0022, 0.0116, 0.0000,  ..., 0.0059, 0.0015, 0.0081],
        [0.0137, 0.0367, 0.0000,  ..., 0.0358, 0.0091, 0.0472],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46680.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0251, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.0344],
        [0.0264, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.0344],
        [0.0461, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0474],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0035, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311643.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2242.3823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-284.5287, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(240.5031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.3693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0709],
        [ 0.0657],
        [ 0.0596],
        ...,
        [-0.1585],
        [-0.1579],
        [-0.1577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44441.9883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0008],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366121.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(521.6303, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0007],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366133., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [ 0.0016,  0.0056, -0.0062,  ...,  0.0042,  0.0010,  0.0059],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1567.7786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.4497, device='cuda:0')



h[100].sum tensor(9.9738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0276, device='cuda:0')



h[200].sum tensor(-28.2084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.9357, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0125, 0.0000,  ..., 0.0061, 0.0014, 0.0089],
        [0.0059, 0.0206, 0.0000,  ..., 0.0157, 0.0039, 0.0216],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78275.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0235],
        [0.0232, 0.0000, 0.0000,  ..., 0.0378, 0.0000, 0.0332],
        [0.0412, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0460],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0037, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478554.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3523.4912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-172.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1727.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(188.0989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.1910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0654],
        [ 0.0689],
        [ 0.0698],
        ...,
        [-0.1617],
        [-0.1610],
        [-0.1609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-37683.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9992],
        ...,
        [1.0012],
        [1.0007],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366133., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366143.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1187.7061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8945, device='cuda:0')



h[100].sum tensor(1.6236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-34.8678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.7705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54643.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.5767e-03, 0.0000e+00, 0.0000e+00,  ..., 6.7113e-03, 0.0000e+00,
         2.0040e-02],
        [5.8341e-03, 0.0000e+00, 0.0000e+00,  ..., 4.9779e-03, 0.0000e+00,
         1.9244e-02],
        [4.4843e-03, 0.0000e+00, 0.0000e+00,  ..., 1.5365e-03, 0.0000e+00,
         1.7821e-02],
        ...,
        [3.8287e-03, 0.0000e+00, 0.0000e+00,  ..., 5.5771e-05, 0.0000e+00,
         1.7151e-02],
        [3.8287e-03, 0.0000e+00, 0.0000e+00,  ..., 5.5818e-05, 0.0000e+00,
         1.7151e-02],
        [3.8286e-03, 0.0000e+00, 0.0000e+00,  ..., 5.5801e-05, 0.0000e+00,
         1.7150e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364341.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2735.8247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.6622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(787.6898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.0330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.8303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0386],
        [ 0.0187],
        [-0.0124],
        ...,
        [-0.1632],
        [-0.1626],
        [-0.1624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53878.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366143.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366154.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0096, -0.0125,  ...,  0.0095,  0.0024,  0.0124],
        [ 0.0016,  0.0056, -0.0063,  ...,  0.0043,  0.0010,  0.0059],
        [ 0.0016,  0.0056, -0.0063,  ...,  0.0043,  0.0010,  0.0059],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1166.9470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1366, device='cuda:0')



h[100].sum tensor(0.8519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-35.4572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.5641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0090, 0.0276, 0.0000,  ..., 0.0237, 0.0058, 0.0320],
        [0.0094, 0.0283, 0.0000,  ..., 0.0247, 0.0061, 0.0331],
        [0.0028, 0.0137, 0.0000,  ..., 0.0076, 0.0018, 0.0107],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50941.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0416, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0450],
        [0.0375, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.0421],
        [0.0235, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0327],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0169],
        [0.0037, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0169],
        [0.0037, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335631.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2468.4783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.6202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(356.1355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.2046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.6868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0760],
        [ 0.0765],
        [ 0.0762],
        ...,
        [-0.1654],
        [-0.1650],
        [-0.1650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45614.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366154.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366164.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038,  0.0099, -0.0129,  ...,  0.0098,  0.0025,  0.0128],
        [ 0.0025,  0.0073, -0.0089,  ...,  0.0065,  0.0016,  0.0087],
        [ 0.0037,  0.0097, -0.0126,  ...,  0.0096,  0.0024,  0.0125],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.6244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5251, device='cuda:0')



h[100].sum tensor(0.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-35.7105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0269, 0.0000,  ..., 0.0227, 0.0055, 0.0307],
        [0.0099, 0.0293, 0.0000,  ..., 0.0260, 0.0064, 0.0347],
        [0.0059, 0.0207, 0.0000,  ..., 0.0157, 0.0038, 0.0213],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49496.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0000, 0.0000,  ..., 0.0918, 0.0000, 0.0549],
        [0.0540, 0.0000, 0.0000,  ..., 0.0893, 0.0000, 0.0538],
        [0.0450, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.0477],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0167],
        [0.0036, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0167],
        [0.0036, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(325634.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2350.8708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.1317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(263.1395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.2808, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.2235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0680],
        [ 0.0688],
        [ 0.0699],
        ...,
        [-0.1695],
        [-0.1688],
        [-0.1686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50179.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366164.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366175.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1140.3137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1912, device='cuda:0')



h[100].sum tensor(-0.3753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-36.5529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46566.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0164],
        [0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0164],
        [0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0165],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0165],
        [0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0165],
        [0.0037, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312951.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2316.7720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.5681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(236.7415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.7589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.3839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1892],
        [-0.1741],
        [-0.1433],
        ...,
        [-0.1701],
        [-0.1695],
        [-0.1693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55146.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366175.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366186.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1240.3584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8162, device='cuda:0')



h[100].sum tensor(1.3386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-35.3735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49591.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0162],
        [0.0040, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0162],
        [0.0072, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0184],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0163],
        [0.0040, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0163],
        [0.0040, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321071.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2458.2959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-288.3347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(386.2905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.5386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.5843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0127],
        [-0.0087],
        [-0.0098],
        ...,
        [-0.1694],
        [-0.1688],
        [-0.1686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54770.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366186.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366197.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1422.2533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5671, device='cuda:0')



h[100].sum tensor(4.7918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0188, device='cuda:0')



h[200].sum tensor(-32.9030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.0359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60941.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0000, 0.0000,  ..., 0.0141, 0.0000, 0.0215],
        [0.0080, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0197],
        [0.0068, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0184],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0161],
        [0.0043, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0161],
        [0.0043, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384983.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2988.6226, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-246.3734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(683.0901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.3651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.8587, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0697],
        [ 0.0595],
        [ 0.0432],
        ...,
        [-0.1682],
        [-0.1676],
        [-0.1674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39771.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366197.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366208.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0127, -0.0173,  ...,  0.0136,  0.0035,  0.0174],
        [ 0.0011,  0.0047, -0.0048,  ...,  0.0031,  0.0007,  0.0044],
        [ 0.0046,  0.0113, -0.0151,  ...,  0.0117,  0.0030,  0.0152],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1605.2036, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.5962, device='cuda:0')



h[100].sum tensor(8.2680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0238, device='cuda:0')



h[200].sum tensor(-30.3544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-29.2324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0228, 0.0000,  ..., 0.0187, 0.0045, 0.0250],
        [0.0193, 0.0469, 0.0000,  ..., 0.0493, 0.0126, 0.0635],
        [0.0116, 0.0322, 0.0000,  ..., 0.0299, 0.0074, 0.0395],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74038.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0607, 0.0000, 0.0000,  ..., 0.0955, 0.0000, 0.0551],
        [0.0923, 0.0000, 0.0000,  ..., 0.1430, 0.0000, 0.0747],
        [0.0949, 0.0000, 0.0000,  ..., 0.1475, 0.0000, 0.0766],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0160],
        [0.0065, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0175],
        [0.0132, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474547.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3718.7935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-198.2353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1698.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(172.0361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.7754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0445],
        [ 0.0260],
        [ 0.0132],
        ...,
        [-0.0974],
        [-0.0330],
        [ 0.0117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39852.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9993],
        ...,
        [1.0011],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366208.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366219.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1420.4514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0704, device='cuda:0')



h[100].sum tensor(4.5903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0174, device='cuda:0')



h[200].sum tensor(-33.5885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.3225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58427.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0165],
        [0.0044, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0159],
        [0.0044, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0158],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0159],
        [0.0044, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0159],
        [0.0044, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378191.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2983.8618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-255.6214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(620.6818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.5900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.2701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0760],
        [-0.1352],
        [-0.1809],
        ...,
        [-0.1710],
        [-0.1704],
        [-0.1702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36952.8945, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366219.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366230.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1271.4246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1920, device='cuda:0')



h[100].sum tensor(1.7337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-36.1386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50088.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157],
        [0.0042, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157],
        [0.0043, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157],
        [0.0042, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157],
        [0.0042, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(336564.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2601.9214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.1177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(339.5710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.9071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.1224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2371],
        [-0.2272],
        [-0.2078],
        ...,
        [-0.1749],
        [-0.1743],
        [-0.1741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46283.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366230.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(581.8447, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366241.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1359.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6796, device='cuda:0')



h[100].sum tensor(3.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-34.9044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0095, 0.0000,  ..., 0.0031, 0.0007, 0.0044],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53227.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0159],
        [0.0074, 0.0000, 0.0000,  ..., 0.0084, 0.0000, 0.0185],
        [0.0138, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0240],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0156],
        [0.0040, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0156],
        [0.0040, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347246.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2643.5256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-274.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(465.2921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.6906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0590],
        [ 0.0055],
        [ 0.0520],
        ...,
        [-0.1795],
        [-0.1788],
        [-0.1786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52727.1914, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366241.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366252.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0090, -0.0115,  ...,  0.0087,  0.0022,  0.0114],
        [ 0.0047,  0.0114, -0.0152,  ...,  0.0119,  0.0030,  0.0153],
        [ 0.0021,  0.0065, -0.0075,  ...,  0.0054,  0.0013,  0.0072],
        ...,
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0005,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1190.5386, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0022, device='cuda:0')



h[100].sum tensor(0.8437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-37.5748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.6680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0213, 0.0507, 0.0000,  ..., 0.0541, 0.0138, 0.0695],
        [0.0175, 0.0434, 0.0000,  ..., 0.0445, 0.0112, 0.0576],
        [0.0148, 0.0373, 0.0000,  ..., 0.0375, 0.0095, 0.0482],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44930.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1393, 0.0000, 0.0000,  ..., 0.2079, 0.0000, 0.1023],
        [0.1259, 0.0000, 0.0000,  ..., 0.1884, 0.0000, 0.0941],
        [0.0996, 0.0000, 0.0000,  ..., 0.1486, 0.0000, 0.0774],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0155],
        [0.0037, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0155],
        [0.0037, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(314878., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2294.1414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.0709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104.3122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.4475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-110.4150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0830],
        [-0.0724],
        [-0.0456],
        ...,
        [-0.1850],
        [-0.1843],
        [-0.1841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52136., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366252.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366264., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0040, -0.0037,  ...,  0.0021,  0.0004,  0.0032],
        [ 0.0026,  0.0074, -0.0090,  ...,  0.0066,  0.0016,  0.0088],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1533.4788, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4694, device='cuda:0')



h[100].sum tensor(7.9089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0204, device='cuda:0')



h[200].sum tensor(-32.3465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.0277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0273, 0.0000,  ..., 0.0233, 0.0055, 0.0313],
        [0.0034, 0.0157, 0.0000,  ..., 0.0090, 0.0019, 0.0129],
        [0.0032, 0.0143, 0.0000,  ..., 0.0082, 0.0018, 0.0113],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60857.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0369, 0.0000, 0.0000,  ..., 0.0616, 0.0000, 0.0408],
        [0.0280, 0.0000, 0.0000,  ..., 0.0475, 0.0000, 0.0349],
        [0.0219, 0.0000, 0.0000,  ..., 0.0357, 0.0000, 0.0300],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0154],
        [0.0035, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0154],
        [0.0035, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0154]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383102.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2819.1899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-252.6596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(950.6300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.1590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.4122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0727],
        [ 0.0735],
        [ 0.0673],
        ...,
        [-0.1886],
        [-0.1843],
        [-0.1769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67427.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366264., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366275.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1244.1136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6612, device='cuda:0')



h[100].sum tensor(2.7620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-36.8652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.4928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46506.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0164],
        [0.0033, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0156],
        [0.0033, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0156],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0156],
        [0.0032, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0156],
        [0.0032, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321061.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2233.2271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(160.1740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(105.3764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-114.9770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1048],
        [-0.1290],
        [-0.1451],
        ...,
        [-0.1944],
        [-0.1937],
        [-0.1935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63219.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366275.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366286.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [ 0.0071,  0.0159, -0.0221,  ...,  0.0178,  0.0046,  0.0227],
        [ 0.0031,  0.0084, -0.0104,  ...,  0.0078,  0.0019,  0.0104],
        ...,
        [ 0.0023,  0.0068, -0.0079,  ...,  0.0058,  0.0014,  0.0078],
        [ 0.0015,  0.0053, -0.0056,  ...,  0.0037,  0.0008,  0.0053],
        [ 0.0023,  0.0068, -0.0079,  ...,  0.0058,  0.0014,  0.0078]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1336.1423, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3912, device='cuda:0')



h[100].sum tensor(4.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-35.6390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.7389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0233, 0.0544, 0.0000,  ..., 0.0589, 0.0151, 0.0756],
        [0.0083, 0.0250, 0.0000,  ..., 0.0212, 0.0052, 0.0281],
        [0.0236, 0.0548, 0.0000,  ..., 0.0594, 0.0152, 0.0762],
        ...,
        [0.0032, 0.0145, 0.0000,  ..., 0.0083, 0.0019, 0.0116],
        [0.0097, 0.0284, 0.0000,  ..., 0.0246, 0.0059, 0.0330],
        [0.0074, 0.0240, 0.0000,  ..., 0.0189, 0.0044, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49630.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1060, 0.0000, 0.0000,  ..., 0.1632, 0.0000, 0.0836],
        [0.0802, 0.0000, 0.0000,  ..., 0.1251, 0.0000, 0.0676],
        [0.1090, 0.0000, 0.0000,  ..., 0.1679, 0.0000, 0.0855],
        ...,
        [0.0218, 0.0000, 0.0000,  ..., 0.0352, 0.0000, 0.0302],
        [0.0343, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0390],
        [0.0373, 0.0000, 0.0000,  ..., 0.0618, 0.0000, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332214.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2335.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(319.1741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.3334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.8817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0043],
        [0.0130],
        [0.0031],
        ...,
        [0.0006],
        [0.0386],
        [0.0461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66918.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366286.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366286.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1258.3446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8518, device='cuda:0')



h[100].sum tensor(3.2919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-36.8207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47264.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0235],
        [0.0065, 0.0000, 0.0000,  ..., 0.0078, 0.0000, 0.0187],
        [0.0042, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0173],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0161],
        [0.0033, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0161],
        [0.0033, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328035.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2290.9309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.1365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(261.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.3903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.3484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0536],
        [ 0.0445],
        [ 0.0381],
        ...,
        [-0.1962],
        [-0.1955],
        [-0.1952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67324.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366286.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366298.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1332.4316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9661, device='cuda:0')



h[100].sum tensor(4.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-35.9646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.7609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51021.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0205],
        [0.0051, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0175],
        [0.0054, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0179],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0165],
        [0.0035, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0165],
        [0.0035, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347238.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2463.8657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-292.9451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(247.6186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.1647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.0134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0172],
        [-0.0141],
        [ 0.0215],
        ...,
        [-0.1956],
        [-0.1951],
        [-0.1949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52250.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366298.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366310.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1222.1790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9324, device='cuda:0')



h[100].sum tensor(2.6820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-37.9133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.9339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0085, 0.0000,  ..., 0.0017, 0.0003, 0.0028],
        [0.0013, 0.0105, 0.0000,  ..., 0.0034, 0.0006, 0.0055],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42732.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0085, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0218],
        [0.0121, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0252],
        [0.0154, 0.0000, 0.0000,  ..., 0.0285, 0.0000, 0.0281],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0170],
        [0.0037, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0170],
        [0.0037, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(305543.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2221.8464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-324.9427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88.7409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.0489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.9174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0761],
        [ 0.0897],
        [ 0.0954],
        ...,
        [-0.1952],
        [-0.1945],
        [-0.1943]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66338.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366310.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366322.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0072, -0.0086,  ...,  0.0064,  0.0016,  0.0086],
        [ 0.0008,  0.0039, -0.0036,  ...,  0.0020,  0.0004,  0.0032],
        [ 0.0013,  0.0049, -0.0051,  ...,  0.0033,  0.0007,  0.0048],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1372.7689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2122, device='cuda:0')



h[100].sum tensor(5.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-35.8827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0157, 0.0000,  ..., 0.0092, 0.0020, 0.0135],
        [0.0093, 0.0273, 0.0000,  ..., 0.0235, 0.0057, 0.0320],
        [0.0040, 0.0157, 0.0000,  ..., 0.0102, 0.0024, 0.0141],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50178.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0366],
        [0.0356, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0424],
        [0.0296, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0376],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0173],
        [0.0037, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0173],
        [0.0037, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342797.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2527.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.1716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(363.4499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.9481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1049],
        [ 0.1064],
        [ 0.1064],
        ...,
        [-0.1947],
        [-0.1941],
        [-0.1940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63763.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9992],
        [0.9994],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366322.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366334.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1364.8539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0384, device='cuda:0')



h[100].sum tensor(5.2940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-36.2394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51002.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0210],
        [0.0046, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0184],
        [0.0038, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0175],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0175],
        [0.0037, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0175],
        [0.0037, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(354279.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2557.1052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-293.1731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(225.2382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.3284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.0787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0580],
        [ 0.0233],
        [-0.0319],
        ...,
        [-0.1958],
        [-0.1951],
        [-0.1949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43031.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366334.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(512.0389, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366345.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0099, -0.0127,  ...,  0.0099,  0.0025,  0.0130],
        [ 0.0054,  0.0126, -0.0170,  ...,  0.0135,  0.0035,  0.0176],
        [ 0.0071,  0.0158, -0.0219,  ...,  0.0177,  0.0046,  0.0228],
        ...,
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0016,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1536.4187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5932, device='cuda:0')



h[100].sum tensor(8.3484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0171, device='cuda:0')



h[200].sum tensor(-33.8613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.9951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0435, 0.0000,  ..., 0.0449, 0.0115, 0.0587],
        [0.0198, 0.0470, 0.0000,  ..., 0.0495, 0.0127, 0.0644],
        [0.0238, 0.0547, 0.0000,  ..., 0.0596, 0.0154, 0.0770],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58364.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0803, 0.0000, 0.0000,  ..., 0.1306, 0.0000, 0.0706],
        [0.0962, 0.0000, 0.0000,  ..., 0.1553, 0.0000, 0.0809],
        [0.1100, 0.0000, 0.0000,  ..., 0.1771, 0.0000, 0.0900],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0177],
        [0.0035, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0177],
        [0.0035, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385031.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2719.8813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-264.6462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(484.2915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.6350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.0643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0940],
        [ 0.1002],
        [ 0.1058],
        ...,
        [-0.1995],
        [-0.1988],
        [-0.1986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44640.7266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366345.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366356.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1410.4519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9716, device='cuda:0')



h[100].sum tensor(5.7131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-35.7823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.4509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0030, 0.0137, 0.0000,  ..., 0.0074, 0.0017, 0.0107],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52543.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0000, 0.0000,  ..., 0.0353, 0.0000, 0.0315],
        [0.0096, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.0239],
        [0.0056, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0203],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0177],
        [0.0029, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0177],
        [0.0029, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(362598.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2423.8040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-284.3572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(299.1815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.1486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 8.2246e-02],
        [ 4.8817e-02],
        [-8.5174e-05],
        ...,
        [-2.0664e-01],
        [-2.0593e-01],
        [-2.0545e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49920.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366356.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366367.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [ 0.0021,  0.0063, -0.0071,  ...,  0.0051,  0.0012,  0.0071],
        [ 0.0014,  0.0050, -0.0051,  ...,  0.0034,  0.0007,  0.0050],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1411.1018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0360, device='cuda:0')



h[100].sum tensor(5.4251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-35.8250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.4951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0176, 0.0000,  ..., 0.0123, 0.0030, 0.0169],
        [0.0044, 0.0165, 0.0000,  ..., 0.0109, 0.0026, 0.0151],
        [0.0105, 0.0295, 0.0000,  ..., 0.0259, 0.0063, 0.0354],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53733.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0265, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0361],
        [0.0278, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.0376],
        [0.0394, 0.0000, 0.0000,  ..., 0.0700, 0.0000, 0.0465],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0176],
        [0.0023, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0176],
        [0.0023, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372209.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2335.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-275.6870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(396.2611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.2494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.7880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0709],
        [ 0.0735],
        [ 0.0782],
        ...,
        [-0.2121],
        [-0.2114],
        [-0.2112]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57376.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366367.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366378.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [ 0.0008,  0.0039, -0.0033,  ...,  0.0019,  0.0003,  0.0031],
        [ 0.0008,  0.0039, -0.0033,  ...,  0.0019,  0.0003,  0.0031],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1440.3091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3949, device='cuda:0')



h[100].sum tensor(5.6534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-35.5693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.4276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0108, 0.0000,  ..., 0.0032, 0.0005, 0.0056],
        [0.0014, 0.0108, 0.0000,  ..., 0.0032, 0.0005, 0.0056],
        [0.0014, 0.0108, 0.0000,  ..., 0.0032, 0.0005, 0.0056],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52961.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0235],
        [0.0105, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0268],
        [0.0203, 0.0000, 0.0000,  ..., 0.0390, 0.0000, 0.0338],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0176],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0176],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363485.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2198.1440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-275.4269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(279.9631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-135.0102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0393],
        [ 0.0200],
        [ 0.0594],
        ...,
        [-0.2148],
        [-0.2140],
        [-0.2138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58868.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366378.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366389.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1347.9304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4679, device='cuda:0')



h[100].sum tensor(3.6875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-37.1711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.3601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46828.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0177],
        [0.0018, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0177],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0178],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0179],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0179],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331701., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1979.9016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.9018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(211.2683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.9342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.4672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3013],
        [-0.2985],
        [-0.2924],
        ...,
        [-0.2161],
        [-0.2154],
        [-0.2151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78193.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9995],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366389.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366401.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0004, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1429.9235, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2311, device='cuda:0')



h[100].sum tensor(5.2248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-36.2177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0158, 0.0000,  ..., 0.0097, 0.0023, 0.0138],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54313.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0349],
        [0.0086, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0247],
        [0.0039, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0205],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0181],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0181],
        [0.0019, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389268.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2400.9456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-271.0191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(695.7756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(106.3469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-142.3078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0200],
        [-0.0421],
        [-0.1181],
        ...,
        [-0.2181],
        [-0.2173],
        [-0.2171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67441.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366401.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366413.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1395.6958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7032, device='cuda:0')



h[100].sum tensor(5.0720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-36.9317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0111, 0.0000,  ..., 0.0046, 0.0011, 0.0067],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50129.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0272],
        [0.0049, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0205],
        [0.0027, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0187],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0184],
        [0.0021, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0184],
        [0.0021, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359234.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2221.3521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.1965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(311.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.8582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-131.0806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0168],
        [-0.0964],
        [-0.1791],
        ...,
        [-0.2198],
        [-0.2191],
        [-0.2188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62481.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366413.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366424.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1363.0317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3466, device='cuda:0')



h[100].sum tensor(5.1443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-37.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.5906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0045, 0.0165, 0.0000,  ..., 0.0107, 0.0025, 0.0153],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47129.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0198],
        [0.0106, 0.0000, 0.0000,  ..., 0.0162, 0.0000, 0.0249],
        [0.0267, 0.0000, 0.0000,  ..., 0.0448, 0.0000, 0.0377],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0185],
        [0.0024, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0185],
        [0.0024, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342059.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2185.6321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.8209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(159.5541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.4651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.7336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0808],
        [-0.0099],
        [ 0.0424],
        ...,
        [-0.2210],
        [-0.2202],
        [-0.2200]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63229.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366424.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366437.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0045, -0.0043,  ...,  0.0027,  0.0005,  0.0043],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.3285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2343, device='cuda:0')



h[100].sum tensor(6.1661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0102, device='cuda:0')



h[200].sum tensor(-37.6284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.5136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0160, 0.0000,  ..., 0.0102, 0.0024, 0.0147],
        [0.0012, 0.0096, 0.0000,  ..., 0.0027, 0.0005, 0.0043],
        [0.0075, 0.0220, 0.0000,  ..., 0.0182, 0.0045, 0.0246],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47107.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0348, 0.0000, 0.0000,  ..., 0.0584, 0.0000, 0.0439],
        [0.0296, 0.0000, 0.0000,  ..., 0.0481, 0.0000, 0.0393],
        [0.0443, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0492],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0185],
        [0.0027, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0185],
        [0.0027, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341755., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2224.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.9130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(119.2362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.4996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.7951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0752],
        [ 0.0727],
        [ 0.0706],
        ...,
        [-0.2211],
        [-0.2204],
        [-0.2203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61338.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366437.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0011],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366449.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1397.2915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2327, device='cuda:0')



h[100].sum tensor(7.4983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-37.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48656.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0182],
        [0.0030, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0186],
        [0.0063, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0215],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0184],
        [0.0027, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0184],
        [0.0027, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352718.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2339.8003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.8353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(311.6443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.4917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.5513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2050],
        [-0.1369],
        [-0.0435],
        ...,
        [-0.2242],
        [-0.2234],
        [-0.2232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70059.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9993],
        [0.9996],
        ...,
        [1.0011],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366449.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(563.1561, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366461.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [ 0.0028,  0.0076, -0.0090,  ...,  0.0068,  0.0016,  0.0095],
        [ 0.0013,  0.0047, -0.0045,  ...,  0.0029,  0.0006,  0.0047],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1489.3452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4152, device='cuda:0')



h[100].sum tensor(10.3505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-36.1709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0150, 0.0000,  ..., 0.0089, 0.0020, 0.0133],
        [0.0040, 0.0162, 0.0000,  ..., 0.0094, 0.0019, 0.0150],
        [0.0169, 0.0408, 0.0000,  ..., 0.0410, 0.0102, 0.0554],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50592.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0236, 0.0000, 0.0000,  ..., 0.0389, 0.0000, 0.0354],
        [0.0401, 0.0000, 0.0000,  ..., 0.0666, 0.0000, 0.0477],
        [0.0752, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.0721],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0181],
        [0.0027, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0181],
        [0.0027, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355318.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2322.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.1455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(212.9015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.9995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.3398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0789],
        [ 0.0937],
        [ 0.0959],
        ...,
        [-0.2256],
        [-0.2249],
        [-0.2247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60240.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366461.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366474.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1575.3577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6715, device='cuda:0')



h[100].sum tensor(12.4882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0155, device='cuda:0')



h[200].sum tensor(-35.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.9900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58555.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5551e-03, 0.0000e+00, 0.0000e+00,  ..., 2.7446e-05, 0.0000e+00,
         1.7732e-02],
        [2.5559e-03, 0.0000e+00, 0.0000e+00,  ..., 2.7552e-05, 0.0000e+00,
         1.7735e-02],
        [2.5904e-03, 0.0000e+00, 0.0000e+00,  ..., 3.7431e-05, 0.0000e+00,
         1.7789e-02],
        ...,
        [2.5906e-03, 0.0000e+00, 0.0000e+00,  ..., 2.0954e-05, 0.0000e+00,
         1.7917e-02],
        [2.5916e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1078e-05, 0.0000e+00,
         1.7919e-02],
        [2.5918e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1130e-05, 0.0000e+00,
         1.7920e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407356.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2671.8533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-276.8804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(609.4183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.7174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.1988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1663],
        [-0.1981],
        [-0.2255],
        ...,
        [-0.2285],
        [-0.2278],
        [-0.2275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57972.7852, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366474.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366474.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.1440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3402, device='cuda:0')



h[100].sum tensor(14.1603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0181, device='cuda:0')



h[200].sum tensor(-33.7205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0089, 0.0000,  ..., 0.0018, 0.0003, 0.0033],
        [0.0016, 0.0110, 0.0000,  ..., 0.0035, 0.0005, 0.0066],
        [0.0008, 0.0089, 0.0000,  ..., 0.0018, 0.0003, 0.0033],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61056.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.9924e-03, 0.0000e+00, 0.0000e+00,  ..., 1.7646e-02, 0.0000e+00,
         2.5847e-02],
        [1.1046e-02, 0.0000e+00, 0.0000e+00,  ..., 2.0286e-02, 0.0000e+00,
         2.7021e-02],
        [1.0967e-02, 0.0000e+00, 0.0000e+00,  ..., 1.9472e-02, 0.0000e+00,
         2.6729e-02],
        ...,
        [2.5906e-03, 0.0000e+00, 0.0000e+00,  ..., 2.0954e-05, 0.0000e+00,
         1.7917e-02],
        [2.5916e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1078e-05, 0.0000e+00,
         1.7919e-02],
        [2.5918e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1130e-05, 0.0000e+00,
         1.7920e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415098.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2748.3247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-268.4286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(787.7596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.9690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.4534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1115],
        [ 0.1131],
        [ 0.1131],
        ...,
        [-0.2285],
        [-0.2278],
        [-0.2275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65984.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366474.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366486.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001],
        ...,
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001],
        [-0.0003,  0.0017,  0.0000,  ..., -0.0010, -0.0005, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1458.5941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7630, device='cuda:0')



h[100].sum tensor(11.0225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-36.6956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50960.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0301],
        [0.0120, 0.0000, 0.0000,  ..., 0.0170, 0.0000, 0.0251],
        [0.0095, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0230],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367834., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2361.9250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-309.5965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(266.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.7025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.3257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0643],
        [ 0.0612],
        [ 0.0601],
        ...,
        [-0.2309],
        [-0.2300],
        [-0.2289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69903.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366486.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366498.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5538e-03,  1.8301e-02, -2.5228e-02,  ...,  2.0930e-02,
          5.3891e-03,  2.7309e-02],
        [ 6.0480e-03,  1.3624e-02, -1.8114e-02,  ...,  1.4733e-02,
          3.7292e-03,  1.9581e-02],
        [ 3.0336e-03,  7.9969e-03, -9.5572e-03,  ...,  7.2784e-03,
          1.7324e-03,  1.0285e-02],
        ...,
        [-3.3316e-04,  1.7126e-03,  0.0000e+00,  ..., -1.0474e-03,
         -4.9776e-04, -9.8325e-05],
        [-3.3316e-04,  1.7126e-03,  0.0000e+00,  ..., -1.0474e-03,
         -4.9776e-04, -9.8325e-05],
        [-3.3316e-04,  1.7126e-03,  0.0000e+00,  ..., -1.0474e-03,
         -4.9776e-04, -9.8325e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.7498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1529, device='cuda:0')



h[100].sum tensor(11.3465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-36.8083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0576, 0.0000,  ..., 0.0631, 0.0160, 0.0835],
        [0.0237, 0.0535, 0.0000,  ..., 0.0576, 0.0146, 0.0767],
        [0.0193, 0.0454, 0.0000,  ..., 0.0469, 0.0117, 0.0634],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49212.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1133, 0.0000, 0.0000,  ..., 0.1740, 0.0000, 0.0956],
        [0.0987, 0.0000, 0.0000,  ..., 0.1522, 0.0000, 0.0858],
        [0.0793, 0.0000, 0.0000,  ..., 0.1240, 0.0000, 0.0731],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356813.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2238.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-319.6328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(206.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.3175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0418],
        [ 0.0534],
        [ 0.0677],
        ...,
        [-0.2366],
        [-0.2359],
        [-0.2356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78065.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9997],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366498.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366510.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6502e-03,  2.0348e-02, -2.8298e-02,  ...,  2.3629e-02,
          6.1005e-03,  3.0727e-02],
        [ 2.7065e-03,  7.3974e-03, -8.6207e-03,  ...,  6.4648e-03,
          1.5069e-03,  9.3134e-03],
        [-3.3560e-04,  1.7236e-03,  0.0000e+00,  ..., -1.0548e-03,
         -5.0553e-04, -6.8013e-05],
        ...,
        [-3.3560e-04,  1.7236e-03,  0.0000e+00,  ..., -1.0548e-03,
         -5.0553e-04, -6.8013e-05],
        [-3.3560e-04,  1.7236e-03,  0.0000e+00,  ..., -1.0548e-03,
         -5.0553e-04, -6.8013e-05],
        [-3.3560e-04,  1.7236e-03,  0.0000e+00,  ..., -1.0548e-03,
         -5.0553e-04, -6.8013e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1461.5898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1922, device='cuda:0')



h[100].sum tensor(11.7683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-36.8623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0381, 0.0000,  ..., 0.0372, 0.0091, 0.0514],
        [0.0189, 0.0439, 0.0000,  ..., 0.0459, 0.0116, 0.0611],
        [0.0073, 0.0217, 0.0000,  ..., 0.0175, 0.0042, 0.0244],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50134.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0815, 0.0000, 0.0000,  ..., 0.1274, 0.0000, 0.0744],
        [0.0817, 0.0000, 0.0000,  ..., 0.1272, 0.0000, 0.0743],
        [0.0598, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.0593],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366005.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2227.8999, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-317.7046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(226.3023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.3140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.6748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0457],
        [ 0.0523],
        [ 0.0609],
        ...,
        [-0.2407],
        [-0.2399],
        [-0.2396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77161.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0011],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366510.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366522.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05],
        [-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05],
        [-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05],
        ...,
        [-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05],
        [-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05],
        [-3.4152e-04,  1.7185e-03,  0.0000e+00,  ..., -1.0599e-03,
         -5.1408e-04, -5.4398e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1705.5488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8458, device='cuda:0')



h[100].sum tensor(16.1322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0183, device='cuda:0')



h[200].sum tensor(-33.8155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.5410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61509.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(428004.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2638.0884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-273.9111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(741.7825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.9881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.2671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2990],
        [-0.3133],
        [-0.3202],
        ...,
        [-0.2406],
        [-0.2398],
        [-0.2395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67292.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366522.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366534.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2654e-03,  8.4311e-03, -1.0211e-02,  ...,  7.8605e-03,
          1.8537e-03,  1.1088e-02],
        [ 7.1833e-03,  1.5725e-02, -2.1277e-02,  ...,  1.7537e-02,
          4.4359e-03,  2.3165e-02],
        [ 5.6692e-03,  1.2907e-02, -1.7000e-02,  ...,  1.3798e-02,
          3.4380e-03,  1.8498e-02],
        ...,
        [-3.4956e-04,  1.7005e-03,  0.0000e+00,  ..., -1.0684e-03,
         -5.2896e-04, -5.6351e-05],
        [-3.4956e-04,  1.7005e-03,  0.0000e+00,  ..., -1.0684e-03,
         -5.2896e-04, -5.6351e-05],
        [-3.4956e-04,  1.7005e-03,  0.0000e+00,  ..., -1.0684e-03,
         -5.2896e-04, -5.6351e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1666.6147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9946, device='cuda:0')



h[100].sum tensor(15.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-34.8062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0412, 0.0000,  ..., 0.0414, 0.0101, 0.0568],
        [0.0182, 0.0434, 0.0000,  ..., 0.0442, 0.0108, 0.0603],
        [0.0229, 0.0520, 0.0000,  ..., 0.0557, 0.0139, 0.0747],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59169.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0000, 0.0000,  ..., 0.1032, 0.0000, 0.0639],
        [0.0817, 0.0000, 0.0000,  ..., 0.1262, 0.0000, 0.0744],
        [0.0926, 0.0000, 0.0000,  ..., 0.1428, 0.0000, 0.0819],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411899.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2518.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.3447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(489.5203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.7936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0484],
        [ 0.0385],
        [ 0.0321],
        ...,
        [-0.2395],
        [-0.2388],
        [-0.2385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57690.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0004],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366534.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366547.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0508e-03,  4.3140e-03, -3.9807e-03,  ...,  2.4152e-03,
          3.9617e-04,  4.2944e-03],
        [ 4.3154e-03,  1.0388e-02, -1.3190e-02,  ...,  1.0480e-02,
          2.5477e-03,  1.4361e-02],
        [-3.6034e-04,  1.6885e-03,  0.0000e+00,  ..., -1.0709e-03,
         -5.3379e-04, -5.6871e-05],
        ...,
        [-3.6034e-04,  1.6885e-03,  0.0000e+00,  ..., -1.0709e-03,
         -5.3379e-04, -5.6871e-05],
        [-3.6034e-04,  1.6885e-03,  0.0000e+00,  ..., -1.0709e-03,
         -5.3379e-04, -5.6871e-05],
        [-3.6034e-04,  1.6885e-03,  0.0000e+00,  ..., -1.0709e-03,
         -5.3379e-04, -5.6871e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1852.9161, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2742, device='cuda:0')



h[100].sum tensor(18.1299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0208, device='cuda:0')



h[200].sum tensor(-32.5759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.5800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0346, 0.0000,  ..., 0.0327, 0.0077, 0.0460],
        [0.0051, 0.0182, 0.0000,  ..., 0.0120, 0.0025, 0.0188],
        [0.0086, 0.0255, 0.0000,  ..., 0.0206, 0.0045, 0.0309],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66222.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0000, 0.0000,  ..., 0.0999, 0.0000, 0.0628],
        [0.0484, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.0525],
        [0.0425, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0481],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0169],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0169],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445056.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2737.5852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-248.5470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(741.8875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.3753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.2381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0483],
        [ 0.0479],
        [ 0.0481],
        ...,
        [-0.2392],
        [-0.2384],
        [-0.2382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53261.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9998],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366547.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366559.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6580e-04,  1.6878e-03,  0.0000e+00,  ..., -1.0830e-03,
         -5.4523e-04, -5.8711e-05],
        [-3.6580e-04,  1.6878e-03,  0.0000e+00,  ..., -1.0830e-03,
         -5.4523e-04, -5.8711e-05],
        [ 1.1366e-03,  4.4812e-03, -4.2317e-03,  ...,  2.6277e-03,
          4.4381e-04,  4.5741e-03],
        ...,
        [-3.6580e-04,  1.6878e-03,  0.0000e+00,  ..., -1.0830e-03,
         -5.4523e-04, -5.8711e-05],
        [-3.6580e-04,  1.6878e-03,  0.0000e+00,  ..., -1.0830e-03,
         -5.4523e-04, -5.8711e-05],
        [-3.6580e-04,  1.6878e-03,  0.0000e+00,  ..., -1.0830e-03,
         -5.4523e-04, -5.8711e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1823.2542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9008, device='cuda:0')



h[100].sum tensor(16.8219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0195, device='cuda:0')



h[200].sum tensor(-33.2153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.9512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0095, 0.0000,  ..., 0.0026, 0.0004, 0.0046],
        [0.0043, 0.0162, 0.0000,  ..., 0.0104, 0.0022, 0.0155],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65808.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0207],
        [0.0137, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0274],
        [0.0249, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0364],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450996.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2799.4409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-248.2048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(836.8314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.8372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.9817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0130],
        [ 0.0486],
        [ 0.0860],
        ...,
        [-0.2417],
        [-0.2410],
        [-0.2407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57191.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366559.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(521.0159, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366570.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05],
        [-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05],
        [-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05],
        ...,
        [-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05],
        [-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05],
        [-3.7029e-04,  1.6923e-03,  0.0000e+00,  ..., -1.1004e-03,
         -5.5628e-04, -5.3606e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1595.3317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3472, device='cuda:0')



h[100].sum tensor(11.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-36.4261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56050.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0189],
        [0.0026, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0183],
        [0.0055, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0209],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414758.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2541.1301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.4130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(466.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.3012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.2017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0450],
        [ 0.0370],
        [ 0.0483],
        ...,
        [-0.2455],
        [-0.2447],
        [-0.2445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58316.9180, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366570.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366582.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05],
        [-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05],
        [-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05],
        ...,
        [-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05],
        [-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05],
        [-3.7300e-04,  1.6979e-03,  0.0000e+00,  ..., -1.1159e-03,
         -5.6755e-04, -6.8948e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1539.3057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7520, device='cuda:0')



h[100].sum tensor(10.3821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-37.3176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49826.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0217],
        [0.0052, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0201],
        [0.0093, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0239],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0175],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0175],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367933.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2237.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.4665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(183.9732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.1491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0328],
        [ 0.0344],
        [ 0.0389],
        ...,
        [-0.2487],
        [-0.2475],
        [-0.2442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74089.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9994],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366582.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366594.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2643e-03,  4.7440e-03, -4.6000e-03,  ...,  2.9215e-03,
          5.0233e-04,  4.9650e-03],
        [ 9.2836e-04,  4.1208e-03, -3.6582e-03,  ...,  2.0926e-03,
          2.8175e-04,  3.9292e-03],
        [ 2.5691e-03,  7.1652e-03, -8.2582e-03,  ...,  6.1416e-03,
          1.3592e-03,  8.9883e-03],
        ...,
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1478.6469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4146, device='cuda:0')



h[100].sum tensor(9.2689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0097, device='cuda:0')



h[200].sum tensor(-38.2363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.9511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0159, 0.0000,  ..., 0.0098, 0.0021, 0.0149],
        [0.0092, 0.0266, 0.0000,  ..., 0.0218, 0.0047, 0.0325],
        [0.0038, 0.0159, 0.0000,  ..., 0.0087, 0.0015, 0.0148],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47069.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0242, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0370],
        [0.0340, 0.0000, 0.0000,  ..., 0.0548, 0.0000, 0.0448],
        [0.0259, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.0389],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358495.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2177.4985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.2322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82.6238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.8531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0795],
        [ 0.0805],
        [ 0.0767],
        ...,
        [-0.2501],
        [-0.2455],
        [-0.2302]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72135.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366594.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366594.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [ 1.4249e-03,  5.0422e-03, -5.0504e-03,  ...,  3.3180e-03,
          6.0783e-04,  5.4604e-03],
        ...,
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05],
        [-3.7649e-04,  1.6996e-03,  0.0000e+00,  ..., -1.1275e-03,
         -5.7508e-04, -9.4167e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1689.9719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8601, device='cuda:0')



h[100].sum tensor(12.7618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-35.4575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.1194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0166, 0.0000,  ..., 0.0119, 0.0029, 0.0162],
        [0.0014, 0.0101, 0.0000,  ..., 0.0033, 0.0006, 0.0055],
        [0.0024, 0.0126, 0.0000,  ..., 0.0054, 0.0009, 0.0094],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57011.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0301, 0.0000, 0.0000,  ..., 0.0418, 0.0000, 0.0380],
        [0.0187, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0308],
        [0.0215, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0341],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408020.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2569.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.6275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(514.7739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.9186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.7294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0490],
        [ 0.0585],
        [ 0.0672],
        ...,
        [-0.2535],
        [-0.2527],
        [-0.2524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75132.0859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [0.9999],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366594.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366606.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [ 0.0015,  0.0052, -0.0053,  ...,  0.0035,  0.0007,  0.0057],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1740.0027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6499, device='cuda:0')



h[100].sum tensor(13.7862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-34.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.3477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0015, 0.0103, 0.0000,  ..., 0.0035, 0.0007, 0.0057],
        [0.0012, 0.0097, 0.0000,  ..., 0.0027, 0.0004, 0.0046],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60954.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0203],
        [0.0081, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0242],
        [0.0113, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0275],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0181],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0181],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434752.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2748.7556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-260.9491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(641.1859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.2087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.4813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0990],
        [-0.0063],
        [ 0.0596],
        ...,
        [-0.2577],
        [-0.2569],
        [-0.2566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70681.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366606.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366618.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0011, -0.0006, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1873.9242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.2479, device='cuda:0')



h[100].sum tensor(16.0224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0197, device='cuda:0')



h[200].sum tensor(-33.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.1894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65365.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0182],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0182],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449546., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2826.2893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-242.9378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(825.5691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.2248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3460],
        [-0.3416],
        [-0.3282],
        ...,
        [-0.2597],
        [-0.2594],
        [-0.2594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66680.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366618.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366630.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0046, -0.0043,  ...,  0.0027,  0.0004,  0.0046],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1610.3325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0463, device='cuda:0')



h[100].sum tensor(11.8024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-36.7774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.8159, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0164, 0.0000,  ..., 0.0104, 0.0022, 0.0156],
        [0.0012, 0.0097, 0.0000,  ..., 0.0027, 0.0004, 0.0046],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52794.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0256, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0394],
        [0.0140, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0297],
        [0.0068, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0230],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386808.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2323.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-290.7223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(346.5612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-137.8048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0810],
        [ 0.0326],
        [-0.0385],
        ...,
        [-0.2657],
        [-0.2648],
        [-0.2646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76746.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366630.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366642.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086,  0.0183, -0.0251,  ...,  0.0210,  0.0053,  0.0275],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [ 0.0049,  0.0116, -0.0149,  ...,  0.0120,  0.0029,  0.0162],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1688.0381, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9222, device='cuda:0')



h[100].sum tensor(13.6818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-35.9047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.7895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0361, 0.0000,  ..., 0.0355, 0.0086, 0.0481],
        [0.0214, 0.0494, 0.0000,  ..., 0.0521, 0.0128, 0.0701],
        [0.0040, 0.0149, 0.0000,  ..., 0.0096, 0.0023, 0.0132],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56573.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0898, 0.0000, 0.0000,  ..., 0.1308, 0.0000, 0.0823],
        [0.0726, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.0701],
        [0.0344, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0430],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412112.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2424.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-275.0856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(513.5145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.2674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.0595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0493],
        [ 0.0557],
        [ 0.0478],
        ...,
        [-0.2673],
        [-0.2664],
        [-0.2661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78215.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366642.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366654.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [ 0.0021,  0.0064, -0.0070,  ...,  0.0050,  0.0011,  0.0075],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1797.4685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1259, device='cuda:0')



h[100].sum tensor(15.4277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0168, device='cuda:0')



h[200].sum tensor(-34.6493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.6744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0114, 0.0000,  ..., 0.0050, 0.0011, 0.0075],
        [0.0080, 0.0230, 0.0000,  ..., 0.0193, 0.0046, 0.0266],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63229.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0272],
        [0.0308, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0416],
        [0.0658, 0.0000, 0.0000,  ..., 0.0954, 0.0000, 0.0660],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448176.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2685.9565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-248.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(820.9850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.3157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.4583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0523],
        [ 0.0439],
        [ 0.0240],
        ...,
        [-0.2681],
        [-0.2672],
        [-0.2670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69576.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366654.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366666.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1514.4282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9768, device='cuda:0')



h[100].sum tensor(10.6792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-38.4113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.6506, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0051, 0.0168, 0.0000,  ..., 0.0123, 0.0030, 0.0166],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47312.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0206],
        [0.0081, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0241],
        [0.0237, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0353],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364667.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2128.7529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-310.6433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(130.2855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.2684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.7075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1927],
        [-0.0761],
        [ 0.0271],
        ...,
        [-0.2337],
        [-0.2597],
        [-0.2657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78157.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0000],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366666.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(477.5860, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366678., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [ 0.0011,  0.0045, -0.0042,  ...,  0.0026,  0.0004,  0.0045],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1549.0049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0565, device='cuda:0')



h[100].sum tensor(11.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-38.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.3916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0096, 0.0000,  ..., 0.0026, 0.0004, 0.0045],
        [0.0008, 0.0090, 0.0000,  ..., 0.0019, 0.0002, 0.0036],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48502.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0209],
        [0.0063, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0240],
        [0.0077, 0.0000, 0.0000,  ..., 0.0093, 0.0000, 0.0258],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0199]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(371703., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2223.7698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.5633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(123.3433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.5045, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.7026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1542],
        [-0.0838],
        [ 0.0035],
        ...,
        [-0.2656],
        [-0.2560],
        [-0.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76132.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366678., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366689.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0048, -0.0046,  ...,  0.0029,  0.0005,  0.0049],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0003],
        [ 0.0013,  0.0048, -0.0046,  ...,  0.0029,  0.0005,  0.0049],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1891.2937, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0526, device='cuda:0')



h[100].sum tensor(16.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0190, device='cuda:0')



h[200].sum tensor(-33.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0162, 0.0000,  ..., 0.0101, 0.0022, 0.0152],
        [0.0068, 0.0223, 0.0000,  ..., 0.0160, 0.0032, 0.0250],
        [0.0022, 0.0115, 0.0000,  ..., 0.0051, 0.0011, 0.0077],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66328.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0278, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0420],
        [0.0271, 0.0000, 0.0000,  ..., 0.0431, 0.0000, 0.0421],
        [0.0170, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0334],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465547.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2957.6675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.4456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(666.4576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.2674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.4245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1184],
        [ 0.1168],
        [ 0.0977],
        ...,
        [-0.2753],
        [-0.2745],
        [-0.2742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56254.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0012],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366689.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366700.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0012, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1761.0322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5842, device='cuda:0')



h[100].sum tensor(12.7843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-35.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.9301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0094, 0.0000,  ..., 0.0022, 0.0003, 0.0041],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58260.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0225],
        [0.0057, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0230],
        [0.0105, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0267],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0200],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422622.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2796.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-273.0996, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(498.6450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.0866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0135],
        [-0.0097],
        [ 0.0203],
        ...,
        [-0.2555],
        [-0.2713],
        [-0.2771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83309.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366700.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366712.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002],
        [ 0.0019,  0.0060, -0.0064,  ...,  0.0045,  0.0009,  0.0070],
        [ 0.0016,  0.0054, -0.0055,  ...,  0.0037,  0.0007,  0.0060],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1514.5620, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9054, device='cuda:0')



h[100].sum tensor(7.1007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0089, device='cuda:0')



h[200].sum tensor(-38.5430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.9153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0111, 0.0000,  ..., 0.0045, 0.0009, 0.0070],
        [0.0047, 0.0170, 0.0000,  ..., 0.0111, 0.0024, 0.0166],
        [0.0121, 0.0321, 0.0000,  ..., 0.0286, 0.0065, 0.0413],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46710.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0247, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0362],
        [0.0349, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0440],
        [0.0533, 0.0000, 0.0000,  ..., 0.0792, 0.0000, 0.0578],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0032, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368037.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2546.0027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-320.7614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(114.8209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.5832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-122.5619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0544],
        [ 0.0585],
        [ 0.0613],
        ...,
        [-0.2823],
        [-0.2812],
        [-0.2791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95717.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366712.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366723.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0026,  0.0072, -0.0082,  ...,  0.0061,  0.0013,  0.0090],
        [ 0.0047,  0.0110, -0.0140,  ...,  0.0112,  0.0027,  0.0154],
        [ 0.0049,  0.0114, -0.0145,  ...,  0.0117,  0.0028,  0.0161],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1627.7729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8015, device='cuda:0')



h[100].sum tensor(7.8897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-37.2396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0141, 0.0358, 0.0000,  ..., 0.0336, 0.0079, 0.0477],
        [0.0193, 0.0454, 0.0000,  ..., 0.0465, 0.0113, 0.0638],
        [0.0169, 0.0410, 0.0000,  ..., 0.0405, 0.0097, 0.0563],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52455.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0831, 0.0000, 0.0000,  ..., 0.1234, 0.0000, 0.0782],
        [0.0892, 0.0000, 0.0000,  ..., 0.1308, 0.0000, 0.0813],
        [0.0757, 0.0000, 0.0000,  ..., 0.1103, 0.0000, 0.0719],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0199],
        [0.0036, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0199],
        [0.0036, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0199]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403999.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2916.9673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.1990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(322.1325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.0906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.6371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0627],
        [ 0.0586],
        [ 0.0540],
        ...,
        [-0.2837],
        [-0.2828],
        [-0.2825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91434.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0001],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366723.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366735.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0088, -0.0106,  ...,  0.0082,  0.0019,  0.0116],
        [ 0.0019,  0.0059, -0.0062,  ...,  0.0043,  0.0009,  0.0067],
        [ 0.0076,  0.0164, -0.0221,  ...,  0.0184,  0.0046,  0.0245],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1662.3315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8540, device='cuda:0')



h[100].sum tensor(8.0374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-36.9261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0098, 0.0277, 0.0000,  ..., 0.0228, 0.0050, 0.0341],
        [0.0184, 0.0436, 0.0000,  ..., 0.0440, 0.0107, 0.0607],
        [0.0076, 0.0230, 0.0000,  ..., 0.0178, 0.0039, 0.0264],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52783.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0448, 0.0000, 0.0000,  ..., 0.0641, 0.0000, 0.0504],
        [0.0621, 0.0000, 0.0000,  ..., 0.0887, 0.0000, 0.0616],
        [0.0502, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0537],
        ...,
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399849.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2943.6270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.6131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(226.9654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.8505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.6234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0240],
        [ 0.0329],
        [ 0.0413],
        ...,
        [-0.2849],
        [-0.2840],
        [-0.2837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82376.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9995],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366735.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366747.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1630.0504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9385, device='cuda:0')



h[100].sum tensor(7.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-37.4448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6831, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0100, 0.0000,  ..., 0.0031, 0.0006, 0.0052],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50813.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0129, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0285],
        [0.0071, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0232],
        [0.0046, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0207],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0201],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0201],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388933.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2921.8035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-310.4290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(170.1536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.0616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.6713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0597],
        [ 0.0181],
        [-0.0295],
        ...,
        [-0.2843],
        [-0.2835],
        [-0.2832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78335.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366747.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366760.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009,  0.0040, -0.0035,  ...,  0.0018,  0.0002,  0.0035],
        [ 0.0009,  0.0040, -0.0035,  ...,  0.0018,  0.0002,  0.0035],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2066.0254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.3190, device='cuda:0')



h[100].sum tensor(14.9255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0220, device='cuda:0')



h[200].sum tensor(-32.1647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.9833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0213, 0.0000,  ..., 0.0144, 0.0028, 0.0231],
        [0.0035, 0.0153, 0.0000,  ..., 0.0077, 0.0013, 0.0134],
        [0.0079, 0.0240, 0.0000,  ..., 0.0180, 0.0038, 0.0276],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73114.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0380, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0496],
        [0.0344, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0467],
        [0.0410, 0.0000, 0.0000,  ..., 0.0619, 0.0000, 0.0508],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514170.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3918.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-228.4876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1322.4364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.1458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.6660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1225],
        [ 0.1216],
        [ 0.1178],
        ...,
        [-0.2843],
        [-0.2834],
        [-0.2832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78226.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0002],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366760.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366772.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1786.4829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6541, device='cuda:0')



h[100].sum tensor(11.2829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-35.6764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2918, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0052, 0.0168, 0.0000,  ..., 0.0124, 0.0031, 0.0167],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58077.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0237],
        [0.0146, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0278],
        [0.0315, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.0395],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426416.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3204.0776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.7132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(440.2787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.9833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.1887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1163],
        [ 0.1125],
        [ 0.1130],
        ...,
        [-0.2629],
        [-0.2698],
        [-0.2756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70610.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366772.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366783.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0005],
        ...,
        [ 0.0030,  0.0079, -0.0093,  ...,  0.0070,  0.0016,  0.0100],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0013, -0.0006, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1912.5676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9174, device='cuda:0')



h[100].sum tensor(13.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0178, device='cuda:0')



h[200].sum tensor(-34.1746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0169, 0.0000,  ..., 0.0096, 0.0018, 0.0155],
        [0.0041, 0.0156, 0.0000,  ..., 0.0092, 0.0019, 0.0139],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66129.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0211],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0198],
        ...,
        [0.0339, 0.0000, 0.0000,  ..., 0.0486, 0.0000, 0.0449],
        [0.0261, 0.0000, 0.0000,  ..., 0.0349, 0.0000, 0.0381],
        [0.0112, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477407.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3551.5593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-259.5854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(870.2380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.6084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0477],
        [-0.1272],
        [-0.1779],
        ...,
        [ 0.0931],
        [ 0.0430],
        [-0.0492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69095.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366783.7812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(541.0077, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366794.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009,  0.0040, -0.0034,  ...,  0.0017,  0.0002,  0.0033],
        [ 0.0012,  0.0046, -0.0044,  ...,  0.0026,  0.0004,  0.0044],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0006, -0.0005],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0006, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0006, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0006, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1782.6898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8033, device='cuda:0')



h[100].sum tensor(11.6009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0150, device='cuda:0')



h[200].sum tensor(-35.8154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0296, 0.0000,  ..., 0.0253, 0.0057, 0.0362],
        [0.0044, 0.0161, 0.0000,  ..., 0.0099, 0.0021, 0.0146],
        [0.0012, 0.0097, 0.0000,  ..., 0.0026, 0.0004, 0.0044],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57659.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0465, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0530],
        [0.0320, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.0420],
        [0.0179, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0312],
        ...,
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0199],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425971.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3175.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-294.4438, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(371.9406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.8073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1257],
        [ 0.1177],
        [ 0.0739],
        ...,
        [-0.2922],
        [-0.2913],
        [-0.2910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77188.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366794.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366804.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [ 0.0009,  0.0040, -0.0034,  ...,  0.0017,  0.0002,  0.0033],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1954.8798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6668, device='cuda:0')



h[100].sum tensor(12.5426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0188, device='cuda:0')



h[200].sum tensor(-33.8737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.5626e-04, 8.6878e-03, 0.0000e+00,  ..., 1.1444e-03, 1.9109e-05,
         2.6095e-03],
        [4.1927e-03, 1.6508e-02, 0.0000e+00,  ..., 8.9045e-03, 1.5152e-03,
         1.4671e-02],
        [8.0521e-03, 2.3581e-02, 0.0000e+00,  ..., 1.8399e-02, 4.0451e-03,
         2.6544e-02],
        ...,
        [0.0000e+00, 6.9058e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.9068e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.9072e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67258.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0306],
        [0.0302, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.0418],
        [0.0422, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0502],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480334.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3718.8154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-253.0571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(769.9001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.6530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.9404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0446],
        [ 0.0938],
        [ 0.1101],
        ...,
        [-0.2989],
        [-0.2980],
        [-0.2977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74933.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9996],
        [1.0003],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366804.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366814.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1682.6411, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2224, device='cuda:0')



h[100].sum tensor(6.3683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-37.3919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53117.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405252.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3319.9370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.4922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(312.6400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.4146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.9718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3798],
        [-0.3375],
        [-0.2724],
        ...,
        [-0.3027],
        [-0.3017],
        [-0.3014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96098.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366814.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366824.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [ 0.0012,  0.0045, -0.0042,  ...,  0.0024,  0.0003,  0.0042],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1808.7360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1628, device='cuda:0')



h[100].sum tensor(6.3334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-36.1299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.9546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0097, 0.0000,  ..., 0.0024, 0.0003, 0.0042],
        [0.0019, 0.0110, 0.0000,  ..., 0.0042, 0.0008, 0.0065],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58509.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0212],
        [0.0143, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0271],
        [0.0206, 0.0000, 0.0000,  ..., 0.0246, 0.0000, 0.0326],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433316., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3634.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.1701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(392.8749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.3958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.3676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1957],
        [-0.0764],
        [ 0.0201],
        ...,
        [-0.3038],
        [-0.3027],
        [-0.3024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78558.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366824.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366835.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1784.7092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1502, device='cuda:0')



h[100].sum tensor(4.2285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0135, device='cuda:0')



h[200].sum tensor(-36.6730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.5735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0121, 0.0000,  ..., 0.0042, 0.0005, 0.0077],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54980.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0198],
        [0.0099, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0230],
        [0.0191, 0.0000, 0.0000,  ..., 0.0219, 0.0000, 0.0311],
        ...,
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411300.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3540.4761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(244.3921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.0405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2151],
        [-0.1041],
        [ 0.0020],
        ...,
        [-0.2965],
        [-0.2942],
        [-0.2933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81527.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9997],
        [1.0004],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366835.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0005],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366846.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0078,  0.0167, -0.0222,  ...,  0.0187,  0.0046,  0.0246],
        [ 0.0017,  0.0054, -0.0056,  ...,  0.0036,  0.0006,  0.0057],
        [ 0.0063,  0.0139, -0.0180,  ...,  0.0149,  0.0036,  0.0199],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2141.1802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.5790, device='cuda:0')



h[100].sum tensor(8.0399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0215, device='cuda:0')



h[200].sum tensor(-32.6860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.4754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0336, 0.0000,  ..., 0.0304, 0.0067, 0.0429],
        [0.0316, 0.0670, 0.0000,  ..., 0.0754, 0.0186, 0.0992],
        [0.0195, 0.0450, 0.0000,  ..., 0.0458, 0.0108, 0.0621],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72347.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0974, 0.0000, 0.0000,  ..., 0.1301, 0.0000, 0.0810],
        [0.1323, 0.0000, 0.0000,  ..., 0.1765, 0.0000, 0.1014],
        [0.1211, 0.0000, 0.0000,  ..., 0.1614, 0.0000, 0.0948],
        ...,
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504064.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4295.8916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-215.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(919.7544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.1427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0221],
        [ 0.0060],
        [-0.0076],
        ...,
        [-0.2953],
        [-0.2916],
        [-0.2886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65248.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0005],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366846.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0005],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366857.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        [ 0.0044,  0.0103, -0.0128,  ...,  0.0102,  0.0024,  0.0139],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1767.9510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0656, device='cuda:0')



h[100].sum tensor(1.4868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-37.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0154, 0.0000,  ..., 0.0102, 0.0024, 0.0139],
        [0.0035, 0.0138, 0.0000,  ..., 0.0081, 0.0018, 0.0113],
        [0.0158, 0.0381, 0.0000,  ..., 0.0366, 0.0083, 0.0504],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54015.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0238, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.0313],
        [0.0289, 0.0000, 0.0000,  ..., 0.0328, 0.0000, 0.0348],
        [0.0461, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.0463],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190],
        [0.0051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0190]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408812.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3499.3831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.1626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(269.9360, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.4241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.1432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0789],
        [-0.0136],
        [ 0.0324],
        ...,
        [-0.2984],
        [-0.2976],
        [-0.2973]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76009.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0005],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366857.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0006],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366868.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1791.0548, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4456, device='cuda:0')



h[100].sum tensor(1.1413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-37.0997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.4037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54035.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        [0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0187],
        [0.0048, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0193],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0188],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0188],
        [0.0047, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407918.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3359.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-284.8332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(231.5284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.3289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.0052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2227],
        [-0.2468],
        [-0.2482],
        ...,
        [-0.2906],
        [-0.2982],
        [-0.2997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71167.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9997],
        [1.0006],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366868.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0006],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366879.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [ 0.0009,  0.0040, -0.0034,  ...,  0.0017,  0.0001,  0.0031],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1743.6959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7271, device='cuda:0')



h[100].sum tensor(0.0376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-37.6958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0016, 0.0109, 0.0000,  ..., 0.0028, 0.0001, 0.0056],
        [0.0022, 0.0127, 0.0000,  ..., 0.0037, 0.0001, 0.0078],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51428.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0219],
        [0.0154, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0288],
        [0.0202, 0.0000, 0.0000,  ..., 0.0249, 0.0000, 0.0335],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393618.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3132.9224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.7832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(237.4246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.2716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0407],
        [ 0.0487],
        [ 0.1004],
        ...,
        [-0.3054],
        [-0.3045],
        [-0.3043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86542.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0006],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366879.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0006],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366891.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [ 0.0011,  0.0045, -0.0041,  ...,  0.0023,  0.0003,  0.0039],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [ 0.0037,  0.0091, -0.0110,  ...,  0.0086,  0.0019,  0.0117],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1921.6780, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2150, device='cuda:0')



h[100].sum tensor(2.3862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-35.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.3630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0095, 0.0000,  ..., 0.0023, 0.0003, 0.0039],
        [0.0043, 0.0160, 0.0000,  ..., 0.0096, 0.0019, 0.0140],
        ...,
        [0.0038, 0.0144, 0.0000,  ..., 0.0087, 0.0020, 0.0119],
        [0.0030, 0.0130, 0.0000,  ..., 0.0068, 0.0015, 0.0095],
        [0.0135, 0.0342, 0.0000,  ..., 0.0311, 0.0069, 0.0429]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61248.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0120, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0249],
        [0.0176, 0.0000, 0.0000,  ..., 0.0200, 0.0000, 0.0299],
        [0.0283, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0380],
        ...,
        [0.0195, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0294],
        [0.0240, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0325],
        [0.0389, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450862.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3340.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-260.1740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(466.4338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.3520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.9501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0933],
        [ 0.1026],
        [ 0.1104],
        ...,
        [-0.0762],
        [-0.0006],
        [ 0.0358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58480.1992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0006],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366891.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(574.1137, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0007],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366902.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        ...,
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008],
        [-0.0004,  0.0017,  0.0000,  ..., -0.0014, -0.0007, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1799.2219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2310, device='cuda:0')



h[100].sum tensor(0.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-36.9436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57003.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0189],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432631., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3077.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-284.4734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(440.7859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.3690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.6181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2250],
        [-0.3196],
        [-0.3821],
        ...,
        [-0.3191],
        [-0.3182],
        [-0.3179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84332.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0007],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366902.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0007],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366913.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1713.6819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7532, device='cuda:0')



h[100].sum tensor(-1.1909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-37.8586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0098, 0.0000,  ..., 0.0023, 0.0003, 0.0038],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0027, 0.0126, 0.0000,  ..., 0.0061, 0.0013, 0.0085],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52936.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0228],
        [0.0069, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0216],
        [0.0138, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0262],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411669.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2725.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-302.9568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(267.5515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.9541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.7074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2660],
        [-0.2687],
        [-0.2355],
        ...,
        [-0.3283],
        [-0.3272],
        [-0.3268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91295.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0007],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366913.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0008],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366923.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1887.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9485, device='cuda:0')



h[100].sum tensor(0.8874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0151, device='cuda:0')



h[200].sum tensor(-35.8691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.4939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58658.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0184],
        [0.0034, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0203],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0179],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0179],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435004.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2739.0776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.8764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(256.3566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.1243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.5247, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2587],
        [-0.1880],
        [-0.0862],
        ...,
        [-0.3342],
        [-0.3333],
        [-0.3330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74903.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9998],
        [1.0008],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366923.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0008],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366934.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1765.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8542, device='cuda:0')



h[100].sum tensor(-1.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-37.2197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0314, 0.0000,  ..., 0.0283, 0.0065, 0.0380],
        [0.0018, 0.0111, 0.0000,  ..., 0.0038, 0.0007, 0.0057],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53621.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0596, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.0560],
        [0.0365, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0413],
        [0.0266, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0346],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410816.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2495.4683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.0252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(209.2395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.5343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.4759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1065],
        [ 0.1065],
        [ 0.1050],
        ...,
        [-0.3405],
        [-0.3394],
        [-0.3391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97324.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9999],
        [1.0008],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366934.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0009],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366945.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1787.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1244, device='cuda:0')



h[100].sum tensor(-2.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-37.0752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55141.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0194],
        [0.0016, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0177],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422545.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2607.6536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.6643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(216.6306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.1208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.0921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2358],
        [-0.3413],
        [-0.4139],
        ...,
        [-0.3419],
        [-0.3409],
        [-0.3406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91177.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0009],
        ...,
        [1.0013],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366945.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0009],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366956.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1828.9873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8435, device='cuda:0')



h[100].sum tensor(-2.6816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-36.7781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6767, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54884.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0235],
        [0.0066, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0218],
        [0.0044, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0202],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415920.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2573.9470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-281.7100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(168.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.8677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.7135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0853],
        [ 0.0761],
        [ 0.0649],
        ...,
        [-0.3406],
        [-0.3397],
        [-0.3397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86144.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9999],
        [1.0009],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366956.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0010],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366968.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1844.5045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2631, device='cuda:0')



h[100].sum tensor(-3.0385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-36.8598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2784, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54236.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0176],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0177],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0016, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413452.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2684.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.0053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(256.4729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.3103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.4003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3089],
        [-0.3278],
        [-0.3373],
        ...,
        [-0.3377],
        [-0.3367],
        [-0.3364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95888.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0010],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366968.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0010],
        ...,
        [1.0014],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366980.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.3652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2207, device='cuda:0')



h[100].sum tensor(-2.9426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-36.8124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55402.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0189],
        [0.0074, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0219],
        [0.0126, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0256],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0183]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423606.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2846.0264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-276.8940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(178.9243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.1652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.1600, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0587],
        [ 0.0853],
        [ 0.1024],
        ...,
        [-0.3336],
        [-0.3327],
        [-0.3324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73189.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0010],
        ...,
        [1.0014],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366980.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0011],
        ...,
        [1.0014],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366992.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1888.2358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4476, device='cuda:0')



h[100].sum tensor(-3.0540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-36.7760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.4050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54962.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0194],
        [0.0024, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0193],
        [0.0033, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0197],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0200],
        [0.0080, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0228],
        [0.0099, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0241]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418654.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2932.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-280.0572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(216.9153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.4680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.7113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2005],
        [-0.1595],
        [-0.0939],
        ...,
        [-0.2006],
        [-0.1182],
        [-0.0667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78187.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0011],
        ...,
        [1.0014],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366992.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0011],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367004.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [ 0.0034,  0.0087, -0.0101,  ...,  0.0080,  0.0018,  0.0109],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2306.2527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.0665, device='cuda:0')



h[100].sum tensor(2.4616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0218, device='cuda:0')



h[200].sum tensor(-32.1371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0062, 0.0198, 0.0000,  ..., 0.0143, 0.0032, 0.0196],
        [0.0062, 0.0198, 0.0000,  ..., 0.0144, 0.0032, 0.0197],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71397.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0301],
        [0.0344, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0397],
        [0.0433, 0.0000, 0.0000,  ..., 0.0543, 0.0000, 0.0451],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0189],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0189],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0189]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506465.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3707.1138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.4614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(816.2478, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.3342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.7181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1205],
        [ 0.1201],
        [ 0.1203],
        ...,
        [-0.3336],
        [-0.3326],
        [-0.3323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65769.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [1.0000],
        [1.0011],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367004.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(582.9348, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367015.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009,  0.0041, -0.0034,  ...,  0.0018,  0.0002,  0.0031],
        [ 0.0052,  0.0120, -0.0148,  ...,  0.0124,  0.0030,  0.0164],
        [ 0.0083,  0.0176, -0.0230,  ...,  0.0200,  0.0050,  0.0259],
        ...,
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0018,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2084.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0144, device='cuda:0')



h[100].sum tensor(-0.8996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0168, device='cuda:0')



h[200].sum tensor(-34.6031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0405, 0.0000,  ..., 0.0396, 0.0093, 0.0530],
        [0.0190, 0.0445, 0.0000,  ..., 0.0450, 0.0108, 0.0598],
        [0.0257, 0.0567, 0.0000,  ..., 0.0616, 0.0152, 0.0805],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64619.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1090, 0.0000, 0.0000,  ..., 0.1415, 0.0000, 0.0849],
        [0.1182, 0.0000, 0.0000,  ..., 0.1536, 0.0000, 0.0902],
        [0.1334, 0.0000, 0.0000,  ..., 0.1734, 0.0000, 0.0987],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479010.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3559.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.3548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(585.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.9351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.7057, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1246],
        [ 0.1230],
        [ 0.1190],
        ...,
        [-0.3407],
        [-0.3397],
        [-0.3394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74986.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0011],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367015.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367026.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0077, -0.0085,  ...,  0.0065,  0.0014,  0.0090],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [ 0.0084,  0.0178, -0.0233,  ...,  0.0203,  0.0051,  0.0262],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2271.3462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4456, device='cuda:0')



h[100].sum tensor(1.5688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0209, device='cuda:0')



h[200].sum tensor(-32.4815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.6977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0122, 0.0000,  ..., 0.0051, 0.0010, 0.0072],
        [0.0178, 0.0424, 0.0000,  ..., 0.0420, 0.0099, 0.0560],
        [0.0169, 0.0401, 0.0000,  ..., 0.0401, 0.0098, 0.0529],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68323.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0331, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0392],
        [0.0737, 0.0000, 0.0000,  ..., 0.0941, 0.0000, 0.0639],
        [0.0932, 0.0000, 0.0000,  ..., 0.1196, 0.0000, 0.0753],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0048, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0208],
        [0.0077, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481334.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3609.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-235.1298, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(659.7352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.4108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.7240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0849],
        [ 0.1006],
        [ 0.1040],
        ...,
        [-0.3126],
        [-0.2513],
        [-0.1641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89308.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367026.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0015],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367038.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [ 0.0027,  0.0075, -0.0082,  ...,  0.0063,  0.0014,  0.0087],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1769.2480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5414, device='cuda:0')



h[100].sum tensor(-5.3236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-38.0016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0153, 0.0000,  ..., 0.0077, 0.0014, 0.0114],
        [0.0028, 0.0147, 0.0000,  ..., 0.0055, 0.0005, 0.0095],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50452.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0232],
        [0.0197, 0.0000, 0.0000,  ..., 0.0231, 0.0000, 0.0324],
        [0.0238, 0.0000, 0.0000,  ..., 0.0294, 0.0000, 0.0366],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405695.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2997.0220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-309.0988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79.2745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.5262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0813],
        [ 0.0244],
        [ 0.0868],
        ...,
        [-0.3634],
        [-0.3622],
        [-0.3618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107842.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0015],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367038.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367050.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0121, -0.0148,  ...,  0.0124,  0.0030,  0.0165],
        [ 0.0053,  0.0122, -0.0150,  ...,  0.0125,  0.0030,  0.0166],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2041.8933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7977, device='cuda:0')



h[100].sum tensor(-1.4277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0161, device='cuda:0')



h[200].sum tensor(-34.9455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.7629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0216, 0.0495, 0.0000,  ..., 0.0512, 0.0124, 0.0675],
        [0.0155, 0.0378, 0.0000,  ..., 0.0367, 0.0088, 0.0485],
        [0.0138, 0.0354, 0.0000,  ..., 0.0320, 0.0073, 0.0435],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62217.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0953, 0.0000, 0.0000,  ..., 0.1216, 0.0000, 0.0764],
        [0.0953, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.0770],
        [0.0900, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.0748],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(468458.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3472.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-263.9141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(493.2570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.5248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.9041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0953],
        [ 0.0967],
        [ 0.0975],
        ...,
        [-0.3712],
        [-0.3700],
        [-0.3697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100964.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0012],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367050.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367062.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0041, -0.0032,  ...,  0.0016,  0.0001,  0.0028],
        [ 0.0013,  0.0049, -0.0044,  ...,  0.0027,  0.0004,  0.0042],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1827.4402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7499, device='cuda:0')



h[100].sum tensor(-4.2557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-37.3622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0274, 0.0000,  ..., 0.0211, 0.0044, 0.0299],
        [0.0028, 0.0141, 0.0000,  ..., 0.0059, 0.0010, 0.0092],
        [0.0013, 0.0107, 0.0000,  ..., 0.0027, 0.0004, 0.0042],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52962.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0510, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0522],
        [0.0282, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0376],
        [0.0141, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0277],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421510.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3047.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.3193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(132.9084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.3212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.3815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1061],
        [ 0.0856],
        [ 0.0289],
        ...,
        [-0.3754],
        [-0.3742],
        [-0.3738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107964.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367062.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367074.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1880.2402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7696, device='cuda:0')



h[100].sum tensor(-3.7130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-36.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0017, 0.0114, 0.0000,  ..., 0.0037, 0.0007, 0.0055],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55277.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0234],
        [0.0132, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0265],
        [0.0214, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0325],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439323.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3223.9814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.6683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(235.5268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.7947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.0593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0882],
        [ 0.0973],
        [ 0.1087],
        ...,
        [-0.3755],
        [-0.3743],
        [-0.3739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103583.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0001],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367074.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367086.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1889.2219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1423, device='cuda:0')



h[100].sum tensor(-3.7976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-37.0608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5092, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54514.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0203],
        [0.0028, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0194],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0189],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(430661.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3143.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.8784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(142.8106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.6190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.0676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0025],
        [-0.0903],
        [-0.2001],
        ...,
        [-0.3703],
        [-0.3709],
        [-0.3714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100857.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0013],
        ...,
        [1.0015],
        [1.0008],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367086.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0008],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367098.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024,  0.0069, -0.0073,  ...,  0.0054,  0.0011,  0.0076],
        [ 0.0028,  0.0076, -0.0082,  ...,  0.0063,  0.0014,  0.0087],
        [ 0.0028,  0.0077, -0.0084,  ...,  0.0065,  0.0014,  0.0090],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1921.6088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0741, device='cuda:0')



h[100].sum tensor(-3.6092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-36.8979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0259, 0.0000,  ..., 0.0193, 0.0039, 0.0275],
        [0.0103, 0.0289, 0.0000,  ..., 0.0234, 0.0050, 0.0326],
        [0.0134, 0.0346, 0.0000,  ..., 0.0311, 0.0070, 0.0423],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55752.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0820, 0.0000, 0.0000,  ..., 0.1045, 0.0000, 0.0708],
        [0.0742, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.0668],
        [0.0658, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.0617],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440397.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3212.0837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-282.2597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(198.3437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(158.2818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.7630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1275],
        [ 0.1283],
        [ 0.1228],
        ...,
        [-0.3734],
        [-0.3722],
        [-0.3717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91259.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0008],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367098.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0009],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367110.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0026,  0.0072, -0.0077,  ...,  0.0058,  0.0012,  0.0081],
        [ 0.0009,  0.0041, -0.0033,  ...,  0.0016,  0.0001,  0.0029],
        [ 0.0013,  0.0050, -0.0045,  ...,  0.0028,  0.0004,  0.0043],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1881.3699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8710, device='cuda:0')



h[100].sum tensor(-4.8251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-37.5074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0037, 0.0163, 0.0000,  ..., 0.0077, 0.0011, 0.0122],
        [0.0094, 0.0272, 0.0000,  ..., 0.0211, 0.0044, 0.0298],
        [0.0041, 0.0163, 0.0000,  ..., 0.0091, 0.0018, 0.0131],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52826.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0000, 0.0000,  ..., 0.0368, 0.0000, 0.0405],
        [0.0397, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0463],
        [0.0282, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0383],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        [0.0027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0193]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424208.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3110.8340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-295.4062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(223.0885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.7525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.0217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1276],
        [ 0.1283],
        [ 0.1011],
        ...,
        [-0.3745],
        [-0.3733],
        [-0.3730]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109538.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0009],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367110.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367122.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0043, -0.0035,  ...,  0.0019,  0.0002,  0.0032],
        [ 0.0027,  0.0074, -0.0080,  ...,  0.0061,  0.0013,  0.0085],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2225.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1969, device='cuda:0')



h[100].sum tensor(-0.8355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0185, device='cuda:0')



h[200].sum tensor(-33.9779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.7819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0267, 0.0000,  ..., 0.0205, 0.0042, 0.0290],
        [0.0037, 0.0163, 0.0000,  ..., 0.0076, 0.0011, 0.0121],
        [0.0034, 0.0151, 0.0000,  ..., 0.0074, 0.0013, 0.0110],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71196.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0466],
        [0.0299, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0403],
        [0.0221, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0340],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0194],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0194],
        [0.0026, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0194]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536853.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4036.4927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-219.9557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(890.2492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(184.1406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.4573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1252],
        [ 0.1160],
        [ 0.0713],
        ...,
        [-0.3756],
        [-0.3743],
        [-0.3739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81716.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0014],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367122.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(529.8867, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367134.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0132, -0.0163,  ...,  0.0140,  0.0034,  0.0183],
        [ 0.0121,  0.0245, -0.0327,  ...,  0.0293,  0.0075,  0.0376],
        [ 0.0059,  0.0133, -0.0164,  ...,  0.0140,  0.0034,  0.0184],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1883.9360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3726, device='cuda:0')



h[100].sum tensor(-5.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-37.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.2947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0290, 0.0627, 0.0000,  ..., 0.0693, 0.0172, 0.0902],
        [0.0269, 0.0589, 0.0000,  ..., 0.0641, 0.0158, 0.0836],
        [0.0294, 0.0634, 0.0000,  ..., 0.0703, 0.0174, 0.0914],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52594.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1239, 0.0000, 0.0000,  ..., 0.1540, 0.0000, 0.0921],
        [0.1316, 0.0000, 0.0000,  ..., 0.1638, 0.0000, 0.0966],
        [0.1313, 0.0000, 0.0000,  ..., 0.1634, 0.0000, 0.0967],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195],
        [0.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0195]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426481.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3075.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-293.7450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(166.0122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.0369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.2202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1025],
        [ 0.1017],
        [ 0.1041],
        ...,
        [-0.3786],
        [-0.3774],
        [-0.3770]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102893.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367134.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367146.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [ 0.0012,  0.0047, -0.0040,  ...,  0.0023,  0.0003,  0.0038],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0014, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1932.5557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9946, device='cuda:0')



h[100].sum tensor(-5.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-37.5173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.4079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0220, 0.0000,  ..., 0.0139, 0.0024, 0.0207],
        [0.0009, 0.0098, 0.0000,  ..., 0.0016, 0.0001, 0.0029],
        [0.0012, 0.0103, 0.0000,  ..., 0.0023, 0.0003, 0.0038],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54212.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0268, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0387],
        [0.0140, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0289],
        [0.0130, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0276],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433847.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3112.9263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-288.6994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(237.6008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(162.6042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.5292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0624],
        [-0.1259],
        [-0.1545],
        ...,
        [-0.3819],
        [-0.3807],
        [-0.3804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105262.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367146.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367158.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [ 0.0036,  0.0090, -0.0103,  ...,  0.0083,  0.0019,  0.0112],
        [ 0.0056,  0.0126, -0.0154,  ...,  0.0131,  0.0032,  0.0172],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2039.5510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9350, device='cuda:0')



h[100].sum tensor(-4.2005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-36.5356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.1120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0211, 0.0000,  ..., 0.0141, 0.0028, 0.0201],
        [0.0096, 0.0269, 0.0000,  ..., 0.0220, 0.0049, 0.0300],
        [0.0149, 0.0371, 0.0000,  ..., 0.0345, 0.0079, 0.0464],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58522.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0712, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.0640],
        [0.0848, 0.0000, 0.0000,  ..., 0.1041, 0.0000, 0.0716],
        [0.1013, 0.0000, 0.0000,  ..., 0.1249, 0.0000, 0.0813],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457428., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3212.0718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-269.4747, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(306.5345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(172.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.9841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0143],
        [ 0.0261],
        [ 0.0360],
        ...,
        [-0.3845],
        [-0.3832],
        [-0.3826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92398.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0015],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367158.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367170.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [ 0.0013,  0.0049, -0.0043,  ...,  0.0026,  0.0004,  0.0041],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1951.9156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3121, device='cuda:0')



h[100].sum tensor(-5.5064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-37.6454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0013, 0.0106, 0.0000,  ..., 0.0026, 0.0004, 0.0041],
        [0.0083, 0.0238, 0.0000,  ..., 0.0192, 0.0045, 0.0257],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55239.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0265],
        [0.0250, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0358],
        [0.0515, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0522],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200],
        [0.0019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445161.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3064.2913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.3201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(244.8429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.7647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.2765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0587],
        [ 0.0994],
        [ 0.1113],
        ...,
        [-0.3874],
        [-0.3861],
        [-0.3858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96270.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367170.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367182.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1795.2743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0442, device='cuda:0')



h[100].sum tensor(-7.8001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0084, device='cuda:0')



h[200].sum tensor(-39.3957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.3243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0211, 0.0000,  ..., 0.0155, 0.0035, 0.0210],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49754.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0394],
        [0.0118, 0.0000, 0.0000,  ..., 0.0107, 0.0000, 0.0263],
        [0.0054, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0222],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421660.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2839.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.3210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47.3738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.8796, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.7064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0485],
        [-0.0624],
        [-0.2239],
        ...,
        [-0.3946],
        [-0.3933],
        [-0.3929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101096.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367182.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367193.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029,  0.0078, -0.0084,  ...,  0.0065,  0.0014,  0.0090],
        [ 0.0062,  0.0137, -0.0169,  ...,  0.0146,  0.0036,  0.0191],
        [ 0.0091,  0.0190, -0.0245,  ...,  0.0218,  0.0055,  0.0281],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1841.4458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8399, device='cuda:0')



h[100].sum tensor(-7.6505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-39.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.5567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0458, 0.0000,  ..., 0.0461, 0.0110, 0.0611],
        [0.0286, 0.0619, 0.0000,  ..., 0.0680, 0.0168, 0.0884],
        [0.0429, 0.0877, 0.0000,  ..., 0.1033, 0.0262, 0.1326],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50267.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1348, 0.0000, 0.0000,  ..., 0.1643, 0.0000, 0.1006],
        [0.1932, 0.0000, 0.0000,  ..., 0.2358, 0.0000, 0.1313],
        [0.2544, 0.0000, 0.0000,  ..., 0.3106, 0.0000, 0.1637],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0205],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0205],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0205]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420687.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2865.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.2268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112.8845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.7154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.3594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0481],
        [-0.0052],
        [-0.0618],
        ...,
        [-0.4018],
        [-0.4005],
        [-0.4001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116664.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0016],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367193.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367205.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0019,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1969.4871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8356, device='cuda:0')



h[100].sum tensor(-6.2964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-37.7564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0111, 0.0000,  ..., 0.0030, 0.0005, 0.0047],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0019, 0.0119, 0.0000,  ..., 0.0041, 0.0008, 0.0060],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56341.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0265],
        [0.0070, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0246],
        [0.0140, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0294],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0207],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0207],
        [0.0017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457525.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3160.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-290.2956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(251.6095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.9729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.8434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1940],
        [-0.1414],
        [-0.0405],
        ...,
        [-0.4093],
        [-0.4080],
        [-0.4076]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110289.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367205.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367217.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.4813e-04,  4.1418e-03, -3.1276e-03,  ...,  1.4976e-03,
          8.5205e-05,  2.7199e-03],
        [-3.6343e-04,  1.9602e-03,  0.0000e+00,  ..., -1.4783e-03,
         -7.0752e-04, -1.0048e-03],
        [-3.6343e-04,  1.9602e-03,  0.0000e+00,  ..., -1.4783e-03,
         -7.0752e-04, -1.0048e-03],
        ...,
        [-3.6343e-04,  1.9602e-03,  0.0000e+00,  ..., -1.4783e-03,
         -7.0752e-04, -1.0048e-03],
        [-3.6343e-04,  1.9602e-03,  0.0000e+00,  ..., -1.4783e-03,
         -7.0752e-04, -1.0048e-03],
        [-3.6343e-04,  1.9602e-03,  0.0000e+00,  ..., -1.4783e-03,
         -7.0752e-04, -1.0048e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2169.7710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9223, device='cuda:0')



h[100].sum tensor(-3.7546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-35.7189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0154, 0.0000,  ..., 0.0074, 0.0013, 0.0110],
        [0.0066, 0.0217, 0.0000,  ..., 0.0145, 0.0029, 0.0207],
        [0.0117, 0.0310, 0.0000,  ..., 0.0271, 0.0063, 0.0364],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63981.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0549],
        [0.0694, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.0652],
        [0.0898, 0.0000, 0.0000,  ..., 0.1081, 0.0000, 0.0763],
        ...,
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0209],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0209],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494642.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3393.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-264.8508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(550.7880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.0654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.5237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0821],
        [ 0.0789],
        [ 0.0754],
        ...,
        [-0.4168],
        [-0.4154],
        [-0.4150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109878.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367217.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367229.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1872.1086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6922, device='cuda:0')



h[100].sum tensor(-7.6003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-38.9872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.1415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0113, 0.0000,  ..., 0.0018, 0.0000, 0.0039],
        [0.0006, 0.0096, 0.0000,  ..., 0.0009, 0.0000, 0.0019],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52774.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0290],
        [0.0058, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0265],
        [0.0037, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0237],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0211],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0211],
        [0.0013, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0211]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441705.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2880.1592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.8078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82.5793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.5137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.6096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3169],
        [-0.3483],
        [-0.3922],
        ...,
        [-0.4227],
        [-0.4214],
        [-0.4210]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108730.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0017],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367229.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0018],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367241.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0047, -0.0039,  ...,  0.0023,  0.0003,  0.0037],
        [ 0.0011,  0.0046, -0.0038,  ...,  0.0022,  0.0003,  0.0035],
        [ 0.0026,  0.0074, -0.0077,  ...,  0.0059,  0.0013,  0.0082],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2034.1702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3902, device='cuda:0')



h[100].sum tensor(-5.5700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-37.5216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0168, 0.0000,  ..., 0.0092, 0.0019, 0.0132],
        [0.0088, 0.0264, 0.0000,  ..., 0.0193, 0.0039, 0.0275],
        [0.0039, 0.0168, 0.0000,  ..., 0.0077, 0.0012, 0.0122],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57465.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0000, 0.0000,  ..., 0.0362, 0.0000, 0.0436],
        [0.0451, 0.0000, 0.0000,  ..., 0.0537, 0.0000, 0.0533],
        [0.0401, 0.0000, 0.0000,  ..., 0.0475, 0.0000, 0.0504],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0214],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0214],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0214]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460162.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3006.6365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-295.7986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(217.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.5265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1254],
        [ 0.1314],
        [ 0.1348],
        ...,
        [-0.4246],
        [-0.4233],
        [-0.4229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104148.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0018],
        ...,
        [1.0015],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367241.7812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(516.8417, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0018],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367254.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2003.6985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1550, device='cuda:0')



h[100].sum tensor(-5.9979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-38.0544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56501.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0211],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0214],
        [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0221],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0217],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0217],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0217]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461026.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3002.8489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-305.9124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(429.5565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.7741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.0261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3339],
        [-0.3472],
        [-0.3497],
        ...,
        [-0.4264],
        [-0.4253],
        [-0.4252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122440.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0018],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367254.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0019],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367267.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1900.5811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4847, device='cuda:0')



h[100].sum tensor(-7.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-39.3321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.9991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0096, 0.0000,  ..., 0.0009, 0.0000, 0.0019],
        [0.0012, 0.0113, 0.0000,  ..., 0.0017, 0.0000, 0.0038],
        [0.0006, 0.0096, 0.0000,  ..., 0.0009, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53330.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0292],
        [0.0095, 0.0000, 0.0000,  ..., 0.0094, 0.0000, 0.0314],
        [0.0071, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0288],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0219],
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0219],
        [0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0219]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450388.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2830.3005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-317.4179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(159.8355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.5952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.4700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0403],
        [ 0.0201],
        [ 0.0486],
        ...,
        [-0.4270],
        [-0.4256],
        [-0.4252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103540.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0019],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367267.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0019],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367280.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [ 0.0028,  0.0077, -0.0082,  ...,  0.0063,  0.0014,  0.0087],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2194.9319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8063, device='cuda:0')



h[100].sum tensor(-3.3445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0155, device='cuda:0')



h[200].sum tensor(-36.5423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.0825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0110, 0.0000,  ..., 0.0027, 0.0005, 0.0043],
        [0.0083, 0.0247, 0.0000,  ..., 0.0185, 0.0041, 0.0257],
        [0.0131, 0.0334, 0.0000,  ..., 0.0303, 0.0073, 0.0404],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62424.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0279, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0412],
        [0.0636, 0.0000, 0.0000,  ..., 0.0765, 0.0000, 0.0629],
        [0.0951, 0.0000, 0.0000,  ..., 0.1151, 0.0000, 0.0808],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0220],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0220],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486677.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3061.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.1963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(405.6112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.0502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.0713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1313],
        [ 0.1404],
        [ 0.1459],
        ...,
        [-0.4264],
        [-0.4250],
        [-0.4247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97455.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0019],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367280.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0020],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367292.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0058, -0.0054,  ...,  0.0037,  0.0007,  0.0055],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1937.7275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7697, device='cuda:0')



h[100].sum tensor(-7.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0099, device='cuda:0')



h[200].sum tensor(-39.3506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.1947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0200, 0.0000,  ..., 0.0135, 0.0031, 0.0186],
        [0.0018, 0.0117, 0.0000,  ..., 0.0037, 0.0007, 0.0055],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52564.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0460, 0.0000, 0.0000,  ..., 0.0545, 0.0000, 0.0522],
        [0.0213, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0367],
        [0.0071, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0272],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440714.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2711.7590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-325.1255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(101.1344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.3265, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.6334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1316],
        [ 0.1120],
        [ 0.0628],
        ...,
        [-0.4301],
        [-0.4288],
        [-0.4284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104094.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0020],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367292.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0020],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367305., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [ 0.0053,  0.0121, -0.0145,  ...,  0.0124,  0.0030,  0.0164],
        [ 0.0063,  0.0139, -0.0170,  ...,  0.0148,  0.0037,  0.0194],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1972.5155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3997, device='cuda:0')



h[100].sum tensor(-7.6443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-39.1312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.6271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0181, 0.0000,  ..., 0.0124, 0.0030, 0.0164],
        [0.0117, 0.0303, 0.0000,  ..., 0.0276, 0.0068, 0.0363],
        [0.0281, 0.0610, 0.0000,  ..., 0.0665, 0.0167, 0.0868],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54098., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0384, 0.0000, 0.0000,  ..., 0.0443, 0.0000, 0.0461],
        [0.0709, 0.0000, 0.0000,  ..., 0.0845, 0.0000, 0.0658],
        [0.1106, 0.0000, 0.0000,  ..., 0.1332, 0.0000, 0.0895],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0229],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0229],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452478.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2852.7554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-321.8402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(219.3043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.1386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.0307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1315],
        [ 0.1375],
        [ 0.1369],
        ...,
        [-0.4345],
        [-0.4332],
        [-0.4328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116902.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0020],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367305., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367317.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2237.1943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9750, device='cuda:0')



h[100].sum tensor(-4.8988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-36.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.1983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61298.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0082, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0293],
        [0.0027, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0250],
        [0.0011, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479605.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3065.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-290.8174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(276.7238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.6320, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.3111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0624],
        [-0.0300],
        [-0.1554],
        ...,
        [-0.4366],
        [-0.4354],
        [-0.4353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97525.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367317.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2030.1372, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0711, device='cuda:0')



h[100].sum tensor(-7.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-38.7838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.7741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55084.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0226],
        [0.0018, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0235],
        [0.0037, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0247],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455651., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2816.9978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.9186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112.4979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.6855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.6538, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4347],
        [-0.3823],
        [-0.2718],
        ...,
        [-0.4450],
        [-0.4436],
        [-0.4432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103036.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0051, -0.0045,  ...,  0.0028,  0.0005,  0.0044],
        [ 0.0038,  0.0094, -0.0105,  ...,  0.0086,  0.0020,  0.0117],
        [ 0.0049,  0.0114, -0.0133,  ...,  0.0113,  0.0028,  0.0150],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2627.5271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.9379, device='cuda:0')



h[100].sum tensor(-0.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0229, device='cuda:0')



h[200].sum tensor(-32.7441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-28.0943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0220, 0.0000,  ..., 0.0161, 0.0038, 0.0220],
        [0.0160, 0.0393, 0.0000,  ..., 0.0367, 0.0088, 0.0496],
        [0.0196, 0.0459, 0.0000,  ..., 0.0457, 0.0112, 0.0608],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78456.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0551, 0.0000, 0.0000,  ..., 0.0663, 0.0000, 0.0593],
        [0.0911, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.0808],
        [0.1173, 0.0000, 0.0000,  ..., 0.1422, 0.0000, 0.0957],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579618.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3866.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.0182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1274.9813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.2485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.0511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1224],
        [ 0.1104],
        [ 0.0945],
        ...,
        [-0.4397],
        [-0.4411],
        [-0.4416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111206.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0108, -0.0126,  ...,  0.0106,  0.0026,  0.0141],
        [ 0.0083,  0.0175, -0.0220,  ...,  0.0197,  0.0050,  0.0256],
        [ 0.0043,  0.0104, -0.0119,  ...,  0.0100,  0.0024,  0.0134],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2013.0160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4762, device='cuda:0')



h[100].sum tensor(-7.9217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-38.9569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.3659, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0514, 0.0000,  ..., 0.0532, 0.0132, 0.0702],
        [0.0235, 0.0528, 0.0000,  ..., 0.0551, 0.0137, 0.0727],
        [0.0249, 0.0554, 0.0000,  ..., 0.0587, 0.0147, 0.0771],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54137.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0959, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0820],
        [0.1116, 0.0000, 0.0000,  ..., 0.1353, 0.0000, 0.0922],
        [0.1140, 0.0000, 0.0000,  ..., 0.1384, 0.0000, 0.0943],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0008, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449291.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2771.3540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-320.8534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(142.5685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.1817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.7570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0985],
        [ 0.1089],
        [ 0.1202],
        ...,
        [-0.4450],
        [-0.4436],
        [-0.4432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110059.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0021],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367329.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0022],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367342.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0133, -0.0160,  ...,  0.0139,  0.0035,  0.0183],
        [ 0.0118,  0.0239, -0.0310,  ...,  0.0284,  0.0074,  0.0365],
        [ 0.0116,  0.0234, -0.0304,  ...,  0.0278,  0.0072,  0.0357],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2031.6122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9993, device='cuda:0')



h[100].sum tensor(-7.6460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-38.8283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.7248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0294, 0.0634, 0.0000,  ..., 0.0696, 0.0176, 0.0908],
        [0.0394, 0.0813, 0.0000,  ..., 0.0940, 0.0242, 0.1214],
        [0.0413, 0.0847, 0.0000,  ..., 0.0987, 0.0254, 0.1273],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55798.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1407, 0.0000, 0.0000,  ..., 0.1706, 0.0000, 0.1060],
        [0.1827, 0.0000, 0.0000,  ..., 0.2221, 0.0000, 0.1301],
        [0.1891, 0.0000, 0.0000,  ..., 0.2300, 0.0000, 0.1344],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463421.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2783.8345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-314.7204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(214.6204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.8175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.4117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0877],
        [ 0.0994],
        [ 0.1111],
        ...,
        [-0.4511],
        [-0.4496],
        [-0.4488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108782.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0022],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367342.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(441.1064, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0023],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367355.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022,  0.0067, -0.0066,  ...,  0.0048,  0.0010,  0.0069],
        [ 0.0059,  0.0132, -0.0158,  ...,  0.0137,  0.0034,  0.0181],
        [ 0.0050,  0.0116, -0.0136,  ...,  0.0116,  0.0028,  0.0154],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0015, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2272.1135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8598, device='cuda:0')



h[100].sum tensor(-4.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0161, device='cuda:0')



h[200].sum tensor(-36.4694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0392, 0.0000,  ..., 0.0364, 0.0088, 0.0492],
        [0.0158, 0.0390, 0.0000,  ..., 0.0361, 0.0087, 0.0488],
        [0.0166, 0.0403, 0.0000,  ..., 0.0379, 0.0092, 0.0510],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63618.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0000, 0.0000,  ..., 0.0799, 0.0000, 0.0651],
        [0.0728, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.0702],
        [0.0722, 0.0000, 0.0000,  ..., 0.0890, 0.0000, 0.0700],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0230],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0230],
        [0.0004, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504112.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3017.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.6545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(458.3956, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.5643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1645],
        [ 0.1671],
        [ 0.1701],
        ...,
        [-0.4561],
        [-0.4553],
        [-0.4551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106075.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0023],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367355.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0023],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367367.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0083, -0.0088,  ...,  0.0070,  0.0016,  0.0096],
        [ 0.0028,  0.0077, -0.0080,  ...,  0.0061,  0.0014,  0.0086],
        [ 0.0031,  0.0082, -0.0087,  ...,  0.0068,  0.0016,  0.0094],
        ...,
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [ 0.0036,  0.0090, -0.0099,  ...,  0.0080,  0.0019,  0.0110],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2195.3389, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1917, device='cuda:0')



h[100].sum tensor(-5.0859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-37.3559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.9744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0297, 0.0000,  ..., 0.0232, 0.0053, 0.0327],
        [0.0110, 0.0302, 0.0000,  ..., 0.0240, 0.0055, 0.0336],
        [0.0106, 0.0295, 0.0000,  ..., 0.0231, 0.0052, 0.0324],
        ...,
        [0.0036, 0.0154, 0.0000,  ..., 0.0082, 0.0019, 0.0112],
        [0.0029, 0.0141, 0.0000,  ..., 0.0064, 0.0015, 0.0089],
        [0.0131, 0.0343, 0.0000,  ..., 0.0292, 0.0068, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60561.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0520, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0602],
        [0.0520, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0598],
        [0.0512, 0.0000, 0.0000,  ..., 0.0642, 0.0000, 0.0594],
        ...,
        [0.0157, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0340],
        [0.0203, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0372],
        [0.0357, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485492.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2725.7183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.1811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(218.5054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.6493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.9089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2029],
        [ 0.2036],
        [ 0.2041],
        ...,
        [-0.0941],
        [-0.0438],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96759.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0005],
        [1.0023],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367367.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0024],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367380.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        ...,
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2315.4932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8262, device='cuda:0')



h[100].sum tensor(-3.7505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0167, device='cuda:0')



h[200].sum tensor(-36.3224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.4687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64656.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0265],
        [0.0014, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0242],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505455.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2820.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-280.2984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(324.8002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.3230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.2370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0687],
        [-0.0350],
        [-0.1516],
        ...,
        [-0.4667],
        [-0.4652],
        [-0.4647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95551.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0024],
        ...,
        [1.0015],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367380.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0024],
        ...,
        [1.0016],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367392.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [ 0.0027,  0.0076, -0.0078,  ...,  0.0060,  0.0014,  0.0085],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2277.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1789, device='cuda:0')



h[100].sum tensor(-5.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(-36.8382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.3382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0049, 0.0183, 0.0000,  ..., 0.0106, 0.0023, 0.0152],
        [0.0067, 0.0222, 0.0000,  ..., 0.0143, 0.0031, 0.0207],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0034, 0.0158, 0.0000,  ..., 0.0069, 0.0013, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61713.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0296],
        [0.0294, 0.0000, 0.0000,  ..., 0.0354, 0.0000, 0.0431],
        [0.0459, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0548],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0251],
        [0.0083, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0299],
        [0.0222, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489492.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2751.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-292.4559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(307.5543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.8115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.3523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0548],
        [ 0.1068],
        [ 0.1439],
        ...,
        [-0.2579],
        [-0.0909],
        [ 0.0470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106993., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0024],
        ...,
        [1.0016],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367392.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0025],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367405., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0071, -0.0071,  ...,  0.0053,  0.0012,  0.0076],
        [ 0.0072,  0.0155, -0.0190,  ...,  0.0168,  0.0042,  0.0220],
        [ 0.0009,  0.0043, -0.0032,  ...,  0.0015,  0.0001,  0.0029],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2056.5435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7198, device='cuda:0')



h[100].sum tensor(-8.8576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-39.2151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0252, 0.0560, 0.0000,  ..., 0.0588, 0.0147, 0.0777],
        [0.0120, 0.0323, 0.0000,  ..., 0.0265, 0.0061, 0.0371],
        [0.0136, 0.0351, 0.0000,  ..., 0.0302, 0.0071, 0.0418],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53565.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0292e-01, 0.0000e+00, 0.0000e+00,  ..., 1.2526e-01, 0.0000e+00,
         8.6870e-02],
        [7.8484e-02, 0.0000e+00, 0.0000e+00,  ..., 9.6031e-02, 0.0000e+00,
         7.4054e-02],
        [6.4867e-02, 0.0000e+00, 0.0000e+00,  ..., 7.9267e-02, 0.0000e+00,
         6.5908e-02],
        ...,
        [3.8701e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2592e-02],
        [3.9366e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2596e-02],
        [3.9764e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2599e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450164.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2439.6379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-325.1629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(132.4164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.4530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.4332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1179],
        [ 0.1215],
        [ 0.1236],
        ...,
        [-0.4806],
        [-0.4792],
        [-0.4787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132898.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0025],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367405., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0026],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367417.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0065, -0.0062,  ...,  0.0044,  0.0009,  0.0066],
        [ 0.0046,  0.0110, -0.0125,  ...,  0.0106,  0.0026,  0.0142],
        [ 0.0021,  0.0065, -0.0063,  ...,  0.0045,  0.0009,  0.0066],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0016, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2367.7622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1957, device='cuda:0')



h[100].sum tensor(-5.6081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-36.3293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.7223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0376, 0.0000,  ..., 0.0352, 0.0087, 0.0472],
        [0.0149, 0.0375, 0.0000,  ..., 0.0336, 0.0080, 0.0461],
        [0.0151, 0.0373, 0.0000,  ..., 0.0348, 0.0085, 0.0467],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66935.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1870e-01, 0.0000e+00, 0.0000e+00,  ..., 1.4290e-01, 0.0000e+00,
         9.3745e-02],
        [1.2990e-01, 0.0000e+00, 0.0000e+00,  ..., 1.5667e-01, 0.0000e+00,
         1.0070e-01],
        [1.2468e-01, 0.0000e+00, 0.0000e+00,  ..., 1.5010e-01, 0.0000e+00,
         9.7325e-02],
        ...,
        [4.8884e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2610e-02],
        [4.9545e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2614e-02],
        [4.9953e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2617e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524501.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3064.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-270.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(594.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.4559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.6207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0314],
        [ 0.0210],
        [ 0.0236],
        ...,
        [-0.4826],
        [-0.4812],
        [-0.4807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114997.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0026],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367417.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0027],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367429.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2116.6240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8871, device='cuda:0')



h[100].sum tensor(-8.9247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-39.0343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.3341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56773.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2970e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.3120e-02],
        [9.5507e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2317e-02],
        [7.8266e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2173e-02],
        ...,
        [6.5306e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2680e-02],
        [6.5960e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2684e-02],
        [6.6379e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2687e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475061.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2656.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-309.9553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(157.3141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.6340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.6384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2155],
        [-0.3501],
        [-0.4594],
        ...,
        [-0.4825],
        [-0.4810],
        [-0.4805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109805.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0027],
        ...,
        [1.0016],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367429.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0028],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367442.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        ...,
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011],
        [-0.0004,  0.0020,  0.0000,  ..., -0.0016, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2082.2476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8732, device='cuda:0')



h[100].sum tensor(-9.3970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-39.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9520, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54423.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.4262e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.1930e-02],
        [8.6512e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.1946e-02],
        [1.3600e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2025e-02],
        ...,
        [1.2456e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2531e-02],
        [1.2522e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2535e-02],
        [1.2566e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2538e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461278.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2578.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.3972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70.4247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.6322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.7560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5439],
        [-0.5761],
        [-0.6006],
        ...,
        [-0.4784],
        [-0.4769],
        [-0.4765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108588.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0028],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367442.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0029],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367455.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032,  0.0083, -0.0088,  ...,  0.0070,  0.0016,  0.0097],
        [ 0.0056,  0.0126, -0.0148,  ...,  0.0128,  0.0032,  0.0171],
        [ 0.0037,  0.0092, -0.0101,  ...,  0.0082,  0.0019,  0.0113],
        ...,
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2873.4175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.7218, device='cuda:0')



h[100].sum tensor(0.1865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0261, device='cuda:0')



h[200].sum tensor(-32.1194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.0636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0087, 0.0255, 0.0000,  ..., 0.0188, 0.0044, 0.0266],
        [0.0129, 0.0335, 0.0000,  ..., 0.0282, 0.0066, 0.0394],
        [0.0260, 0.0568, 0.0000,  ..., 0.0601, 0.0151, 0.0794],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89390.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.4828e-02, 0.0000e+00, 0.0000e+00,  ..., 7.8764e-02, 0.0000e+00,
         6.5140e-02],
        [9.3312e-02, 0.0000e+00, 0.0000e+00,  ..., 1.1352e-01, 0.0000e+00,
         8.1068e-02],
        [1.3480e-01, 0.0000e+00, 0.0000e+00,  ..., 1.6354e-01, 0.0000e+00,
         1.0284e-01],
        ...,
        [8.4684e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2205e-02],
        [8.5332e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2208e-02],
        [8.5772e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.2211e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686703.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4383.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.9450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1846.2551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(205.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-272.7250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1665],
        [ 0.1571],
        [ 0.1457],
        ...,
        [-0.4740],
        [-0.4725],
        [-0.4721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80769.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0029],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367455.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0030],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367467.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0075, -0.0077,  ...,  0.0059,  0.0013,  0.0084],
        [ 0.0011,  0.0045, -0.0035,  ...,  0.0017,  0.0002,  0.0032],
        [ 0.0050,  0.0116, -0.0134,  ...,  0.0114,  0.0028,  0.0154],
        ...,
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2355.2659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9395, device='cuda:0')



h[100].sum tensor(-6.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0156, device='cuda:0')



h[200].sum tensor(-37.1865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.1739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0205, 0.0000,  ..., 0.0101, 0.0017, 0.0169],
        [0.0110, 0.0302, 0.0000,  ..., 0.0235, 0.0052, 0.0337],
        [0.0059, 0.0205, 0.0000,  ..., 0.0119, 0.0024, 0.0181],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66864.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0586],
        [0.0492, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0581],
        [0.0404, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0522],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542243.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3180.0471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.8889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(524.8459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.9447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1703],
        [ 0.1784],
        [ 0.1808],
        ...,
        [-0.4807],
        [-0.4792],
        [-0.4787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83745.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0030],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367467.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(580.4739, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0031],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367479.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0061, -0.0057,  ...,  0.0039,  0.0008,  0.0060],
        [ 0.0020,  0.0062, -0.0058,  ...,  0.0040,  0.0008,  0.0061],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0010],
        ...,
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0010],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0010],
        [-0.0003,  0.0020,  0.0000,  ..., -0.0017, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2333.7466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5213, device='cuda:0')



h[100].sum tensor(-7.7289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-37.3678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.8869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0308, 0.0000,  ..., 0.0240, 0.0053, 0.0347],
        [0.0035, 0.0157, 0.0000,  ..., 0.0068, 0.0013, 0.0108],
        [0.0020, 0.0124, 0.0000,  ..., 0.0040, 0.0008, 0.0061],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65211.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0441, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0526],
        [0.0234, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0394],
        [0.0115, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0313],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533567.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3079.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-276.8184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(523.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.6685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.4875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0556],
        [-0.0728],
        [-0.2452],
        ...,
        [-0.4918],
        [-0.4903],
        [-0.4899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101479.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0031],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367479.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0032],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367492., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        [ 0.0067,  0.0146, -0.0175,  ...,  0.0153,  0.0038,  0.0205],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2311.5913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7560, device='cuda:0')



h[100].sum tensor(-8.4364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-37.5148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0367, 0.0000,  ..., 0.0318, 0.0073, 0.0447],
        [0.0051, 0.0185, 0.0000,  ..., 0.0105, 0.0022, 0.0155],
        [0.0153, 0.0379, 0.0000,  ..., 0.0334, 0.0078, 0.0468],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63861.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0000, 0.0000,  ..., 0.0528, 0.0000, 0.0527],
        [0.0356, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.0467],
        [0.0467, 0.0000, 0.0000,  ..., 0.0545, 0.0000, 0.0536],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516629.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2860.2231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.9760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(427.0556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.0434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.3493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0114],
        [-0.0197],
        [-0.0443],
        ...,
        [-0.4964],
        [-0.4950],
        [-0.4952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117879.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0032],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367492., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0033],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367504.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0053, -0.0045,  ...,  0.0026,  0.0004,  0.0045],
        [ 0.0051,  0.0117, -0.0134,  ...,  0.0114,  0.0027,  0.0155],
        [ 0.0063,  0.0140, -0.0166,  ...,  0.0144,  0.0035,  0.0194],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2189.3696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7456, device='cuda:0')



h[100].sum tensor(-9.9562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-38.6349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6095, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0223, 0.0000,  ..., 0.0155, 0.0036, 0.0220],
        [0.0172, 0.0413, 0.0000,  ..., 0.0379, 0.0089, 0.0526],
        [0.0294, 0.0630, 0.0000,  ..., 0.0674, 0.0168, 0.0898],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59158.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0591],
        [0.0958, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.0830],
        [0.1412, 0.0000, 0.0000,  ..., 0.1683, 0.0000, 0.1084],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492795.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2556.8762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-305.5828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(191.9656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.7975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0859],
        [ 0.0656],
        [ 0.0442],
        ...,
        [-0.5077],
        [-0.5061],
        [-0.5057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107805.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0033],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367504.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0070, -0.0069,  ...,  0.0049,  0.0010,  0.0074],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2157.1958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1434, device='cuda:0')



h[100].sum tensor(-10.0507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-38.9066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0183, 0.0000,  ..., 0.0099, 0.0021, 0.0150],
        [0.0025, 0.0133, 0.0000,  ..., 0.0049, 0.0010, 0.0074],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57397.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0436],
        [0.0148, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.0345],
        [0.0040, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480465.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2388.7908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-319.2234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(173.5017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.7445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.2519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0606],
        [-0.0164],
        [-0.1104],
        ...,
        [-0.5099],
        [-0.5083],
        [-0.5078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124262.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6985e-03,  7.4427e-03, -7.4444e-03,  ...,  5.5004e-03,
          1.1672e-03,  8.1447e-03],
        [ 1.0027e-03,  4.4229e-03, -3.2441e-03,  ...,  1.3862e-03,
          7.1195e-05,  2.9520e-03],
        [ 1.3887e-03,  5.1104e-03, -4.2003e-03,  ...,  2.3227e-03,
          3.2069e-04,  4.1340e-03],
        ...,
        [-3.0711e-04,  2.0906e-03,  0.0000e+00,  ..., -1.7915e-03,
         -7.7532e-04, -1.0587e-03],
        [-3.0711e-04,  2.0906e-03,  0.0000e+00,  ..., -1.7915e-03,
         -7.7532e-04, -1.0587e-03],
        [-3.0711e-04,  2.0906e-03,  0.0000e+00,  ..., -1.7915e-03,
         -7.7532e-04, -1.0587e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2107.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2344, device='cuda:0')



h[100].sum tensor(-10.6385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-39.3858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0172, 0.0000,  ..., 0.0067, 0.0010, 0.0120],
        [0.0097, 0.0279, 0.0000,  ..., 0.0194, 0.0040, 0.0293],
        [0.0044, 0.0172, 0.0000,  ..., 0.0085, 0.0017, 0.0131],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55613.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0262, 0.0000, 0.0000,  ..., 0.0307, 0.0000, 0.0444],
        [0.0358, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.0508],
        [0.0242, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.0429],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473348.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2319.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-326.5944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(157.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.0245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.3312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1267],
        [ 0.1514],
        [ 0.1426],
        ...,
        [-0.5099],
        [-0.5083],
        [-0.5078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131421.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0048, -0.0037,  ...,  0.0019,  0.0002,  0.0035],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2152.3953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8587, device='cuda:0')



h[100].sum tensor(-10.1073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0122, device='cuda:0')



h[200].sum tensor(-38.9527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.0009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0174, 0.0000,  ..., 0.0087, 0.0017, 0.0134],
        [0.0012, 0.0110, 0.0000,  ..., 0.0019, 0.0002, 0.0035],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57864.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0252, 0.0000, 0.0000,  ..., 0.0294, 0.0000, 0.0432],
        [0.0124, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0337],
        [0.0038, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0268],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483859.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2414.9324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.4829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(155.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.3537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.8915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1346],
        [ 0.0734],
        [-0.0236],
        ...,
        [-0.4998],
        [-0.5044],
        [-0.5057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120340.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367516.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367528.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0046, -0.0035,  ...,  0.0017,  0.0001,  0.0033],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2162.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0857, device='cuda:0')



h[100].sum tensor(-9.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-38.8418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.1567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0147, 0.0000,  ..., 0.0032, 0.0001, 0.0077],
        [0.0020, 0.0130, 0.0000,  ..., 0.0027, 0.0001, 0.0058],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60036.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0394],
        [0.0108, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0345],
        [0.0030, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502230.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2469.2134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.8346, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(333.3243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.6296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1431],
        [ 0.0765],
        [-0.0449],
        ...,
        [-0.5115],
        [-0.5099],
        [-0.5094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118541.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0012],
        [1.0034],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367528.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0035],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367540.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0134, -0.0156,  ...,  0.0135,  0.0033,  0.0183],
        [ 0.0099,  0.0202, -0.0251,  ...,  0.0229,  0.0058,  0.0301],
        [ 0.0070,  0.0150, -0.0178,  ...,  0.0157,  0.0039,  0.0210],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1999.8446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7726, device='cuda:0')



h[100].sum tensor(-11.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-40.3760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.5104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0571, 0.0000,  ..., 0.0590, 0.0146, 0.0793],
        [0.0340, 0.0708, 0.0000,  ..., 0.0776, 0.0195, 0.1028],
        [0.0361, 0.0745, 0.0000,  ..., 0.0827, 0.0209, 0.1092],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52717.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1180, 0.0000, 0.0000,  ..., 0.1447, 0.0000, 0.0953],
        [0.1497, 0.0000, 0.0000,  ..., 0.1840, 0.0000, 0.1136],
        [0.1544, 0.0000, 0.0000,  ..., 0.1900, 0.0000, 0.1171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458912.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2023.4646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-343.9315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(98.5436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.6838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1279],
        [ 0.1235],
        [ 0.1294],
        ...,
        [-0.5166],
        [-0.5149],
        [-0.5145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137097.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0035],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367540.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0013],
        [1.0036],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367552.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0075, -0.0074,  ...,  0.0055,  0.0012,  0.0081],
        [ 0.0012,  0.0047, -0.0036,  ...,  0.0017,  0.0002,  0.0034],
        [ 0.0013,  0.0049, -0.0038,  ...,  0.0020,  0.0002,  0.0037],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2221.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3639, device='cuda:0')



h[100].sum tensor(-8.4917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-38.3145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.7201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0173, 0.0000,  ..., 0.0066, 0.0009, 0.0119],
        [0.0094, 0.0270, 0.0000,  ..., 0.0180, 0.0037, 0.0275],
        [0.0045, 0.0173, 0.0000,  ..., 0.0084, 0.0017, 0.0130],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61483.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0228, 0.0000, 0.0000,  ..., 0.0285, 0.0000, 0.0435],
        [0.0312, 0.0000, 0.0000,  ..., 0.0392, 0.0000, 0.0490],
        [0.0208, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0412],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505381.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2333.4326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.3485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(320.3102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.7655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1536],
        [ 0.1453],
        [ 0.0552],
        ...,
        [-0.5186],
        [-0.5170],
        [-0.5165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112027.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0013],
        [1.0036],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367552.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0037],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367563.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0108, -0.0120,  ...,  0.0100,  0.0024,  0.0138],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2277.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9860, device='cuda:0')



h[100].sum tensor(-8.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-37.9105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.8333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0404, 0.0000,  ..., 0.0380, 0.0093, 0.0517],
        [0.0114, 0.0301, 0.0000,  ..., 0.0240, 0.0056, 0.0340],
        [0.0065, 0.0214, 0.0000,  ..., 0.0121, 0.0024, 0.0190],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62709.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0964, 0.0000, 0.0000,  ..., 0.1200, 0.0000, 0.0853],
        [0.0796, 0.0000, 0.0000,  ..., 0.0992, 0.0000, 0.0761],
        [0.0630, 0.0000, 0.0000,  ..., 0.0787, 0.0000, 0.0674],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510121.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2479.1392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-302.1341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(276.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.3949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.2819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1447],
        [ 0.1518],
        [ 0.1600],
        ...,
        [-0.5221],
        [-0.5204],
        [-0.5200]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105760.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0037],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367563.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(458.4992, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0014],
        [1.0038],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367574.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0018, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2232.5962, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3362, device='cuda:0')



h[100].sum tensor(-9.4854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-38.4440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.7011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0013, 0.0112, 0.0000,  ..., 0.0020, 0.0003, 0.0038],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60868.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0234],
        [0.0017, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0257],
        [0.0090, 0.0000, 0.0000,  ..., 0.0099, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504472.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2620.9304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-310.1874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(346.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.1615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.9419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2223],
        [-0.0765],
        [ 0.0608],
        ...,
        [-0.5287],
        [-0.5271],
        [-0.5267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126069.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0014],
        [1.0038],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367574.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0039],
        ...,
        [1.0016],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367584.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0054, -0.0045,  ...,  0.0026,  0.0004,  0.0046],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [ 0.0016,  0.0054, -0.0045,  ...,  0.0026,  0.0004,  0.0046],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2886.0962, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.7830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.3233, device='cuda:0')



h[100].sum tensor(-2.7134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0253, device='cuda:0')



h[200].sum tensor(-32.3663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-31.1038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0033, 0.0152, 0.0000,  ..., 0.0056, 0.0010, 0.0096],
        [0.0068, 0.0225, 0.0000,  ..., 0.0117, 0.0020, 0.0199],
        [0.0012, 0.0111, 0.0000,  ..., 0.0018, 0.0002, 0.0036],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88824.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0218, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0428],
        [0.0253, 0.0000, 0.0000,  ..., 0.0309, 0.0000, 0.0456],
        [0.0143, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0369],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678965.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4148.3892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-199.9773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2151.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.8416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-275.5577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1546],
        [ 0.1525],
        [ 0.1197],
        ...,
        [-0.5354],
        [-0.5338],
        [-0.5334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135795.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0039],
        ...,
        [1.0016],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367584.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0039],
        ...,
        [1.0016],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367584.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0047, -0.0035,  ...,  0.0016,  0.0002,  0.0033],
        [ 0.0012,  0.0047, -0.0035,  ...,  0.0016,  0.0002,  0.0033],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2269.2915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9785, device='cuda:0')



h[100].sum tensor(-9.8435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0140, device='cuda:0')



h[200].sum tensor(-38.1809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.1419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0166, 0.0000,  ..., 0.0055, 0.0008, 0.0109],
        [0.0028, 0.0148, 0.0000,  ..., 0.0031, 0.0002, 0.0079],
        [0.0021, 0.0131, 0.0000,  ..., 0.0026, 0.0002, 0.0059],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61011.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0228, 0.0000, 0.0000,  ..., 0.0281, 0.0000, 0.0453],
        [0.0186, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0422],
        [0.0131, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0374],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503746.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2723.7056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.2172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(278.2545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.9649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.1101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1748],
        [ 0.1573],
        [ 0.1102],
        ...,
        [-0.5349],
        [-0.5332],
        [-0.5327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120172.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0039],
        ...,
        [1.0016],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367584.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0040],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367595.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0010],
        [ 0.0035,  0.0087, -0.0092,  ...,  0.0072,  0.0016,  0.0104],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0010],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0010],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2160.7212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6049, device='cuda:0')



h[100].sum tensor(-11.7143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-39.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0224, 0.0000,  ..., 0.0115, 0.0020, 0.0199],
        [0.0025, 0.0138, 0.0000,  ..., 0.0036, 0.0004, 0.0073],
        [0.0084, 0.0253, 0.0000,  ..., 0.0155, 0.0031, 0.0249],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57171.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0294, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0488],
        [0.0261, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0463],
        [0.0358, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485604.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2689.9875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.6666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(141.8452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.6888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.7001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1205],
        [ 0.1213],
        [ 0.1210],
        ...,
        [-0.5374],
        [-0.5355],
        [-0.5344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125448.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0015],
        [1.0040],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367595.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0041],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367607.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0082, -0.0084,  ...,  0.0064,  0.0014,  0.0094],
        [ 0.0027,  0.0074, -0.0074,  ...,  0.0054,  0.0012,  0.0082],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2295.0383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9394, device='cuda:0')



h[100].sum tensor(-10.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-38.2102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.1151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0126, 0.0328, 0.0000,  ..., 0.0258, 0.0058, 0.0379],
        [0.0053, 0.0188, 0.0000,  ..., 0.0105, 0.0023, 0.0159],
        [0.0027, 0.0137, 0.0000,  ..., 0.0054, 0.0012, 0.0082],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59887.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0437, 0.0000, 0.0000,  ..., 0.0529, 0.0000, 0.0548],
        [0.0283, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0445],
        [0.0155, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491736.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2841.1060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(259.4739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.5792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.4831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0034],
        [-0.0365],
        [-0.0885],
        ...,
        [-0.5379],
        [-0.5364],
        [-0.5360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134460.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0016],
        [1.0041],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367607.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0041],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367618.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0045, -0.0033,  ...,  0.0014,  0.0001,  0.0031],
        [ 0.0030,  0.0080, -0.0081,  ...,  0.0062,  0.0014,  0.0091],
        [ 0.0015,  0.0053, -0.0044,  ...,  0.0025,  0.0004,  0.0044],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0008, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2150.0898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1680, device='cuda:0')



h[100].sum tensor(-12.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-39.7028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1544, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0254, 0.0000,  ..., 0.0157, 0.0032, 0.0252],
        [0.0047, 0.0187, 0.0000,  ..., 0.0067, 0.0008, 0.0138],
        [0.0086, 0.0257, 0.0000,  ..., 0.0161, 0.0034, 0.0257],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56019.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.0379, 0.0000, 0.0496],
        [0.0310, 0.0000, 0.0000,  ..., 0.0391, 0.0000, 0.0513],
        [0.0398, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0570],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0246],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480085.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2763.1226, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.3179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(120.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.9208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.7382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0702],
        [ 0.0089],
        [ 0.0576],
        ...,
        [-0.3741],
        [-0.4533],
        [-0.5047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116214.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0041],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367618.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0042],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367630.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2086.8970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7343, device='cuda:0')



h[100].sum tensor(-13.1773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-40.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.4842, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54002.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0247],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470460.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2665.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-323.9082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44.2435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.7430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.8024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5003],
        [-0.3983],
        [-0.2591],
        ...,
        [-0.5298],
        [-0.5253],
        [-0.5218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114517.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0042],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367630.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0043],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367642.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2151.1062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7350, device='cuda:0')



h[100].sum tensor(-12.2212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-39.8668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.8572, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55704.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477500.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2650.3425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-319.0001, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(174.9983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.8255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4781],
        [-0.5510],
        [-0.6076],
        ...,
        [-0.5342],
        [-0.5328],
        [-0.5325]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114816.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0017],
        [1.0043],
        ...,
        [1.0016],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367642.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0018],
        [1.0043],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367653.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [ 0.0029,  0.0077, -0.0077,  ...,  0.0058,  0.0014,  0.0086],
        [ 0.0045,  0.0105, -0.0115,  ...,  0.0096,  0.0024,  0.0134],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2087.6545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6787, device='cuda:0')



h[100].sum tensor(-12.7116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-40.4739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.4460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0241, 0.0000,  ..., 0.0141, 0.0030, 0.0228],
        [0.0068, 0.0213, 0.0000,  ..., 0.0140, 0.0034, 0.0202],
        [0.0065, 0.0208, 0.0000,  ..., 0.0133, 0.0032, 0.0193],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53624.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0279, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.0490],
        [0.0340, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0522],
        [0.0388, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0246],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0246]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(468519., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2486.3647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-329.6825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.7800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.9042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1964],
        [ 0.2212],
        [ 0.2321],
        ...,
        [-0.5397],
        [-0.5381],
        [-0.5378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121015.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0018],
        [1.0043],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367653.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0044],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367665.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2194.8501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1820, device='cuda:0')



h[100].sum tensor(-11.4238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-39.4789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59349.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.2051e-04, 0.0000e+00, 0.0000e+00,  ..., 4.7725e-05, 0.0000e+00,
         2.4979e-02],
        [1.4473e-03, 0.0000e+00, 0.0000e+00,  ..., 1.7259e-03, 0.0000e+00,
         2.7199e-02],
        [2.2676e-03, 0.0000e+00, 0.0000e+00,  ..., 2.8594e-03, 0.0000e+00,
         2.8442e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4720e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4725e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4730e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505649.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2691.5259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.2840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(351.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.3669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.9163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0510],
        [-0.0418],
        [-0.0026],
        ...,
        [-0.5456],
        [-0.5440],
        [-0.5436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118435., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0018],
        [1.0044],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367665.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(483.7585, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0019],
        [1.0045],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367676.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0099, -0.0107,  ...,  0.0088,  0.0022,  0.0123],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2206.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5302, device='cuda:0')



h[100].sum tensor(-11.3297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-39.3852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.0892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0328, 0.0000,  ..., 0.0295, 0.0076, 0.0397],
        [0.0075, 0.0226, 0.0000,  ..., 0.0157, 0.0039, 0.0222],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58864.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0000, 0.0000,  ..., 0.0775, 0.0000, 0.0649],
        [0.0390, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.0531],
        [0.0180, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501739.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2588.0229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-309.9870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(229.9476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.8647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.1029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1569],
        [ 0.1601],
        [ 0.1674],
        ...,
        [-0.5542],
        [-0.5526],
        [-0.5522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116227.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0019],
        [1.0045],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367676.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0046],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367687.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0110, -0.0122,  ...,  0.0102,  0.0026,  0.0142],
        [ 0.0020,  0.0061, -0.0055,  ...,  0.0036,  0.0008,  0.0057],
        [ 0.0026,  0.0073, -0.0070,  ...,  0.0051,  0.0012,  0.0077],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2378.5674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5850, device='cuda:0')



h[100].sum tensor(-9.6846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-37.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0341, 0.0000,  ..., 0.0274, 0.0067, 0.0395],
        [0.0134, 0.0342, 0.0000,  ..., 0.0275, 0.0067, 0.0397],
        [0.0041, 0.0167, 0.0000,  ..., 0.0074, 0.0016, 0.0118],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64571.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.8862e-02, 0.0000e+00, 0.0000e+00,  ..., 1.1909e-01, 0.0000e+00,
         8.8867e-02],
        [7.0188e-02, 0.0000e+00, 0.0000e+00,  ..., 9.4853e-02, 0.0000e+00,
         7.7321e-02],
        [4.3562e-02, 0.0000e+00, 0.0000e+00,  ..., 5.9718e-02, 0.0000e+00,
         5.9646e-02],
        ...,
        [4.1774e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.5663e-02],
        [5.8061e-04, 0.0000e+00, 0.0000e+00,  ..., 5.3225e-04, 0.0000e+00,
         2.7415e-02],
        [1.0785e-03, 0.0000e+00, 0.0000e+00,  ..., 1.0649e-03, 0.0000e+00,
         2.8295e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528523.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2772.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.0374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(350.4104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.2347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.7347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1998],
        [ 0.2018],
        [ 0.2029],
        ...,
        [-0.4961],
        [-0.4420],
        [-0.4058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117581.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0019],
        [1.0046],
        ...,
        [1.0016],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367687.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367698.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2225.1978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3553, device='cuda:0')



h[100].sum tensor(-12.2312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-39.2326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6554, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57889.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0584e-02, 0.0000e+00, 0.0000e+00,  ..., 1.4054e-02, 0.0000e+00,
         3.3856e-02],
        [1.3809e-03, 0.0000e+00, 0.0000e+00,  ..., 2.1518e-03, 0.0000e+00,
         2.7161e-02],
        [9.5023e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4734e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4530e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4535e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4540e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491922., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2512.5425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.3089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(251.8685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.7574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.0947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0642],
        [-0.0338],
        [-0.1515],
        ...,
        [-0.5746],
        [-0.5730],
        [-0.5725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144689.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367698.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367698.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023,  0.0068, -0.0064,  ...,  0.0045,  0.0010,  0.0069],
        [ 0.0016,  0.0056, -0.0047,  ...,  0.0027,  0.0005,  0.0048],
        [ 0.0010,  0.0045, -0.0033,  ...,  0.0013,  0.0002,  0.0030],
        ...,
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012],
        [-0.0003,  0.0021,  0.0000,  ..., -0.0019, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2143.5244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7061, device='cuda:0')



h[100].sum tensor(-13.1474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-39.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.8374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0080, 0.0248, 0.0000,  ..., 0.0145, 0.0031, 0.0235],
        [0.0055, 0.0204, 0.0000,  ..., 0.0084, 0.0015, 0.0158],
        [0.0041, 0.0181, 0.0000,  ..., 0.0052, 0.0007, 0.0118],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55108.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0310, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0525],
        [0.0302, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0536],
        [0.0270, 0.0000, 0.0000,  ..., 0.0378, 0.0000, 0.0520],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480307.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2403.6511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-321.1211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(124.6770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.0847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.2294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1731],
        [ 0.1925],
        [ 0.1968],
        ...,
        [-0.5746],
        [-0.5730],
        [-0.5725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137114.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0020],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367698.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367709.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0003,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [ 0.0035,  0.0089, -0.0092,  ...,  0.0072,  0.0017,  0.0105],
        ...,
        [-0.0003,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0003,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0003,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2472.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9711, device='cuda:0')



h[100].sum tensor(-10.2710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-37.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.8819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0154, 0.0000,  ..., 0.0073, 0.0017, 0.0105],
        [0.0120, 0.0310, 0.0000,  ..., 0.0266, 0.0067, 0.0364],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67761.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0305],
        [0.0334, 0.0000, 0.0000,  ..., 0.0423, 0.0000, 0.0476],
        [0.0791, 0.0000, 0.0000,  ..., 0.0993, 0.0000, 0.0765],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553700.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2998.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-268.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(730.5018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.1476, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.3468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0594],
        [ 0.0917],
        [ 0.1099],
        ...,
        [-0.5806],
        [-0.5790],
        [-0.5785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136529.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0047],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367709.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0048],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367721.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2241.3303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5944, device='cuda:0')



h[100].sum tensor(-13.4557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-39.2590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.8195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58500.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503327.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2619.2986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-301.7801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115.0883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.3783, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6321],
        [-0.5505],
        [-0.4293],
        ...,
        [-0.5827],
        [-0.5807],
        [-0.5799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122890.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0048],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367721.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0049],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367732.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0101, -0.0109,  ...,  0.0089,  0.0022,  0.0126],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2064.5010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1193, device='cuda:0')



h[100].sum tensor(-15.5594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0084, device='cuda:0')



h[200].sum tensor(-40.9610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.3758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0117, 0.0311, 0.0000,  ..., 0.0247, 0.0059, 0.0353],
        [0.0075, 0.0232, 0.0000,  ..., 0.0158, 0.0038, 0.0227],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52377.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0000, 0.0000,  ..., 0.0745, 0.0000, 0.0668],
        [0.0381, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0515],
        [0.0110, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0325],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470274.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2363.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-329.3577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70.4176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.7292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.9748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1092],
        [ 0.0679],
        [-0.0534],
        ...,
        [-0.5844],
        [-0.5827],
        [-0.5823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143822.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0049],
        ...,
        [1.0017],
        [1.0008],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367732.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0050],
        ...,
        [1.0017],
        [1.0009],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367744.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [ 0.0012,  0.0048, -0.0037,  ...,  0.0017,  0.0002,  0.0035],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2968.2095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.0264, device='cuda:0')



h[100].sum tensor(-5.4085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0252, device='cuda:0')



h[200].sum tensor(-32.9110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-30.9001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.5468e-03, 2.6183e-02, 0.0000e+00,  ..., 1.6021e-02, 3.4009e-03,
         2.5578e-02],
        [8.8932e-04, 1.0784e-02, 0.0000e+00,  ..., 1.0135e-03, 4.9070e-05,
         2.6028e-03],
        [8.5716e-03, 2.5613e-02, 0.0000e+00,  ..., 1.7190e-02, 3.9342e-03,
         2.5753e-02],
        ...,
        [0.0000e+00, 8.7752e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7766e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7780e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82919.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0558, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0661],
        [0.0495, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0610],
        [0.0729, 0.0000, 0.0000,  ..., 0.0895, 0.0000, 0.0754],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627653.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3699.9065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-207.4671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1217.8733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.0021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-262.2205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0386],
        [ 0.0268],
        [ 0.0151],
        ...,
        [-0.5806],
        [-0.5792],
        [-0.5790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114806.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0050],
        ...,
        [1.0017],
        [1.0009],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367744.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0051],
        ...,
        [1.0018],
        [1.0009],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367755.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0124, -0.0140,  ...,  0.0120,  0.0030,  0.0165],
        [ 0.0060,  0.0133, -0.0151,  ...,  0.0132,  0.0033,  0.0180],
        [ 0.0055,  0.0125, -0.0141,  ...,  0.0122,  0.0030,  0.0167],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2406.8511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1289, device='cuda:0')



h[100].sum tensor(-11.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-38.0974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.9313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0476, 0.0000,  ..., 0.0452, 0.0112, 0.0624],
        [0.0241, 0.0535, 0.0000,  ..., 0.0533, 0.0134, 0.0727],
        [0.0252, 0.0554, 0.0000,  ..., 0.0558, 0.0141, 0.0759],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64914.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1326, 0.0000, 0.0000,  ..., 0.1625, 0.0000, 0.1097],
        [0.1357, 0.0000, 0.0000,  ..., 0.1663, 0.0000, 0.1114],
        [0.1288, 0.0000, 0.0000,  ..., 0.1578, 0.0000, 0.1077],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537523.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2935.2649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.6035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(386.0419, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.7877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.6087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0872],
        [ 0.0916],
        [ 0.0997],
        ...,
        [-0.5783],
        [-0.5767],
        [-0.5763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100824.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0051],
        ...,
        [1.0018],
        [1.0009],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367755.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0052],
        ...,
        [1.0018],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367766.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0139, -0.0160,  ...,  0.0141,  0.0036,  0.0191],
        [ 0.0043,  0.0104, -0.0111,  ...,  0.0092,  0.0023,  0.0129],
        [ 0.0012,  0.0049, -0.0037,  ...,  0.0018,  0.0003,  0.0035],
        ...,
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [-0.0004,  0.0021,  0.0000,  ..., -0.0020, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2325.3325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5120, device='cuda:0')



h[100].sum tensor(-12.3529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-38.8882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0512, 0.0000,  ..., 0.0502, 0.0126, 0.0687],
        [0.0121, 0.0324, 0.0000,  ..., 0.0246, 0.0057, 0.0362],
        [0.0102, 0.0291, 0.0000,  ..., 0.0199, 0.0045, 0.0303],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61124.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1049, 0.0000, 0.0000,  ..., 0.1298, 0.0000, 0.0943],
        [0.0772, 0.0000, 0.0000,  ..., 0.0962, 0.0000, 0.0795],
        [0.0602, 0.0000, 0.0000,  ..., 0.0755, 0.0000, 0.0705],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515959.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2702.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-294.3688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(206.5011, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.6720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.3415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1911],
        [ 0.2049],
        [ 0.2182],
        ...,
        [-0.5806],
        [-0.5790],
        [-0.5786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103578.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0052],
        ...,
        [1.0018],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367766.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(499.4043, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0053],
        ...,
        [1.0018],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367777.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [ 0.0014,  0.0053, -0.0043,  ...,  0.0023,  0.0004,  0.0042],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2243.4336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9121, device='cuda:0')



h[100].sum tensor(-13.6993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-39.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.3513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0118, 0.0000,  ..., 0.0023, 0.0004, 0.0042],
        [0.0029, 0.0151, 0.0000,  ..., 0.0048, 0.0009, 0.0086],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60102.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0287],
        [0.0074, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0334],
        [0.0156, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0408],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516713.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2641.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-303.9759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(300.5289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.4201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.9572, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0091],
        [ 0.1098],
        [ 0.1694],
        ...,
        [-0.5885],
        [-0.5869],
        [-0.5865]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115750.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0053],
        ...,
        [1.0018],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367777.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0054],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367788.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6738e-03,  1.1049e-02, -1.1995e-02,  ...,  1.0077e-02,
          2.4902e-03,  1.4068e-02],
        [ 4.3064e-03,  1.0403e-02, -1.1122e-02,  ...,  9.1973e-03,
          2.2544e-03,  1.2953e-02],
        [ 8.1122e-04,  4.2603e-03, -2.8186e-03,  ...,  8.2325e-04,
          1.0892e-05,  2.3410e-03],
        ...,
        [-3.7527e-04,  2.1751e-03,  0.0000e+00,  ..., -2.0194e-03,
         -7.5070e-04, -1.2613e-03],
        [-3.7527e-04,  2.1751e-03,  0.0000e+00,  ..., -2.0194e-03,
         -7.5070e-04, -1.2613e-03],
        [-3.7527e-04,  2.1751e-03,  0.0000e+00,  ..., -2.0194e-03,
         -7.5070e-04, -1.2613e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2383.2708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5068, device='cuda:0')



h[100].sum tensor(-12.8294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-38.3351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.5045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0213, 0.0488, 0.0000,  ..., 0.0465, 0.0116, 0.0642],
        [0.0152, 0.0382, 0.0000,  ..., 0.0320, 0.0077, 0.0458],
        [0.0198, 0.0461, 0.0000,  ..., 0.0429, 0.0106, 0.0595],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62523.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1123, 0.0000, 0.0000,  ..., 0.1391, 0.0000, 0.0980],
        [0.1038, 0.0000, 0.0000,  ..., 0.1288, 0.0000, 0.0936],
        [0.1045, 0.0000, 0.0000,  ..., 0.1296, 0.0000, 0.0941],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522347.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2647.3540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.6059, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(303.8549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.1512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.6665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1249],
        [ 0.1234],
        [ 0.1254],
        ...,
        [-0.5971],
        [-0.5954],
        [-0.5949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124238.7891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0054],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367788.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0055],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367798.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0013],
        [ 0.0015,  0.0055, -0.0045,  ...,  0.0025,  0.0005,  0.0045],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0013],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0020, -0.0008, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2308.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2942, device='cuda:0')



h[100].sum tensor(-14.4230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-38.9906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0121, 0.0000,  ..., 0.0025, 0.0005, 0.0045],
        [0.0012, 0.0115, 0.0000,  ..., 0.0017, 0.0002, 0.0035],
        [0.0064, 0.0228, 0.0000,  ..., 0.0110, 0.0021, 0.0192],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62169.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0079, 0.0000, 0.0318],
        [0.0106, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0365],
        [0.0211, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0455],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529864.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2657.0278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.9152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(453.1776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.3173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.0350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0012],
        [ 0.1050],
        [ 0.1609],
        ...,
        [-0.6085],
        [-0.6068],
        [-0.6064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134365.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0055],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367798.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0056],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367809.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2376.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2697, device='cuda:0')



h[100].sum tensor(-14.6404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0141, device='cuda:0')



h[200].sum tensor(-38.3998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.3417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60893.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511391.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2463.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-310.2294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(263.2527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.0493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.4459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4372],
        [-0.5143],
        [-0.5369],
        ...,
        [-0.6150],
        [-0.6135],
        [-0.6134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142212.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0056],
        ...,
        [1.0019],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367809.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0057],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367820.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [ 0.0011,  0.0048, -0.0035,  ...,  0.0015,  0.0002,  0.0033],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2233.8872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3250, device='cuda:0')



h[100].sum tensor(-17.1228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-39.7181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.9484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0115, 0.0000,  ..., 0.0015, 0.0002, 0.0033],
        [0.0042, 0.0177, 0.0000,  ..., 0.0079, 0.0017, 0.0127],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58704.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0200, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0407],
        [0.0242, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0448],
        [0.0321, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.0513],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0236],
        [0.0008, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0260],
        [0.0063, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510250.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2423.0005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-317.4062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(215.9803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.0837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.4120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0736],
        [ 0.0885],
        [ 0.1054],
        ...,
        [-0.4935],
        [-0.3322],
        [-0.1330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0057],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367820.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0058],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367831.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0050, -0.0037,  ...,  0.0017,  0.0003,  0.0036],
        [ 0.0011,  0.0048, -0.0035,  ...,  0.0015,  0.0002,  0.0033],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0008, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2294.1265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7171, device='cuda:0')



h[100].sum tensor(-17.3606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-39.3111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.9037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0234, 0.0000,  ..., 0.0116, 0.0023, 0.0203],
        [0.0033, 0.0160, 0.0000,  ..., 0.0057, 0.0011, 0.0100],
        [0.0011, 0.0114, 0.0000,  ..., 0.0015, 0.0002, 0.0033],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59841.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0240, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.0475],
        [0.0157, 0.0000, 0.0000,  ..., 0.0220, 0.0000, 0.0408],
        [0.0069, 0.0000, 0.0000,  ..., 0.0096, 0.0000, 0.0329],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516204.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2498.1550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-313.9163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(302.9031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.2982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.5503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1565],
        [ 0.1114],
        [ 0.0063],
        ...,
        [-0.6186],
        [-0.6171],
        [-0.6170]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143096.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0058],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367831.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0059],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367843.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2260.8589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3853, device='cuda:0')



h[100].sum tensor(-18.4686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-39.7228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.9898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58479.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0274],
        [0.0004, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0268],
        [0.0002, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508416.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2413.8970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-318.5879, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(169.1425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.9847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.3272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0032],
        [-0.0257],
        [-0.0123],
        ...,
        [-0.6208],
        [-0.6191],
        [-0.6187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134314.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0059],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367843.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0027],
        [1.0060],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367855., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2676.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9960, device='cuda:0')



h[100].sum tensor(-14.6314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0190, device='cuda:0')



h[200].sum tensor(-36.2270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.3303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72733.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0232],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588951.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3035.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-263.0844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(966.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(161.3815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.9523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6237],
        [-0.6963],
        [-0.7452],
        ...,
        [-0.6178],
        [-0.6165],
        [-0.6164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129460.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0027],
        [1.0060],
        ...,
        [1.0019],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367855., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0027],
        [1.0061],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367866.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0053, -0.0042,  ...,  0.0022,  0.0004,  0.0041],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2255.3589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4042, device='cuda:0')



h[100].sum tensor(-19.7453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-40.0870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.3165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0028, 0.0152, 0.0000,  ..., 0.0048, 0.0009, 0.0088],
        [0.0013, 0.0118, 0.0000,  ..., 0.0022, 0.0004, 0.0041],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57097.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0410],
        [0.0073, 0.0000, 0.0000,  ..., 0.0108, 0.0000, 0.0325],
        [0.0007, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0259],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500561.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2291.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-324.6915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(160.3841, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.4892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.4215, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0442],
        [-0.1520],
        [-0.3669],
        ...,
        [-0.6162],
        [-0.6145],
        [-0.6141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125405.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0027],
        [1.0061],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367866.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0028],
        [1.0062],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367877.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [ 0.0028,  0.0078, -0.0076,  ...,  0.0057,  0.0014,  0.0086],
        [ 0.0024,  0.0071, -0.0066,  ...,  0.0047,  0.0011,  0.0073],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2345.7383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9524, device='cuda:0')



h[100].sum tensor(-19.4232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-39.4328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.0652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0028, 0.0143, 0.0000,  ..., 0.0057, 0.0014, 0.0086],
        [0.0046, 0.0182, 0.0000,  ..., 0.0090, 0.0021, 0.0141],
        [0.0154, 0.0388, 0.0000,  ..., 0.0329, 0.0082, 0.0472],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58687.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0360],
        [0.0282, 0.0000, 0.0000,  ..., 0.0389, 0.0000, 0.0464],
        [0.0581, 0.0000, 0.0000,  ..., 0.0772, 0.0000, 0.0651],
        ...,
        [0.0046, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0307],
        [0.0031, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0291],
        [0.0006, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505220.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2335.7612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-318.9578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(179.5108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.5448, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.6171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1334],
        [ 0.1760],
        [ 0.2028],
        ...,
        [-0.1845],
        [-0.2719],
        [-0.4043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124048., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0028],
        [1.0062],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367877.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(526.9920, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0028],
        [1.0062],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367888.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2128.7690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7580, device='cuda:0')



h[100].sum tensor(-22.3780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0082, device='cuda:0')



h[200].sum tensor(-41.4403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53456.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.0898e-05, 0.0000e+00,
         2.4698e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5291e-04, 0.0000e+00,
         2.4633e-02],
        [8.9409e-04, 0.0000e+00, 0.0000e+00,  ..., 2.3776e-03, 0.0000e+00,
         2.5952e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4245e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4251e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.4255e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486967.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.4758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-339.6578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37.8923, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.0369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.5286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0652],
        [-0.0923],
        [-0.0591],
        ...,
        [-0.6242],
        [-0.6225],
        [-0.6220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124909.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0028],
        [1.0062],
        ...,
        [1.0019],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367888.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0029],
        [1.0063],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367899.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0011,  0.0048, -0.0036,  ...,  0.0016,  0.0003,  0.0034],
        [ 0.0011,  0.0049, -0.0036,  ...,  0.0017,  0.0003,  0.0035],
        ...,
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0004,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2392.5559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1820, device='cuda:0')



h[100].sum tensor(-20.1016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-39.1941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0114, 0.0000,  ..., 0.0016, 0.0003, 0.0034],
        [0.0041, 0.0176, 0.0000,  ..., 0.0080, 0.0019, 0.0129],
        [0.0102, 0.0299, 0.0000,  ..., 0.0207, 0.0050, 0.0318],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61818.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0359],
        [0.0230, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0468],
        [0.0388, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0585],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526691.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2444.7104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-307.8377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(290.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.2088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.2003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1451],
        [ 0.2021],
        [ 0.2341],
        ...,
        [-0.6354],
        [-0.6337],
        [-0.6332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128320.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0029],
        [1.0063],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367899.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0029],
        [1.0064],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367910.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2423.1614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3046, device='cuda:0')



h[100].sum tensor(-20.3374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-38.9799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.6794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63344.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0238],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0245]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544170.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2528.7349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-303.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(450.1281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.9108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.4905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4871],
        [-0.6318],
        [-0.7273],
        ...,
        [-0.6467],
        [-0.6450],
        [-0.6445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139203.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0029],
        [1.0064],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367910.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0030],
        [1.0065],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367921.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0025,  0.0075, -0.0070,  ...,  0.0051,  0.0012,  0.0079],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2345.0793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6113, device='cuda:0')



h[100].sum tensor(-21.7665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-39.6962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.8311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0142, 0.0000,  ..., 0.0051, 0.0012, 0.0079],
        [0.0043, 0.0173, 0.0000,  ..., 0.0095, 0.0024, 0.0134],
        [0.0210, 0.0490, 0.0000,  ..., 0.0467, 0.0121, 0.0648],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59298.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0530],
        [0.0586, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0667],
        [0.0990, 0.0000, 0.0000,  ..., 0.1338, 0.0000, 0.0921],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515466.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2237.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-318.9315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(162.7083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.6156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.0025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2243],
        [ 0.2226],
        [ 0.2227],
        ...,
        [-0.6576],
        [-0.6557],
        [-0.6552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144891.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0030],
        [1.0065],
        ...,
        [1.0020],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367921.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0030],
        [1.0065],
        ...,
        [1.0020],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367932.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138,  0.0273, -0.0334,  ...,  0.0323,  0.0086,  0.0424],
        [ 0.0052,  0.0122, -0.0133,  ...,  0.0116,  0.0030,  0.0161],
        [ 0.0025,  0.0075, -0.0070,  ...,  0.0051,  0.0012,  0.0079],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2414.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3204, device='cuda:0')



h[100].sum tensor(-21.7730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-39.2208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0222, 0.0512, 0.0000,  ..., 0.0497, 0.0129, 0.0686],
        [0.0288, 0.0628, 0.0000,  ..., 0.0656, 0.0172, 0.0889],
        [0.0159, 0.0403, 0.0000,  ..., 0.0346, 0.0088, 0.0496],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62636.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1343, 0.0000, 0.0000,  ..., 0.1786, 0.0000, 0.1135],
        [0.1367, 0.0000, 0.0000,  ..., 0.1819, 0.0000, 0.1155],
        [0.1005, 0.0000, 0.0000,  ..., 0.1357, 0.0000, 0.0960],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0249]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537631., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2432.7498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-305.4814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(229.5580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.8553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.2257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1975],
        [ 0.2029],
        [ 0.2117],
        ...,
        [-0.6635],
        [-0.6616],
        [-0.6610]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140301.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0030],
        [1.0065],
        ...,
        [1.0020],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367932.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0031],
        [1.0066],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0075,  0.0163, -0.0188,  ...,  0.0173,  0.0045,  0.0233],
        [ 0.0053,  0.0124, -0.0135,  ...,  0.0118,  0.0031,  0.0164],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0025,  0.0075, -0.0070,  ...,  0.0051,  0.0012,  0.0079]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2777.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7422, device='cuda:0')



h[100].sum tensor(-18.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0194, device='cuda:0')



h[200].sum tensor(-36.2643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.8424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0623, 0.0000,  ..., 0.0649, 0.0170, 0.0880],
        [0.0133, 0.0348, 0.0000,  ..., 0.0292, 0.0075, 0.0413],
        [0.0070, 0.0230, 0.0000,  ..., 0.0151, 0.0038, 0.0220],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0146, 0.0000,  ..., 0.0053, 0.0013, 0.0081],
        [0.0046, 0.0189, 0.0000,  ..., 0.0091, 0.0022, 0.0145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75420.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1375, 0.0000, 0.0000,  ..., 0.1816, 0.0000, 0.1146],
        [0.0815, 0.0000, 0.0000,  ..., 0.1096, 0.0000, 0.0810],
        [0.0398, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.0551],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0303],
        [0.0133, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0384],
        [0.0275, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0491]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615185.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3065.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-254.3801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(792.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.6169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.6809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1543],
        [ 0.1334],
        [ 0.0232],
        ...,
        [-0.1941],
        [-0.0509],
        [ 0.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132210.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0031],
        [1.0066],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367943.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367954.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2632.6321, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7265, device='cuda:0')



h[100].sum tensor(-20.4618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-37.7061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.4003, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66390.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0305],
        [0.0026, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0306],
        [0.0020, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547753.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2538.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-294.1139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(435.5120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.1264, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.8830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0349],
        [ 0.0706],
        [ 0.1020],
        ...,
        [-0.6696],
        [-0.6677],
        [-0.6671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145920.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367954.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367954.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0035,  0.0092, -0.0093,  ...,  0.0075,  0.0019,  0.0109],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2734.1714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9470, device='cuda:0')



h[100].sum tensor(-19.4045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0178, device='cuda:0')



h[200].sum tensor(-36.8382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0342, 0.0000,  ..., 0.0264, 0.0066, 0.0392],
        [0.0028, 0.0146, 0.0000,  ..., 0.0057, 0.0014, 0.0087],
        [0.0035, 0.0159, 0.0000,  ..., 0.0075, 0.0019, 0.0109],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73376.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0301, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0502],
        [0.0151, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0396],
        [0.0113, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0364],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600958.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2940.7725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-264.0687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(861.6400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.0230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.5240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1213],
        [-0.1979],
        [-0.3114],
        ...,
        [-0.6691],
        [-0.6671],
        [-0.6664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125412.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367954.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367966.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0052, -0.0040,  ...,  0.0020,  0.0004,  0.0040],
        [ 0.0029,  0.0081, -0.0079,  ...,  0.0060,  0.0015,  0.0091],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2258.5334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5551, device='cuda:0')



h[100].sum tensor(-24.5650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-41.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.0475, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0240, 0.0000,  ..., 0.0144, 0.0035, 0.0226],
        [0.0035, 0.0167, 0.0000,  ..., 0.0065, 0.0015, 0.0112],
        [0.0104, 0.0305, 0.0000,  ..., 0.0212, 0.0051, 0.0326],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55982.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0664],
        [0.0312, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0533],
        [0.0300, 0.0000, 0.0000,  ..., 0.0419, 0.0000, 0.0521],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0257]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504457.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2211.5386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.5089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109.3282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.0130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2144],
        [ 0.2094],
        [ 0.1876],
        ...,
        [-0.6677],
        [-0.6658],
        [-0.6653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146321.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0032],
        [1.0067],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367966.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0032],
        [1.0068],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367978.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2537.6978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5154, device='cuda:0')



h[100].sum tensor(-21.8304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0143, device='cuda:0')



h[200].sum tensor(-38.9258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.5104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66433.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0252],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563392.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2724.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-290.0233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(265.2052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(164.0292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.5272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8087],
        [-0.8035],
        [-0.7857],
        ...,
        [-0.6306],
        [-0.6516],
        [-0.6597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98615.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0032],
        [1.0068],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367978.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(490.0213, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0033],
        [1.0069],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367989.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0079, -0.0075,  ...,  0.0056,  0.0014,  0.0086],
        [ 0.0028,  0.0080, -0.0077,  ...,  0.0058,  0.0014,  0.0088],
        [ 0.0039,  0.0099, -0.0102,  ...,  0.0084,  0.0021,  0.0121],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2458.1917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3285, device='cuda:0')



h[100].sum tensor(-22.9469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-39.7182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0345, 0.0000,  ..., 0.0267, 0.0066, 0.0397],
        [0.0120, 0.0332, 0.0000,  ..., 0.0249, 0.0061, 0.0374],
        [0.0135, 0.0350, 0.0000,  ..., 0.0295, 0.0075, 0.0418],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61432.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0689],
        [0.0662, 0.0000, 0.0000,  ..., 0.0845, 0.0000, 0.0765],
        [0.0721, 0.0000, 0.0000,  ..., 0.0915, 0.0000, 0.0794],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529617.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2485.4775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-320.1433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(280.9188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.3007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.9609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2151],
        [ 0.2183],
        [ 0.2179],
        ...,
        [-0.6677],
        [-0.6659],
        [-0.6654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135917.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0033],
        [1.0069],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367989.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0033],
        [1.0070],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368000.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0115, -0.0123,  ...,  0.0106,  0.0027,  0.0150],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0021, -0.0007, -0.0013],
        ...,
        [ 0.0018,  0.0062, -0.0052,  ...,  0.0033,  0.0007,  0.0056],
        [ 0.0045,  0.0110, -0.0116,  ...,  0.0099,  0.0025,  0.0140],
        [ 0.0031,  0.0084, -0.0082,  ...,  0.0064,  0.0016,  0.0095]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2544.5911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6703, device='cuda:0')



h[100].sum tensor(-22.3320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0143, device='cuda:0')



h[200].sum tensor(-39.0608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.6167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0182, 0.0433, 0.0000,  ..., 0.0409, 0.0105, 0.0563],
        [0.0113, 0.0312, 0.0000,  ..., 0.0242, 0.0061, 0.0351],
        [0.0048, 0.0190, 0.0000,  ..., 0.0096, 0.0023, 0.0151],
        ...,
        [0.0084, 0.0256, 0.0000,  ..., 0.0182, 0.0046, 0.0261],
        [0.0145, 0.0380, 0.0000,  ..., 0.0309, 0.0077, 0.0453],
        [0.0174, 0.0431, 0.0000,  ..., 0.0379, 0.0096, 0.0542]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63819.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1245, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.1113],
        [0.0930, 0.0000, 0.0000,  ..., 0.1151, 0.0000, 0.0926],
        [0.0742, 0.0000, 0.0000,  ..., 0.0924, 0.0000, 0.0809],
        ...,
        [0.0440, 0.0000, 0.0000,  ..., 0.0558, 0.0000, 0.0626],
        [0.0643, 0.0000, 0.0000,  ..., 0.0805, 0.0000, 0.0772],
        [0.0665, 0.0000, 0.0000,  ..., 0.0830, 0.0000, 0.0778]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542340.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2566.6501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-313.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(390.3550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.4774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.7012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0754],
        [0.0866],
        [0.0949],
        ...,
        [0.1220],
        [0.1967],
        [0.1899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142291.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0033],
        [1.0070],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368000.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0070],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368011.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0147, -0.0164,  ...,  0.0149,  0.0039,  0.0204],
        [ 0.0072,  0.0156, -0.0178,  ...,  0.0163,  0.0042,  0.0221],
        [ 0.0066,  0.0146, -0.0163,  ...,  0.0148,  0.0038,  0.0202],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0013],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0013],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2408.2695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4329, device='cuda:0')



h[100].sum tensor(-24.1139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-40.3036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.7087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0529, 0.0000,  ..., 0.0517, 0.0133, 0.0716],
        [0.0238, 0.0541, 0.0000,  ..., 0.0533, 0.0137, 0.0737],
        [0.0219, 0.0506, 0.0000,  ..., 0.0486, 0.0124, 0.0676],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59961.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1104, 0.0000, 0.0000,  ..., 0.1337, 0.0000, 0.1027],
        [0.1061, 0.0000, 0.0000,  ..., 0.1287, 0.0000, 0.0996],
        [0.0930, 0.0000, 0.0000,  ..., 0.1130, 0.0000, 0.0914],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525475.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2434.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-328.1987, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(156.5425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.4324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.0945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1607],
        [ 0.1569],
        [ 0.1520],
        ...,
        [-0.6794],
        [-0.6776],
        [-0.6771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126929.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0070],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368011.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0034],
        [1.0071],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368022.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0055, -0.0042,  ...,  0.0022,  0.0004,  0.0043],
        [ 0.0024,  0.0073, -0.0066,  ...,  0.0047,  0.0011,  0.0075],
        [ 0.0056,  0.0129, -0.0141,  ...,  0.0124,  0.0032,  0.0173],
        ...,
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0022,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.7065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7951, device='cuda:0')



h[100].sum tensor(-23.6402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-39.7150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0205, 0.0000,  ..., 0.0094, 0.0021, 0.0164],
        [0.0132, 0.0355, 0.0000,  ..., 0.0277, 0.0068, 0.0411],
        [0.0172, 0.0426, 0.0000,  ..., 0.0374, 0.0094, 0.0535],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62786.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0622],
        [0.0699, 0.0000, 0.0000,  ..., 0.0852, 0.0000, 0.0820],
        [0.0918, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0956],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541365.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2530.9282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-319.8991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(291.7457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.8076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.2306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1977],
        [ 0.1923],
        [ 0.1809],
        ...,
        [-0.6888],
        [-0.6869],
        [-0.6862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135364.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0034],
        [1.0071],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368022.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0034],
        [1.0071],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368033.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2672e-03,  7.0955e-03, -6.3805e-03,  ...,  4.4652e-03,
          1.0445e-03,  7.1755e-03],
        [ 4.6541e-03,  1.1272e-02, -1.1890e-02,  ...,  1.0194e-02,
          2.5825e-03,  1.4448e-02],
        [ 7.8633e-04,  4.5044e-03, -2.9625e-03,  ...,  9.1133e-04,
          9.0324e-05,  2.6641e-03],
        ...,
        [-4.9714e-04,  2.2587e-03,  0.0000e+00,  ..., -2.1689e-03,
         -7.3666e-04, -1.2461e-03],
        [-4.9714e-04,  2.2587e-03,  0.0000e+00,  ..., -2.1689e-03,
         -7.3666e-04, -1.2461e-03],
        [-4.9714e-04,  2.2587e-03,  0.0000e+00,  ..., -2.1689e-03,
         -7.3666e-04, -1.2461e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2414.0215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4176, device='cuda:0')



h[100].sum tensor(-24.7599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-40.3941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6982, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0190, 0.0459, 0.0000,  ..., 0.0418, 0.0106, 0.0591],
        [0.0095, 0.0291, 0.0000,  ..., 0.0188, 0.0044, 0.0299],
        [0.0088, 0.0279, 0.0000,  ..., 0.0172, 0.0041, 0.0278],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60435.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0754, 0.0000, 0.0000,  ..., 0.0911, 0.0000, 0.0855],
        [0.0609, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.0780],
        [0.0496, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0716],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532268., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2423.6833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-333.9284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(288.3624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.2571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.5803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1925],
        [ 0.1942],
        [ 0.1982],
        ...,
        [-0.6988],
        [-0.6969],
        [-0.6963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148220.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0034],
        [1.0071],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368033.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0072],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368044.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0142, -0.0157,  ...,  0.0142,  0.0036,  0.0195],
        [ 0.0065,  0.0146, -0.0162,  ...,  0.0147,  0.0038,  0.0202],
        [ 0.0027,  0.0078, -0.0073,  ...,  0.0054,  0.0013,  0.0084],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2478.1833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3986, device='cuda:0')



h[100].sum tensor(-24.5479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-39.9868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0189, 0.0456, 0.0000,  ..., 0.0414, 0.0105, 0.0586],
        [0.0189, 0.0457, 0.0000,  ..., 0.0416, 0.0105, 0.0588],
        [0.0180, 0.0442, 0.0000,  ..., 0.0393, 0.0100, 0.0560],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62435.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0793, 0.0000, 0.0000,  ..., 0.0950, 0.0000, 0.0853],
        [0.0798, 0.0000, 0.0000,  ..., 0.0955, 0.0000, 0.0859],
        [0.0687, 0.0000, 0.0000,  ..., 0.0824, 0.0000, 0.0791],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542876.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2501.9158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-325.0941, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(256.5521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.5820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.1571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1745],
        [ 0.1862],
        [ 0.2011],
        ...,
        [-0.7046],
        [-0.7004],
        [-0.6905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129900.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0072],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368044.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0035],
        [1.0072],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368056.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0050, -0.0036,  ...,  0.0016,  0.0003,  0.0035],
        [ 0.0009,  0.0048, -0.0034,  ...,  0.0013,  0.0002,  0.0032],
        [ 0.0025,  0.0075, -0.0069,  ...,  0.0051,  0.0012,  0.0079],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2272.9858, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8318, device='cuda:0')



h[100].sum tensor(-27.1042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-41.8283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.5511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0178, 0.0000,  ..., 0.0076, 0.0017, 0.0127],
        [0.0084, 0.0274, 0.0000,  ..., 0.0164, 0.0038, 0.0269],
        [0.0035, 0.0179, 0.0000,  ..., 0.0054, 0.0010, 0.0115],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55823.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.0504],
        [0.0338, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0596],
        [0.0248, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510766.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2230.2427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-356.4857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(148.8248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.9479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.1790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1785],
        [ 0.1944],
        [ 0.1586],
        ...,
        [-0.7095],
        [-0.7074],
        [-0.7068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164079.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0035],
        [1.0072],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368056.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0073],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368067.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2594.6399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3492, device='cuda:0')



h[100].sum tensor(-24.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-39.2802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.0826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65602.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7813e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.8301e-05, 0.0000e+00,
         2.8349e-02],
        [1.6907e-04, 0.0000e+00, 0.0000e+00,  ..., 2.3935e-04, 0.0000e+00,
         2.8902e-02],
        ...,
        [7.0615e-04, 0.0000e+00, 0.0000e+00,  ..., 1.8368e-03, 0.0000e+00,
         3.0198e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7848e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7159e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560453.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2637.7766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.0337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(391.7370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.4652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.3322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4234],
        [-0.3504],
        [-0.2820],
        ...,
        [-0.5573],
        [-0.6474],
        [-0.6911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139474.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0073],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368067.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0074],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368079.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0026,  0.0078, -0.0072,  ...,  0.0054,  0.0013,  0.0083],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2360.9761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2037, device='cuda:0')



h[100].sum tensor(-26.7722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-41.3844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0176, 0.0000,  ..., 0.0073, 0.0017, 0.0123],
        [0.0026, 0.0146, 0.0000,  ..., 0.0054, 0.0013, 0.0083],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59144.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0396, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0597],
        [0.0276, 0.0000, 0.0000,  ..., 0.0335, 0.0000, 0.0497],
        [0.0168, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0416],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531349.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2387.9587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-343.1854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(289.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.7970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1080],
        [ 0.1120],
        [ 0.1193],
        ...,
        [-0.7167],
        [-0.7147],
        [-0.7140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163907.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0074],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368079.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0074],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368091.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0054, -0.0042,  ...,  0.0022,  0.0005,  0.0043],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2514.4546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5096, device='cuda:0')



h[100].sum tensor(-25.4084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-40.2755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0142, 0.0000,  ..., 0.0027, 0.0005, 0.0063],
        [0.0013, 0.0123, 0.0000,  ..., 0.0022, 0.0005, 0.0043],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65483.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0405],
        [0.0043, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0354],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0312],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577081.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2724.5728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-313.1022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(526.5316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.8522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.1141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3440],
        [-0.3601],
        [-0.3236],
        ...,
        [-0.7189],
        [-0.7169],
        [-0.7162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136400.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0074],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368091.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(511.5113, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0075],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368103.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0075, -0.0068,  ...,  0.0050,  0.0012,  0.0078],
        [ 0.0047,  0.0113, -0.0118,  ...,  0.0103,  0.0027,  0.0145],
        [ 0.0011,  0.0052, -0.0038,  ...,  0.0019,  0.0004,  0.0038],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0013],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0013],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0022, -0.0007, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2522.4285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3034, device='cuda:0')



h[100].sum tensor(-25.5529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-40.3708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.9923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0502, 0.0000,  ..., 0.0479, 0.0125, 0.0666],
        [0.0142, 0.0376, 0.0000,  ..., 0.0306, 0.0078, 0.0446],
        [0.0078, 0.0246, 0.0000,  ..., 0.0169, 0.0044, 0.0244],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64560.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0929, 0.0000, 0.0000,  ..., 0.1130, 0.0000, 0.0979],
        [0.0758, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0871],
        [0.0481, 0.0000, 0.0000,  ..., 0.0597, 0.0000, 0.0674],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564608.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2576.1968, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-313.0102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(250.4450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.4454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.1619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2027],
        [ 0.2062],
        [ 0.2039],
        ...,
        [-0.7211],
        [-0.7191],
        [-0.7185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119133.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0036],
        [1.0075],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368103.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0076],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368115.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0023,  0.0072, -0.0065,  ...,  0.0046,  0.0012,  0.0073],
        [ 0.0050,  0.0119, -0.0126,  ...,  0.0111,  0.0029,  0.0155],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2638.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7960, device='cuda:0')



h[100].sum tensor(-24.9319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0150, device='cuda:0')



h[200].sum tensor(-39.4994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0181, 0.0000,  ..., 0.0080, 0.0020, 0.0131],
        [0.0084, 0.0267, 0.0000,  ..., 0.0176, 0.0045, 0.0267],
        [0.0129, 0.0355, 0.0000,  ..., 0.0275, 0.0070, 0.0408],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67512.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0243, 0.0000, 0.0000,  ..., 0.0322, 0.0000, 0.0508],
        [0.0507, 0.0000, 0.0000,  ..., 0.0635, 0.0000, 0.0697],
        [0.0755, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.0873],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578499., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2738.5991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.0592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(421.1252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.1321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.5775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2069],
        [ 0.1995],
        [ 0.1892],
        ...,
        [-0.5125],
        [-0.6374],
        [-0.7003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130631.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0076],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368115.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0077],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368126.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0051, -0.0037,  ...,  0.0017,  0.0004,  0.0037],
        [ 0.0036,  0.0095, -0.0095,  ...,  0.0078,  0.0020,  0.0114],
        [ 0.0045,  0.0111, -0.0116,  ...,  0.0100,  0.0026,  0.0142],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2704.7688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6702, device='cuda:0')



h[100].sum tensor(-24.7210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0160, device='cuda:0')



h[200].sum tensor(-39.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.6754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0240, 0.0000,  ..., 0.0138, 0.0035, 0.0219],
        [0.0134, 0.0364, 0.0000,  ..., 0.0288, 0.0074, 0.0423],
        [0.0206, 0.0490, 0.0000,  ..., 0.0460, 0.0121, 0.0643],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68870.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.0743],
        [0.0723, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.0858],
        [0.0848, 0.0000, 0.0000,  ..., 0.1042, 0.0000, 0.0940],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582413.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2766.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-302.9345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(482.7222, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.8734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.9125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0462],
        [ 0.0583],
        [ 0.0717],
        ...,
        [-0.7423],
        [-0.7401],
        [-0.7394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156148.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0077],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368126.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0077],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368126.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2367.2334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7523, device='cuda:0')



h[100].sum tensor(-28.1084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-41.7801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.8690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58207.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529697.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2293.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-345.3408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(125.4660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.3069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.4190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9051],
        [-0.9127],
        [-0.9164],
        ...,
        [-0.7394],
        [-0.7382],
        [-0.7376]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153373.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0077],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368126.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368138.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2357.2976, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9814, device='cuda:0')



h[100].sum tensor(-28.5790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-41.9709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.3400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57450.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524875.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2279.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-349.6627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(127.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.6622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7943],
        [-0.8528],
        [-0.8973],
        ...,
        [-0.7466],
        [-0.7445],
        [-0.7438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166954.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368138.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368138.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [ 0.0012,  0.0053, -0.0039,  ...,  0.0020,  0.0005,  0.0040],
        ...,
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012],
        [-0.0005,  0.0023,  0.0000,  ..., -0.0021, -0.0007, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2468.7271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3520, device='cuda:0')



h[100].sum tensor(-27.4636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-41.0591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0137, 0.0000,  ..., 0.0018, 0.0003, 0.0052],
        [0.0012, 0.0123, 0.0000,  ..., 0.0020, 0.0005, 0.0040],
        [0.0009, 0.0118, 0.0000,  ..., 0.0012, 0.0003, 0.0030],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60819.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0097, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0435],
        [0.0051, 0.0000, 0.0000,  ..., 0.0103, 0.0000, 0.0386],
        [0.0034, 0.0000, 0.0000,  ..., 0.0081, 0.0000, 0.0369],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541946.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2408.1650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-335.4787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(240.8329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.4814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.3197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0966],
        [-0.0377],
        [-0.1965],
        ...,
        [-0.7466],
        [-0.7445],
        [-0.7438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164320.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368138.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368150.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0046, -0.0030,  ...,  0.0010,  0.0002,  0.0027],
        [ 0.0008,  0.0046, -0.0030,  ...,  0.0010,  0.0002,  0.0027],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2701.0271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3494, device='cuda:0')



h[100].sum tensor(-25.4694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-39.3320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.4552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0175, 0.0000,  ..., 0.0050, 0.0012, 0.0107],
        [0.0019, 0.0155, 0.0000,  ..., 0.0022, 0.0004, 0.0071],
        [0.0013, 0.0134, 0.0000,  ..., 0.0015, 0.0003, 0.0048],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70171.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0177, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.0511],
        [0.0145, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0487],
        [0.0098, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0428],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604916.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2954.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.6491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(814.0525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.3984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.8785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1510],
        [ 0.1208],
        [-0.0020],
        ...,
        [-0.7465],
        [-0.7444],
        [-0.7437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155698.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0078],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368150.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0079],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368162.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2371.0896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4676, device='cuda:0')



h[100].sum tensor(-29.0870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-42.2458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.9874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58663.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535827.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2400.0278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-341.4416, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(154.9965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.6962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.7183, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5228],
        [-0.6630],
        [-0.7699],
        ...,
        [-0.7387],
        [-0.7366],
        [-0.7358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140816.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0079],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368162.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0079],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368174.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2482.5115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4100, device='cuda:0')



h[100].sum tensor(-28.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-41.5533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.0067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61124.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7277e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.8307e-02],
        [8.2668e-05, 0.0000e+00, 0.0000e+00,  ..., 1.8674e-03, 0.0000e+00,
         3.0894e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7843e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7847e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7849e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547058.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2509.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-329.8190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(186.5376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.8184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5296],
        [-0.3651],
        [-0.1743],
        ...,
        [-0.7353],
        [-0.7333],
        [-0.7327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120639.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0079],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368174.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0080],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368186.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3186.3721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.1956, device='cuda:0')



h[100].sum tensor(-21.6904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0225, device='cuda:0')



h[200].sum tensor(-36.0151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.5849, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86096.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0290],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0286],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697703.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3705.3657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-230.6230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1546.8904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(191.4281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.4089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1060],
        [-0.3117],
        [-0.4176],
        ...,
        [-0.7321],
        [-0.7302],
        [-0.7296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107034.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0080],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368186.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(454.3569, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0080],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368197.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0060, -0.0049,  ...,  0.0031,  0.0008,  0.0054],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2738.6194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3525, device='cuda:0')



h[100].sum tensor(-26.4774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-39.7097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.7711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0187, 0.0000,  ..., 0.0091, 0.0024, 0.0143],
        [0.0016, 0.0129, 0.0000,  ..., 0.0032, 0.0008, 0.0054],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70133.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0000, 0.0000,  ..., 0.0441, 0.0000, 0.0569],
        [0.0114, 0.0000, 0.0000,  ..., 0.0179, 0.0000, 0.0406],
        [0.0014, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0308],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602676.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2940.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.0030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(762.2286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(162.9541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.6911, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1605],
        [ 0.0170],
        [-0.1522],
        ...,
        [-0.7403],
        [-0.7383],
        [-0.7377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137735.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0080],
        ...,
        [1.0022],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368197.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0081],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368208.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [ 0.0014,  0.0057, -0.0044,  ...,  0.0026,  0.0007,  0.0047],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2931.6353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4912, device='cuda:0')



h[100].sum tensor(-24.9868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0187, device='cuda:0')



h[200].sum tensor(-38.1814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.9838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0126, 0.0000,  ..., 0.0026, 0.0007, 0.0047],
        [0.0010, 0.0120, 0.0000,  ..., 0.0017, 0.0004, 0.0036],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74738.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0291],
        [0.0027, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0344],
        [0.0055, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0391],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625857.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3104.3713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-281.2469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(667.8629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(166.0233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.1719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5325],
        [-0.3099],
        [-0.0805],
        ...,
        [-0.7479],
        [-0.7459],
        [-0.7453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118347.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0081],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368208.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0041],
        [1.0081],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368218.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0071, -0.0062,  ...,  0.0045,  0.0012,  0.0071],
        [ 0.0042,  0.0107, -0.0109,  ...,  0.0096,  0.0026,  0.0135],
        [ 0.0082,  0.0175, -0.0197,  ...,  0.0190,  0.0051,  0.0254],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2574.0127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8566, device='cuda:0')



h[100].sum tensor(-29.0434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-41.0733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0248, 0.0000,  ..., 0.0173, 0.0046, 0.0248],
        [0.0186, 0.0457, 0.0000,  ..., 0.0421, 0.0113, 0.0590],
        [0.0235, 0.0542, 0.0000,  ..., 0.0538, 0.0145, 0.0739],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63599.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0000, 0.0000,  ..., 0.0662, 0.0000, 0.0699],
        [0.0871, 0.0000, 0.0000,  ..., 0.1157, 0.0000, 0.0975],
        [0.1171, 0.0000, 0.0000,  ..., 0.1530, 0.0000, 0.1174],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562325.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2540.2622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-330.7336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(274.3552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.1364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.9648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2491],
        [ 0.2552],
        [ 0.2540],
        ...,
        [-0.3172],
        [-0.5441],
        [-0.6827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145128.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0041],
        [1.0081],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368218.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0082],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368229.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0065, -0.0054,  ...,  0.0037,  0.0010,  0.0062],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [ 0.0018,  0.0065, -0.0054,  ...,  0.0037,  0.0010,  0.0062],
        ...,
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0023,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2687.6543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3778, device='cuda:0')



h[100].sum tensor(-28.3979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-40.1861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.1022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0128, 0.0000,  ..., 0.0027, 0.0007, 0.0048],
        [0.0064, 0.0247, 0.0000,  ..., 0.0128, 0.0033, 0.0221],
        [0.0014, 0.0129, 0.0000,  ..., 0.0027, 0.0007, 0.0049],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68213.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0380],
        [0.0164, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0485],
        [0.0126, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0460],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595968.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2752.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-314.9350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(522.1365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.1332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.4128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1054],
        [ 0.0895],
        [ 0.1953],
        ...,
        [-0.7717],
        [-0.7696],
        [-0.7689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134391.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0082],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368229.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368240.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [ 0.0009,  0.0049, -0.0033,  ...,  0.0015,  0.0004,  0.0034],
        [ 0.0008,  0.0048, -0.0032,  ...,  0.0013,  0.0003,  0.0031],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2480.9614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9046, device='cuda:0')



h[100].sum tensor(-30.8574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-41.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0120, 0.0000,  ..., 0.0015, 0.0004, 0.0034],
        [0.0036, 0.0179, 0.0000,  ..., 0.0075, 0.0020, 0.0125],
        [0.0077, 0.0271, 0.0000,  ..., 0.0161, 0.0042, 0.0263],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60908.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0080, 0.0000, 0.0000,  ..., 0.0149, 0.0000, 0.0400],
        [0.0188, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.0510],
        [0.0296, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0606],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551548.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2365.2834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-345.8186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(169.6063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.6992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.6221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0302],
        [ 0.1605],
        [ 0.2162],
        ...,
        [-0.7783],
        [-0.7761],
        [-0.7754]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150631.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0022],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368240.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368252.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2730.9167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1316, device='cuda:0')



h[100].sum tensor(-28.8699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0152, device='cuda:0')



h[200].sum tensor(-40.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66586.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574604.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2523.6460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-323.3828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(347.9044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.2071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.7051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7155],
        [-0.8285],
        [-0.9032],
        ...,
        [-0.7818],
        [-0.7796],
        [-0.7789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144260.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368252.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368252.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2932.1089, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6997, device='cuda:0')



h[100].sum tensor(-26.9141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0183, device='cuda:0')



h[200].sum tensor(-38.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.4407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75665.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0310],
        [0.0130, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0402],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641646.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3053.7629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.5722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(768.4116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.4008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.7025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1568],
        [ 0.1676],
        [ 0.1967],
        ...,
        [-0.7818],
        [-0.7796],
        [-0.7789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130432.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368252.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0043],
        [1.0083],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368263.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030,  0.0086, -0.0080,  ...,  0.0066,  0.0017,  0.0098],
        [ 0.0046,  0.0115, -0.0118,  ...,  0.0106,  0.0028,  0.0149],
        [ 0.0030,  0.0086, -0.0080,  ...,  0.0066,  0.0017,  0.0098],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2895.3228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4751, device='cuda:0')



h[100].sum tensor(-27.7701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0176, device='cuda:0')



h[200].sum tensor(-38.9057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.6003, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0429, 0.0000,  ..., 0.0381, 0.0102, 0.0542],
        [0.0181, 0.0453, 0.0000,  ..., 0.0414, 0.0111, 0.0584],
        [0.0329, 0.0709, 0.0000,  ..., 0.0769, 0.0207, 0.1034],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72687.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0924, 0.0000, 0.0000,  ..., 0.1266, 0.0000, 0.1007],
        [0.1103, 0.0000, 0.0000,  ..., 0.1494, 0.0000, 0.1123],
        [0.1469, 0.0000, 0.0000,  ..., 0.1960, 0.0000, 0.1360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611158.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2815.7546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.2193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(547.0526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.6940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.4483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2703],
        [ 0.2707],
        [ 0.2695],
        ...,
        [-0.7638],
        [-0.7188],
        [-0.6354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144241.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0043],
        [1.0083],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368263.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0043],
        [1.0083],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368274.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2729.1802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9863, device='cuda:0')



h[100].sum tensor(-29.8502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0145, device='cuda:0')



h[200].sum tensor(-40.3096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.8335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67882.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589891.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2686.4460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-317.3799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(314.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.9822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3042],
        [-0.5154],
        [-0.6799],
        ...,
        [-0.7840],
        [-0.7817],
        [-0.7809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135814.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0043],
        [1.0083],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368274.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0044],
        [1.0084],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368286.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0160, -0.0175,  ...,  0.0168,  0.0045,  0.0229],
        [ 0.0080,  0.0174, -0.0193,  ...,  0.0187,  0.0050,  0.0252],
        [ 0.0048,  0.0119, -0.0123,  ...,  0.0111,  0.0029,  0.0157],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2450.6128, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8346, device='cuda:0')



h[100].sum tensor(-32.8874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-42.5981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.2393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0339, 0.0728, 0.0000,  ..., 0.0793, 0.0212, 0.1069],
        [0.0279, 0.0624, 0.0000,  ..., 0.0650, 0.0173, 0.0886],
        [0.0206, 0.0498, 0.0000,  ..., 0.0474, 0.0125, 0.0664],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58533., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1505, 0.0000, 0.0000,  ..., 0.1955, 0.0000, 0.1352],
        [0.1360, 0.0000, 0.0000,  ..., 0.1773, 0.0000, 0.1264],
        [0.1117, 0.0000, 0.0000,  ..., 0.1468, 0.0000, 0.1113],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540492.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2332.0034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-357.4501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81.6655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.4797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2262],
        [ 0.2237],
        [ 0.2239],
        ...,
        [-0.7863],
        [-0.7841],
        [-0.7834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148195.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0044],
        [1.0084],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368286.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(475.1469, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0084],
        ...,
        [1.0020],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368297.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0044, -0.0026,  ...,  0.0007,  0.0001,  0.0025],
        [ 0.0005,  0.0044, -0.0026,  ...,  0.0007,  0.0001,  0.0025],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0007, -0.0011],
        ...,
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0007, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0007, -0.0011],
        [-0.0006,  0.0024,  0.0000,  ..., -0.0021, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2563.2700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9983, device='cuda:0')



h[100].sum tensor(-31.9054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-41.7653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.4104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0133, 0.0000,  ..., 0.0009, 0.0001, 0.0043],
        [0.0008, 0.0133, 0.0000,  ..., 0.0009, 0.0001, 0.0043],
        [0.0008, 0.0133, 0.0000,  ..., 0.0009, 0.0001, 0.0043],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61146.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0088, 0.0000, 0.0387],
        [0.0047, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0400],
        [0.0044, 0.0000, 0.0000,  ..., 0.0089, 0.0000, 0.0391],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553772.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2428.9507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-344.1350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.5101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.9047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5915e-02],
        [ 1.0741e-02],
        [-1.9337e-05],
        ...,
        [-7.8783e-01],
        [-7.8572e-01],
        [-7.8505e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121016.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0044],
        [1.0084],
        ...,
        [1.0020],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368297.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0045],
        [1.0085],
        ...,
        [1.0020],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368308.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [ 0.0021,  0.0072, -0.0062,  ...,  0.0045,  0.0011,  0.0074],
        [ 0.0047,  0.0116, -0.0118,  ...,  0.0106,  0.0028,  0.0151],
        ...,
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2426.9729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0879, device='cuda:0')



h[100].sum tensor(-33.2030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0095, device='cuda:0')



h[200].sum tensor(-42.8258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.7268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0145, 0.0000,  ..., 0.0045, 0.0011, 0.0074],
        [0.0084, 0.0265, 0.0000,  ..., 0.0191, 0.0050, 0.0275],
        [0.0214, 0.0514, 0.0000,  ..., 0.0492, 0.0129, 0.0690],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57818.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0000, 0.0000,  ..., 0.0258, 0.0000, 0.0432],
        [0.0505, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0662],
        [0.0978, 0.0000, 0.0000,  ..., 0.1265, 0.0000, 0.0984],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540305., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2308.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.2216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(102.5728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.9691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0336],
        [ 0.1333],
        [ 0.2031],
        ...,
        [-0.7945],
        [-0.7923],
        [-0.7916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147271.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0045],
        [1.0085],
        ...,
        [1.0020],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368308.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0045],
        [1.0086],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368319.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0057, -0.0042,  ...,  0.0024,  0.0005,  0.0047],
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [ 0.0024,  0.0078, -0.0069,  ...,  0.0053,  0.0013,  0.0083],
        ...,
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0024,  0.0000,  ..., -0.0022, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2663.9019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0742, device='cuda:0')



h[100].sum tensor(-30.8747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0134, device='cuda:0')



h[200].sum tensor(-40.9231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.5213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0267, 0.0000,  ..., 0.0169, 0.0043, 0.0264],
        [0.0064, 0.0255, 0.0000,  ..., 0.0131, 0.0031, 0.0232],
        [0.0050, 0.0208, 0.0000,  ..., 0.0109, 0.0027, 0.0171],
        ...,
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0034, 0.0172, 0.0000,  ..., 0.0076, 0.0019, 0.0114]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63993.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0554, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.0733],
        [0.0412, 0.0000, 0.0000,  ..., 0.0558, 0.0000, 0.0647],
        [0.0374, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0610],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0272],
        [0.0032, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0313],
        [0.0169, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0430]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571571.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2497.8127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-337.2989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(230.8062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.6683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.9553, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2644],
        [ 0.2655],
        [ 0.2659],
        ...,
        [-0.6706],
        [-0.4882],
        [-0.2465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145928.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0045],
        [1.0086],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368319.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0045],
        [1.0086],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368331.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0082, -0.0074,  ...,  0.0058,  0.0015,  0.0091],
        [ 0.0022,  0.0075, -0.0064,  ...,  0.0048,  0.0012,  0.0077],
        [ 0.0022,  0.0074, -0.0063,  ...,  0.0046,  0.0011,  0.0076],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2653.5010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3264, device='cuda:0')



h[100].sum tensor(-31.1198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-41.0088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0307, 0.0000,  ..., 0.0202, 0.0050, 0.0323],
        [0.0132, 0.0374, 0.0000,  ..., 0.0294, 0.0075, 0.0441],
        [0.0125, 0.0362, 0.0000,  ..., 0.0277, 0.0070, 0.0419],
        ...,
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64663.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.0769],
        [0.0772, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0886],
        [0.0799, 0.0000, 0.0000,  ..., 0.1035, 0.0000, 0.0904],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578337.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2515.4370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.6019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(301.1570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.7786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.8286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2344],
        [ 0.2480],
        [ 0.2549],
        ...,
        [-0.7647],
        [-0.7275],
        [-0.7027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143028.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0045],
        [1.0086],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368331.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0087],
        ...,
        [1.0019],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368343.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0050, -0.0032,  ...,  0.0013,  0.0002,  0.0033],
        [ 0.0008,  0.0050, -0.0033,  ...,  0.0013,  0.0002,  0.0034],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2524.9739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6760, device='cuda:0')



h[100].sum tensor(-32.6688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-42.0384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0274, 0.0000,  ..., 0.0154, 0.0037, 0.0265],
        [0.0035, 0.0184, 0.0000,  ..., 0.0073, 0.0017, 0.0127],
        [0.0008, 0.0125, 0.0000,  ..., 0.0013, 0.0002, 0.0034],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59783.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0303, 0.0000, 0.0000,  ..., 0.0409, 0.0000, 0.0572],
        [0.0198, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0488],
        [0.0089, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0395],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549379.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2299.8823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-360.4315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(135.0783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.7824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2402],
        [ 0.2386],
        [ 0.2280],
        ...,
        [-0.8215],
        [-0.8191],
        [-0.8181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168489.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0087],
        ...,
        [1.0019],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368343.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0087],
        ...,
        [1.0019],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368354.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0022, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2784.8354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5636, device='cuda:0')



h[100].sum tensor(-30.5479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0148, device='cuda:0')



h[200].sum tensor(-40.0968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0134, 0.0000,  ..., 0.0003, 0.0000, 0.0039],
        [0.0003, 0.0117, 0.0000,  ..., 0.0001, 0.0000, 0.0019],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67785.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0348],
        [0.0012, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0287],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596972.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2685.1714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-328.4023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(428.8067, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.4549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.2517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5310],
        [-0.6303],
        [-0.7395],
        ...,
        [-0.8290],
        [-0.8267],
        [-0.8259]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156475.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0046],
        [1.0087],
        ...,
        [1.0019],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368354.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0088],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368366., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2572.0500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9018, device='cuda:0')



h[100].sum tensor(-32.9253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-41.8621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62543.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0252],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573337.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2498.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-348.8898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(182.6170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.3688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.0788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9472],
        [-0.9821],
        [-1.0110],
        ...,
        [-0.8302],
        [-0.8287],
        [-0.8284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158489.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0088],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368366., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0088],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368377.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2542.1045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5788, device='cuda:0')



h[100].sum tensor(-33.5160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-42.2550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.7500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0126, 0.0000,  ..., 0.0013, 0.0002, 0.0036],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59998.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0106, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0385],
        [0.0009, 0.0000, 0.0000,  ..., 0.0026, 0.0000, 0.0305],
        [0.0004, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0292],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557094.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2404.0933, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-362.2274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(149.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.2593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.5764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0212],
        [-0.1388],
        [-0.2318],
        ...,
        [-0.8372],
        [-0.8348],
        [-0.8340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175285.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0088],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368377.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0089],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368389.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2742.1228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8166, device='cuda:0')



h[100].sum tensor(-31.7655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-40.8841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63944.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0253],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0255],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571674.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2546.8804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-345.6229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(164.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.9955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.2723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9739],
        [-0.9945],
        [-1.0087],
        ...,
        [-0.8370],
        [-0.8344],
        [-0.8334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149220.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0047],
        [1.0089],
        ...,
        [1.0020],
        [1.0009],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368389.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0048],
        [1.0090],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368400.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [ 0.0040,  0.0106, -0.0102,  ...,  0.0089,  0.0023,  0.0132],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2694.1973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5372, device='cuda:0')



h[100].sum tensor(-32.2420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-41.4576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.7802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0071, 0.0247, 0.0000,  ..., 0.0158, 0.0040, 0.0239],
        [0.0150, 0.0396, 0.0000,  ..., 0.0341, 0.0088, 0.0491],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63268.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0403],
        [0.0441, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0618],
        [0.0763, 0.0000, 0.0000,  ..., 0.0964, 0.0000, 0.0839],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573097.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2594.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-350.2978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(230.8578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.7309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.4509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1491],
        [ 0.1846],
        [ 0.1897],
        ...,
        [-0.8366],
        [-0.8343],
        [-0.8336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154379.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0048],
        [1.0090],
        ...,
        [1.0021],
        [1.0010],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368400.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(513.9338, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0048],
        [1.0090],
        ...,
        [1.0022],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368412.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0006,  0.0048, -0.0029,  ...,  0.0009,  0.0001,  0.0030],
        [ 0.0009,  0.0053, -0.0035,  ...,  0.0016,  0.0003,  0.0039],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2950.4485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3340, device='cuda:0')



h[100].sum tensor(-29.8567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-39.6752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.4447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0283, 0.0000,  ..., 0.0163, 0.0040, 0.0282],
        [0.0035, 0.0185, 0.0000,  ..., 0.0072, 0.0017, 0.0129],
        [0.0018, 0.0156, 0.0000,  ..., 0.0031, 0.0006, 0.0077],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73068.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0334, 0.0000, 0.0000,  ..., 0.0455, 0.0000, 0.0595],
        [0.0229, 0.0000, 0.0000,  ..., 0.0325, 0.0000, 0.0515],
        [0.0156, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0457],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644871.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3200.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.8263, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(831.7839, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(167.8923, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.2225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2543],
        [ 0.2487],
        [ 0.2369],
        ...,
        [-0.8356],
        [-0.8333],
        [-0.8326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149952.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0048],
        [1.0090],
        ...,
        [1.0022],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368412.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0049],
        [1.0091],
        ...,
        [1.0022],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368424.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3030.6880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6988, device='cuda:0')



h[100].sum tensor(-29.1050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-39.2014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.3813, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73807.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0278],
        [0.0018, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0307]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635951.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3115.3296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.5836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(773.0060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.3261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.6822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6845],
        [-0.6204],
        [-0.5204],
        ...,
        [-0.8040],
        [-0.7295],
        [-0.5802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153859.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0049],
        [1.0091],
        ...,
        [1.0022],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368424.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0049],
        [1.0091],
        ...,
        [1.0023],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368435.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [ 0.0010,  0.0054, -0.0037,  ...,  0.0018,  0.0004,  0.0041],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2706.7229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6753, device='cuda:0')



h[100].sum tensor(-32.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-41.7881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0130, 0.0000,  ..., 0.0018, 0.0004, 0.0042],
        [0.0007, 0.0125, 0.0000,  ..., 0.0010, 0.0002, 0.0032],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64551.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0283],
        [0.0019, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0324],
        [0.0029, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0345],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588890.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2754.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-343.9599, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(267.8419, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(158.2531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.5976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3932],
        [-0.5176],
        [-0.5913],
        ...,
        [-0.8433],
        [-0.8410],
        [-0.8402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138031.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0049],
        [1.0091],
        ...,
        [1.0023],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368435.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0050],
        [1.0092],
        ...,
        [1.0023],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368446.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3075.0208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8971, device='cuda:0')



h[100].sum tensor(-29.1380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0173, device='cuda:0')



h[200].sum tensor(-39.0344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.2036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73799.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        ...,
        [0.0048, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0352],
        [0.0043, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0348],
        [0.0026, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640092.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3163.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.1929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(659.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(164.1372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.0035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9339],
        [-0.9662],
        [-0.9833],
        ...,
        [-0.1067],
        [-0.1142],
        [-0.1432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137460.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0050],
        [1.0092],
        ...,
        [1.0023],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368446.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0050],
        [1.0092],
        ...,
        [1.0024],
        [1.0013],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368458.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [ 0.0014,  0.0062, -0.0046,  ...,  0.0028,  0.0007,  0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2799.6042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0437, device='cuda:0')



h[100].sum tensor(-31.9855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-41.1851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.8141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0015, 0.0143, 0.0000,  ..., 0.0029, 0.0007, 0.0057],
        [0.0011, 0.0136, 0.0000,  ..., 0.0019, 0.0005, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65871.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0293],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0279],
        [0.0005, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0297],
        ...,
        [0.0001, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0298],
        [0.0040, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0354],
        [0.0056, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592425.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2758.3064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-339.0660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(289.2819, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.9868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.7332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0351],
        [-0.1209],
        [-0.1541],
        ...,
        [-0.7352],
        [-0.6221],
        [-0.5228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162844.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0050],
        [1.0092],
        ...,
        [1.0024],
        [1.0013],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368458.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0051],
        [1.0093],
        ...,
        [1.0024],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368469.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0007, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2724.5767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6547, device='cuda:0')



h[100].sum tensor(-32.8848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-41.8327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63391.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0288],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581477.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2639.7153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-346.4881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(114.3179, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.1005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.7975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0364],
        [-0.0578],
        [-0.1136],
        ...,
        [-0.8725],
        [-0.8699],
        [-0.8690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153336., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0051],
        [1.0093],
        ...,
        [1.0024],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368469.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0051],
        [1.0093],
        ...,
        [1.0025],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368480.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2605.8442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1229, device='cuda:0')



h[100].sum tensor(-33.9403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0096, device='cuda:0')



h[200].sum tensor(-42.8259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.7508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59941.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0263],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566749.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2477.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.7706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59.6322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.6540, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.5271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9354],
        [-0.8687],
        [-0.7556],
        ...,
        [-0.8791],
        [-0.8765],
        [-0.8756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167485.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0051],
        [1.0093],
        ...,
        [1.0025],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368480.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0052],
        [1.0094],
        ...,
        [1.0025],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368492.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0079,  0.0174, -0.0186,  ...,  0.0184,  0.0050,  0.0253],
        [ 0.0061,  0.0142, -0.0147,  ...,  0.0140,  0.0038,  0.0197],
        [ 0.0050,  0.0125, -0.0125,  ...,  0.0115,  0.0031,  0.0166],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.9690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5305, device='cuda:0')



h[100].sum tensor(-30.7300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0148, device='cuda:0')



h[200].sum tensor(-40.3846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0540, 0.0000,  ..., 0.0518, 0.0140, 0.0737],
        [0.0290, 0.0652, 0.0000,  ..., 0.0673, 0.0182, 0.0934],
        [0.0308, 0.0683, 0.0000,  ..., 0.0716, 0.0194, 0.0989],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67006.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1343, 0.0000, 0.0000,  ..., 0.1764, 0.0000, 0.1251],
        [0.1623, 0.0000, 0.0000,  ..., 0.2115, 0.0000, 0.1424],
        [0.1745, 0.0000, 0.0000,  ..., 0.2266, 0.0000, 0.1499],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593026., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2706.1343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-337.2696, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(326.7496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.3596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-217.0868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2270],
        [ 0.2105],
        [ 0.1951],
        ...,
        [-0.8826],
        [-0.8799],
        [-0.8790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183455.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0052],
        [1.0094],
        ...,
        [1.0025],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368492.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0052],
        [1.0094],
        ...,
        [1.0026],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368504.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0145, -0.0150,  ...,  0.0144,  0.0039,  0.0203],
        [ 0.0040,  0.0106, -0.0101,  ...,  0.0090,  0.0024,  0.0133],
        [ 0.0088,  0.0189, -0.0205,  ...,  0.0205,  0.0056,  0.0280],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3111.4622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6309, device='cuda:0')



h[100].sum tensor(-29.3344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0171, device='cuda:0')



h[200].sum tensor(-39.3092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.0210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0448, 0.0000,  ..., 0.0390, 0.0106, 0.0573],
        [0.0269, 0.0615, 0.0000,  ..., 0.0622, 0.0169, 0.0869],
        [0.0134, 0.0383, 0.0000,  ..., 0.0299, 0.0082, 0.0458],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73037.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0900, 0.0000, 0.0000,  ..., 0.1219, 0.0000, 0.0982],
        [0.1102, 0.0000, 0.0000,  ..., 0.1476, 0.0000, 0.1116],
        [0.0856, 0.0000, 0.0000,  ..., 0.1164, 0.0000, 0.0959],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631937.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3015.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.0396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(419.0666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.1529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.5363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2773],
        [ 0.2762],
        [ 0.2676],
        ...,
        [-0.8812],
        [-0.8785],
        [-0.8773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143531.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0052],
        [1.0094],
        ...,
        [1.0026],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368504.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0053],
        [1.0095],
        ...,
        [1.0026],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368516.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0107, -0.0103,  ...,  0.0091,  0.0025,  0.0136],
        [ 0.0067,  0.0154, -0.0161,  ...,  0.0156,  0.0043,  0.0218],
        [ 0.0078,  0.0172, -0.0183,  ...,  0.0181,  0.0049,  0.0250],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2827.7092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1719, device='cuda:0')



h[100].sum tensor(-32.0770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-41.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0199, 0.0494, 0.0000,  ..., 0.0455, 0.0125, 0.0656],
        [0.0207, 0.0508, 0.0000,  ..., 0.0474, 0.0130, 0.0681],
        [0.0238, 0.0562, 0.0000,  ..., 0.0548, 0.0150, 0.0776],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63587.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1173, 0.0000, 0.0000,  ..., 0.1572, 0.0000, 0.1154],
        [0.1143, 0.0000, 0.0000,  ..., 0.1533, 0.0000, 0.1137],
        [0.1087, 0.0000, 0.0000,  ..., 0.1458, 0.0000, 0.1094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580544.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2621.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-353.6945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(168.8552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.1477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.4044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2609],
        [ 0.2607],
        [ 0.2571],
        ...,
        [-0.8831],
        [-0.8806],
        [-0.8799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161969.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0053],
        [1.0095],
        ...,
        [1.0026],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368516.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(540.8248, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0053],
        [1.0096],
        ...,
        [1.0026],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368528.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2796.7278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5024, device='cuda:0')



h[100].sum tensor(-32.5702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-42.0188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.0701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63338.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0014, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0320],
        [0.0074, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0378],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581460.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2659.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-357.4609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(166.3177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.7512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.1138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3647],
        [-0.1153],
        [ 0.0940],
        ...,
        [-0.8819],
        [-0.8792],
        [-0.8768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164259.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0053],
        [1.0096],
        ...,
        [1.0026],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368528.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0054],
        [1.0097],
        ...,
        [1.0026],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368540.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0025,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2707.3271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5508, device='cuda:0')



h[100].sum tensor(-33.6193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0098, device='cuda:0')



h[200].sum tensor(-42.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.0445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0128, 0.0000,  ..., 0.0014, 0.0004, 0.0037],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0131, 0.0000,  ..., 0.0016, 0.0005, 0.0040],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60764.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0313],
        [0.0016, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573412.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2609.3696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-371.0979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(128.5971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.9257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.8273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6500],
        [-0.7393],
        [-0.7575],
        ...,
        [-0.8868],
        [-0.8842],
        [-0.8833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164096.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0054],
        [1.0097],
        ...,
        [1.0026],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368540.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0055],
        [1.0098],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368552.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3020.1802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3654, device='cuda:0')



h[100].sum tensor(-30.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0147, device='cuda:0')



h[200].sum tensor(-40.4749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.0937, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68501.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0303],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608334.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2839.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-340.5172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(227.1896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.5652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.8327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7046],
        [-0.5853],
        [-0.4344],
        ...,
        [-0.8925],
        [-0.8899],
        [-0.8889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145987.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0055],
        [1.0098],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368552.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0056],
        [1.0099],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368564.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0055, -0.0036,  ...,  0.0018,  0.0005,  0.0042],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [ 0.0010,  0.0055, -0.0036,  ...,  0.0018,  0.0005,  0.0042],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3322.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5226, device='cuda:0')



h[100].sum tensor(-28.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0193, device='cuda:0')



h[200].sum tensor(-38.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.6917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0127, 0.0000,  ..., 0.0010, 0.0003, 0.0033],
        [0.0033, 0.0209, 0.0000,  ..., 0.0057, 0.0018, 0.0151],
        [0.0007, 0.0128, 0.0000,  ..., 0.0010, 0.0003, 0.0033],
        ...,
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78238.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0137, 0.0000, 0.0396],
        [0.0088, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0446],
        [0.0047, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0410],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666477.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3252.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-303.0757, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(512.5558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.5259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.4956, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0902],
        [ 0.0783],
        [ 0.1045],
        ...,
        [-0.9018],
        [-0.8991],
        [-0.8982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140336.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0056],
        [1.0099],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368564.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0057],
        [1.0100],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368576.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [ 0.0011,  0.0057, -0.0038,  ...,  0.0020,  0.0006,  0.0045],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2823.9741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5548, device='cuda:0')



h[100].sum tensor(-32.8179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-41.8809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1061, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0195, 0.0000,  ..., 0.0059, 0.0018, 0.0134],
        [0.0032, 0.0185, 0.0000,  ..., 0.0066, 0.0019, 0.0124],
        [0.0046, 0.0233, 0.0000,  ..., 0.0088, 0.0026, 0.0191],
        ...,
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63602.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0000, 0.0000,  ..., 0.0297, 0.0000, 0.0513],
        [0.0173, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0522],
        [0.0177, 0.0000, 0.0000,  ..., 0.0318, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588156.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2562.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-365.9949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.0386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1063],
        [ 0.1637],
        [ 0.1319],
        ...,
        [-0.9126],
        [-0.9099],
        [-0.9090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160875.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0057],
        [1.0100],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368576.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0101],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368587.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0120, -0.0117,  ...,  0.0109,  0.0030,  0.0158],
        [ 0.0029,  0.0088, -0.0077,  ...,  0.0064,  0.0018,  0.0101],
        [ 0.0035,  0.0099, -0.0090,  ...,  0.0079,  0.0022,  0.0120],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2853.4390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5362, device='cuda:0')



h[100].sum tensor(-32.6018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0120, device='cuda:0')



h[200].sum tensor(-41.5875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.7796, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0124, 0.0368, 0.0000,  ..., 0.0276, 0.0078, 0.0431],
        [0.0146, 0.0407, 0.0000,  ..., 0.0330, 0.0093, 0.0501],
        [0.0130, 0.0380, 0.0000,  ..., 0.0291, 0.0082, 0.0451],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66221.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0701, 0.0000, 0.0000,  ..., 0.1007, 0.0000, 0.0878],
        [0.0702, 0.0000, 0.0000,  ..., 0.1006, 0.0000, 0.0876],
        [0.0639, 0.0000, 0.0000,  ..., 0.0924, 0.0000, 0.0835],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0283]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611218.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2660.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-356.3331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(245.2836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.8309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2797],
        [ 0.2846],
        [ 0.2901],
        ...,
        [-0.9249],
        [-0.9222],
        [-0.9212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168973.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0101],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368587.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0101],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368599.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2766.0815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9754, device='cuda:0')



h[100].sum tensor(-33.3410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-42.2029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0127, 0.0000,  ..., 0.0006, 0.0003, 0.0028],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62017.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0409],
        [0.0019, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0343],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0296],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583219.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2350.0054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-374.3236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(124.0209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.2932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.7379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [-0.0614],
        [-0.2920],
        ...,
        [-0.9342],
        [-0.9314],
        [-0.9304]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184554.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0101],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368599.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0102],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368611.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3490.8647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1905, device='cuda:0')



h[100].sum tensor(-26.8458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0219, device='cuda:0')



h[200].sum tensor(-36.9147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-26.8951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0239, 0.0000,  ..., 0.0139, 0.0040, 0.0217],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81079.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0654],
        [0.0119, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0390],
        [0.0003, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0304],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677851.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3085.4907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.5819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(798.2216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.4516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.8414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2357],
        [ 0.1641],
        [ 0.1018],
        ...,
        [-0.9389],
        [-0.9360],
        [-0.9351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170135.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0102],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368611.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0059],
        [1.0103],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368623.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0056, -0.0037,  ...,  0.0019,  0.0006,  0.0044],
        [ 0.0033,  0.0095, -0.0085,  ...,  0.0073,  0.0021,  0.0113],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.0498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5267, device='cuda:0')



h[100].sum tensor(-31.9152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-41.0578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0430, 0.0000,  ..., 0.0361, 0.0102, 0.0539],
        [0.0075, 0.0274, 0.0000,  ..., 0.0165, 0.0048, 0.0270],
        [0.0080, 0.0281, 0.0000,  ..., 0.0176, 0.0051, 0.0283],
        ...,
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65359.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0765, 0.0000, 0.0000,  ..., 0.1077, 0.0000, 0.0911],
        [0.0527, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.0752],
        [0.0396, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0655],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596348.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2492.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.3722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(168.6209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.6627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.9459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2929],
        [ 0.2816],
        [ 0.2201],
        ...,
        [-0.9389],
        [-0.9361],
        [-0.9351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166679.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0059],
        [1.0103],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368623.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0104],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368635.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2767.4819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9756, device='cuda:0')



h[100].sum tensor(-33.7160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-42.5052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.3361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0031, 0.0184, 0.0000,  ..., 0.0063, 0.0019, 0.0120],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62009.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0283],
        [0.0085, 0.0000, 0.0000,  ..., 0.0142, 0.0000, 0.0364],
        [0.0280, 0.0000, 0.0000,  ..., 0.0418, 0.0000, 0.0542],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587758., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2470.9634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-374.3615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86.4103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.2910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.1431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0462],
        [ 0.1268],
        [ 0.2050],
        ...,
        [-0.9373],
        [-0.9345],
        [-0.9335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165920.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0104],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368635.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0104],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368635.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2883.5146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0048, device='cuda:0')



h[100].sum tensor(-32.6700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-41.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.4149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0184, 0.0000,  ..., 0.0062, 0.0019, 0.0119],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65080.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0304],
        [0.0111, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0399],
        [0.0276, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0555],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601575.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2591.6528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-363.7682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(252.8261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.2963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.3201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0640],
        [ 0.1652],
        [ 0.2452],
        ...,
        [-0.9137],
        [-0.9208],
        [-0.9256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176208.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0104],
        ...,
        [1.0027],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368635.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0105],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368647.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2954.3269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5604, device='cuda:0')



h[100].sum tensor(-32.1030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-41.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.4825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66382.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0000, 0.0000,  ..., 0.0119, 0.0000, 0.0362],
        [0.0063, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0378],
        [0.0074, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0379],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606191.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2643.7471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-357.5851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(273.7719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.2483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.7166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1937],
        [ 0.2431],
        [ 0.2632],
        ...,
        [-0.9363],
        [-0.9336],
        [-0.9327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169266.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0105],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368647.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0061],
        [1.0106],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368659.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0055, -0.0036,  ...,  0.0018,  0.0006,  0.0042],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [ 0.0010,  0.0055, -0.0036,  ...,  0.0018,  0.0006,  0.0042],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2939.6167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9576, device='cuda:0')



h[100].sum tensor(-32.3300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-41.4380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.0687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0150, 0.0000,  ..., 0.0039, 0.0012, 0.0070],
        [0.0057, 0.0254, 0.0000,  ..., 0.0114, 0.0034, 0.0224],
        [0.0019, 0.0151, 0.0000,  ..., 0.0040, 0.0012, 0.0070],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65975.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0427],
        [0.0198, 0.0000, 0.0000,  ..., 0.0311, 0.0000, 0.0516],
        [0.0110, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0438],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605716.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2660.1440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-356.4341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(219.4615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.8898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.3102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0183],
        [ 0.1411],
        [ 0.1301],
        ...,
        [-0.9343],
        [-0.9316],
        [-0.9289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145106.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0061],
        [1.0106],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368659.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0107],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368671.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030,  0.0091, -0.0080,  ...,  0.0067,  0.0019,  0.0105],
        [ 0.0010,  0.0056, -0.0037,  ...,  0.0018,  0.0006,  0.0043],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0026,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2806.0205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4283, device='cuda:0')



h[100].sum tensor(-33.5565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-42.3996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.6467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0033, 0.0189, 0.0000,  ..., 0.0069, 0.0020, 0.0127],
        [0.0037, 0.0196, 0.0000,  ..., 0.0078, 0.0023, 0.0139],
        [0.0056, 0.0254, 0.0000,  ..., 0.0113, 0.0033, 0.0222],
        ...,
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61818.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0284, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0551],
        [0.0256, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.0545],
        [0.0297, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0595],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585508.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2496.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-375.4401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.1750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2944],
        [ 0.2998],
        [ 0.3011],
        ...,
        [-0.9481],
        [-0.9455],
        [-0.9445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163541.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0107],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368671.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0109],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368682.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0069, -0.0052,  ...,  0.0035,  0.0010,  0.0065],
        [ 0.0026,  0.0083, -0.0070,  ...,  0.0056,  0.0016,  0.0091],
        [ 0.0025,  0.0082, -0.0069,  ...,  0.0054,  0.0015,  0.0089],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3367.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.7187, device='cuda:0')



h[100].sum tensor(-28.4715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0188, device='cuda:0')



h[200].sum tensor(-38.2918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.1400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0198, 0.0000,  ..., 0.0081, 0.0023, 0.0142],
        [0.0062, 0.0252, 0.0000,  ..., 0.0132, 0.0038, 0.0227],
        [0.0155, 0.0424, 0.0000,  ..., 0.0348, 0.0098, 0.0523],
        ...,
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78399.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0332, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0598],
        [0.0508, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0711],
        [0.0788, 0.0000, 0.0000,  ..., 0.1002, 0.0000, 0.0881],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675903.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3205.0107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-312.0587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(840.8911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.6013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.8027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2622],
        [ 0.2561],
        [ 0.2429],
        ...,
        [-0.9607],
        [-0.9579],
        [-0.9569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179369.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0109],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368682.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0110],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368693.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [ 0.0021,  0.0076, -0.0061,  ...,  0.0046,  0.0013,  0.0078],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2819.7922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1175, device='cuda:0')



h[100].sum tensor(-33.3866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-42.3051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0022, 0.0158, 0.0000,  ..., 0.0046, 0.0013, 0.0078],
        [0.0016, 0.0149, 0.0000,  ..., 0.0033, 0.0010, 0.0062],
        ...,
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62225.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0297],
        [0.0086, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0373],
        [0.0201, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.0467],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591414.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2515.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-377.4364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(145.7975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.0685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.6309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4060],
        [-0.3227],
        [-0.1256],
        ...,
        [-0.9696],
        [-0.9667],
        [-0.9631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191562.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0064],
        [1.0110],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368693.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0111],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368704.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0101, -0.0092,  ...,  0.0081,  0.0023,  0.0122],
        [ 0.0093,  0.0199, -0.0212,  ...,  0.0217,  0.0060,  0.0297],
        [ 0.0034,  0.0097, -0.0087,  ...,  0.0075,  0.0021,  0.0115],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2997.6807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5346, device='cuda:0')



h[100].sum tensor(-31.8636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0131, device='cuda:0')



h[200].sum tensor(-41.0449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0183, 0.0472, 0.0000,  ..., 0.0414, 0.0115, 0.0607],
        [0.0138, 0.0395, 0.0000,  ..., 0.0307, 0.0086, 0.0470],
        [0.0148, 0.0401, 0.0000,  ..., 0.0338, 0.0094, 0.0490],
        ...,
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66973.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0815, 0.0000, 0.0000,  ..., 0.1015, 0.0000, 0.0889],
        [0.0797, 0.0000, 0.0000,  ..., 0.0994, 0.0000, 0.0878],
        [0.0708, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.0808],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613759.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2703.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-356.3966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(183.8668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.8304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.4616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2348],
        [ 0.2264],
        [ 0.1885],
        ...,
        [-0.9782],
        [-0.9753],
        [-0.9743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178225.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0111],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368704.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0111],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368715.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024,  0.0081, -0.0067,  ...,  0.0052,  0.0015,  0.0086],
        [ 0.0011,  0.0058, -0.0038,  ...,  0.0020,  0.0006,  0.0045],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2878.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4078, device='cuda:0')



h[100].sum tensor(-32.9918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0114, device='cuda:0')



h[200].sum tensor(-41.9637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.0052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0290, 0.0000,  ..., 0.0160, 0.0046, 0.0282],
        [0.0043, 0.0208, 0.0000,  ..., 0.0091, 0.0026, 0.0156],
        [0.0011, 0.0141, 0.0000,  ..., 0.0020, 0.0006, 0.0046],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63855.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0341, 0.0000, 0.0000,  ..., 0.0451, 0.0000, 0.0605],
        [0.0219, 0.0000, 0.0000,  ..., 0.0303, 0.0000, 0.0504],
        [0.0091, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0388],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600581.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2588.3354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-370.3173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(189.1594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.0743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2515],
        [ 0.1891],
        [ 0.0695],
        ...,
        [-0.9854],
        [-0.9825],
        [-0.9815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187683., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0111],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368715.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368726.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2828.9116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6532, device='cuda:0')



h[100].sum tensor(-33.4933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0104, device='cuda:0')



h[200].sum tensor(-42.4081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.8011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61882.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0256],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589778.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2514.7554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-377.4059, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79.9986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.6555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.3189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4394],
        [-0.6847],
        [-0.8724],
        ...,
        [-0.9895],
        [-0.9866],
        [-0.9856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186464.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368726.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368726.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2973.4995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8656, device='cuda:0')



h[100].sum tensor(-32.2103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-41.3624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66986.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.5615e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.5940e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.6755e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.7940e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.9842e-05, 0.0000e+00,
         3.0117e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9981e-04, 0.0000e+00,
         3.1207e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619569.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2760.0364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-355.3601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(264.8987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.8489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.3150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0062],
        [-0.9018],
        [-0.7394],
        ...,
        [-0.8823],
        [-0.8033],
        [-0.7504]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166530.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368726.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(929.7847, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368738.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [ 0.0015,  0.0066, -0.0048,  ...,  0.0031,  0.0009,  0.0059],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2718.5459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4244, device='cuda:0')



h[100].sum tensor(-34.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0086, device='cuda:0')



h[200].sum tensor(-43.3407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.5852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0015, 0.0148, 0.0000,  ..., 0.0031, 0.0009, 0.0059],
        [0.0061, 0.0240, 0.0000,  ..., 0.0135, 0.0038, 0.0211],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60030.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0340],
        [0.0146, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0423],
        [0.0423, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.0619],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587296.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2533.5212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-384.1294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93.3714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.1819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.1216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1112],
        [ 0.1658],
        [ 0.2187],
        ...,
        [-0.9886],
        [-0.9857],
        [-0.9847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187042.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0112],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368738.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368749.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0144, -0.0144,  ...,  0.0141,  0.0039,  0.0199],
        [ 0.0055,  0.0134, -0.0131,  ...,  0.0127,  0.0035,  0.0181],
        [ 0.0052,  0.0128, -0.0123,  ...,  0.0117,  0.0033,  0.0169],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3068.3877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5004, device='cuda:0')



h[100].sum tensor(-31.4288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0137, device='cuda:0')



h[200].sum tensor(-40.9453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.8138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0354, 0.0000,  ..., 0.0273, 0.0077, 0.0406],
        [0.0174, 0.0457, 0.0000,  ..., 0.0393, 0.0110, 0.0578],
        [0.0191, 0.0487, 0.0000,  ..., 0.0434, 0.0121, 0.0631],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68563.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0894, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.0945],
        [0.1013, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.1029],
        [0.1053, 0.0000, 0.0000,  ..., 0.1293, 0.0000, 0.1061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629663.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2925.4519, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-347.6778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(295.5009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.6071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.1606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2236],
        [ 0.2257],
        [ 0.2296],
        ...,
        [-0.9914],
        [-0.9885],
        [-0.9876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170264.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368749.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368761.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0086, -0.0072,  ...,  0.0059,  0.0017,  0.0094],
        [ 0.0008,  0.0053, -0.0031,  ...,  0.0013,  0.0004,  0.0035],
        [ 0.0007,  0.0052, -0.0030,  ...,  0.0012,  0.0004,  0.0034],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2894.1812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5872, device='cuda:0')



h[100].sum tensor(-33.0099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0109, device='cuda:0')



h[200].sum tensor(-42.3211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.4420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0225, 0.0000,  ..., 0.0068, 0.0022, 0.0163],
        [0.0064, 0.0269, 0.0000,  ..., 0.0130, 0.0039, 0.0242],
        [0.0047, 0.0241, 0.0000,  ..., 0.0090, 0.0028, 0.0191],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63981.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0374, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0659],
        [0.0402, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.0677],
        [0.0350, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0638],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0271]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605525.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2744.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-366.3518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(171.1356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.3854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.4181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2938],
        [ 0.2946],
        [ 0.2968],
        ...,
        [-0.9947],
        [-0.9918],
        [-0.9908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175267.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368761.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0114],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368772.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0110, -0.0102,  ...,  0.0093,  0.0026,  0.0138],
        [ 0.0015,  0.0065, -0.0046,  ...,  0.0029,  0.0009,  0.0056],
        [ 0.0017,  0.0069, -0.0051,  ...,  0.0035,  0.0010,  0.0063],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.2856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7104, device='cuda:0')



h[100].sum tensor(-32.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-42.0517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0314, 0.0000,  ..., 0.0194, 0.0056, 0.0323],
        [0.0102, 0.0333, 0.0000,  ..., 0.0220, 0.0064, 0.0357],
        [0.0056, 0.0242, 0.0000,  ..., 0.0115, 0.0034, 0.0204],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65005.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0000, 0.0000,  ..., 0.0552, 0.0000, 0.0648],
        [0.0423, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.0647],
        [0.0338, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0586],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610610.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2809.2778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-360.7991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(155.3527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.8579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.9833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2799],
        [ 0.2620],
        [ 0.1941],
        ...,
        [-0.9991],
        [-0.9962],
        [-0.9952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163276.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0114],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368772.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0114],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368783.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0071, -0.0053,  ...,  0.0038,  0.0011,  0.0067],
        [ 0.0009,  0.0056, -0.0035,  ...,  0.0016,  0.0005,  0.0040],
        [ 0.0006,  0.0049, -0.0027,  ...,  0.0008,  0.0003,  0.0028],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3231.0405, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3634, device='cuda:0')



h[100].sum tensor(-30.1146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0158, device='cuda:0')



h[200].sum tensor(-40.1042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.4648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0259, 0.0000,  ..., 0.0140, 0.0041, 0.0235],
        [0.0053, 0.0238, 0.0000,  ..., 0.0109, 0.0033, 0.0196],
        [0.0103, 0.0336, 0.0000,  ..., 0.0223, 0.0065, 0.0360],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72247.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0000, 0.0000,  ..., 0.0863, 0.0000, 0.0823],
        [0.0526, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0737],
        [0.0586, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.0785],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646427.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3129.4194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-333.6065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(441.3508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.6984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.3811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2306],
        [ 0.2477],
        [ 0.2579],
        ...,
        [-1.0052],
        [-1.0023],
        [-1.0013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180268.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0114],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368783.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0115],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2812.7051, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8147, device='cuda:0')



h[100].sum tensor(-33.7627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-43.1628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.5394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0265, 0.0000,  ..., 0.0170, 0.0049, 0.0255],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61497.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0600],
        [0.0111, 0.0000, 0.0000,  ..., 0.0160, 0.0000, 0.0390],
        [0.0019, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0319],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595061.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2705.7241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-377.2654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60.1531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.2125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.9275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2187],
        [ 0.0805],
        [-0.1380],
        ...,
        [-1.0108],
        [-1.0074],
        [-1.0056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186664.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0115],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0116],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368806.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        ...,
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0027,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3223.4507, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4329, device='cuda:0')



h[100].sum tensor(-30.2645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-40.3203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.8263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0031, 0.0189, 0.0000,  ..., 0.0063, 0.0019, 0.0118],
        ...,
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73147.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0303],
        [0.0056, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0342],
        [0.0207, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.0492],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658899.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3270.7859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-327.9124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(329.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.4904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.7321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4227],
        [-0.2888],
        [-0.0413],
        ...,
        [-1.0169],
        [-1.0140],
        [-1.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142236.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0116],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368806.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0117],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368817.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0007,  0.0053, -0.0030,  ...,  0.0012,  0.0004,  0.0034],
        [ 0.0008,  0.0054, -0.0032,  ...,  0.0013,  0.0005,  0.0036],
        [ 0.0023,  0.0079, -0.0062,  ...,  0.0048,  0.0014,  0.0080],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3342.4067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2720, device='cuda:0')



h[100].sum tensor(-29.2716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0169, device='cuda:0')



h[200].sum tensor(-39.5473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.7746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0196, 0.0000,  ..., 0.0073, 0.0022, 0.0130],
        [0.0072, 0.0284, 0.0000,  ..., 0.0149, 0.0045, 0.0266],
        [0.0053, 0.0252, 0.0000,  ..., 0.0103, 0.0033, 0.0207],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76562.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0355, 0.0000, 0.0000,  ..., 0.0482, 0.0000, 0.0616],
        [0.0378, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0651],
        [0.0402, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0669],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683938.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3485.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.9100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(614.0005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.0051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.0575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2883],
        [ 0.3005],
        [ 0.3059],
        ...,
        [-1.0229],
        [-1.0196],
        [-1.0182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169735.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0117],
        ...,
        [1.0027],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368817.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0117],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368828.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [ 0.0021,  0.0077, -0.0060,  ...,  0.0046,  0.0014,  0.0077],
        [ 0.0055,  0.0134, -0.0129,  ...,  0.0125,  0.0036,  0.0179],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3071.3955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9261, device='cuda:0')



h[100].sum tensor(-31.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-41.5850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.7334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0235, 0.0000,  ..., 0.0127, 0.0037, 0.0200],
        [0.0105, 0.0327, 0.0000,  ..., 0.0233, 0.0067, 0.0354],
        [0.0078, 0.0283, 0.0000,  ..., 0.0170, 0.0050, 0.0274],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67530.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0445, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0660],
        [0.0592, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.0763],
        [0.0548, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0739],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627945.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3064.3484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-353.3099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(163.3274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.0266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.4656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2670],
        [ 0.2648],
        [ 0.2605],
        ...,
        [-1.0279],
        [-1.0250],
        [-1.0240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164539.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0117],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368828.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0118],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368839.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [ 0.0021,  0.0076, -0.0058,  ...,  0.0044,  0.0013,  0.0075],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3333.4351, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5481, device='cuda:0')



h[100].sum tensor(-29.4316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0165, device='cuda:0')



h[200].sum tensor(-39.8302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.2778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0286, 0.0000,  ..., 0.0153, 0.0047, 0.0269],
        [0.0016, 0.0151, 0.0000,  ..., 0.0032, 0.0010, 0.0059],
        [0.0021, 0.0160, 0.0000,  ..., 0.0045, 0.0013, 0.0076],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73734.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0260, 0.0000, 0.0000,  ..., 0.0372, 0.0000, 0.0542],
        [0.0133, 0.0000, 0.0000,  ..., 0.0213, 0.0000, 0.0439],
        [0.0198, 0.0000, 0.0000,  ..., 0.0291, 0.0000, 0.0486],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659170.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3326.2493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-328.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(406.7098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.6101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.9757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2604],
        [ 0.2658],
        [ 0.2673],
        ...,
        [-1.0318],
        [-1.0289],
        [-1.0279]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170304.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0118],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368839.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(521.7401, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0119],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368850.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3151.5747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3838, device='cuda:0')



h[100].sum tensor(-31.0580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-41.2374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.7338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69845.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0298]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642677.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3195.6921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-342.9182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(256.7318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.4815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.2700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9782],
        [-1.0719],
        [-1.1441],
        ...,
        [-1.0209],
        [-0.9910],
        [-0.9361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164187.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0119],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368850.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0120],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368861.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0114, -0.0105,  ...,  0.0098,  0.0028,  0.0144],
        [ 0.0023,  0.0079, -0.0063,  ...,  0.0049,  0.0015,  0.0081],
        [ 0.0008,  0.0053, -0.0031,  ...,  0.0012,  0.0005,  0.0034],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2921.5537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0862, device='cuda:0')



h[100].sum tensor(-33.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-42.9577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.4119, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0361, 0.0000,  ..., 0.0258, 0.0076, 0.0403],
        [0.0114, 0.0355, 0.0000,  ..., 0.0248, 0.0073, 0.0391],
        [0.0097, 0.0327, 0.0000,  ..., 0.0208, 0.0062, 0.0340],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63006.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0865, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.0965],
        [0.0778, 0.0000, 0.0000,  ..., 0.1011, 0.0000, 0.0914],
        [0.0710, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.0877],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606760.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2870.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-370.4876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117.2121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.8776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.5924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2845],
        [ 0.2933],
        [ 0.2980],
        ...,
        [-1.0379],
        [-1.0350],
        [-1.0340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171614.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0120],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368861.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368872.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0098, -0.0084,  ...,  0.0075,  0.0022,  0.0114],
        [ 0.0036,  0.0101, -0.0089,  ...,  0.0080,  0.0023,  0.0120],
        [ 0.0051,  0.0128, -0.0121,  ...,  0.0117,  0.0033,  0.0167],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3314.5850, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0737, device='cuda:0')



h[100].sum tensor(-29.6190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(-40.2560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.2660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0498, 0.0000,  ..., 0.0449, 0.0129, 0.0646],
        [0.0147, 0.0400, 0.0000,  ..., 0.0335, 0.0096, 0.0482],
        [0.0077, 0.0268, 0.0000,  ..., 0.0172, 0.0050, 0.0257],
        ...,
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0148, 0.0000,  ..., 0.0021, 0.0007, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73805.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1223, 0.0000, 0.0000,  ..., 0.1548, 0.0000, 0.1158],
        [0.0911, 0.0000, 0.0000,  ..., 0.1168, 0.0000, 0.0956],
        [0.0582, 0.0000, 0.0000,  ..., 0.0764, 0.0000, 0.0740],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282],
        [0.0003, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0309],
        [0.0091, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664216.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3310.4526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-323.2860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(390.8912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(161.3552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.8777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2825],
        [ 0.2969],
        [ 0.3116],
        ...,
        [-0.9068],
        [-0.6854],
        [-0.3531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143341.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368872.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368872.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0101, -0.0089,  ...,  0.0080,  0.0023,  0.0121],
        [ 0.0018,  0.0071, -0.0052,  ...,  0.0037,  0.0011,  0.0065],
        [ 0.0008,  0.0053, -0.0031,  ...,  0.0012,  0.0005,  0.0034],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3021.7007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5689, device='cuda:0')



h[100].sum tensor(-32.1563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-42.3154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1157, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0098, 0.0328, 0.0000,  ..., 0.0210, 0.0063, 0.0342],
        [0.0074, 0.0288, 0.0000,  ..., 0.0154, 0.0048, 0.0271],
        [0.0031, 0.0215, 0.0000,  ..., 0.0050, 0.0019, 0.0138],
        ...,
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65769.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0672],
        [0.0377, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0647],
        [0.0293, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0606],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621157.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2965.1470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-358.3961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(162.1111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.9740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.9163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3310],
        [ 0.3399],
        [ 0.3330],
        ...,
        [-1.0448],
        [-1.0419],
        [-1.0409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170965.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368872.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368884.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [ 0.0034,  0.0098, -0.0084,  ...,  0.0075,  0.0022,  0.0113],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3094.6841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7045, device='cuda:0')



h[100].sum tensor(-31.6207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-41.8120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.5813, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0043, 0.0210, 0.0000,  ..., 0.0090, 0.0027, 0.0152],
        [0.0090, 0.0304, 0.0000,  ..., 0.0198, 0.0058, 0.0308],
        ...,
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67197.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0344],
        [0.0272, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.0525],
        [0.0555, 0.0000, 0.0000,  ..., 0.0734, 0.0000, 0.0728],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630774.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2996.7478, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-353.2052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(137.4742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.2097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-223.7104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4743],
        [-0.1732],
        [ 0.0905],
        ...,
        [-1.0560],
        [-1.0529],
        [-1.0518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166915.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0121],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368884.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0122],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368895.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0087, -0.0071,  ...,  0.0059,  0.0017,  0.0094],
        [ 0.0024,  0.0081, -0.0063,  ...,  0.0050,  0.0015,  0.0083],
        [ 0.0024,  0.0081, -0.0064,  ...,  0.0051,  0.0015,  0.0084],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0023, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3246.7056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5974, device='cuda:0')



h[100].sum tensor(-30.5814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-40.7490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2529, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0345, 0.0000,  ..., 0.0232, 0.0069, 0.0370],
        [0.0062, 0.0269, 0.0000,  ..., 0.0125, 0.0039, 0.0234],
        [0.0065, 0.0275, 0.0000,  ..., 0.0132, 0.0041, 0.0244],
        ...,
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73290.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0611, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.0769],
        [0.0424, 0.0000, 0.0000,  ..., 0.0578, 0.0000, 0.0660],
        [0.0352, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0613],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670812.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3298.0266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-332.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(520.9591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.5388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.5544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1964],
        [ 0.1416],
        [ 0.0812],
        ...,
        [-1.0670],
        [-1.0640],
        [-1.0628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175656.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0122],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368895.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0123],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368906.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0059, -0.0037,  ...,  0.0020,  0.0007,  0.0044],
        [ 0.0005,  0.0049, -0.0024,  ...,  0.0005,  0.0003,  0.0025],
        [ 0.0023,  0.0079, -0.0061,  ...,  0.0048,  0.0014,  0.0080],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2926.0059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0963, device='cuda:0')



h[100].sum tensor(-33.7564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-43.0479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.4189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0198, 0.0000,  ..., 0.0072, 0.0022, 0.0130],
        [0.0097, 0.0329, 0.0000,  ..., 0.0208, 0.0062, 0.0341],
        [0.0052, 0.0241, 0.0000,  ..., 0.0107, 0.0033, 0.0194],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63215.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0263, 0.0000, 0.0000,  ..., 0.0377, 0.0000, 0.0539],
        [0.0437, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0675],
        [0.0371, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0630],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615967.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2841.6357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-378.1620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(121.7400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.0342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.3078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1763],
        [ 0.2778],
        [ 0.2903],
        ...,
        [-1.0748],
        [-1.0718],
        [-1.0707]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206457.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0123],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368906.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0123],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368917.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0089, -0.0073,  ...,  0.0062,  0.0018,  0.0098],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        ...,
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0007,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3395.1284, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6408, device='cuda:0')



h[100].sum tensor(-30.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0166, device='cuda:0')



h[200].sum tensor(-39.8655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.3414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0164, 0.0000,  ..., 0.0046, 0.0014, 0.0078],
        [0.0028, 0.0175, 0.0000,  ..., 0.0062, 0.0018, 0.0099],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75797.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0000, 0.0000,  ..., 0.0354, 0.0000, 0.0506],
        [0.0147, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0428],
        [0.0071, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0374],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685822.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3448.6255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-330.9142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(561.8735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.6468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.2346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2527],
        [ 0.2490],
        [ 0.2469],
        ...,
        [-1.0808],
        [-1.0778],
        [-1.0767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187641.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0123],
        ...,
        [1.0027],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368917.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0124],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368929.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3143.6948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7961, device='cuda:0')



h[100].sum tensor(-32.7315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-41.7306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.6442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0139, 0.0000,  ..., 0.0011, 0.0004, 0.0034],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68880.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0289],
        [0.0050, 0.0000, 0.0000,  ..., 0.0086, 0.0000, 0.0355],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645488.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3167.4014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-360.0311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(179.1858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.9257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.0106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0469],
        [-0.8349],
        [-0.5315],
        ...,
        [-1.0837],
        [-1.0808],
        [-1.0797]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181258.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0124],
        ...,
        [1.0028],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368929.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0125],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368940.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [ 0.0023,  0.0079, -0.0061,  ...,  0.0048,  0.0014,  0.0081],
        [ 0.0010,  0.0058, -0.0036,  ...,  0.0018,  0.0006,  0.0043],
        ...,
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3296.7896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7650, device='cuda:0')



h[100].sum tensor(-31.7058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0150, device='cuda:0')



h[200].sum tensor(-40.7951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0234, 0.0000,  ..., 0.0075, 0.0025, 0.0174],
        [0.0020, 0.0186, 0.0000,  ..., 0.0030, 0.0011, 0.0097],
        [0.0081, 0.0304, 0.0000,  ..., 0.0171, 0.0051, 0.0298],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73389.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0000, 0.0000,  ..., 0.0309, 0.0000, 0.0521],
        [0.0246, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0545],
        [0.0479, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0710],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673186.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3445.9780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-343.1947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(350.7513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.2459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.2334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3011],
        [ 0.2934],
        [ 0.2875],
        ...,
        [-1.0847],
        [-1.0817],
        [-1.0806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169413.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0125],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368940.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(471.4366, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0125],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368951.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2972.6248, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9127, device='cuda:0')



h[100].sum tensor(-34.5839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0100, device='cuda:0')



h[200].sum tensor(-43.1633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.2929, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63359.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618416.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3000.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-385.4080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56.1057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.9768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5927],
        [-0.5653],
        [-0.4772],
        ...,
        [-1.0845],
        [-1.0815],
        [-1.0804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184389.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0125],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368951.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0076],
        [1.0126],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368962.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3836.1079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.5437, device='cuda:0')



h[100].sum tensor(-27.1772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0226, device='cuda:0')



h[200].sum tensor(-37.2790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-27.8238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85248.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0263],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733148.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3916.2954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.3965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1024.5865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.0770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-283.2916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0948],
        [-1.1321],
        [-1.1337],
        ...,
        [-1.0877],
        [-1.0847],
        [-1.0836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176676.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0076],
        [1.0126],
        ...,
        [1.0028],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368962.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0076],
        [1.0127],
        ...,
        [1.0028],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368973.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [ 0.0008,  0.0054, -0.0031,  ...,  0.0013,  0.0005,  0.0036],
        ...,
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0028,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2874.7397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5982, device='cuda:0')



h[100].sum tensor(-35.2743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-43.9711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.7045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0140, 0.0000,  ..., 0.0013, 0.0005, 0.0036],
        [0.0033, 0.0196, 0.0000,  ..., 0.0067, 0.0021, 0.0124],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61148.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0313],
        [0.0120, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0414],
        [0.0260, 0.0000, 0.0000,  ..., 0.0373, 0.0000, 0.0547],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610318.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2844.6687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-397.0077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(69.4057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.3286, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.0297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0607],
        [ 0.1780],
        [ 0.2727],
        ...,
        [-1.0961],
        [-1.0930],
        [-1.0919]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194194.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0076],
        [1.0127],
        ...,
        [1.0028],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368973.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0077],
        [1.0128],
        ...,
        [1.0029],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368984.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0140, -0.0133,  ...,  0.0133,  0.0038,  0.0189],
        [ 0.0048,  0.0123, -0.0113,  ...,  0.0110,  0.0031,  0.0159],
        [ 0.0046,  0.0119, -0.0108,  ...,  0.0104,  0.0030,  0.0152],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3084.9612, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7946, device='cuda:0')



h[100].sum tensor(-33.5835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-42.5677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0144, 0.0410, 0.0000,  ..., 0.0321, 0.0093, 0.0487],
        [0.0211, 0.0524, 0.0000,  ..., 0.0482, 0.0138, 0.0692],
        [0.0138, 0.0401, 0.0000,  ..., 0.0308, 0.0090, 0.0471],
        ...,
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66502.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0000, 0.0000,  ..., 0.0927, 0.0000, 0.0849],
        [0.0835, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0935],
        [0.0699, 0.0000, 0.0000,  ..., 0.0930, 0.0000, 0.0847],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636405.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3022.4761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-375.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(159.1239, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.0860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.8980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3418],
        [ 0.3338],
        [ 0.3253],
        ...,
        [-1.1048],
        [-1.1017],
        [-1.1005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186400.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0077],
        [1.0128],
        ...,
        [1.0029],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368984.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0077],
        [1.0128],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368994.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3362.3916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7077, device='cuda:0')



h[100].sum tensor(-31.5143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0155, device='cuda:0')



h[200].sum tensor(-40.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.0149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0178, 0.0000,  ..., 0.0040, 0.0013, 0.0090],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75071.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0312],
        [0.0068, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0375],
        [0.0192, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.0497],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687366.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3410.1169, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-340.4625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(367.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.2351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.5464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1568],
        [ 0.0798],
        [ 0.2234],
        ...,
        [-1.1163],
        [-1.1132],
        [-1.1119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153201.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0077],
        [1.0128],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368994.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0078],
        [1.0129],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369005.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0111, -0.0097,  ...,  0.0092,  0.0026,  0.0137],
        [ 0.0020,  0.0076, -0.0056,  ...,  0.0042,  0.0013,  0.0074],
        [ 0.0032,  0.0095, -0.0079,  ...,  0.0070,  0.0020,  0.0110],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [ 0.0016,  0.0069, -0.0048,  ...,  0.0033,  0.0010,  0.0062],
        [ 0.0016,  0.0069, -0.0048,  ...,  0.0033,  0.0010,  0.0062]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3114.7588, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2629, device='cuda:0')



h[100].sum tensor(-34.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-42.3992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0230, 0.0000,  ..., 0.0089, 0.0028, 0.0174],
        [0.0114, 0.0361, 0.0000,  ..., 0.0250, 0.0073, 0.0400],
        [0.0060, 0.0257, 0.0000,  ..., 0.0126, 0.0038, 0.0221],
        ...,
        [0.0029, 0.0196, 0.0000,  ..., 0.0058, 0.0018, 0.0116],
        [0.0029, 0.0196, 0.0000,  ..., 0.0058, 0.0018, 0.0116],
        [0.0029, 0.0196, 0.0000,  ..., 0.0058, 0.0018, 0.0116]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66408.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0354, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0617],
        [0.0452, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0682],
        [0.0356, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0616],
        ...,
        [0.0123, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0445],
        [0.0160, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0480],
        [0.0160, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636920., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2996.2859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-381.0596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(195.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.4592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2742],
        [ 0.2774],
        [ 0.2742],
        ...,
        [-0.5328],
        [-0.3843],
        [-0.3835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200377.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0078],
        [1.0129],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369005.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0078],
        [1.0129],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369016.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2993.4595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0040, device='cuda:0')



h[100].sum tensor(-35.6154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-43.2828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.3555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63489.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0290],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625903.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2919.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-394.2185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(120.9106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.9290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-223.8298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0775],
        [-0.9190],
        [-0.6836],
        ...,
        [-1.1263],
        [-1.1233],
        [-1.1222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208370.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0078],
        [1.0129],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369016.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0079],
        [1.0130],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369027.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        ...,
        [ 0.0004,  0.0049, -0.0024,  ...,  0.0005,  0.0002,  0.0026],
        [ 0.0012,  0.0063, -0.0040,  ...,  0.0024,  0.0008,  0.0051],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3133.2078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3572, device='cuda:0')



h[100].sum tensor(-34.7321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0119, device='cuda:0')



h[200].sum tensor(-42.3868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.6567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0081, 0.0312, 0.0000,  ..., 0.0171, 0.0051, 0.0303],
        [0.0024, 0.0187, 0.0000,  ..., 0.0045, 0.0015, 0.0100],
        [0.0013, 0.0155, 0.0000,  ..., 0.0025, 0.0008, 0.0053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68866.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0330],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        ...,
        [0.0409, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0679],
        [0.0239, 0.0000, 0.0000,  ..., 0.0362, 0.0000, 0.0540],
        [0.0104, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661896.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3225.5869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-371.7121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(290.8123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.8926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.9618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1823],
        [-0.5368],
        [-0.8414],
        ...,
        [ 0.2168],
        [ 0.0112],
        [-0.3370]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187466.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0079],
        [1.0130],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369027.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0079],
        [1.0130],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369038.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2933.3916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1817, device='cuda:0')



h[100].sum tensor(-36.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-43.8067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.1050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62336.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0324],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0295],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622243.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2883.2146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-398.5967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50.8935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.6891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.3187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9643],
        [-0.8087],
        [-0.5706],
        ...,
        [-0.7494],
        [-0.9170],
        [-1.0377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190045.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0079],
        [1.0130],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369038.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0080],
        [1.0131],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369049.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [ 0.0028,  0.0089, -0.0071,  ...,  0.0061,  0.0018,  0.0097]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3787.9517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5901, device='cuda:0')



h[100].sum tensor(-29.5354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0210, device='cuda:0')



h[200].sum tensor(-38.0524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-25.7968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0029, 0.0182, 0.0000,  ..., 0.0063, 0.0019, 0.0101],
        [0.0051, 0.0233, 0.0000,  ..., 0.0110, 0.0033, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85622.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0297],
        [0.0092, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0384],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0331],
        [0.0163, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0451],
        [0.0347, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0597]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750992.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3895.9858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-302.0064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(747.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(175.0760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-284.7808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7780],
        [-0.4740],
        [-0.0866],
        ...,
        [-0.7500],
        [-0.4086],
        [-0.0657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159568.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0080],
        [1.0131],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369049.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(521.0114, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0080],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369060.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.3232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5203, device='cuda:0')



h[100].sum tensor(-36.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0087, device='cuda:0')



h[200].sum tensor(-44.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.6511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.9671e-04, 1.4576e-02, 0.0000e+00,  ..., 0.0000e+00, 4.2722e-05,
         3.2671e-03],
        [9.8525e-05, 1.3093e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1398e-05,
         1.6364e-03],
        [6.3628e-03, 2.5103e-02, 0.0000e+00,  ..., 1.4189e-02, 4.1118e-03,
         2.2112e-02],
        ...,
        [0.0000e+00, 1.1988e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.1988e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.1988e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61372.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0396],
        [0.0107, 0.0000, 0.0000,  ..., 0.0186, 0.0000, 0.0435],
        [0.0336, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0604],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618018.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2773.0591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-405.9088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81.0272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.5907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.7499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2605],
        [ 0.0444],
        [ 0.2433],
        ...,
        [-1.1398],
        [-1.1367],
        [-1.1356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209627.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0080],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369060.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369070.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3671.6611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6781, device='cuda:0')



h[100].sum tensor(-30.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0194, device='cuda:0')



h[200].sum tensor(-38.8755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-23.7984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0198, 0.0000,  ..., 0.0067, 0.0020, 0.0125],
        [0.0034, 0.0201, 0.0000,  ..., 0.0071, 0.0021, 0.0131],
        [0.0009, 0.0146, 0.0000,  ..., 0.0017, 0.0006, 0.0042],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80613.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0000, 0.0000,  ..., 0.0287, 0.0000, 0.0494],
        [0.0165, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.0494],
        [0.0088, 0.0000, 0.0000,  ..., 0.0168, 0.0000, 0.0417],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723449.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3582.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-323.6018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(542.9669, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(172.5013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.2221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1194],
        [-0.1721],
        [-0.3727],
        ...,
        [-1.1484],
        [-1.1453],
        [-1.1440]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145767.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369070.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369070.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0055, -0.0031,  ...,  0.0013,  0.0004,  0.0036],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3613.0244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8728, device='cuda:0')



h[100].sum tensor(-31.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0184, device='cuda:0')



h[200].sum tensor(-39.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.5595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0202, 0.0000,  ..., 0.0073, 0.0022, 0.0134],
        [0.0008, 0.0143, 0.0000,  ..., 0.0013, 0.0005, 0.0037],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82802.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0209, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0527],
        [0.0107, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0437],
        [0.0072, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0399],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(747629., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3767.5181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-319.4495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(969.4782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.9306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-277.6250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2806],
        [ 0.2127],
        [ 0.1414],
        ...,
        [-1.1450],
        [-1.1418],
        [-1.1405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179969.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0132],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369070.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0133],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369081.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0077, -0.0057,  ...,  0.0044,  0.0013,  0.0076],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0024, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3159.2249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7012, device='cuda:0')



h[100].sum tensor(-34.8998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-42.3095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.8928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0339, 0.0000,  ..., 0.0241, 0.0069, 0.0367],
        [0.0032, 0.0199, 0.0000,  ..., 0.0067, 0.0020, 0.0125],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69363.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0719, 0.0000, 0.0000,  ..., 0.0999, 0.0000, 0.0866],
        [0.0476, 0.0000, 0.0000,  ..., 0.0685, 0.0000, 0.0697],
        [0.0314, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0575],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664617.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3065.6960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-370.3087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(153.2195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.7620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.8143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3289],
        [ 0.3325],
        [ 0.3365],
        ...,
        [-1.1581],
        [-1.1549],
        [-1.1537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162996.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0081],
        [1.0133],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369081.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0082],
        [1.0134],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369091.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3621.1587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4232, device='cuda:0')



h[100].sum tensor(-31.0988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0187, device='cuda:0')



h[200].sum tensor(-39.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.9372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83998.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(766702.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3861.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.9052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1075.1936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(166.0731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-281.2176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5452],
        [-0.7730],
        [-0.9824],
        ...,
        [-1.1721],
        [-1.1689],
        [-1.1677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198448.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0082],
        [1.0134],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369091.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0082],
        [1.0134],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369102.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0051, -0.0025,  ...,  0.0005,  0.0002,  0.0027],
        [ 0.0027,  0.0088, -0.0069,  ...,  0.0058,  0.0017,  0.0095],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3156.2295, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1542, device='cuda:0')



h[100].sum tensor(-35.0303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0124, device='cuda:0')



h[200].sum tensor(-42.2022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.2037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0434, 0.0000,  ..., 0.0346, 0.0097, 0.0525],
        [0.0035, 0.0205, 0.0000,  ..., 0.0073, 0.0021, 0.0135],
        [0.0027, 0.0178, 0.0000,  ..., 0.0059, 0.0017, 0.0096],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67923.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0000, 0.0000,  ..., 0.0810, 0.0000, 0.0783],
        [0.0304, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0576],
        [0.0152, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0448],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656875.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2960.2605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-381.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(129.7581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.1861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.6833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2844],
        [ 0.1683],
        [-0.0624],
        ...,
        [-1.1769],
        [-1.1773],
        [-1.1768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191520.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0082],
        [1.0134],
        ...,
        [1.0029],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369102.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0135],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369113.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [ 0.0011,  0.0062, -0.0038,  ...,  0.0021,  0.0006,  0.0048],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3403.5610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4700, device='cuda:0')



h[100].sum tensor(-33.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0159, device='cuda:0')



h[200].sum tensor(-40.5218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.5380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0237, 0.0000,  ..., 0.0067, 0.0021, 0.0170],
        [0.0008, 0.0145, 0.0000,  ..., 0.0013, 0.0004, 0.0037],
        [0.0049, 0.0230, 0.0000,  ..., 0.0106, 0.0030, 0.0177],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72550.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0445],
        [0.0139, 0.0000, 0.0000,  ..., 0.0234, 0.0000, 0.0448],
        [0.0345, 0.0000, 0.0000,  ..., 0.0486, 0.0000, 0.0590],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674870.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3131.6838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-365.3629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(276.7820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.3066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.9442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4876],
        [-0.2083],
        [ 0.0537],
        ...,
        [-1.1897],
        [-1.1864],
        [-1.1851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200584.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0135],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369113.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0083],
        [1.0135],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369124.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009,  0.0058, -0.0034,  ...,  0.0016,  0.0005,  0.0041],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3101.1194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2992, device='cuda:0')



h[100].sum tensor(-35.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-42.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.9306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0207, 0.0000,  ..., 0.0075, 0.0022, 0.0138],
        [0.0009, 0.0147, 0.0000,  ..., 0.0016, 0.0005, 0.0041],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67564.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0424, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0657],
        [0.0220, 0.0000, 0.0000,  ..., 0.0310, 0.0000, 0.0492],
        [0.0108, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0396],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659688.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3046.2593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-386.4864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(186.1888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.0825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.7350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1486],
        [ 0.1447],
        [ 0.1395],
        ...,
        [-1.1873],
        [-1.1839],
        [-1.1826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196741.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0083],
        [1.0135],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369124.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0084],
        [1.0136],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369135., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3288.0227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4319, device='cuda:0')



h[100].sum tensor(-33.9358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0136, device='cuda:0')



h[200].sum tensor(-41.4773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.7668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.3611e-03, 1.6804e-02, 0.0000e+00,  ..., 2.4615e-03, 7.2848e-04,
         6.8253e-03],
        [8.1424e-05, 1.3286e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.5845e-03],
        [8.2186e-04, 1.4595e-02, 0.0000e+00,  ..., 1.3625e-03, 4.2833e-04,
         3.8584e-03],
        ...,
        [0.0000e+00, 1.2189e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2187e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2187e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72757.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0112, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0467],
        [0.0084, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0440],
        [0.0103, 0.0000, 0.0000,  ..., 0.0193, 0.0000, 0.0454],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0291],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0291],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691066.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3366.4292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-364.7828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(349.7286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.9431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.8524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1448],
        [ 0.2085],
        [ 0.2413],
        ...,
        [-1.1892],
        [-1.1859],
        [-1.1847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179350.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0084],
        [1.0136],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369135., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0084],
        [1.0137],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369145.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [ 0.0005,  0.0051, -0.0026,  ...,  0.0006,  0.0002,  0.0028],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3391.8057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5574, device='cuda:0')



h[100].sum tensor(-32.8762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0148, device='cuda:0')



h[200].sum tensor(-40.8865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.2254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0019, 0.0178, 0.0000,  ..., 0.0034, 0.0011, 0.0085],
        [0.0023, 0.0198, 0.0000,  ..., 0.0037, 0.0012, 0.0110],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73803.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0470],
        [0.0167, 0.0000, 0.0000,  ..., 0.0284, 0.0000, 0.0514],
        [0.0234, 0.0000, 0.0000,  ..., 0.0370, 0.0000, 0.0579],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0295],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0295],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691867.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3403.9341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.9368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(326.1164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(158.4151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.5935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2993],
        [ 0.3027],
        [ 0.2990],
        ...,
        [-1.1913],
        [-1.1880],
        [-1.1867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184299.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0084],
        [1.0137],
        ...,
        [1.0029],
        [1.0015],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369145.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(468.2764, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0084],
        [1.0137],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369156.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0142, -0.0132,  ...,  0.0135,  0.0038,  0.0193],
        [ 0.0037,  0.0105, -0.0089,  ...,  0.0082,  0.0023,  0.0126],
        [ 0.0016,  0.0070, -0.0047,  ...,  0.0032,  0.0010,  0.0061],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3279.2881, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5962, device='cuda:0')



h[100].sum tensor(-33.5921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0132, device='cuda:0')



h[200].sum tensor(-41.7291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.1933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0462, 0.0000,  ..., 0.0385, 0.0109, 0.0575],
        [0.0168, 0.0457, 0.0000,  ..., 0.0377, 0.0107, 0.0565],
        [0.0083, 0.0285, 0.0000,  ..., 0.0185, 0.0052, 0.0278],
        ...,
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71226.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0930, 0.0000, 0.0000,  ..., 0.1221, 0.0000, 0.1009],
        [0.0866, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.0969],
        [0.0597, 0.0000, 0.0000,  ..., 0.0811, 0.0000, 0.0785],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680698.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3296.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-372.1634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(277.3941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.8876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.7824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2568],
        [ 0.2579],
        [ 0.2578],
        ...,
        [-1.1943],
        [-1.1910],
        [-1.1897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180671.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0084],
        [1.0137],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369156.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0138],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369166.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.9412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4719, device='cuda:0')



h[100].sum tensor(-32.5051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0148, device='cuda:0')



h[200].sum tensor(-41.0505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.1668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73252.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0301]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690803.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3387.1887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-366.5414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(386.3206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(158.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.9668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3141],
        [-1.2569],
        [-1.1495],
        ...,
        [-1.2010],
        [-1.1976],
        [-1.1964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180681.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0138],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369166.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0139],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3143.3730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8384, device='cuda:0')



h[100].sum tensor(-34.4118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-42.7314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68034.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0330],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0293],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0290],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666265.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3178.1245, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-389.7765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(210.4396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.2523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-231.9936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5248],
        [-0.7932],
        [-0.9500],
        ...,
        [-1.2091],
        [-1.2057],
        [-1.2043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195848.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0139],
        ...,
        [1.0029],
        [1.0014],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0140],
        ...,
        [1.0028],
        [1.0014],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369187.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0068, -0.0045,  ...,  0.0030,  0.0009,  0.0059],
        [ 0.0019,  0.0075, -0.0053,  ...,  0.0039,  0.0012,  0.0070],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3464.3306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6443, device='cuda:0')



h[100].sum tensor(-31.7089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0154, device='cuda:0')



h[200].sum tensor(-40.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.9713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0511, 0.0000,  ..., 0.0453, 0.0129, 0.0661],
        [0.0065, 0.0255, 0.0000,  ..., 0.0143, 0.0041, 0.0224],
        [0.0019, 0.0165, 0.0000,  ..., 0.0039, 0.0012, 0.0071],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75047.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1359, 0.0000, 0.0000,  ..., 0.1754, 0.0000, 0.1263],
        [0.0750, 0.0000, 0.0000,  ..., 0.1006, 0.0000, 0.0866],
        [0.0346, 0.0000, 0.0000,  ..., 0.0499, 0.0000, 0.0596],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701028.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3427.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.0071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(382.5560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.4510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.3291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0889],
        [ 0.1354],
        [ 0.1760],
        ...,
        [-1.2143],
        [-1.2110],
        [-1.2100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185480.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0085],
        [1.0140],
        ...,
        [1.0028],
        [1.0014],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369187.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0141],
        ...,
        [1.0028],
        [1.0014],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3269.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6682, device='cuda:0')



h[100].sum tensor(-33.4036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-41.9614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.5564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0159, 0.0000,  ..., 0.0005, 0.0004, 0.0048],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71662.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0464],
        [0.0238, 0.0000, 0.0000,  ..., 0.0374, 0.0000, 0.0534],
        [0.0340, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.0630],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687875.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3306.9875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-375.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(255.1216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.7760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1937],
        [ 0.1993],
        [ 0.2036],
        ...,
        [-1.2278],
        [-1.2243],
        [-1.2229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181946.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0141],
        ...,
        [1.0028],
        [1.0014],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0141],
        ...,
        [1.0028],
        [1.0013],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369209., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [ 0.0038,  0.0107, -0.0091,  ...,  0.0085,  0.0025,  0.0129],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3166.2568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9199, device='cuda:0')



h[100].sum tensor(-34.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-42.7495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0197, 0.0000,  ..., 0.0086, 0.0025, 0.0130],
        [0.0030, 0.0183, 0.0000,  ..., 0.0065, 0.0019, 0.0104],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66321.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0336],
        [0.0155, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0448],
        [0.0238, 0.0000, 0.0000,  ..., 0.0374, 0.0000, 0.0516],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653478.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3002.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-398.9433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(122.7943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.9345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.4986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0105],
        [-0.7325],
        [-0.4524],
        ...,
        [-1.2313],
        [-1.2280],
        [-1.2267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197411.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0141],
        ...,
        [1.0028],
        [1.0013],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369209., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0087],
        [1.0142],
        ...,
        [1.0028],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369219.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030,  0.0094, -0.0075,  ...,  0.0066,  0.0020,  0.0105],
        [ 0.0022,  0.0080, -0.0059,  ...,  0.0047,  0.0014,  0.0080],
        [ 0.0075,  0.0170, -0.0163,  ...,  0.0174,  0.0049,  0.0243],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3411.0010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9733, device='cuda:0')



h[100].sum tensor(-32.4639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-41.2869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0264, 0.0000,  ..., 0.0156, 0.0045, 0.0240],
        [0.0202, 0.0513, 0.0000,  ..., 0.0456, 0.0131, 0.0666],
        [0.0188, 0.0490, 0.0000,  ..., 0.0423, 0.0121, 0.0623],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72548.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0533, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.0720],
        [0.0970, 0.0000, 0.0000,  ..., 0.1299, 0.0000, 0.1023],
        [0.1140, 0.0000, 0.0000,  ..., 0.1515, 0.0000, 0.1150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0307],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0307],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0307]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688959.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3340.5452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-375.3764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(390.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.8018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.1443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1813],
        [ 0.2616],
        [ 0.2728],
        ...,
        [-1.2315],
        [-1.2281],
        [-1.2268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198621.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0087],
        [1.0142],
        ...,
        [1.0028],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369219.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0087],
        [1.0143],
        ...,
        [1.0028],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369230.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0068, -0.0044,  ...,  0.0029,  0.0009,  0.0058],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [ 0.0004,  0.0049, -0.0023,  ...,  0.0003,  0.0002,  0.0025],
        ...,
        [ 0.0032,  0.0096, -0.0077,  ...,  0.0069,  0.0020,  0.0109],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0006, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3736.7104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0909, device='cuda:0')



h[100].sum tensor(-29.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0179, device='cuda:0')



h[200].sum tensor(-39.2829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0258, 0.0000,  ..., 0.0122, 0.0038, 0.0218],
        [0.0031, 0.0225, 0.0000,  ..., 0.0052, 0.0019, 0.0146],
        [0.0009, 0.0175, 0.0000,  ..., 0.0006, 0.0005, 0.0067],
        ...,
        [0.0025, 0.0179, 0.0000,  ..., 0.0054, 0.0016, 0.0090],
        [0.0033, 0.0192, 0.0000,  ..., 0.0072, 0.0021, 0.0114],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79275.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0515, 0.0000, 0.0000,  ..., 0.0759, 0.0000, 0.0770],
        [0.0360, 0.0000, 0.0000,  ..., 0.0569, 0.0000, 0.0684],
        [0.0232, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0599],
        ...,
        [0.0168, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0491],
        [0.0125, 0.0000, 0.0000,  ..., 0.0227, 0.0000, 0.0449],
        [0.0023, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723235.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3633.8088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-347.6869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(628.0479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.6615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.0997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3120],
        [ 0.3276],
        [ 0.3415],
        ...,
        [-0.4480],
        [-0.6602],
        [-0.9083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182846.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0087],
        [1.0143],
        ...,
        [1.0028],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369230.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0088],
        [1.0144],
        ...,
        [1.0027],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369240.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        ...,
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0029,  0.0000,  ..., -0.0025, -0.0005, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3470.6104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6879, device='cuda:0')



h[100].sum tensor(-32.2033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0143, device='cuda:0')



h[200].sum tensor(-41.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.6287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75181.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0293],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0294],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0296],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712682.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3529.2104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-363.0348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(414.7888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.1268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.9381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3813],
        [-1.3887],
        [-1.3864],
        ...,
        [-1.2347],
        [-1.2314],
        [-1.2301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179140.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0088],
        [1.0144],
        ...,
        [1.0027],
        [1.0013],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369240.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0089],
        [1.0145],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369250.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3768.4055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2118, device='cuda:0')



h[100].sum tensor(-29.8761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0180, device='cuda:0')



h[200].sum tensor(-39.2231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-22.1058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80896.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0294],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0297],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(738316.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3675.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-339.6189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(644.2177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(162.7265, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.3130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2880],
        [-1.2020],
        [-1.0677],
        ...,
        [-1.2439],
        [-1.2405],
        [-1.2391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173058.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0089],
        [1.0145],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369250.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(524.6953, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0089],
        [1.0146],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369260.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [ 0.0013,  0.0064, -0.0040,  ...,  0.0024,  0.0009,  0.0051],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3635.9834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0464, device='cuda:0')



h[100].sum tensor(-30.9622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0162, device='cuda:0')



h[200].sum tensor(-40.1015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.9336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0049, 0.0234, 0.0000,  ..., 0.0104, 0.0033, 0.0175],
        [0.0013, 0.0160, 0.0000,  ..., 0.0025, 0.0009, 0.0053],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78370.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0315],
        [0.0036, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0374],
        [0.0108, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0450],
        ...,
        [0.0411, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0691],
        [0.0189, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0508],
        [0.0038, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(736189.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3555.7517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-348.2402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(473.6205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.1525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-262.4723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1731],
        [ 0.0779],
        [ 0.2201],
        ...,
        [ 0.1946],
        [-0.1451],
        [-0.6029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173591.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0089],
        [1.0146],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369260.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0090],
        [1.0147],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369270.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3251.4678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8805, device='cuda:0')



h[100].sum tensor(-34.0477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-42.6665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0147, 0.0000,  ..., 0.0013, 0.0006, 0.0038],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66579.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0402],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0327],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661656.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2872.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-398.6930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87.5258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.7712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2972],
        [-0.5720],
        [-0.7114],
        ...,
        [-1.2696],
        [-1.2661],
        [-1.2647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209069.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0090],
        [1.0147],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369270.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0091],
        [1.0148],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369281.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0066, -0.0042,  ...,  0.0027,  0.0009,  0.0054],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0054,  0.0133, -0.0119,  ...,  0.0122,  0.0036,  0.0176],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3505.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6714, device='cuda:0')



h[100].sum tensor(-31.9802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0143, device='cuda:0')



h[200].sum tensor(-41.0741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.6174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0151, 0.0000,  ..., 0.0017, 0.0007, 0.0042],
        [0.0087, 0.0321, 0.0000,  ..., 0.0185, 0.0059, 0.0317],
        [0.0043, 0.0222, 0.0000,  ..., 0.0092, 0.0029, 0.0158],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74429.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.0550, 0.0000, 0.0608],
        [0.0444, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.0722],
        [0.0439, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.0713],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706525.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3125.6726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-363.2748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(297.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.4935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.8967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3690],
        [ 0.3743],
        [ 0.3798],
        ...,
        [-1.2799],
        [-1.2763],
        [-1.2749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182297.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0091],
        [1.0148],
        ...,
        [1.0027],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369281.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0149],
        ...,
        [1.0027],
        [1.0011],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369291.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0079, -0.0056,  ...,  0.0044,  0.0014,  0.0077],
        [ 0.0010,  0.0061, -0.0035,  ...,  0.0019,  0.0007,  0.0045],
        [ 0.0006,  0.0054, -0.0028,  ...,  0.0009,  0.0004,  0.0032],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3240.4634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7795, device='cuda:0')



h[100].sum tensor(-34.3768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0105, device='cuda:0')



h[200].sum tensor(-42.9596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.8878, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0273, 0.0000,  ..., 0.0116, 0.0039, 0.0231],
        [0.0050, 0.0260, 0.0000,  ..., 0.0096, 0.0034, 0.0205],
        [0.0014, 0.0173, 0.0000,  ..., 0.0022, 0.0010, 0.0069],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66459.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0619],
        [0.0209, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0566],
        [0.0094, 0.0000, 0.0000,  ..., 0.0236, 0.0000, 0.0459],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0309],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0309],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665055.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2835.9402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-398.1039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97.8675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.4762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.2878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2619],
        [ 0.0469],
        [-0.3003],
        ...,
        [-1.2808],
        [-1.2775],
        [-1.2765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208738.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0149],
        ...,
        [1.0027],
        [1.0011],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369291.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0092],
        [1.0150],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369302.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3513.5483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6673, device='cuda:0')



h[100].sum tensor(-32.4607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-41.3576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.9284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0047, 0.0228, 0.0000,  ..., 0.0100, 0.0031, 0.0170],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73700.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.5387e-05, 0.0000e+00, 0.0000e+00,  ..., 3.7337e-03, 0.0000e+00,
         3.1602e-02],
        [7.2832e-03, 0.0000e+00, 0.0000e+00,  ..., 1.5586e-02, 0.0000e+00,
         3.8659e-02],
        [3.1512e-02, 0.0000e+00, 0.0000e+00,  ..., 5.3511e-02, 0.0000e+00,
         5.9650e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.0761e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.0755e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.0749e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702075., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3172.3096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-366.2430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(304.1100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.0869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.7079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4663],
        [-0.0813],
        [ 0.1674],
        ...,
        [-1.2815],
        [-1.2780],
        [-1.2766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187286.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0092],
        [1.0150],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369302.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369312.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0007,  0.0056, -0.0029,  ...,  0.0011,  0.0005,  0.0035],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3300.0691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2457, device='cuda:0')



h[100].sum tensor(-34.4163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0108, device='cuda:0')



h[200].sum tensor(-42.9246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.2077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0026, 0.0205, 0.0000,  ..., 0.0043, 0.0016, 0.0119],
        [0.0023, 0.0187, 0.0000,  ..., 0.0042, 0.0015, 0.0097],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69091.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0172, 0.0000, 0.0000,  ..., 0.0353, 0.0000, 0.0536],
        [0.0121, 0.0000, 0.0000,  ..., 0.0269, 0.0000, 0.0481],
        [0.0028, 0.0000, 0.0000,  ..., 0.0102, 0.0000, 0.0375],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685943.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3086.1704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-385.2823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(193.1489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.8559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.8872, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1900],
        [ 0.0698],
        [-0.1452],
        ...,
        [-1.2816],
        [-1.2781],
        [-1.2767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202386.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369312.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369312.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3559.3325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4457, device='cuda:0')



h[100].sum tensor(-32.3366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0142, device='cuda:0')



h[200].sum tensor(-41.2382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.4625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0152, 0.0000,  ..., 0.0000, 0.0001, 0.0035],
        [0.0014, 0.0172, 0.0000,  ..., 0.0023, 0.0009, 0.0069],
        [0.0025, 0.0191, 0.0000,  ..., 0.0047, 0.0016, 0.0103],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74429.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0465],
        [0.0103, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0474],
        [0.0128, 0.0000, 0.0000,  ..., 0.0283, 0.0000, 0.0487],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708611.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3251.5337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-363.2320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(411.2602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.3338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2708],
        [ 0.2296],
        [ 0.1600],
        ...,
        [-1.2816],
        [-1.2781],
        [-1.2767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192227.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0011],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369312.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0010],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369323.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0010,  0.0060, -0.0034,  ...,  0.0017,  0.0006,  0.0043],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [ 0.0047,  0.0122, -0.0105,  ...,  0.0105,  0.0030,  0.0156],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.0811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8081, device='cuda:0')



h[100].sum tensor(-33.9398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0116, device='cuda:0')



h[200].sum tensor(-42.5147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.2799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0146, 0.0000,  ..., 0.0009, 0.0004, 0.0033],
        [0.0057, 0.0245, 0.0000,  ..., 0.0123, 0.0037, 0.0201],
        [0.0037, 0.0198, 0.0000,  ..., 0.0082, 0.0024, 0.0127],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69033.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0480],
        [0.0251, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.0546],
        [0.0252, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0534],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675479., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2983.2222, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-382.4295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(153.9290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.9503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.2891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2400],
        [ 0.2463],
        [ 0.1717],
        ...,
        [-1.2824],
        [-1.2789],
        [-1.2775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187513.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0093],
        [1.0151],
        ...,
        [1.0026],
        [1.0010],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369323.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0094],
        [1.0152],
        ...,
        [1.0025],
        [1.0010],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369333.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0009,  0.0059, -0.0033,  ...,  0.0015,  0.0005,  0.0041],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3397.1104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8795, device='cuda:0')



h[100].sum tensor(-34.0296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0117, device='cuda:0')



h[200].sum tensor(-42.5574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.3289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0183, 0.0000,  ..., 0.0035, 0.0012, 0.0089],
        [0.0009, 0.0151, 0.0000,  ..., 0.0015, 0.0005, 0.0041],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70089.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0239, 0.0000, 0.0000,  ..., 0.0403, 0.0000, 0.0544],
        [0.0077, 0.0000, 0.0000,  ..., 0.0166, 0.0000, 0.0410],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0331],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0301],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0301],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0301]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684414.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3022.1272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-377.1010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(176.2417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.0067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.6613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1584],
        [-0.0758],
        [-0.3963],
        ...,
        [-1.2892],
        [-1.2857],
        [-1.2842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180312.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0094],
        [1.0152],
        ...,
        [1.0025],
        [1.0010],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369333.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0094],
        [1.0152],
        ...,
        [1.0025],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369343.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0057, -0.0030,  ...,  0.0012,  0.0004,  0.0037],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011],
        [-0.0008,  0.0030,  0.0000,  ..., -0.0025, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3316.4121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9418, device='cuda:0')



h[100].sum tensor(-34.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-43.1340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0187, 0.0000,  ..., 0.0040, 0.0013, 0.0095],
        [0.0008, 0.0149, 0.0000,  ..., 0.0012, 0.0004, 0.0037],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68419.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0499],
        [0.0066, 0.0000, 0.0000,  ..., 0.0170, 0.0000, 0.0415],
        [0.0016, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0364],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679301.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2931.5486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-383.8858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92.3802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.8683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.3187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2235],
        [ 0.1499],
        [ 0.1002],
        ...,
        [-1.3015],
        [-1.2981],
        [-1.2967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182485.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0094],
        [1.0152],
        ...,
        [1.0025],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369343.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(479.1067, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0095],
        [1.0153],
        ...,
        [1.0025],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369353.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0137, -0.0121,  ...,  0.0125,  0.0035,  0.0182],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [ 0.0031,  0.0097, -0.0075,  ...,  0.0068,  0.0020,  0.0109],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3688.3145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0707, device='cuda:0')



h[100].sum tensor(-31.7607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0157, device='cuda:0')



h[200].sum tensor(-40.7433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.2640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0150, 0.0416, 0.0000,  ..., 0.0337, 0.0095, 0.0498],
        [0.0174, 0.0472, 0.0000,  ..., 0.0389, 0.0111, 0.0587],
        [0.0032, 0.0205, 0.0000,  ..., 0.0063, 0.0019, 0.0125],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76543.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1165, 0.0000, 0.0000,  ..., 0.1564, 0.0000, 0.1117],
        [0.0906, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.0967],
        [0.0468, 0.0000, 0.0000,  ..., 0.0694, 0.0000, 0.0702],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719303.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3194.5874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-356.9121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(403.4522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.3964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.6056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1446],
        [ 0.1883],
        [ 0.2324],
        ...,
        [-1.3191],
        [-1.3154],
        [-1.3139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207363.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0095],
        [1.0153],
        ...,
        [1.0025],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369353.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0096],
        [1.0153],
        ...,
        [1.0025],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369364.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [ 0.0028,  0.0092, -0.0069,  ...,  0.0061,  0.0018,  0.0100],
        [ 0.0028,  0.0092, -0.0069,  ...,  0.0061,  0.0018,  0.0100],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3481.1782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1244, device='cuda:0')



h[100].sum tensor(-33.5711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0129, device='cuda:0')



h[200].sum tensor(-42.1271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.8695, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0028, 0.0186, 0.0000,  ..., 0.0061, 0.0018, 0.0100],
        [0.0050, 0.0236, 0.0000,  ..., 0.0106, 0.0031, 0.0181],
        [0.0173, 0.0471, 0.0000,  ..., 0.0385, 0.0109, 0.0583],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70633.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0445],
        [0.0341, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0584],
        [0.0698, 0.0000, 0.0000,  ..., 0.0968, 0.0000, 0.0818],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0298]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690255.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2914.2874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-381.3346, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(190.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.0336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.0525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2410],
        [-0.0065],
        [ 0.1866],
        ...,
        [-1.3288],
        [-1.3251],
        [-1.3235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197087.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0096],
        [1.0153],
        ...,
        [1.0025],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369364.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0153],
        ...,
        [1.0024],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369374.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0070, -0.0044,  ...,  0.0030,  0.0009,  0.0060],
        [ 0.0069,  0.0161, -0.0147,  ...,  0.0158,  0.0044,  0.0225],
        [ 0.0028,  0.0092, -0.0069,  ...,  0.0060,  0.0017,  0.0099],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3282.0542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1058, device='cuda:0')



h[100].sum tensor(-35.3061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-43.5080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.4254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0199, 0.0513, 0.0000,  ..., 0.0446, 0.0126, 0.0662],
        [0.0140, 0.0415, 0.0000,  ..., 0.0306, 0.0088, 0.0482],
        [0.0143, 0.0421, 0.0000,  ..., 0.0315, 0.0090, 0.0494],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67198.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0772, 0.0000, 0.0000,  ..., 0.1063, 0.0000, 0.0880],
        [0.0804, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0912],
        [0.0799, 0.0000, 0.0000,  ..., 0.1102, 0.0000, 0.0911],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677188.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2800.8960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-397.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(151.9352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.9197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.2202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2248],
        [ 0.2142],
        [ 0.2036],
        ...,
        [-1.3347],
        [-1.3309],
        [-1.3293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216525.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0153],
        ...,
        [1.0024],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369374.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369385.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3845.4795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0019, device='cuda:0')



h[100].sum tensor(-31.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0173, device='cuda:0')



h[200].sum tensor(-40.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.2756, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0152, 0.0000,  ..., 0.0013, 0.0005, 0.0040],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0048, 0.0233, 0.0000,  ..., 0.0100, 0.0029, 0.0174],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80365.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0136, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0475],
        [0.0204, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0505],
        [0.0576, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.0746],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(751650.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3452.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-342.6121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(650.2489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.8139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-275.3423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2510],
        [ 0.2106],
        [ 0.1567],
        ...,
        [-1.3347],
        [-1.3310],
        [-1.3294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203162.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369385.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0097],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369396.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0077, -0.0052,  ...,  0.0040,  0.0012,  0.0073],
        [ 0.0005,  0.0053, -0.0025,  ...,  0.0006,  0.0003,  0.0030],
        [ 0.0035,  0.0104, -0.0083,  ...,  0.0078,  0.0022,  0.0122],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3596.7568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7027, device='cuda:0')



h[100].sum tensor(-33.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0138, device='cuda:0')



h[200].sum tensor(-41.7899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.9527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0275, 0.0000,  ..., 0.0111, 0.0035, 0.0232],
        [0.0084, 0.0320, 0.0000,  ..., 0.0173, 0.0052, 0.0312],
        [0.0052, 0.0267, 0.0000,  ..., 0.0097, 0.0031, 0.0215],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72802.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0460, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0739],
        [0.0496, 0.0000, 0.0000,  ..., 0.0735, 0.0000, 0.0760],
        [0.0471, 0.0000, 0.0000,  ..., 0.0704, 0.0000, 0.0749],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699515.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3032.2388, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-371.5731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(280.1766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.9918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.5162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2849],
        [ 0.2603],
        [ 0.2270],
        ...,
        [-1.3338],
        [-1.3304],
        [-1.3289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202018.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0097],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369396.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369407.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0096, -0.0073,  ...,  0.0066,  0.0019,  0.0107],
        [ 0.0047,  0.0123, -0.0105,  ...,  0.0106,  0.0030,  0.0158],
        [ 0.0040,  0.0112, -0.0092,  ...,  0.0089,  0.0026,  0.0137],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3699.8879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7249, device='cuda:0')



h[100].sum tensor(-32.2565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0149, device='cuda:0')



h[200].sum tensor(-41.2751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.3404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0383, 0.0000,  ..., 0.0263, 0.0077, 0.0427],
        [0.0153, 0.0436, 0.0000,  ..., 0.0339, 0.0098, 0.0524],
        [0.0162, 0.0452, 0.0000,  ..., 0.0360, 0.0104, 0.0551],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73896.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.0831],
        [0.0791, 0.0000, 0.0000,  ..., 0.1109, 0.0000, 0.0930],
        [0.0842, 0.0000, 0.0000,  ..., 0.1172, 0.0000, 0.0961],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703624.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3069.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-365.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(266.4200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.0917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.1630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2714],
        [ 0.2649],
        [ 0.2576],
        ...,
        [-1.1683],
        [-1.1350],
        [-1.1356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199830.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0024],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369407.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0023],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369417.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071,  0.0164, -0.0150,  ...,  0.0163,  0.0046,  0.0231],
        [ 0.0023,  0.0082, -0.0058,  ...,  0.0047,  0.0014,  0.0083],
        [ 0.0034,  0.0102, -0.0080,  ...,  0.0075,  0.0022,  0.0119],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3550.3267, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7126, device='cuda:0')



h[100].sum tensor(-33.3138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0127, device='cuda:0')



h[200].sum tensor(-42.3279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.5869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0381, 0.0000,  ..., 0.0261, 0.0077, 0.0424],
        [0.0156, 0.0440, 0.0000,  ..., 0.0345, 0.0100, 0.0531],
        [0.0139, 0.0412, 0.0000,  ..., 0.0304, 0.0089, 0.0479],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71654.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0792, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.0903],
        [0.0777, 0.0000, 0.0000,  ..., 0.1099, 0.0000, 0.0902],
        [0.0696, 0.0000, 0.0000,  ..., 0.1001, 0.0000, 0.0869],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697684.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2988.0911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-373.4457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(254.5726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.7221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.5260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2892],
        [ 0.3026],
        [ 0.3177],
        ...,
        [-1.3360],
        [-1.3326],
        [-1.3312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194500.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0023],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369417.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0098],
        [1.0155],
        ...,
        [1.0023],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369427., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0104, -0.0082,  ...,  0.0078,  0.0023,  0.0121],
        [ 0.0028,  0.0091, -0.0068,  ...,  0.0060,  0.0018,  0.0098],
        [ 0.0035,  0.0103, -0.0081,  ...,  0.0076,  0.0022,  0.0120],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0006, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3421.8391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7722, device='cuda:0')



h[100].sum tensor(-34.1466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0110, device='cuda:0')



h[200].sum tensor(-43.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.5690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0341, 0.0000,  ..., 0.0205, 0.0062, 0.0351],
        [0.0136, 0.0407, 0.0000,  ..., 0.0298, 0.0088, 0.0471],
        [0.0157, 0.0443, 0.0000,  ..., 0.0347, 0.0102, 0.0534],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68289.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.0000, 0.0000,  ..., 0.1244, 0.0000, 0.0990],
        [0.1056, 0.0000, 0.0000,  ..., 0.1486, 0.0000, 0.1106],
        [0.1100, 0.0000, 0.0000,  ..., 0.1537, 0.0000, 0.1128],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0315],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0315],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0315]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681052.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2796.1714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-386.9978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109.8173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.5907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1998],
        [ 0.1866],
        [ 0.1865],
        ...,
        [-1.3489],
        [-1.3454],
        [-1.3438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190870.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0098],
        [1.0155],
        ...,
        [1.0023],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369427., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0155],
        ...,
        [1.0023],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369436.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0058, -0.0030,  ...,  0.0012,  0.0005,  0.0037],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0026, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4083.8271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8943, device='cuda:0')



h[100].sum tensor(-28.6103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0201, device='cuda:0')



h[200].sum tensor(-38.8974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.6331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0184, 0.0000,  ..., 0.0032, 0.0012, 0.0085],
        [0.0008, 0.0152, 0.0000,  ..., 0.0012, 0.0005, 0.0038],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88557.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0132, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0505],
        [0.0045, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0415],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0350],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0318],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0329],
        [0.0017, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0364]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(811204.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3775.1221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.6863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1110.0730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.0062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.7802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0712],
        [-0.2386],
        [-0.5731],
        ...,
        [-1.3185],
        [-1.2135],
        [-1.0090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194084.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0155],
        ...,
        [1.0023],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369436.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0156],
        ...,
        [1.0022],
        [1.0007],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369445.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3419.0032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0547, device='cuda:0')



h[100].sum tensor(-33.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0112, device='cuda:0')



h[200].sum tensor(-43.0895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.7628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68417.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0322]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686098.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2709.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-392.5791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110.0224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.0800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.4291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5500],
        [-1.5695],
        [-1.5818],
        ...,
        [-1.3884],
        [-1.3847],
        [-1.3831]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227300.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0156],
        ...,
        [1.0022],
        [1.0007],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369445.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(501.2734, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0156],
        ...,
        [1.0022],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369455.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0004,  0.0052, -0.0023,  ...,  0.0004,  0.0003,  0.0027],
        [ 0.0004,  0.0052, -0.0023,  ...,  0.0004,  0.0003,  0.0027],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3411.8547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8572, device='cuda:0')



h[100].sum tensor(-33.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-43.1422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0165, 0.0000,  ..., 0.0004, 0.0005, 0.0047],
        [0.0007, 0.0165, 0.0000,  ..., 0.0004, 0.0005, 0.0048],
        [0.0007, 0.0166, 0.0000,  ..., 0.0004, 0.0005, 0.0048],
        ...,
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68842.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0448],
        [0.0021, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0438],
        [0.0021, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0435],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694335.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2773.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-393.6974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(162.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.1407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4534],
        [-0.7369],
        [-0.9823],
        ...,
        [-1.3990],
        [-1.3951],
        [-1.3933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247934.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0156],
        ...,
        [1.0022],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369455.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0157],
        ...,
        [1.0022],
        [1.0006],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369465.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3682.3579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5256, device='cuda:0')



h[100].sum tensor(-31.3926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0143, device='cuda:0')



h[200].sum tensor(-41.5397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.5173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76030.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0420],
        [0.0078, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0410],
        [0.0064, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0393],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0329],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0329],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(735491., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3194.2219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-364.7236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(374.9949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.8694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.9783, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2276],
        [ 0.2063],
        [ 0.1556],
        ...,
        [-1.2416],
        [-1.2283],
        [-1.1727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224137.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0157],
        ...,
        [1.0022],
        [1.0006],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369465.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0157],
        ...,
        [1.0021],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369476.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0051, -0.0022,  ...,  0.0003,  0.0003,  0.0025],
        [ 0.0010,  0.0060, -0.0033,  ...,  0.0016,  0.0007,  0.0043],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3908.9795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0131, device='cuda:0')



h[100].sum tensor(-29.6276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0168, device='cuda:0')



h[200].sum tensor(-40.2399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0288, 0.0000,  ..., 0.0130, 0.0045, 0.0254],
        [0.0020, 0.0186, 0.0000,  ..., 0.0035, 0.0014, 0.0088],
        [0.0010, 0.0156, 0.0000,  ..., 0.0017, 0.0007, 0.0043],
        ...,
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81421.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0000, 0.0000,  ..., 0.0582, 0.0000, 0.0671],
        [0.0182, 0.0000, 0.0000,  ..., 0.0370, 0.0000, 0.0548],
        [0.0069, 0.0000, 0.0000,  ..., 0.0177, 0.0000, 0.0433],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0345],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0345]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(762532.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3500.3018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-341.5336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(527.6405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.3141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-274.9910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3062],
        [ 0.1440],
        [-0.1588],
        ...,
        [-1.2208],
        [-1.1928],
        [-1.2158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197071.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0157],
        ...,
        [1.0021],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369476.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0099],
        [1.0158],
        ...,
        [1.0020],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369486.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0014,  0.0067, -0.0041,  ...,  0.0026,  0.0010,  0.0055],
        [ 0.0030,  0.0095, -0.0072,  ...,  0.0066,  0.0021,  0.0106],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3589.8433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4925, device='cuda:0')



h[100].sum tensor(-32.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-42.4007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.4358, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0023, 0.0191, 0.0000,  ..., 0.0043, 0.0017, 0.0099],
        [0.0046, 0.0242, 0.0000,  ..., 0.0090, 0.0032, 0.0181],
        [0.0088, 0.0328, 0.0000,  ..., 0.0185, 0.0061, 0.0326],
        ...,
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71495.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0000, 0.0000,  ..., 0.0367, 0.0000, 0.0537],
        [0.0385, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0700],
        [0.0607, 0.0000, 0.0000,  ..., 0.0960, 0.0000, 0.0868],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0336],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0336],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701730.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3113.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-385.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(263.8726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.7990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0245],
        [ 0.2343],
        [ 0.3488],
        ...,
        [-1.3775],
        [-1.3739],
        [-1.3723]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212982.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0099],
        [1.0158],
        ...,
        [1.0020],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369486.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0100],
        [1.0159],
        ...,
        [1.0020],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369496.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023,  0.0083, -0.0058,  ...,  0.0049,  0.0016,  0.0084],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0016,  0.0071, -0.0045,  ...,  0.0032,  0.0011,  0.0063],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3352.3325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4114, device='cuda:0')



h[100].sum tensor(-33.9447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0092, device='cuda:0')



h[200].sum tensor(-43.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.2626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0206, 0.0000,  ..., 0.0041, 0.0019, 0.0118],
        [0.0074, 0.0303, 0.0000,  ..., 0.0152, 0.0052, 0.0283],
        [0.0126, 0.0377, 0.0000,  ..., 0.0282, 0.0086, 0.0427],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66727.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0750],
        [0.0716, 0.0000, 0.0000,  ..., 0.1110, 0.0000, 0.0939],
        [0.1072, 0.0000, 0.0000,  ..., 0.1570, 0.0000, 0.1144],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681767.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3035.2661, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-405.4166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78.3009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.3369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3502],
        [ 0.3330],
        [ 0.3104],
        ...,
        [-1.3708],
        [-1.3674],
        [-1.3659]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196084.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0100],
        [1.0159],
        ...,
        [1.0020],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369496.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0100],
        [1.0159],
        ...,
        [1.0019],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369506.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0014,  0.0068, -0.0042,  ...,  0.0028,  0.0010,  0.0058],
        [ 0.0007,  0.0056, -0.0028,  ...,  0.0011,  0.0005,  0.0036],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3477.3032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1437, device='cuda:0')



h[100].sum tensor(-33.1013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-43.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0161, 0.0000,  ..., 0.0028, 0.0010, 0.0058],
        [0.0017, 0.0180, 0.0000,  ..., 0.0029, 0.0013, 0.0081],
        [0.0100, 0.0346, 0.0000,  ..., 0.0213, 0.0069, 0.0361],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67735.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0115, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0475],
        [0.0245, 0.0000, 0.0000,  ..., 0.0459, 0.0000, 0.0596],
        [0.0502, 0.0000, 0.0000,  ..., 0.0820, 0.0000, 0.0793],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682472.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3097.8015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-403.8764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(120.0480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.0440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.9849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2572],
        [ 0.3146],
        [ 0.3383],
        ...,
        [-1.3694],
        [-1.3659],
        [-1.3624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184372.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0100],
        [1.0159],
        ...,
        [1.0019],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369506.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0100],
        [1.0160],
        ...,
        [1.0019],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369515.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3465.9092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4834, device='cuda:0')



h[100].sum tensor(-33.2219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0103, device='cuda:0')



h[200].sum tensor(-43.2854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.6845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70811.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0323],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0330],
        [0.0019, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0370],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711193.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3361.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-391.2406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(151.5526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.7933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.5655, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7222],
        [-0.5271],
        [-0.2116],
        ...,
        [-1.3754],
        [-1.3724],
        [-1.3711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164679.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0100],
        [1.0160],
        ...,
        [1.0019],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369515.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0100],
        [1.0160],
        ...,
        [1.0019],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369524.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3610.3628, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4070, device='cuda:0')



h[100].sum tensor(-32.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0125, device='cuda:0')



h[200].sum tensor(-42.3312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.3771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72940.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0323],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0324],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0326],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718444.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3393.1362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-386.0926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(224.0634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.3670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4237],
        [-1.3988],
        [-1.3437],
        ...,
        [-1.3890],
        [-1.3854],
        [-1.3838]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177335.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0100],
        [1.0160],
        ...,
        [1.0019],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369524.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0101],
        [1.0161],
        ...,
        [1.0018],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369533.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3478.7607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1995, device='cuda:0')



h[100].sum tensor(-33.0819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-43.0672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69898.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0325],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702982.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3193.7573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-400.0207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56.8443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.8540, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.8621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3365],
        [-1.4196],
        [-1.4922],
        ...,
        [-1.4033],
        [-1.3993],
        [-1.3976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174269.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0101],
        [1.0161],
        ...,
        [1.0018],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369533.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0101],
        [1.0162],
        ...,
        [1.0018],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369542.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        ...,
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010],
        [-0.0008,  0.0031,  0.0000,  ..., -0.0025, -0.0005, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3523.9927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5620, device='cuda:0')



h[100].sum tensor(-32.6684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0115, device='cuda:0')



h[200].sum tensor(-42.6937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.1110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71820.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0324],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719218.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3209.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-396.7152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(264.3393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.5651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.5887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3995],
        [-1.4920],
        [-1.5518],
        ...,
        [-1.4221],
        [-1.4183],
        [-1.4167]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194136.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0101],
        [1.0162],
        ...,
        [1.0018],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369542.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(483.4305, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0102],
        [1.0163],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369551.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [ 0.0008,  0.0059, -0.0031,  ...,  0.0014,  0.0006,  0.0040],
        [ 0.0004,  0.0052, -0.0023,  ...,  0.0004,  0.0003,  0.0027],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3348.6023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7797, device='cuda:0')



h[100].sum tensor(-33.7423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0094, device='cuda:0')



h[200].sum tensor(-43.6770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.5154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0155, 0.0000,  ..., 0.0014, 0.0006, 0.0040],
        [0.0020, 0.0188, 0.0000,  ..., 0.0035, 0.0014, 0.0089],
        [0.0093, 0.0338, 0.0000,  ..., 0.0195, 0.0063, 0.0339],
        ...,
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68376.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0455],
        [0.0220, 0.0000, 0.0000,  ..., 0.0433, 0.0000, 0.0586],
        [0.0493, 0.0000, 0.0000,  ..., 0.0820, 0.0000, 0.0793],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702278.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2927.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-414.7610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(170.5987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.5669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.5074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1801],
        [ 0.2720],
        [ 0.3411],
        ...,
        [-1.4422],
        [-1.4383],
        [-1.4366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237374.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0102],
        [1.0163],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369551.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0102],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369560.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0008,  0.0059, -0.0030,  ...,  0.0013,  0.0006,  0.0038],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3769.8511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1753, device='cuda:0')



h[100].sum tensor(-30.4676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0152, device='cuda:0')



h[200].sum tensor(-40.9603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.6495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0210, 0.0000,  ..., 0.0065, 0.0022, 0.0127],
        [0.0063, 0.0274, 0.0000,  ..., 0.0131, 0.0043, 0.0234],
        [0.0056, 0.0251, 0.0000,  ..., 0.0121, 0.0038, 0.0200],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76583.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0329, 0.0000, 0.0000,  ..., 0.0599, 0.0000, 0.0679],
        [0.0456, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.0755],
        [0.0486, 0.0000, 0.0000,  ..., 0.0802, 0.0000, 0.0766],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744570.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3136.6248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-383.0101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(424.4836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.0659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.2523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3618],
        [ 0.3551],
        [ 0.3482],
        ...,
        [-1.4600],
        [-1.4559],
        [-1.4542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218276.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0102],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369560.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0102],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369560.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0110, -0.0087,  ...,  0.0086,  0.0026,  0.0132],
        [ 0.0059,  0.0144, -0.0123,  ...,  0.0134,  0.0039,  0.0193],
        [ 0.0040,  0.0112, -0.0089,  ...,  0.0089,  0.0027,  0.0136],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3797.3223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4439, device='cuda:0')



h[100].sum tensor(-30.2554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0153, device='cuda:0')



h[200].sum tensor(-40.7892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-18.8338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0217, 0.0545, 0.0000,  ..., 0.0491, 0.0145, 0.0718],
        [0.0185, 0.0492, 0.0000,  ..., 0.0415, 0.0124, 0.0620],
        [0.0148, 0.0432, 0.0000,  ..., 0.0328, 0.0100, 0.0509],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78505.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1248, 0.0000, 0.0000,  ..., 0.1818, 0.0000, 0.1257],
        [0.1159, 0.0000, 0.0000,  ..., 0.1702, 0.0000, 0.1207],
        [0.1026, 0.0000, 0.0000,  ..., 0.1532, 0.0000, 0.1136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750399., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3184.3174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-373.9218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(365.7406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.7442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.3378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2445],
        [ 0.2450],
        [ 0.2502],
        ...,
        [-1.3405],
        [-1.3016],
        [-1.3000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214567.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0102],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369560.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0103],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369570.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3736.8027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0712, device='cuda:0')



h[100].sum tensor(-30.9478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0146, device='cuda:0')



h[200].sum tensor(-41.1480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.8917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76313.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0462],
        [0.0099, 0.0000, 0.0000,  ..., 0.0241, 0.0000, 0.0469],
        [0.0111, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0488],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(736366.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3003.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-381.7734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(217.6345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.0829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2421],
        [ 0.2464],
        [ 0.2535],
        ...,
        [-1.4702],
        [-1.4661],
        [-1.4644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194034.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0103],
        [1.0164],
        ...,
        [1.0017],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369570.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0104],
        [1.0165],
        ...,
        [1.0017],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369581., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0005, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3300.7988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1013, device='cuda:0')



h[100].sum tensor(-34.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0090, device='cuda:0')



h[200].sum tensor(-43.8913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.0498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0162, 0.0000,  ..., 0.0019, 0.0007, 0.0048],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65192.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0323],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0353],
        [0.0069, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0442],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680133., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2546.9585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-434.7085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.8825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.4090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1947],
        [-0.8710],
        [-0.4672],
        ...,
        [-1.4753],
        [-1.4712],
        [-1.4695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261748.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0104],
        [1.0165],
        ...,
        [1.0017],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369581., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0105],
        [1.0166],
        ...,
        [1.0016],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369591.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [ 0.0037,  0.0107, -0.0083,  ...,  0.0082,  0.0024,  0.0127],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3551.9519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6189, device='cuda:0')



h[100].sum tensor(-33.6420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0121, device='cuda:0')



h[200].sum tensor(-42.4326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.8363, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0037, 0.0205, 0.0000,  ..., 0.0082, 0.0024, 0.0128],
        [0.0029, 0.0192, 0.0000,  ..., 0.0063, 0.0019, 0.0103],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71119.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0362],
        [0.0118, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0464],
        [0.0162, 0.0000, 0.0000,  ..., 0.0321, 0.0000, 0.0515],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711960.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2828.4009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-406.6022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(157.1712, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.5254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.3787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0767],
        [-0.6788],
        [-0.2911],
        ...,
        [-1.4715],
        [-1.4675],
        [-1.4661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226732.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0105],
        [1.0166],
        ...,
        [1.0016],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369591.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0106],
        [1.0166],
        ...,
        [1.0016],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369602.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [ 0.0007,  0.0058, -0.0028,  ...,  0.0011,  0.0004,  0.0037],
        [ 0.0005,  0.0055, -0.0026,  ...,  0.0007,  0.0003,  0.0032],
        ...,
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0008,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3467.5347, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0928, device='cuda:0')



h[100].sum tensor(-35.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-43.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0154, 0.0000,  ..., 0.0011, 0.0004, 0.0037],
        [0.0010, 0.0173, 0.0000,  ..., 0.0011, 0.0006, 0.0061],
        [0.0034, 0.0243, 0.0000,  ..., 0.0058, 0.0021, 0.0167],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69815.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0000, 0.0000,  ..., 0.0228, 0.0000, 0.0486],
        [0.0178, 0.0000, 0.0000,  ..., 0.0350, 0.0000, 0.0572],
        [0.0269, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0659],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709343.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2886.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-411.8602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(154.6434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.6807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.7794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1710],
        [ 0.2675],
        [ 0.2976],
        ...,
        [-1.4712],
        [-1.4672],
        [-1.4656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224381.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0106],
        [1.0166],
        ...,
        [1.0016],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369602.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0107],
        [1.0166],
        ...,
        [1.0015],
        [0.9999],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369613.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0137, -0.0115,  ...,  0.0123,  0.0035,  0.0182],
        [ 0.0076,  0.0173, -0.0155,  ...,  0.0175,  0.0050,  0.0249],
        [ 0.0057,  0.0141, -0.0120,  ...,  0.0129,  0.0037,  0.0189],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3753.7725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9570, device='cuda:0')



h[100].sum tensor(-33.3652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0139, device='cuda:0')



h[200].sum tensor(-41.4571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-17.1272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0251, 0.0606, 0.0000,  ..., 0.0576, 0.0163, 0.0833],
        [0.0287, 0.0666, 0.0000,  ..., 0.0661, 0.0187, 0.0943],
        [0.0282, 0.0658, 0.0000,  ..., 0.0649, 0.0183, 0.0928],
        ...,
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76549.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1454, 0.0000, 0.0000,  ..., 0.1931, 0.0000, 0.1350],
        [0.1635, 0.0000, 0.0000,  ..., 0.2152, 0.0000, 0.1455],
        [0.1612, 0.0000, 0.0000,  ..., 0.2125, 0.0000, 0.1446],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(748088.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3254.8623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-383.3949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(396.2405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.8497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.3368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0744],
        [ 0.0585],
        [ 0.0418],
        ...,
        [-1.4717],
        [-1.4677],
        [-1.4660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202394.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0107],
        [1.0166],
        ...,
        [1.0015],
        [0.9999],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369613.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0108],
        [1.0167],
        ...,
        [1.0015],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369623.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3551.4019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1758, device='cuda:0')



h[100].sum tensor(-35.3279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-42.7946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.8460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.3629e-04, 1.6429e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6788e-04,
         4.6736e-03],
        [2.1852e-04, 1.4643e-02, 0.0000e+00,  ..., 0.0000e+00, 8.4086e-05,
         2.3408e-03],
        [7.1349e-04, 1.5540e-02, 0.0000e+00,  ..., 1.1495e-03, 4.0931e-04,
         3.8721e-03],
        ...,
        [0.0000e+00, 1.3343e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3340e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3339e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70787.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0458],
        [0.0025, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0439],
        [0.0038, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0461],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714889.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2963.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-405.9347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(136.7037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.2526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3260],
        [-0.2253],
        [-0.1118],
        ...,
        [-1.4736],
        [-1.4697],
        [-1.4682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209324.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0108],
        [1.0167],
        ...,
        [1.0015],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369623.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0109],
        [1.0168],
        ...,
        [1.0015],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369633.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0077, -0.0050,  ...,  0.0039,  0.0012,  0.0073],
        [ 0.0010,  0.0064, -0.0035,  ...,  0.0019,  0.0006,  0.0049],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3654.8652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9838, device='cuda:0')



h[100].sum tensor(-34.8425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0123, device='cuda:0')



h[200].sum tensor(-42.2183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.0867, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0308, 0.0000,  ..., 0.0152, 0.0045, 0.0290],
        [0.0026, 0.0200, 0.0000,  ..., 0.0050, 0.0016, 0.0112],
        [0.0010, 0.0161, 0.0000,  ..., 0.0020, 0.0006, 0.0049],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72168.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0382, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0694],
        [0.0227, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0588],
        [0.0119, 0.0000, 0.0000,  ..., 0.0240, 0.0000, 0.0504],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719273.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2965.6265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-400.6124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(157.4314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.0007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.7494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2678],
        [ 0.2445],
        [ 0.1595],
        ...,
        [-1.4789],
        [-1.4749],
        [-1.4733]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200307.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0109],
        [1.0168],
        ...,
        [1.0015],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369633.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(451.5469, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0169],
        ...,
        [1.0014],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369643.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3545.6489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1838, device='cuda:0')



h[100].sum tensor(-35.8494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0107, device='cuda:0')



h[200].sum tensor(-42.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.1652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69789.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0330],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710228.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2839.4509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-412.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(142.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.5604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.3177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5183],
        [-1.4142],
        [-1.2365],
        ...,
        [-1.3330],
        [-1.4387],
        [-1.4700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223010.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0169],
        ...,
        [1.0014],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369643.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0169],
        ...,
        [1.0014],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369643.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3909.6968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7362, device='cuda:0')



h[100].sum tensor(-33.0846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0155, device='cuda:0')



h[200].sum tensor(-40.6854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-19.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78620.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0420],
        [0.0002, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0380],
        [0.0000, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0366],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(757692.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3247.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-373.2775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(330.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.1515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-265.3734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0400],
        [-0.2008],
        [-0.3982],
        ...,
        [-1.4894],
        [-1.4810],
        [-1.4565]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189399.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0169],
        ...,
        [1.0014],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369643.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0111],
        [1.0170],
        ...,
        [1.0014],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369651.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023,  0.0084, -0.0057,  ...,  0.0049,  0.0014,  0.0086],
        [ 0.0010,  0.0063, -0.0034,  ...,  0.0019,  0.0006,  0.0048],
        [ 0.0033,  0.0102, -0.0076,  ...,  0.0073,  0.0021,  0.0118],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4031.7273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8935, device='cuda:0')



h[100].sum tensor(-32.0420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0167, device='cuda:0')



h[200].sum tensor(-39.8401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.5149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0257, 0.0000,  ..., 0.0077, 0.0025, 0.0194],
        [0.0076, 0.0301, 0.0000,  ..., 0.0165, 0.0049, 0.0284],
        [0.0036, 0.0220, 0.0000,  ..., 0.0075, 0.0023, 0.0144],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80342.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0332, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.0690],
        [0.0390, 0.0000, 0.0000,  ..., 0.0627, 0.0000, 0.0712],
        [0.0315, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0653],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(764233.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3186.7866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-367.7682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(408.6616, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.9304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.3847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3445],
        [ 0.3689],
        [ 0.3654],
        ...,
        [-1.4932],
        [-1.4829],
        [-1.4785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222178.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0111],
        [1.0170],
        ...,
        [1.0014],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369651.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0112],
        [1.0171],
        ...,
        [1.0014],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369660.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0006,  0.0057, -0.0027,  ...,  0.0010,  0.0004,  0.0036],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3500.9976, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1301, device='cuda:0')



h[100].sum tensor(-35.9676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0101, device='cuda:0')



h[200].sum tensor(-43.0112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.4420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0033, 0.0214, 0.0000,  ..., 0.0068, 0.0021, 0.0135],
        [0.0006, 0.0155, 0.0000,  ..., 0.0010, 0.0004, 0.0036],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69289.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0000, 0.0000,  ..., 0.0498, 0.0000, 0.0626],
        [0.0126, 0.0000, 0.0000,  ..., 0.0249, 0.0000, 0.0493],
        [0.0014, 0.0000, 0.0000,  ..., 0.0082, 0.0000, 0.0397],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0342],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711020.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2644.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-412.5117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.3623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.3521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2751],
        [ 0.1603],
        [-0.0304],
        ...,
        [-1.5178],
        [-1.5137],
        [-1.5120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213235.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0112],
        [1.0171],
        ...,
        [1.0014],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369660.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0112],
        [1.0172],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369670.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [ 0.0020,  0.0080, -0.0052,  ...,  0.0043,  0.0013,  0.0079],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0026, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3451.5288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7367, device='cuda:0')



h[100].sum tensor(-36.3139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0093, device='cuda:0')



h[200].sum tensor(-43.2875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-11.4858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0199, 0.0000,  ..., 0.0046, 0.0015, 0.0106],
        [0.0012, 0.0195, 0.0000,  ..., 0.0014, 0.0008, 0.0089],
        ...,
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67628.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0426],
        [0.0094, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0501],
        [0.0160, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0565],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703874.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2551.8494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-422.9222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(119.4571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.0780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.5953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0990],
        [ 0.1369],
        [ 0.2351],
        ...,
        [-1.5109],
        [-1.5131],
        [-1.5122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251316.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0112],
        [1.0172],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369670.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0113],
        [1.0173],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369679.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3657.0173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0703, device='cuda:0')



h[100].sum tensor(-34.8369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-42.0340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.4598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71149.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0355],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0347],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0347]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720302.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2701.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-409.4887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(190.1859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.4455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.8702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7354],
        [-0.6925],
        [-0.5218],
        ...,
        [-1.3986],
        [-1.4763],
        [-1.5072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246314.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0113],
        [1.0173],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369679.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0114],
        [1.0174],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369689.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3786.7212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3321, device='cuda:0')



h[100].sum tensor(-33.8401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0130, device='cuda:0')



h[200].sum tensor(-41.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.0120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74350.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0333],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0332],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0349]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734161.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2777.1594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-394.2410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(216.6539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.2160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-254.2192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8118],
        [-1.1049],
        [-1.3124],
        ...,
        [-1.5224],
        [-1.5184],
        [-1.5168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223040.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0114],
        [1.0174],
        ...,
        [1.0013],
        [0.9997],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369689.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0115],
        [1.0175],
        ...,
        [1.0012],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369698.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3644.6157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8242, device='cuda:0')



h[100].sum tensor(-35.0255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0111, device='cuda:0')



h[200].sum tensor(-41.9160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.6047, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70488.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0332],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0335],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716850.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2659.5408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-410.1256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49.0581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.4763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.6451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6312],
        [-1.5741],
        [-1.4724],
        ...,
        [-1.5247],
        [-1.5207],
        [-1.5191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216983.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0115],
        [1.0175],
        ...,
        [1.0012],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369698.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0116],
        [1.0176],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369708.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [ 0.0021,  0.0083, -0.0055,  ...,  0.0048,  0.0015,  0.0084],
        [ 0.0023,  0.0086, -0.0058,  ...,  0.0052,  0.0017,  0.0090],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3834.6499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8006, device='cuda:0')



h[100].sum tensor(-33.8053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0133, device='cuda:0')



h[200].sum tensor(-40.6214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-16.3335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0180, 0.0000,  ..., 0.0048, 0.0015, 0.0084],
        [0.0040, 0.0225, 0.0000,  ..., 0.0088, 0.0028, 0.0158],
        [0.0136, 0.0416, 0.0000,  ..., 0.0311, 0.0095, 0.0490],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74895.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0466],
        [0.0243, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0586],
        [0.0539, 0.0000, 0.0000,  ..., 0.0939, 0.0000, 0.0785],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744755.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2911.4216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-393.2452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(182.1238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.1526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.6497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4277],
        [-0.1479],
        [ 0.1465],
        ...,
        [-1.5312],
        [-1.5272],
        [-1.5256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209127.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0116],
        [1.0176],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369708.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0118],
        [1.0177],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369717.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4101.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5352, device='cuda:0')



h[100].sum tensor(-32.0932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0165, device='cuda:0')



h[200].sum tensor(-38.8733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-20.2690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82487.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0334],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0341],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(784911.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3236.5203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.8945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(344.1333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.4469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.4927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8500],
        [-0.7498],
        [-0.5038],
        ...,
        [-1.5349],
        [-1.5336],
        [-1.5328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192524.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0118],
        [1.0177],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369717.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(459.2484, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0119],
        [1.0178],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369727.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        ...,
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009],
        [-0.0009,  0.0032,  0.0000,  ..., -0.0025, -0.0005, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3790.1299, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5568, device='cuda:0')



h[100].sum tensor(-34.8260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0126, device='cuda:0')



h[200].sum tensor(-40.7238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.4800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75992.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0334],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0335],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0338],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(761537.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3113.4924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-397.2835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(417.9386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.3640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.0678, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2705],
        [-1.3760],
        [-1.4416],
        ...,
        [-1.5507],
        [-1.5466],
        [-1.5451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248494.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0119],
        [1.0178],
        ...,
        [1.0012],
        [0.9996],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369727.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0120],
        [1.0179],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369736.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3817.8369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9507, device='cuda:0')



h[100].sum tensor(-34.8205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0128, device='cuda:0')



h[200].sum tensor(-40.5378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-15.7503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74351.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0000, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0359],
        [0.0013, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0404],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739107.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2940.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-399.3104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67.1192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.1178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1087],
        [-0.8423],
        [-0.5170],
        ...,
        [-1.5600],
        [-1.5559],
        [-1.5543]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207518.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0120],
        [1.0179],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369736.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0121],
        [1.0179],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369745.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023,  0.0087, -0.0058,  ...,  0.0053,  0.0016,  0.0090],
        [ 0.0050,  0.0132, -0.0107,  ...,  0.0118,  0.0034,  0.0174],
        [ 0.0063,  0.0152, -0.0129,  ...,  0.0148,  0.0043,  0.0212],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3682.6768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3163, device='cuda:0')



h[100].sum tensor(-35.6792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0113, device='cuda:0')



h[200].sum tensor(-41.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.9424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0480, 0.0000,  ..., 0.0404, 0.0118, 0.0607],
        [0.0241, 0.0595, 0.0000,  ..., 0.0568, 0.0164, 0.0818],
        [0.0241, 0.0595, 0.0000,  ..., 0.0567, 0.0163, 0.0817],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72410.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1507e-01, 0.0000e+00, 0.0000e+00,  ..., 1.7487e-01, 0.0000e+00,
         1.1884e-01],
        [1.3503e-01, 0.0000e+00, 0.0000e+00,  ..., 2.0128e-01, 0.0000e+00,
         1.3026e-01],
        [1.4253e-01, 0.0000e+00, 0.0000e+00,  ..., 2.1111e-01, 0.0000e+00,
         1.3485e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2255e-04, 0.0000e+00,
         3.5363e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2285e-04, 0.0000e+00,
         3.5357e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2312e-04, 0.0000e+00,
         3.5352e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(734771.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2866.5737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-409.3537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80.4468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.3487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.7481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2937],
        [ 0.2882],
        [ 0.2885],
        ...,
        [-1.5658],
        [-1.5615],
        [-1.5598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216056.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0121],
        [1.0179],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369745.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0122],
        [1.0180],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369754.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0079, -0.0050,  ...,  0.0043,  0.0013,  0.0077],
        [ 0.0014,  0.0071, -0.0041,  ...,  0.0030,  0.0010,  0.0061],
        [ 0.0043,  0.0119, -0.0093,  ...,  0.0100,  0.0029,  0.0150],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3356.7886, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3535, device='cuda:0')



h[100].sum tensor(-37.8109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0075, device='cuda:0')



h[200].sum tensor(-43.3342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0028, 0.0208, 0.0000,  ..., 0.0061, 0.0019, 0.0122],
        [0.0109, 0.0374, 0.0000,  ..., 0.0251, 0.0075, 0.0409],
        [0.0115, 0.0385, 0.0000,  ..., 0.0265, 0.0078, 0.0428],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64405.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0000, 0.0000,  ..., 0.0418, 0.0000, 0.0556],
        [0.0444, 0.0000, 0.0000,  ..., 0.0775, 0.0000, 0.0732],
        [0.0545, 0.0000, 0.0000,  ..., 0.0915, 0.0000, 0.0804],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689787.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2413.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-446.8682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.0283, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.7272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3400],
        [ 0.0724],
        [ 0.3035],
        ...,
        [-1.5685],
        [-1.5644],
        [-1.5628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255026.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0122],
        [1.0180],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369754.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0123],
        [1.0180],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369763.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0006,  0.0057, -0.0026,  ...,  0.0011,  0.0004,  0.0035],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [ 0.0006,  0.0057, -0.0026,  ...,  0.0011,  0.0004,  0.0035],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3600.1797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9563, device='cuda:0')



h[100].sum tensor(-35.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0106, device='cuda:0')



h[200].sum tensor(-41.8763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-13.0090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0151, 0.0000,  ..., 0.0004, 0.0002, 0.0027],
        [0.0017, 0.0221, 0.0000,  ..., 0.0030, 0.0013, 0.0125],
        [0.0003, 0.0152, 0.0000,  ..., 0.0004, 0.0002, 0.0027],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69709.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0409],
        [0.0017, 0.0000, 0.0000,  ..., 0.0188, 0.0000, 0.0467],
        [0.0009, 0.0000, 0.0000,  ..., 0.0104, 0.0000, 0.0414],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713470.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2491.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-423.4854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(99.4351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.2697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.6420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0997],
        [-1.1417],
        [-1.2771],
        ...,
        [-1.5739],
        [-1.5698],
        [-1.5682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240471.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0123],
        [1.0180],
        ...,
        [1.0011],
        [0.9995],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369763.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0124],
        [1.0180],
        ...,
        [1.0011],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369772.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [ 0.0011,  0.0066, -0.0036,  ...,  0.0024,  0.0008,  0.0052],
        [ 0.0066,  0.0158, -0.0134,  ...,  0.0156,  0.0044,  0.0220],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0024, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4839.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.9890, device='cuda:0')



h[100].sum tensor(-26.2146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0262, device='cuda:0')



h[200].sum tensor(-34.3891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-32.2470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0221, 0.0000,  ..., 0.0079, 0.0024, 0.0144],
        [0.0130, 0.0395, 0.0000,  ..., 0.0305, 0.0088, 0.0454],
        [0.0175, 0.0486, 0.0000,  ..., 0.0410, 0.0118, 0.0609],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100516.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0493, 0.0000, 0.0000,  ..., 0.0829, 0.0000, 0.0753],
        [0.1016, 0.0000, 0.0000,  ..., 0.1536, 0.0000, 0.1084],
        [0.1449, 0.0000, 0.0000,  ..., 0.2117, 0.0000, 0.1356],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(890797.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3893.7085, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.5710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1167.2581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.6836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-334.7817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2773],
        [ 0.2645],
        [ 0.2399],
        ...,
        [-1.5819],
        [-1.5778],
        [-1.5762]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217754.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0124],
        [1.0180],
        ...,
        [1.0011],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369772.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0125],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369781., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3416.4932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1615, device='cuda:0')



h[100].sum tensor(-36.9284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0085, device='cuda:0')



h[200].sum tensor(-42.9485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.4048, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66338.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0330],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.0000, 0.0341],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700184.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2365.0012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-444.4722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87.9083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.5145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6124],
        [-1.4897],
        [-1.2690],
        ...,
        [-1.5909],
        [-1.5867],
        [-1.5851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279730.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0125],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369781., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0126],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369789.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3684.4441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2120, device='cuda:0')



h[100].sum tensor(-35.0886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0118, device='cuda:0')



h[200].sum tensor(-41.3244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.5571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71967.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0330],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0333],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727334.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2600.3525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-418.6554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(103.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.3431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.1234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6293],
        [-1.6587],
        [-1.6553],
        ...,
        [-1.6006],
        [-1.5964],
        [-1.5949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238599.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0126],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369789.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0127],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369798.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0163, -0.0140,  ...,  0.0163,  0.0046,  0.0230],
        [ 0.0042,  0.0119, -0.0092,  ...,  0.0099,  0.0028,  0.0148],
        [ 0.0018,  0.0078, -0.0048,  ...,  0.0040,  0.0012,  0.0072],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4188.1943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5138, device='cuda:0')



h[100].sum tensor(-31.6349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0176, device='cuda:0')



h[200].sum tensor(-38.3530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.6268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0522, 0.0000,  ..., 0.0462, 0.0131, 0.0676],
        [0.0134, 0.0418, 0.0000,  ..., 0.0311, 0.0089, 0.0484],
        [0.0068, 0.0293, 0.0000,  ..., 0.0155, 0.0045, 0.0263],
        ...,
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84421.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1106, 0.0000, 0.0000,  ..., 0.1611, 0.0000, 0.1160],
        [0.0925, 0.0000, 0.0000,  ..., 0.1377, 0.0000, 0.1055],
        [0.0696, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.0919],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0351]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(799874.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3293.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-365.1218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(344.9562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.3413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.8955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2958],
        [ 0.3066],
        [ 0.3206],
        ...,
        [-1.6051],
        [-1.6009],
        [-1.5994]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203853.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0127],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369798.6875, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:35.539137
evaluation loss: 510.0099792480469
epoch: 0 mean loss: 513.276611328125
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



training loss:
 [513.27661133] 

\evaluation loss:
 [510.00997925]



eval_efficiency:
 [0.91283146 0.91071373 0.90777016 0.90674001 0.90437934 0.90180034
 0.89757639 0.8879102  0.87479847 0.86428849 0.85365584 0.84415339
 0.83642269 0.83025912 0.82172265 0.81367682 0.8037987  0.79509065
 0.77603337 0.75714949 0.74594969 0.73419058 0.72103883 0.70775182
 0.69720185 0.68674554 0.67333785 0.65771989 0.64161314 0.62693762
 0.61407925 0.60274759 0.59218272 0.57850687 0.56637334 0.55278377
 0.53887695 0.52424303 0.50886007 0.49676129 0.48109168 0.4654718
 0.45050491 0.43494787 0.41838762 0.40422838 0.3891532  0.37261254
 0.36068599 0.34822668 0.33720766 0.32603122 0.31512228 0.30241944
 0.2885137  0.2745033  0.26291216 0.25375678 0.24462678 0.23231342
 0.22335624 0.21491119 0.20893843 0.20256947 0.19503076 0.18710248
 0.18030011 0.17451057 0.16959812 0.16539964 0.16111425 0.15538933
 0.15061715 0.14580527 0.14017534 0.13667449 0.13232744 0.12828554
 0.125871   0.1224756  0.11984496 0.11595077 0.11351239 0.11094558
 0.10731393 0.10491154 0.10217158 0.09908598 0.09737956 0.09510031
 0.0919314  0.08949829 0.08751862 0.08523209 0.08286881 0.08051152
 0.07919101 0.07692039 0.07497006 0.07272234] 


eval_purity:
 [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0128],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73961.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(691.0750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(3.1105, device='cuda:0')



h[100].sum tensor(-7.4659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0017, device='cuda:0')



h[200].sum tensor(-8.5580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(13094.9678, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0332],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0338],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0363],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(138553.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(483.9372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.9852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(26.6077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-47.7538, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1627],
        [-1.0313],
        [-0.7248],
        ...,
        [-1.4569],
        [-1.4528],
        [-1.4859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-56029.5977, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0128],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73961.5938, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network after training 
result1: tensor([[-1.1627],
        [-1.0313],
        [-0.7248],
        ...,
        [-1.4569],
        [-1.4528],
        [-1.4859]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0128],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(147923.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        ...,
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010],
        [-0.0009,  0.0033,  0.0000,  ..., -0.0025, -0.0006, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(1683.3032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4166, device='cuda:0')



h[100].sum tensor(-12.7001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.0069, device='cuda:0')



h[200].sum tensor(-15.3125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(34612.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0397],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0348],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0335],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(328007.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1394.5283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-145.5646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(365.0354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.4694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.9663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4541],
        [-0.9152],
        [-1.2509],
        ...,
        [-1.6102],
        [-1.6060],
        [-1.6044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-103999.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0128],
        [1.0181],
        ...,
        [1.0010],
        [0.9994],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(147923.1719, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network after training 
result1: tensor([[-1.1627],
        [-1.0313],
        [-0.7248],
        ...,
        [-1.4569],
        [-1.4528],
        [-1.4859]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



total time: 0:00:39.546819 hpmesh elements: 44 to 45

real	1m6.834s
user	0m49.709s
sys	0m16.674s
